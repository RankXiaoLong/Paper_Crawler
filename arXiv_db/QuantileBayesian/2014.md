# 2014

## TOC

- [2014-01](#2014-01)
- [2014-02](#2014-02)
- [2014-03](#2014-03)
- [2014-04](#2014-04)
- [2014-05](#2014-05)
- [2014-06](#2014-06)
- [2014-07](#2014-07)
- [2014-08](#2014-08)
- [2014-09](#2014-09)
- [2014-10](#2014-10)
- [2014-11](#2014-11)
- [2014-12](#2014-12)

## 2014-01

<details>

<summary>2014-01-01 08:59:39 - Approximate Bayesian Computation for a Class of Time Series Models</summary>

- *Ajay Jasra*

- `1401.0265v1` - [abs](http://arxiv.org/abs/1401.0265v1) - [pdf](http://arxiv.org/pdf/1401.0265v1)

> In the following article we consider approximate Bayesian computation (ABC) for certain classes of time series models. In particular, we focus upon scenarios where the likelihoods of the observations and parameter are intractable, by which we mean that one cannot evaluate the likelihood even up-to a positive unbiased estimate. This paper reviews and develops a class of approximation procedures based upon the idea of ABC, but, specifically maintains the probabilistic structure of the original statistical model. This idea is useful, in that it can facilitate an analysis of the bias of the approximation and the adaptation of established computational methods for parameter inference. Several existing results in the literature are surveyed and novel developments with regards to computation are given.

</details>

<details>

<summary>2014-01-02 09:31:54 - Theory and Applications of Proper Scoring Rules</summary>

- *A. Philip Dawid, Monica Musio*

- `1401.0398v1` - [abs](http://arxiv.org/abs/1401.0398v1) - [pdf](http://arxiv.org/pdf/1401.0398v1)

> We give an overview of some uses of proper scoring rules in statistical inference, including frequentist estimation theory and Bayesian model selection with improper priors.

</details>

<details>

<summary>2014-01-03 08:17:51 - Particle Gibbs with Ancestor Sampling</summary>

- *Fredrik Lindsten, Michael I. Jordan, Thomas B. SchÃ¶n*

- `1401.0604v1` - [abs](http://arxiv.org/abs/1401.0604v1) - [pdf](http://arxiv.org/pdf/1401.0604v1)

> Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.

</details>

<details>

<summary>2014-01-03 15:59:20 - Finite sample posterior concentration in high-dimensional regression</summary>

- *Nate Strawn, Artin Armagan, Rayan Saab, Lawrence Carin, David Dunson*

- `1207.4854v3` - [abs](http://arxiv.org/abs/1207.4854v3) - [pdf](http://arxiv.org/pdf/1207.4854v3)

> We study the behavior of the posterior distribution in high-dimensional Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number of predictors and $n$ the sample size. Our focus is on obtaining quantitative finite sample bounds ensuring sufficient posterior probability assigned in neighborhoods of the true regression coefficient vector, $\beta^0$, with high probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain universal bounds, which provide insight into the role of the prior in controlling concentration of the posterior. Based on these finite sample bounds, we examine the implied asymptotic contraction rates for several examples showing that sparsely-structured and heavy-tail shrinkage priors exhibit rapid contraction rates. We also demonstrate that a stronger result holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators ($\gamma$) is drawn from the uniform distribution on the set of binary sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$ if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite sample bounds provide guidelines for designing and evaluating priors for high-dimensional problems.

</details>

<details>

<summary>2014-01-03 19:32:25 - Evaluating the dependence of a non-leaky intervention's partial efficacy on a categorical mark</summary>

- *Paul T. Edlefsen*

- `1206.6701v2` - [abs](http://arxiv.org/abs/1206.6701v2) - [pdf](http://arxiv.org/pdf/1206.6701v2)

> We address discrete-marks survival analysis, also known as categorical sieve analysis, for a setting of a randomized placebo-controlled treatment intervention to prevent infection by a pathogen to which multiple exposures are possible, with a finite number of types of "failure". In particular, we address the case of interventions that are partially efficacious due to a combination of failure-type-dependent efficacy and subject-dependent efficacy, for an intervention that is "non-leaky" (where "leaky" interventions are those for which each exposure event has a chance of resulting in a "failure" outcome, so multiple exposures to pathogens of a single type increase the chance of failure). We introduce the notion of some-or-none interventions, which are completely effective only against some of the failure types, and are completely ineffective against the others. Under conditions of no intervention-induced failures, we introduce a framework and Bayesian and frequentist methods to detect and quantify the extent to which an intervention's partial efficacy is attributable to uneven efficacy across the failure types rather than to incomplete "take" of the intervention. These new methods provide more power than existing methods to detect sieve effects when the conditions hold. We demonstrate the new framework and methods with simulation results and new analyses of genomic signatures of HIV-1 vaccine effects in the STEP and RV144 vaccine efficacy trials.

</details>

<details>

<summary>2014-01-06 11:58:05 - Inverse Bayesian Estimation of Gravitational Mass Density in Galaxies from Missing Kinematic Data</summary>

- *Dalia Chakrabarty, Prasenjit Saha*

- `1401.1052v1` - [abs](http://arxiv.org/abs/1401.1052v1) - [pdf](http://arxiv.org/pdf/1401.1052v1)

> In this paper we focus on a type of inverse problem in which the data is expressed as an unknown function of the sought and unknown model function (or its discretised representation as a model parameter vector). In particular, we deal with situations in which training data is not available. Then we cannot model the unknown functional relationship between data and the unknown model function (or parameter vector) with a Gaussian Process of appropriate dimensionality. A Bayesian method based on state space modelling is advanced instead. Within this framework, the likelihood is expressed in terms of the probability density function ($pdf$) of the state space variable and the sought model parameter vector is embedded within the domain of this $pdf$. As the measurable vector lives only inside an identified sub-volume of the system state space, the $pdf$ of the state space variable is projected onto the space of the measurables, and it is in terms of the projected state space density that the likelihood is written; the final form of the likelihood is achieved after convolution with the distribution of measurement errors. Application motivated vague priors are invoked and the posterior probability density of the model parameter vectors, given the data is computed. Inference is performed by taking posterior samples with adaptive MCMC. The method is illustrated on synthetic as well as real galactic data.

</details>

<details>

<summary>2014-01-07 00:33:30 - Quantile Regression for Large-scale Applications</summary>

- *Jiyan Yang, Xiangrui Meng, Michael W. Mahoney*

- `1305.0087v3` - [abs](http://arxiv.org/abs/1305.0087v3) - [pdf](http://arxiv.org/pdf/1305.0087v3)

> Quantile regression is a method to estimate the quantiles of the conditional distribution of a response variable, and as such it permits a much more accurate portrayal of the relationship between the response variable and observed covariates than methods such as Least-squares or Least Absolute Deviations regression. It can be expressed as a linear program, and, with appropriate preprocessing, interior-point methods can be used to find a solution for moderately large problems. Dealing with very large problems, \emph(e.g.), involving data up to and beyond the terabyte regime, remains a challenge. Here, we present a randomized algorithm that runs in nearly linear time in the size of the input and that, with constant probability, computes a $(1+\epsilon)$ approximate solution to an arbitrary quantile regression problem. As a key step, our algorithm computes a low-distortion subspace-preserving embedding with respect to the loss function of quantile regression. Our empirical evaluation illustrates that our algorithm is competitive with the best previous work on small to medium-sized problems, and that in addition it can be implemented in MapReduce-like environments and applied to terabyte-sized problems.

</details>

<details>

<summary>2014-01-07 04:32:47 - Bayesian Robust Inference of Sample Selection Using Selection-t Models</summary>

- *Peng Ding*

- `1401.1269v1` - [abs](http://arxiv.org/abs/1401.1269v1) - [pdf](http://arxiv.org/pdf/1401.1269v1)

> Heckman selection model is the most popular econometric model in analysis of data with sample selection. However, selection models with Normal errors cannot accommodate heavy tails in the error distribution. Recently, Marchenko and Genton proposed a selection-t model to perform frequentist' robust analysis of sample selection. Instead of using their maximum likelihood estimates, our paper develops new Bayesian procedures for the selection-t models with either continuous or binary outcomes. By exploiting the Normal mixture representation of the t distribution, we can use data augmentation to impute the missing data, and use parameter expansion to sample the restricted covariance matrices. The Bayesian procedures only involve simple steps, without calculating analytical or numerical derivatives of the complicated log likelihood functions. Simulation studies show the vulnerability of the selection models with Normal errors, as well as the robustness of the selection models with t errors. Interestingly, we find evidence of heavy-tailedness in three real examples analyzed by previous studies, and the conclusions about the existence of selection effect are very sensitive to the distributional assumptions of the error terms.

</details>

<details>

<summary>2014-01-07 08:44:31 - Non-stationary patterns of isolation-by-distance: inferring measures of local genetic differentiation with Bayesian kriging</summary>

- *Nicolas Duforet-Frebourg, Michael G. B. Blum*

- `1209.5242v3` - [abs](http://arxiv.org/abs/1209.5242v3) - [pdf](http://arxiv.org/pdf/1209.5242v3)

> Patterns of isolation-by-distance arise when population differentiation increases with increasing geographic distances. Patterns of isolation-by-distance are usually caused by local spatial dispersal, which explains why differences of allele frequencies between populations accumulate with distance. However, spatial variations of demographic parameters such as migration rate or population density can generate non-stationary patterns of isolation-by-distance where the rate at which genetic differentiation accumulates varies across space. To characterize non-stationary patterns of isolation-by-distance, we infer local genetic differentiation based on Bayesian kriging. Local genetic differentiation for a sampled population is defined as the average genetic differentiation between the sampled population and fictive neighboring populations. To avoid defining populations in advance, the method can also be applied at the scale of individuals making it relevant for landscape genetics. Inference of local genetic differentiation relies on a matrix of pairwise similarity or dissimilarity between populations or individuals such as matrices of FST between pairs of populations. Simulation studies show that maps of local genetic differentiation can reveal barriers to gene flow but also other patterns such as continuous variations of gene flow across habitat. The potential of the method is illustrated with 2 data sets: genome-wide SNP data for human Swedish populations and AFLP markers for alpine plant species. The software LocalDiff implementing the method is available at http://membres-timc.imag.fr/Michael.Blum/LocalDiff.html

</details>

<details>

<summary>2014-01-07 18:28:12 - Bayesian Multi--Dipole Modeling of a Single Topography in MEG by Adaptive Sequential Monte Carlo Samplers</summary>

- *Alberto Sorrentino, Gianvittorio Luria, Riccardo Aramini*

- `1305.4511v2` - [abs](http://arxiv.org/abs/1305.4511v2) - [pdf](http://arxiv.org/pdf/1305.4511v2)

> In the present paper, we develop a novel Bayesian approach to the problem of estimating neural currents in the brain from a fixed distribution of magnetic field (called \emph{topography}), measured by magnetoencephalography. Differently from recent studies that describe inversion techniques, such as spatio-temporal regularization/filtering, in which neural dynamics always plays a role, we face here a purely static inverse problem. Neural currents are modelled as an unknown number of current dipoles, whose state space is described in terms of a variable--dimension model. Within the resulting Bayesian framework, we set up a sequential Monte Carlo sampler to explore the posterior distribution. An adaptation technique is employed in order to effectively balance the computational cost and the quality of the sample approximation. Then, both the number and the parameters of the unknown current dipoles are simultaneously estimated. The performance of the method is assessed by means of synthetic data, generated by source configurations containing up to four dipoles. Eventually, we describe the results obtained by analyzing data from a real experiment, involving somatosensory evoked fields, and compare them to those provided by three other methods.

</details>

<details>

<summary>2014-01-08 14:09:10 - Bayesian data augmentation dose finding with continual reassessment method and delayed toxicity</summary>

- *Suyu Liu, Guosheng Yin, Ying Yuan*

- `1401.1706v1` - [abs](http://arxiv.org/abs/1401.1706v1) - [pdf](http://arxiv.org/pdf/1401.1706v1)

> A major practical impediment when implementing adaptive dose-finding designs is that the toxicity outcome used by the decision rules may not be observed shortly after the initiation of the treatment. To address this issue, we propose the data augmentation continual reassessment method (DA-CRM) for dose finding. By naturally treating the unobserved toxicities as missing data, we show that such missing data are nonignorable in the sense that the missingness depends on the unobserved outcomes. The Bayesian data augmentation approach is used to sample both the missing data and model parameters from their posterior full conditional distributions. We evaluate the performance of the DA-CRM through extensive simulation studies and also compare it with other existing methods. The results show that the proposed design satisfactorily resolves the issues related to late-onset toxicities and possesses desirable operating characteristics: treating patients more safely and also selecting the maximum tolerated dose with a higher probability. The new DA-CRM is illustrated with two phase I cancer clinical trials.

</details>

<details>

<summary>2014-01-09 19:44:36 - Implementing and Automating Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression</summary>

- *Tim Salimans*

- `1401.2135v1` - [abs](http://arxiv.org/abs/1401.2135v1) - [pdf](http://arxiv.org/pdf/1401.2135v1)

> We recently proposed a general algorithm for approximating nonstandard Bayesian posterior distributions by minimization of their Kullback-Leibler divergence with respect to a more convenient approximating distribution. In this note we offer details on how to efficiently implement this algorithm in practice. We also suggest default choices for the form of the posterior approximation, the number of iterations, the step size, and other user choices. By using these defaults it becomes possible to construct good posterior approximations for hierarchical models completely automatically.

</details>

<details>

<summary>2014-01-10 10:28:56 - An empirical Bayes testing procedure for detecting variants in analysis of next generation sequencing data</summary>

- *Zhigen Zhao, Wei Wang, Zhi Wei*

- `1401.2278v1` - [abs](http://arxiv.org/abs/1401.2278v1) - [pdf](http://arxiv.org/pdf/1401.2278v1)

> Because of the decreasing cost and high digital resolution, next-generation sequencing (NGS) is expected to replace the traditional hybridization-based microarray technology. For genetics study, the first-step analysis of NGS data is often to identify genomic variants among sequenced samples. Several statistical models and tests have been developed for variant calling in NGS study. The existing approaches, however, are based on either conventional Bayesian or frequentist methods, which are unable to address the multiplicity and testing efficiency issues simultaneously. In this paper, we derive an optimal empirical Bayes testing procedure to detect variants for NGS study. We utilize the empirical Bayes technique to exploit the across-site information among many testing sites in NGS data. We prove that our testing procedure is valid and optimal in the sense of rejecting the maximum number of nonnulls while the Bayesian false discovery rate is controlled at a given nominal level. We show by both simulation studies and real data analysis that our testing efficiency can be greatly enhanced over the existing frequentist approaches that fail to pool and utilize information across the multiple testing sites.

</details>

<details>

<summary>2014-01-10 13:29:39 - Bayesian shrinkage methods for partially observed data with many predictors</summary>

- *Philip S. Boonstra, Bhramar Mukherjee, Jeremy M. G. Taylor*

- `1401.2324v1` - [abs](http://arxiv.org/abs/1401.2324v1) - [pdf](http://arxiv.org/pdf/1401.2324v1)

> Motivated by the increasing use of and rapid changes in array technologies, we consider the prediction problem of fitting a linear regression relating a continuous outcome $Y$ to a large number of covariates $\mathbf {X}$, for example, measurements from current, state-of-the-art technology. For most of the samples, only the outcome $Y$ and surrogate covariates, $\mathbf {W}$, are available. These surrogates may be data from prior studies using older technologies. Owing to the dimension of the problem and the large fraction of missing information, a critical issue is appropriate shrinkage of model parameters for an optimal bias-variance trade-off. We discuss a variety of fully Bayesian and Empirical Bayes algorithms which account for uncertainty in the missing data and adaptively shrink parameter estimates for superior prediction. These methods are evaluated via a comprehensive simulation study. In addition, we apply our methods to a lung cancer data set, predicting survival time ($Y$) using qRT-PCR ($\mathbf {X}$) and microarray ($\mathbf {W}$) measurements.

</details>

<details>

<summary>2014-01-10 14:34:12 - Exploiting multiple outcomes in Bayesian principal stratification analysis with application to the evaluation of a job training program</summary>

- *Alessandra Mattei, Fan Li, Fabrizia Mealli*

- `1401.2344v1` - [abs](http://arxiv.org/abs/1401.2344v1) - [pdf](http://arxiv.org/pdf/1401.2344v1)

> The causal effect of a randomized job training program, the JOBS II study, on trainees' depression is evaluated. Principal stratification is used to deal with noncompliance to the assigned treatment. Due to the latent nature of the principal strata, strong structural assumptions are often invoked to identify principal causal effects. Alternatively, distributional assumptions may be invoked using a model-based approach. These often lead to weakly identified models with substantial regions of flatness in the posterior distribution of the causal effects. Information on multiple outcomes is routinely collected in practice, but is rarely used to improve inference. This article develops a Bayesian approach to exploit multivariate outcomes to sharpen inferences in weakly identified principal stratification models. We show that inference for the causal effect on depression is significantly improved by using the re-employment status as a secondary outcome in the JOBS II study. Simulation studies are also performed to illustrate the potential gains in the estimation of principal causal effects from jointly modeling more than one outcome. This approach can also be used to assess plausibility of structural assumptions and sensitivity to deviations from these structural assumptions. Two model checking procedures via posterior predictive checks are also discussed.

</details>

<details>

<summary>2014-01-11 03:13:16 - Multiscale Shrinkage and LÃ©vy Processes</summary>

- *Xin Yuan, Vinayak Rao, Shaobo Han, Lawrence Carin*

- `1401.2497v1` - [abs](http://arxiv.org/abs/1401.2497v1) - [pdf](http://arxiv.org/pdf/1401.2497v1)

> A new shrinkage-based construction is developed for a compressible vector $\boldsymbol{x}\in\mathbb{R}^n$, for cases in which the components of $\xv$ are naturally associated with a tree structure. Important examples are when $\xv$ corresponds to the coefficients of a wavelet or block-DCT representation of data. The method we consider in detail, and for which numerical results are presented, is based on increments of a gamma process. However, we demonstrate that the general framework is appropriate for many other types of shrinkage priors, all within the L\'{e}vy process family, with the gamma process a special case. Bayesian inference is carried out by approximating the posterior with samples from an MCMC algorithm, as well as by constructing a heuristic variational approximation to the posterior. We also consider expectation-maximization (EM) for a MAP (point) solution. State-of-the-art results are manifested for compressive sensing and denoising applications, the latter with spiky (non-Gaussian) noise.

</details>

<details>

<summary>2014-01-12 09:52:17 - On Using Control Variates with Stochastic Approximation for Variational Bayes and its Connection to Stochastic Linear Regression</summary>

- *Tim Salimans, David A. Knowles*

- `1401.1022v3` - [abs](http://arxiv.org/abs/1401.1022v3) - [pdf](http://arxiv.org/pdf/1401.1022v3)

> Recently, we and several other authors have written about the possibilities of using stochastic approximation techniques for fitting variational approximations to intractable Bayesian posterior distributions. Naive implementations of stochastic approximation suffer from high variance in this setting. Several authors have therefore suggested using control variates to reduce this variance, while we have taken a different but analogous approach to reducing the variance which we call stochastic linear regression. In this note we take the former perspective and derive the ideal set of control variates for stochastic approximation variational Bayes under a certain set of assumptions. We then show that using these control variates is closely related to using the stochastic linear regression approximation technique we proposed earlier. A simple example shows that our method for constructing control variates leads to stochastic estimators with much lower variance compared to other approaches.

</details>

<details>

<summary>2014-01-12 16:53:43 - Hierarchical modelling of faecal egg counts to assess anthelmintic efficacy</summary>

- *Michaela Paul, Paul R. Torgerson, Johan HÃ¶glund, Reinhard Furrer*

- `1401.2642v1` - [abs](http://arxiv.org/abs/1401.2642v1) - [pdf](http://arxiv.org/pdf/1401.2642v1)

> Counting the number of parasite eggs in faecal samples is a widely used diagnostic method to evaluate parasite burden. Typically a sub-sample of the diluted faeces is examined for eggs. The resulting egg counts are multiplied by a specific correction factor to estimate the mean parasite burden. To detect anthelmintic resistance, the mean parasite burden from treated and untreated animals are compared. However, this standard method has some limitations. In particular, the analysis of repeated samples may produce quite variable results as the sampling variability due to the counting technique is ignored. We propose a hierarchical model that takes this sampling variability as well as between-animal variation into account. Bayesian inference is done via Markov chain Monte Carlo sampling. The performance of the hierarchical model is illustrated by a re-analysis of faecal egg count data from a Swedish study assessing the anthelmintic resistance of nematode parasite in sheep. A simulation study shows that the hierarchical model provides better classification of anthelmintic resistance compared to the standard method.

</details>

<details>

<summary>2014-01-13 07:07:41 - A semiparametric approach to mixed outcome latent variable models: Estimating the association between cognition and regional brain volumes</summary>

- *Jonathan Gruhl, Elena A. Erosheva, Paul K. Crane*

- `1401.2728v1` - [abs](http://arxiv.org/abs/1401.2728v1) - [pdf](http://arxiv.org/pdf/1401.2728v1)

> Multivariate data that combine binary, categorical, count and continuous outcomes are common in the social and health sciences. We propose a semiparametric Bayesian latent variable model for multivariate data of arbitrary type that does not require specification of conditional distributions. Drawing on the extended rank likelihood method by Hoff [Ann. Appl. Stat. 1 (2007) 265-283], we develop a semiparametric approach for latent variable modeling with mixed outcomes and propose associated Markov chain Monte Carlo estimation methods. Motivated by cognitive testing data, we focus on bifactor models, a special case of factor analysis. We employ our semiparametric Bayesian latent variable model to investigate the association between cognitive outcomes and MRI-measured regional brain volumes.

</details>

<details>

<summary>2014-01-13 09:11:59 - Bayesian spline method for assessing extreme loads on wind turbines</summary>

- *Giwhyun Lee, Eunshin Byon, Lewis Ntaimo, Yu Ding*

- `1401.2760v1` - [abs](http://arxiv.org/abs/1401.2760v1) - [pdf](http://arxiv.org/pdf/1401.2760v1)

> This study presents a Bayesian parametric model for the purpose of estimating the extreme load on a wind turbine. The extreme load is the highest stress level imposed on a turbine structure that the turbine would experience during its service lifetime. A wind turbine should be designed to resist such a high load to avoid catastrophic structural failures. To assess the extreme load, turbine structural responses are evaluated by conducting field measurement campaigns or performing aeroelastic simulation studies. In general, data obtained in either case are not sufficient to represent various loading responses under all possible weather conditions. An appropriate extrapolation is necessary to characterize the structural loads in a turbine's service life. This study devises a Bayesian spline method for this extrapolation purpose, using load data collected in a period much shorter than a turbine's service life. The spline method is applied to three sets of turbine's load response data to estimate the corresponding extreme loads at the roots of the turbine blades. Compared to the current industry practice, the spline method appears to provide better extreme load assessment.

</details>

<details>

<summary>2014-01-13 10:14:08 - A variational Bayes framework for sparse adaptive estimation</summary>

- *Konstantinos E. Themelis, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas*

- `1401.2771v1` - [abs](http://arxiv.org/abs/1401.2771v1) - [pdf](http://arxiv.org/pdf/1401.2771v1)

> Recently, a number of mostly $\ell_1$-norm regularized least squares type deterministic algorithms have been proposed to address the problem of \emph{sparse} adaptive signal estimation and system identification. From a Bayesian perspective, this task is equivalent to maximum a posteriori probability estimation under a sparsity promoting heavy-tailed prior for the parameters of interest. Following a different approach, this paper develops a unifying framework of sparse \emph{variational Bayes} algorithms that employ heavy-tailed priors in conjugate hierarchical form to facilitate posterior inference. The resulting fully automated variational schemes are first presented in a batch iterative form. Then it is shown that by properly exploiting the structure of the batch estimation task, new sparse adaptive variational Bayes algorithms can be derived, which have the ability to impose and track sparsity during real-time processing in a time-varying environment. The most important feature of the proposed algorithms is that they completely eliminate the need for computationally costly parameter fine-tuning, a necessary ingredient of sparse adaptive deterministic algorithms. Extensive simulation results are provided to demonstrate the effectiveness of the new sparse variational Bayes algorithms against state-of-the-art deterministic techniques for adaptive channel estimation. The results show that the proposed algorithms are numerically robust and exhibit in general superior estimation performance compared to their deterministic counterparts.

</details>

<details>

<summary>2014-01-13 14:02:37 - GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation</summary>

- *Edward Meeds, Max Welling*

- `1401.2838v1` - [abs](http://arxiv.org/abs/1401.2838v1) - [pdf](http://arxiv.org/pdf/1401.2838v1)

> Scientists often express their understanding of the world through a computationally demanding simulation program. Analyzing the posterior distribution of the parameters given observations (the inverse problem) can be extremely challenging. The Approximate Bayesian Computation (ABC) framework is the standard statistical tool to handle these likelihood free problems, but they require a very large number of simulations. In this work we develop two new ABC sampling algorithms that significantly reduce the number of simulations necessary for posterior inference. Both algorithms use confidence estimates for the accept probability in the Metropolis Hastings step to adaptively choose the number of necessary simulations. Our GPS-ABC algorithm stores the information obtained from every simulation in a Gaussian process which acts as a surrogate function for the simulated statistics. Experiments on a challenging realistic biological problem illustrate the potential of these algorithms.

</details>

<details>

<summary>2014-01-13 15:00:33 - Probabilistic Mortality Forecasting with Varying Age-Specific Survival Improvements</summary>

- *Christina Bohk, Roland Rau*

- `1311.5380v2` - [abs](http://arxiv.org/abs/1311.5380v2) - [pdf](http://arxiv.org/pdf/1311.5380v2)

> We propose a probabilistic mortality forecasting model that can be applied to derive forecasts for populations with regular and irregular mortality developments. Our model (1) uses rates of mortality improvement to model dynamic age patterns of mortality change and it can (2) optionally complement the mortality trend of a country of interest with that of at least one reference country. Retrospective mortality forecasts for British and Danish women from 1991 to 2011 suggest that our model can generate smaller forecast errors than other widely accepted approaches like, for instance, the Lee-Carter model or the UN Bayesian approach.

</details>

<details>

<summary>2014-01-13 16:10:07 - Exact Bayesian Inference for the Bingham Distribution</summary>

- *Christopher J. Fallaize, Theodore Kypraios*

- `1401.2894v1` - [abs](http://arxiv.org/abs/1401.2894v1) - [pdf](http://arxiv.org/pdf/1401.2894v1)

> This paper is concerned with making Bayesian inference from data that are assumed to be drawn from a Bingham distribution. A barrier to the Bayesian approach is the parameter-dependent normalising constant of the Bingham distribution, which, even when it can be evaluated or accurately approximated, would have to be calculated at each iteration of an MCMC scheme, thereby greatly increasing the computational burden. We propose a method which enables exact (in Monte Carlo sense) Bayesian inference for the unknown parameters of the Bingham distribution by completely avoiding the need to evaluate this constant. We apply the method to simulated and real data, and illustrate that it is simpler to implement, faster, and performs better than an alternative algorithm that has recently been proposed in the literature.

</details>

<details>

<summary>2014-01-13 19:04:13 - Binary Classifier Calibration: Bayesian Non-Parametric Approach</summary>

- *Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht*

- `1401.2955v1` - [abs](http://arxiv.org/abs/1401.2955v1) - [pdf](http://arxiv.org/pdf/1401.2955v1)

> A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classification models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicable to a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods.

</details>

<details>

<summary>2014-01-14 06:47:59 - A comparison of Bayesian and frequentist interval estimators in regression that utilize uncertain prior information</summary>

- *Paul Kabaila, Gayan Dharmarathne*

- `1401.3084v1` - [abs](http://arxiv.org/abs/1401.3084v1) - [pdf](http://arxiv.org/pdf/1401.3084v1)

> Consider a linear regression model with regression parameter beta and normally distributed errors. Suppose that the parameter of interest is theta = a^T beta where a is a specified vector. Define the parameter tau = c^T beta - t where c and t are specified and a and c are linearly independent. Also suppose that we have uncertain prior information that tau = 0. Kabaila and Giri, 2009, JSPI, describe a new frequentist 1-alpha confidence interval for theta that utilizes this uncertain prior information. We compare this confidence interval with Bayesian 1-alpha equi-tailed and shortest credible intervals for theta that result from a prior density for tau that is a mixture of a rectangular "slab" and a Dirac delta function "spike", combined with noninformative prior densities for the other parameters of the model. We show that these frequentist and Bayesian interval estimators depend on the data in very different ways. We also consider some close variants of this prior distribution that lead to Bayesian and frequentist interval estimators with greater similarity. Nonetheless, as we show, substantial differences between these interval estimators remain.

</details>

<details>

<summary>2014-01-14 15:45:45 - Principal Component Analysis in an Asymmetric Norm</summary>

- *Ngoc Mai Tran, Maria Osipenko, Wolfgang Karl Haerdle*

- `1401.3229v1` - [abs](http://arxiv.org/abs/1401.3229v1) - [pdf](http://arxiv.org/pdf/1401.3229v1)

> Principal component analysis (PCA) is a widely used dimension reduction tool in the analysis of many kind of high-dimensional data. It is used in signal processing, mechanical engineering, psychometrics, and other fields under different names. It still bears the same mathematical idea: the decomposition of variation of a high dimensional object into uncorrelated factors or components. However, in many of the above applications, one is interested in capturing the tail variables of the data rather than variation around the mean. Such applications include weather related event curves, expected shortfalls, and speeding analysis among others. These are all high dimensional tail objects which one would like to study in a PCA fashion. The tail character though requires to do the dimension reduction in an asymmetric norm rather than the classical $L_2$-type orthogonal projection. We develop an analogue of PCA in an asymmetric norm. These norms cover both quantiles and expectiles, another tail event measure. The difficulty is that there is no natural basis, no `principal components', to the $k$-dimensional subspace found. We propose two definitions of principal components and provide algorithms based on iterative least squares. We prove upper bounds on their convergence times, and compare their performances in a simulation study. We apply the algorithms to a Chinese weather dataset with a view to weather derivative pricing

</details>

<details>

<summary>2014-01-14 21:17:21 - Survey On The Estimation Of Mutual Information Methods as a Measure of Dependency Versus Correlation Analysis</summary>

- *D. Gencaga, N. K. Malakar, D. J. Lary*

- `1401.3358v1` - [abs](http://arxiv.org/abs/1401.3358v1) - [pdf](http://arxiv.org/pdf/1401.3358v1)

> In this survey, we present and compare different approaches to estimate Mutual Information (MI) from data to analyse general dependencies between variables of interest in a system. We demonstrate the performance difference of MI versus correlation analysis, which is only optimal in case of linear dependencies. First, we use a piece-wise constant Bayesian methodology using a general Dirichlet prior. In this estimation method, we use a two-stage approach where we approximate the probability distribution first and then calculate the marginal and joint entropies. Here, we demonstrate the performance of this Bayesian approach versus the others for computing the dependency between different variables. We also compare these with linear correlation analysis. Finally, we apply MI and correlation analysis to the identification of the bias in the determination of the aerosol optical depth (AOD) by the satellite based Moderate Resolution Imaging Spectroradiometer (MODIS) and the ground based AErosol RObotic NETwork (AERONET). Here, we observe that the AOD measurements by these two instruments might be different for the same location. The reason of this bias is explored by quantifying the dependencies between the bias and 15 other variables including cloud cover, surface reflectivity and others.

</details>

<details>

<summary>2014-01-14 23:14:00 - A Log Probability Weighted Moment Estimator of Extreme Quantiles</summary>

- *Frederico Caeiro, Dora Prata Gomes*

- `1401.3383v1` - [abs](http://arxiv.org/abs/1401.3383v1) - [pdf](http://arxiv.org/pdf/1401.3383v1)

> In this paper we consider the semi-parametric estimation of extreme quantiles of a right heavy-tail model. We propose a new Log Probability Weighted Moment estimator for extreme quantiles, which is obtained from the estimators of the shape and scale parameters of the tail. Under a second-order regular variation condition on the tail, of the underlying distribution function, we deduce the non degenerate asymptotic behaviour of the estimators under study and present an asymptotic comparison at their optimal levels. In addition, the performance of the estimators is illustrated through an application to real data.

</details>

<details>

<summary>2014-01-15 04:54:14 - Transductive Rademacher Complexity and its Applications</summary>

- *Ran El-Yaniv, Dmitry Pechyony*

- `1401.3441v1` - [abs](http://arxiv.org/abs/1401.3441v1) - [pdf](http://arxiv.org/pdf/1401.3441v1)

> We develop a technique for deriving data-dependent error bounds for transductive learning algorithms based on transductive Rademacher complexity. Our technique is based on a novel general error bound for transduction in terms of transductive Rademacher complexity, together with a novel bounding technique for Rademacher averages for particular algorithms, in terms of their "unlabeled-labeled" representation. This technique is relevant to many advanced graph-based transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms. Finally, we present a new PAC-Bayesian bound for mixtures of transductive algorithms based on our Rademacher bounds.

</details>

<details>

<summary>2014-01-15 05:33:29 - Efficient Markov Network Structure Discovery Using Independence Tests</summary>

- *Facundo Bromberg, Dimitris Margaritis, Vasant Honavar*

- `1401.3478v1` - [abs](http://arxiv.org/abs/1401.3478v1) - [pdf](http://arxiv.org/pdf/1401.3478v1)

> We present two algorithms for learning the structure of a Markov network from data: GSMN* and GSIMN. Both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests. Until very recently, algorithms for structure learning were based on maximum likelihood estimation, which has been proved to be NP-hard for Markov networks due to the difficulty of estimating the parameters of the network, needed for the computation of the data likelihood. The independence-based approach does not require the computation of the likelihood, and thus both GSMN* and GSIMN can compute the structure efficiently (as shown in our experiments). GSMN* is an adaptation of the Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of Bayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls well-known properties of the conditional independence relation to infer novel independences from known ones, thus avoiding the performance of statistical tests to estimate them. To accomplish this efficiently GSIMN uses the Triangle theorem, also introduced in this work, which is a simplified version of the set of Markov axioms. Experimental comparisons on artificial and real-world data sets show GSIMN can yield significant savings with respect to GSMN*, while generating a Markov network with comparable or in some cases improved quality. We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH, that produces all possible conditional independences resulting from repeatedly applying Pearls theorems on the known conditional independence tests. The results of this comparison show that GSIMN, by the sole use of the Triangle theorem, is nearly optimal in terms of the set of independences tests that it infers.

</details>

<details>

<summary>2014-01-15 08:52:29 - Percolation under Noise: Detecting Explosive Percolation Using the Second Largest Component</summary>

- *Wes Viles, Cedric E. Ginestet, Ariana Tang, Mark A. Kramer, Eric D. Kolaczyk*

- `1401.3518v1` - [abs](http://arxiv.org/abs/1401.3518v1) - [pdf](http://arxiv.org/pdf/1401.3518v1)

> We consider the problem of distinguishing classical (Erd\H{o}s-R\'{e}nyi) percolation from explosive (Achlioptas) percolation, under noise. A statistical model of percolation is constructed allowing for the birth and death of edges as well as the presence of noise in the observations. This graph-valued stochastic process is composed of a latent and an observed non-stationary process, where the observed graph process is corrupted by Type I and Type II errors. This produces a hidden Markov graph model. We show that for certain choices of parameters controlling the noise, the classical (ER) percolation is visually indistinguishable from the explosive (Achlioptas) percolation model. In this setting, we compare two different criteria for discriminating between these two percolation models, based on a quantile difference (QD) of the first component's size and on the maximal size of the second largest component. We show through data simulations that this second criterion outperforms the QD of the first component's size, in terms of discriminatory power. The maximal size of the second component therefore provides a useful statistic for distinguishing between the ER and Achlioptas models of percolation, under physically motivated conditions for the birth and death of edges, and under noise. The potential application of the proposed criteria for percolation detection in clinical neuroscience is also discussed.

</details>

<details>

<summary>2014-01-16 12:09:11 - Asymptotic theory of cepstral random fields</summary>

- *Tucker S. McElroy, Scott H. Holan*

- `1112.1977v4` - [abs](http://arxiv.org/abs/1112.1977v4) - [pdf](http://arxiv.org/pdf/1112.1977v4)

> Random fields play a central role in the analysis of spatially correlated data and, as a result, have a significant impact on a broad array of scientific applications. This paper studies the cepstral random field model, providing recursive formulas that connect the spatial cepstral coefficients to an equivalent moving-average random field, which facilitates easy computation of the autocovariance matrix. We also provide a comprehensive treatment of the asymptotic theory for two-dimensional random field models: we establish asymptotic results for Bayesian, maximum likelihood and quasi-maximum likelihood estimation of random field parameters and regression parameters. The theoretical results are presented generally and are of independent interest, pertaining to a wide class of random field models. The results for the cepstral model facilitate model-building: because the cepstral coefficients are unconstrained in practice, numerical optimization is greatly simplified, and we are always guaranteed a positive definite covariance matrix. We show that inference for individual coefficients is possible, and one can refine models in a disciplined manner. Our results are illustrated through simulation and the analysis of straw yield data in an agricultural field experiment.

</details>

<details>

<summary>2014-01-17 01:20:13 - Bayesian Hypothesis Assessment in Two-arm Trials Using Relative Belief Ratios</summary>

- *Saman Muthukumarana, Michael Evans*

- `1401.4215v1` - [abs](http://arxiv.org/abs/1401.4215v1) - [pdf](http://arxiv.org/pdf/1401.4215v1)

> This paper develops a Bayesian approach for assessing equivalence and non-inferiority hypotheses in two-arm trials using relative belief ratios. A relative belief ratio is a measure of statistical evidence and can indicate evidence either for or against a hypothesis. In addition to the relative belief ratio, we also compute a measure of the strength of this evidence as a calibration of the relative belief ratio. Furthermore, we make use of the relative belief ratio as a measure of evidence, to assess whether a given prior induces bias either for or against a hypothesis. Prior elicitation, model checking and checking for prior-data conflict procedures are developed to ensure that the choices of model and prior made are relevant to the specific application. We highlight the applicability of the approach and illustrate the proposed method by applying it to a data set obtained from a two-arm clinical trial.

</details>

<details>

<summary>2014-01-21 08:24:07 - Consistency of weighted majority votes</summary>

- *Daniel Berend, Aryeh Kontorovich*

- `1312.0451v5` - [abs](http://arxiv.org/abs/1312.0451v5) - [pdf](http://arxiv.org/pdf/1312.0451v5)

> We revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory.

</details>

<details>

<summary>2014-01-21 17:37:37 - Dirichlet-Laplace priors for optimal shrinkage</summary>

- *Anirban Bhattacharya, Debdeep Pati, Natesh S. Pillai, David B. Dunson*

- `1401.5398v1` - [abs](http://arxiv.org/abs/1401.5398v1) - [pdf](http://arxiv.org/pdf/1401.5398v1)

> Penalized regression methods, such as $L_1$ regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality properties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated an amazing variety of continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, facilitating computation. In sharp contrast to the frequentist literature, little is known about the properties of such priors and the convergence and concentration of the corresponding posterior distribution. In this article, we propose a new class of Dirichlet--Laplace (DL) priors, which possess optimal posterior concentration and lead to efficient posterior computation exploiting results from normalized random measure theory. Finite sample performance of Dirichlet--Laplace priors relative to alternatives is assessed in simulated and real data examples.

</details>

<details>

<summary>2014-01-21 23:51:01 - Guaranteed Model Order Estimation and Sample Complexity Bounds for LDA</summary>

- *E. D. GutiÃ©rrez*

- `1312.2646v4` - [abs](http://arxiv.org/abs/1312.2646v4) - [pdf](http://arxiv.org/pdf/1312.2646v4)

> The question of how to determine the number of independent latent factors (topics) in mixture models such as Latent Dirichlet Allocation (LDA) is of great practical importance. In most applications, the exact number of topics is unknown, and depends on the application and the size of the data set. Bayesian nonparametric methods can avoid the problem of topic number selection, but they can be impracticably slow for large sample sizes and are subject to local optima. We develop a guaranteed procedure for topic number recovery that does not necessitate learning the model's latent parameters beforehand. Our procedure relies on adapting results from random matrix theory. Performance of our topic number recovery procedure is superior to hLDA, a nonparametric method. We also discuss some implications of our results on the sample complexity and accuracy of popular spectral learning algorithms for LDA. Our results and procedure can be extended to spectral learning algorithms for other exchangeable mixture models as well as Hidden Markov Models.

</details>

<details>

<summary>2014-01-22 03:33:45 - On Bayesian inference for the M/G/1 queue with efficient MCMC sampling</summary>

- *Alexander Y. Shestopaloff, Radford M. Neal*

- `1401.5548v1` - [abs](http://arxiv.org/abs/1401.5548v1) - [pdf](http://arxiv.org/pdf/1401.5548v1)

> We introduce an efficient MCMC sampling scheme to perform Bayesian inference in the M/G/1 queueing model given only observations of interdeparture times. Our MCMC scheme uses a combination of Gibbs sampling and simple Metropolis updates together with three novel "shift" and "scale" updates. We show that our novel updates improve the speed of sampling considerably, by factors of about 60 to about 180 on a variety of simulated data sets.

</details>

<details>

<summary>2014-01-22 15:17:39 - Bayesian modeling and forecasting of 24-hour high-frequency volatility: A case study of the financial crisis</summary>

- *Jonathan R. Stroud, Michael S. Johannes*

- `1211.2961v2` - [abs](http://arxiv.org/abs/1211.2961v2) - [pdf](http://arxiv.org/pdf/1211.2961v2)

> This paper estimates models of high frequency index futures returns using `around the clock' 5-minute returns that incorporate the following key features: multiple persistent stochastic volatility factors, jumps in prices and volatilities, seasonal components capturing time of the day patterns, correlations between return and volatility shocks, and announcement effects. We develop an integrated MCMC approach to estimate interday and intraday parameters and states using high-frequency data without resorting to various aggregation measures like realized volatility. We provide a case study using financial crisis data from 2007 to 2009, and use particle filters to construct likelihood functions for model comparison and out-of-sample forecasting from 2009 to 2012. We show that our approach improves realized volatility forecasts by up to 50% over existing benchmarks.

</details>

<details>

<summary>2014-01-22 21:24:05 - Statistical modelling of summary values leads to accurate Approximate Bayesian Computations</summary>

- *Oliver Ratmann, Anton Camacho, Adam Meijer, GÃ© Donker*

- `1305.4283v2` - [abs](http://arxiv.org/abs/1305.4283v2) - [pdf](http://arxiv.org/pdf/1305.4283v2)

> Approximate Bayesian Computation (ABC) methods rely on asymptotic arguments, implying that parameter inference can be systematically biased even when sufficient statistics are available. We propose to construct the ABC accept/reject step from decision theoretic arguments on a suitable auxiliary space. This framework, referred to as ABC*, fully specifies which test statistics to use, how to combine them, how to set the tolerances and how long to simulate in order to obtain accuracy properties on the auxiliary space. Akin to maximum-likelihood indirect inference, regularity conditions establish when the ABC* approximation to the posterior density is accurate on the original parameter space in terms of the Kullback-Leibler divergence and the maximum a posteriori point estimate. Fundamentally, escaping asymptotic arguments requires knowledge of the distribution of test statistics, which we obtain through modelling the distribution of summary values, data points on a summary level. Synthetic examples and an application to time series data of influenza A (H3N2) infections in the Netherlands illustrate ABC* in action.

</details>

<details>

<summary>2014-01-26 11:00:46 - Painting Analysis Using Wavelets and Probabilistic Topic Models</summary>

- *Tong Wu, Gungor Polatkan, David Steel, William Brown, Ingrid Daubechies, Robert Calderbank*

- `1401.6638v1` - [abs](http://arxiv.org/abs/1401.6638v1) - [pdf](http://arxiv.org/pdf/1401.6638v1)

> In this paper, computer-based techniques for stylistic analysis of paintings are applied to the five panels of the 14th century Peruzzi Altarpiece by Giotto di Bondone. Features are extracted by combining a dual-tree complex wavelet transform with a hidden Markov tree (HMT) model. Hierarchical clustering is used to identify stylistic keywords in image patches, and keyword frequencies are calculated for sub-images that each contains many patches. A generative hierarchical Bayesian model learns stylistic patterns of keywords; these patterns are then used to characterize the styles of the sub-images; this in turn, permits to discriminate between paintings. Results suggest that such unsupervised probabilistic topic models can be useful to distill characteristic elements of style.

</details>

<details>

<summary>2014-01-28 09:06:30 - Bayesian Properties of Normalized Maximum Likelihood and its Fast Computation</summary>

- *Andrew Barron, Teemu Roos, Kazuho Watanabe*

- `1401.7116v1` - [abs](http://arxiv.org/abs/1401.7116v1) - [pdf](http://arxiv.org/pdf/1401.7116v1)

> The normalized maximized likelihood (NML) provides the minimax regret solution in universal data compression, gambling, and prediction, and it plays an essential role in the minimum description length (MDL) method of statistical modeling and estimation. Here we show that the normalized maximum likelihood has a Bayes-like representation as a mixture of the component models, even in finite samples, though the weights of linear combination may be both positive and negative. This representation addresses in part the relationship between MDL and Bayes modeling. This representation has the advantage of speeding the calculation of marginals and conditionals required for coding and prediction applications.

</details>

<details>

<summary>2014-01-28 11:44:29 - Tempering by Subsampling</summary>

- *Jan-Willem van de Meent, Brooks Paige, Frank Wood*

- `1401.7145v1` - [abs](http://arxiv.org/abs/1401.7145v1) - [pdf](http://arxiv.org/pdf/1401.7145v1)

> In this paper we demonstrate that tempering Markov chain Monte Carlo samplers for Bayesian models by recursively subsampling observations without replacement can improve the performance of baseline samplers in terms of effective sample size per computation. We present two tempering by subsampling algorithms, subsampled parallel tempering and subsampled tempered transitions. We provide an asymptotic analysis of the computational cost of tempering by subsampling, verify that tempering by subsampling costs less than traditional tempering, and demonstrate both algorithms on Bayesian approaches to learning the mean of a high dimensional multivariate Normal and estimating Gaussian process hyperparameters.

</details>

<details>

<summary>2014-01-28 16:45:25 - Bergm: Bayesian Exponential Random Graphs in R</summary>

- *Alberto Caimo, Nial Friel*

- `1201.2770v3` - [abs](http://arxiv.org/abs/1201.2770v3) - [pdf](http://arxiv.org/pdf/1201.2770v3)

> In this paper we describe the main featuress of the Bergm package for the open-source R software which provides a comprehensive framework for Bayesian analysis for exponential random graph models: tools for parameter estimation, model selection and goodness-of-fit diagnostics. We illustrate the capabilities of this package describing the algorithms through a tutorial analysis of two well-known network datasets.

</details>

<details>

<summary>2014-01-29 01:54:57 - Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts</summary>

- *Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui*

- `1401.1974v4` - [abs](http://arxiv.org/abs/1401.1974v4) - [pdf](http://arxiv.org/pdf/1401.1974v4)

> We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.

</details>

<details>

<summary>2014-01-29 18:46:38 - Bayesian nonparametric comorbidity analysis of psychiatric disorders</summary>

- *Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fernando Perez-Cruz*

- `1401.7620v1` - [abs](http://arxiv.org/abs/1401.7620v1) - [pdf](http://arxiv.org/pdf/1401.7620v1)

> The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database.

</details>

<details>

<summary>2014-01-30 15:54:03 - Inference with penalized likelihood</summary>

- *Daniel Commenges, JÃ©rÃ©mie Bureau, Hein Putter*

- `1401.7893v1` - [abs](http://arxiv.org/abs/1401.7893v1) - [pdf](http://arxiv.org/pdf/1401.7893v1)

> This work studies the statistical properties of the maximum penalized likelihood approach in a semi-parametric framework. We recall the penalized likelihood approach for estimating a function and review some asymptotic results. We investigate the properties of two estimators of the variance of maximum penalized likelihood estimators: sandwich estimator and a Bayesian estimator. The coverage rates of confidence intervals based on these estimators are studied through a simulation study of survival data. In a first simulation the coverage rates for the survival function and the hazard function are evaluated. In a second simulation data are generated from a proportional hazard model with covariates. The estimators of the variances of the regression coefficients are studied. As for the survival and hazard functions, both sandwich and Bayesian estimators exhibit relatively good properties, but the Bayesian estimator seems to be more accurate. As for the regression coefficients, we focussed on the Bayesian estimator and found that it yielded good coverage rates.

</details>

<details>

<summary>2014-01-30 22:40:35 - Sparse Bayesian Unsupervised Learning</summary>

- *Stephane Gaiffas, Bertrand Michel*

- `1401.8017v1` - [abs](http://arxiv.org/abs/1401.8017v1) - [pdf](http://arxiv.org/pdf/1401.8017v1)

> This paper is about variable selection, clustering and estimation in an unsupervised high-dimensional setting. Our approach is based on fitting constrained Gaussian mixture models, where we learn the number of clusters $K$ and the set of relevant variables $S$ using a generalized Bayesian posterior with a sparsity inducing prior. We prove a sparsity oracle inequality which shows that this procedure selects the optimal parameters $K$ and $S$. This procedure is implemented using a Metropolis-Hastings algorithm, based on a clustering-oriented greedy proposal, which makes the convergence to the posterior very fast.

</details>

<details>

<summary>2014-01-31 07:54:14 - Marginal and simultaneous predictive classification using stratified graphical models</summary>

- *Henrik Nyman, Jie Xiong, Johan Pensar, Jukka Corander*

- `1401.8078v1` - [abs](http://arxiv.org/abs/1401.8078v1) - [pdf](http://arxiv.org/pdf/1401.8078v1)

> An inductive probabilistic classification rule must generally obey the principles of Bayesian predictive inference, such that all observed and unobserved stochastic quantities are jointly modeled and the parameter uncertainty is fully acknowledged through the posterior predictive distribution. Several such rules have been recently considered and their asymptotic behavior has been characterized under the assumption that the observed features or variables used for building a classifier are conditionally independent given a simultaneous labeling of both the training samples and those from an unknown origin. Here we extend the theoretical results to predictive classifiers acknowledging feature dependencies either through graphical models or sparser alternatives defined as stratified graphical models. We also show through experimentation with both synthetic and real data that the predictive classifiers based on stratified graphical models have consistently best accuracy compared with the predictive classifiers based on either conditionally independent features or on ordinary graphical models.

</details>

<details>

<summary>2014-01-31 16:43:28 - A Bayesian Non-Parametric Approach to Asymmetric Dynamic Conditional Correlation Model With Application to Portfolio Selection</summary>

- *Audrone Virbickaite, M. ConcepciÃ³n AusÃ­n, Pedro Galeano*

- `1301.5129v2` - [abs](http://arxiv.org/abs/1301.5129v2) - [pdf](http://arxiv.org/pdf/1301.5129v2)

> We propose a Bayesian non-parametric approach for modeling the distribution of multiple returns. In particular, we use an asymmetric dynamic conditional correlation (ADCC) model to estimate the time-varying correlations of financial returns where the individual volatilities are driven by GJR-GARCH models. The ADCC-GJR-GARCH model takes into consideration the asymmetries in individual assets' volatilities, as well as in the correlations. The errors are modeled using a Dirichlet location-scale mixture of multivariate Gaussian distributions allowing for a great flexibility in the return distribution in terms of skewness and kurtosis. Model estimation and prediction are developed using MCMC methods based on slice sampling techniques. We carry out a simulation study to illustrate the flexibility of the proposed approach. We find that the proposed DPM model is able to adapt to several frequently used distribution models and also accurately estimates the posterior distribution of the volatilities of the returns, without assuming any underlying distribution. Finally, we present a financial application using Apple and NASDAQ Industrial index data to solve a portfolio allocation problem. We find that imposing a restrictive parametric distribution can result into underestimation of the portfolio variance, whereas DPM model is able to overcome this problem.

</details>

<details>

<summary>2014-01-31 20:01:23 - Empirical Bayes unfolding of elementary particle spectra at the Large Hadron Collider</summary>

- *Mikael Kuusela, Victor M. Panaretos*

- `1401.8274v1` - [abs](http://arxiv.org/abs/1401.8274v1) - [pdf](http://arxiv.org/pdf/1401.8274v1)

> We consider the so-called unfolding problem in experimental high energy physics, where the goal is to estimate the true spectrum of elementary particles given observations distorted by measurement error due to the limited resolution of a particle detector. This an important statistical inverse problem arising in the analysis of data at the Large Hadron Collider at CERN. Mathematically, the problem is formalized as one of estimating the intensity function of an indirectly observed Poisson point process. Particle physicists are particularly keen on unfolding methods that feature a principled way of choosing the regularization strength and allow for the quantification of the uncertainty inherent in the solution. Though there are many approaches that have been considered by experimental physicists, it can be argued that few -- if any -- of these deal with these two key issues in a satisfactory manner. In this paper, we propose to attack the unfolding problem within the framework of empirical Bayes estimation: we consider Bayes estimators of the coefficients of a basis expansion of the unknown intensity, using a regularizing prior; and employ a Monte Carlo expectation-maximization algorithm to find the marginal maximum likelihood estimate of the hyperparameter controlling the strength of the regularization. Due to the data-driven choice of the hyperparameter, credible intervals derived using the empirical Bayes posterior lose their subjective Bayesian interpretation. Since the properties and meaning of such intervals are poorly understood, we explore instead the use of bootstrap resampling for constructing purely frequentist confidence bands for the true intensity. The performance of the proposed methodology is demonstrated using both simulations and real data from the Large Hadron Collider.

</details>


## 2014-02

<details>

<summary>2014-02-01 14:35:04 - DinTucker: Scaling up Gaussian process models on multidimensional arrays with billions of elements</summary>

- *Shandian Zhe, Yuan Qi, Youngja Park, Ian Molloy, Suresh Chari*

- `1311.2663v5` - [abs](http://arxiv.org/abs/1311.2663v5) - [pdf](http://arxiv.org/pdf/1311.2663v5)

> Infinite Tucker Decomposition (InfTucker) and random function prior models, as nonparametric Bayesian models on infinite exchangeable arrays, are more powerful models than widely-used multilinear factorization methods including Tucker and PARAFAC decomposition, (partly) due to their capability of modeling nonlinear relationships between array elements. Despite their great predictive performance and sound theoretical foundations, they cannot handle massive data due to a prohibitively high training time. To overcome this limitation, we present Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensor decomposition algorithm on MAPREDUCE. While maintaining the predictive accuracy of InfTucker, it is scalable on massive data. DINTUCKER is based on a new hierarchical Bayesian model that enables local training of InfTucker on subarrays and information integration from all local training results. We use distributed stochastic gradient descent, coupled with variational inference, to train this model. We apply DINTUCKER to multidimensional arrays with billions of elements from applications in the "Read the Web" project (Carlson et al., 2010) and in information security and compare it with the state-of-the-art large-scale tensor decomposition method, GigaTensor. On both datasets, DINTUCKER achieves significantly higher prediction accuracy with less computational time.

</details>

<details>

<summary>2014-02-02 21:19:22 - Interval estimations in metrology</summary>

- *Giovanni Mana, CArlo Palmisano*

- `1402.0248v1` - [abs](http://arxiv.org/abs/1402.0248v1) - [pdf](http://arxiv.org/pdf/1402.0248v1)

> This paper investigates interval estimation for a measurand that is known to be positive. Both the Neyman and Bayesian procedures are considered and the difference between the two, not always perceived, is discussed in detail. A solution is proposed to a paradox originated by the frequentist assessment of the long-run success rate of Bayesian intervals.

</details>

<details>

<summary>2014-02-03 02:28:46 - Simultaneous prediction for independent Poisson processes with different durations</summary>

- *Fumiyasu Komaki*

- `1401.8080v2` - [abs](http://arxiv.org/abs/1401.8080v2) - [pdf](http://arxiv.org/pdf/1401.8080v2)

> Simultaneous predictive densities for independent Poisson observables are investigated. The observed data and the target variables to be predicted are independently distributed according to different Poisson distributions parametrized by the same parameter. The performance of predictive densities is evaluated by the Kullback-Leibler divergence. A class of prior distributions depending on the objective of prediction is introduced. A Bayesian predictive density based on a prior in this class dominates the Bayesian predictive density based on the Jeffreys prior.

</details>

<details>

<summary>2014-02-03 04:56:58 - Principled Graph Matching Algorithms for Integrating Multiple Data Sources</summary>

- *Duo Zhang, Benjamin I. P. Rubinstein, Jim Gemmell*

- `1402.0282v1` - [abs](http://arxiv.org/abs/1402.0282v1) - [pdf](http://arxiv.org/pdf/1402.0282v1)

> This paper explores combinatorial optimization for problems of max-weight graph matching on multi-partite graphs, which arise in integrating multiple data sources. Entity resolution-the data integration problem of performing noisy joins on structured data-typically proceeds by first hashing each record into zero or more blocks, scoring pairs of records that are co-blocked for similarity, and then matching pairs of sufficient similarity. In the most common case of matching two sources, it is often desirable for the final matching to be one-to-one (a record may be matched with at most one other); members of the database and statistical record linkage communities accomplish such matchings in the final stage by weighted bipartite graph matching on similarity scores. Such matchings are intuitively appealing: they leverage a natural global property of many real-world entity stores-that of being nearly deduped-and are known to provide significant improvements to precision and recall. Unfortunately unlike the bipartite case, exact max-weight matching on multi-partite graphs is known to be NP-hard. Our two-fold algorithmic contributions approximate multi-partite max-weight matching: our first algorithm borrows optimization techniques common to Bayesian probabilistic inference; our second is a greedy approximation algorithm. In addition to a theoretical guarantee on the latter, we present comparisons on a real-world ER problem from Bing significantly larger than typically found in the literature, publication data, and on a series of synthetic problems. Our results quantify significant improvements due to exploiting multiple sources, which are made possible by global one-to-one constraints linking otherwise independent matching sub-problems. We also discover that our algorithms are complementary: one being much more robust under noise, and the other being simple to implement and very fast to run.

</details>

<details>

<summary>2014-02-03 07:09:03 - Thompson Sampling for Contextual Bandits with Linear Payoffs</summary>

- *Shipra Agrawal, Navin Goyal*

- `1209.3352v4` - [abs](http://arxiv.org/abs/1209.3352v4) - [pdf](http://arxiv.org/pdf/1209.3352v4)

> Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of $\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of $\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound for this problem.

</details>

<details>

<summary>2014-02-03 11:32:05 - Bayesian Inference Methods for Univariate and Multivariate GARCH Models: a Survey</summary>

- *AudronÄ VirbickaitÄ, M. ConcepciÃ³n AusÃ­n, Pedro Galeano*

- `1402.0346v1` - [abs](http://arxiv.org/abs/1402.0346v1) - [pdf](http://arxiv.org/pdf/1402.0346v1)

> This survey reviews the existing literature on the most relevant Bayesian inference methods for univariate and multivariate GARCH models. The advantages and drawbacks of each procedure are outlined as well as the advantages of the Bayesian approach versus classical procedures. The paper makes emphasis on recent Bayesian non-parametric approaches for GARCH models that avoid imposing arbitrary parametric distributional assumptions. These novel approaches implicitly assume infinite mixture of Gaussian distributions on the standardized returns which have been shown to be more flexible and describe better the uncertainty about future volatilities. Finally, the survey presents an illustration using real data to show the flexibility and usefulness of the non-parametric approach.

</details>

<details>

<summary>2014-02-04 10:50:29 - On the Computation of Multivariate Scenario Sets for the Skew-t and Generalized Hyperbolic Families</summary>

- *Emanuele Giorgi, Alexander J. McNeil*

- `1402.0686v1` - [abs](http://arxiv.org/abs/1402.0686v1) - [pdf](http://arxiv.org/pdf/1402.0686v1)

> We examine the problem of computing multivariate scenarios sets for skewed distributions. Our interest is motivated by the potential use of such sets in the "stress testing" of insurance companies and banks whose solvency is dependent on changes in a set of financial "risk factors". We define multivariate scenario sets based on the notion of half-space depth (HD) and also introduce the notion of expectile depth (ED) where half-spaces are defined by expectiles rather than quantiles. We then use the HD and ED functions to define convex scenario sets that generalize the concepts of quantile and expectile to higher dimensions. In the case of elliptical distributions these sets coincide with the regions encompassed by the contours of the density function. In the context of multivariate skewed distributions, the equivalence of depth contours and density contours does not hold in general. We consider two parametric families that account for skewness and heavy tails: the generalized hyperbolic and the skew-t distributions. By making use of a canonical form representation, where skewness is completely absorbed by one component, we show that the HD contours of these distributions are "near-elliptical" and, in the case of the skew-Cauchy distribution, we prove that the HD contours are exactly elliptical. We propose a measure of multivariate skewness as a deviation from angular symmetry and show that it can explain the quality of the elliptical approximation for the HD contours.

</details>

<details>

<summary>2014-02-04 17:01:16 - Sequential Model-Based Ensemble Optimization</summary>

- *Alexandre Lacoste, Hugo Larochelle, FranÃ§ois Laviolette, Mario Marchand*

- `1402.0796v1` - [abs](http://arxiv.org/abs/1402.0796v1) - [pdf](http://arxiv.org/pdf/1402.0796v1)

> One of the most tedious tasks in the application of machine learning is model selection, i.e. hyperparameter selection. Fortunately, recent progress has been made in the automation of this process, through the use of sequential model-based optimization (SMBO) methods. This can be used to optimize a cross-validation performance of a learning algorithm over the value of its hyperparameters. However, it is well known that ensembles of learned models almost consistently outperform a single model, even if properly selected. In this paper, we thus propose an extension of SMBO methods that automatically constructs such ensembles. This method builds on a recently proposed ensemble construction paradigm known as agnostic Bayesian learning. In experiments on 22 regression and 39 classification data sets, we confirm the success of this proposed approach, which is able to outperform model selection with SMBO.

</details>

<details>

<summary>2014-02-04 19:02:27 - Null hypothesis significance tests: A mix-up of two different theories, the basis for widespread confusion and numerous misinterpretations</summary>

- *Jesper W. Schneider*

- `1402.1089v1` - [abs](http://arxiv.org/abs/1402.1089v1) - [pdf](http://arxiv.org/pdf/1402.1089v1)

> Null hypothesis statistical significance tests (NHST) are widely used in quantitative research in the empirical sciences including scientometrics. Nevertheless, since their introduction nearly a century ago significance tests have been controversial. Many researchers are not aware of the numerous criticisms raised against NHST. As practiced, NHST has been characterized as a null ritual that is overused and too often misapplied and misinterpreted. NHST is in fact a patchwork of two fundamentally different classical statistical testing models, often blended with some wishful quasi-Bayesian interpretations. This is undoubtedly a major reason why NHST is very often misunderstood. But NHST also has intrinsic logical problems and the epistemic range of the information provided by such tests is much more limited than most researchers recognize. In this article we introduce to the scientometric community the theoretical origins of NHST, which is mostly absent from standard statistical textbooks, and we discuss some of the most prevalent problems relating to the practice of NHST and trace these problems back to the mixup of the two different theoretical origins. Finally, we illustrate some of the misunderstandings with examples from the scientometric literature and bring forward some modest recommendations for a more sound practice in quantitative data analysis.

</details>

<details>

<summary>2014-02-04 23:48:23 - Discovering Latent Network Structure in Point Process Data</summary>

- *Scott W. Linderman, Ryan P. Adams*

- `1402.0914v1` - [abs](http://arxiv.org/abs/1402.0914v1) - [pdf](http://arxiv.org/pdf/1402.0914v1)

> Networks play a central role in modern data analysis, enabling us to reason about systems by studying the relationships between their parts. Most often in network analysis, the edges are given. However, in many systems it is difficult or impossible to measure the network directly. Examples of latent networks include economic interactions linking financial instruments and patterns of reciprocity in gang violence. In these cases, we are limited to noisy observations of events associated with each node. To enable analysis of these implicit networks, we develop a probabilistic model that combines mutually-exciting point processes with random graph models. We show how the Poisson superposition principle enables an elegant auxiliary variable formulation and a fully-Bayesian, parallel inference algorithm. We evaluate this new model empirically on several datasets.

</details>

<details>

<summary>2014-02-05 07:41:40 - Estimating spatial quantile regression with functional coefficients: A robust semiparametric framework</summary>

- *Zudi Lu, Qingguo Tang, Longsheng Cheng*

- `1402.0958v1` - [abs](http://arxiv.org/abs/1402.0958v1) - [pdf](http://arxiv.org/pdf/1402.0958v1)

> This paper considers an estimation of semiparametric functional (varying)-coefficient quantile regression with spatial data. A general robust framework is developed that treats quantile regression for spatial data in a natural semiparametric way. The local M-estimators of the unknown functional-coefficient functions are proposed by using local linear approximation, and their asymptotic distributions are then established under weak spatial mixing conditions allowing the data processes to be either stationary or nonstationary with spatial trends. Application to a soil data set is demonstrated with interesting findings that go beyond traditional analysis.

</details>

<details>

<summary>2014-02-05 14:27:44 - An SIR Graph Growth Model for the Epidemics of Communicable Diseases</summary>

- *Charanpal Dhanjal, StÃ©phan ClÃ©menÃ§on*

- `1312.2565v2` - [abs](http://arxiv.org/abs/1312.2565v2) - [pdf](http://arxiv.org/pdf/1312.2565v2)

> It is the main purpose of this paper to introduce a graph-valued stochastic process in order to model the spread of a communicable infectious disease. The major novelty of the SIR model we promote lies in the fact that the social network on which the epidemics is taking place is not specified in advance but evolves through time, accounting for the temporal evolution of the interactions involving infective individuals. Without assuming the existence of a fixed underlying network model, the stochastic process introduced describes, in a flexible and realistic manner, epidemic spread in non-uniformly mixing and possibly heterogeneous populations. It is shown how to fit such a (parametrised) model by means of Approximate Bayesian Computation methods based on graph-valued statistics. The concepts and statistical methods described in this paper are finally applied to a real epidemic dataset, related to the spread of HIV in Cuba in presence of a contact tracing system, which permits one to reconstruct partly the evolution of the graph of sexual partners diagnosed HIV positive between 1986 and 2006.

</details>

<details>

<summary>2014-02-09 02:10:47 - A Bayesian Nonparametric Hypothesis Testing Approach for Regression Discontinuity Designs</summary>

- *George Karabatsos, Stephen G. Walker*

- `1402.1909v1` - [abs](http://arxiv.org/abs/1402.1909v1) - [pdf](http://arxiv.org/pdf/1402.1909v1)

> The regression discontinuity (RD) design is a popular approach to causal inference in non-randomized studies. This is because it can be used to identify and estimate causal effects under mild conditions. Specifically, for each subject, the RD design assigns a treatment or non-treatment, depending on whether or not an observed value of an assignment variable exceeds a fixed and known cutoff value.   In this paper, we propose a Bayesian nonparametric regression modeling approach to RD designs, which exploits a local randomization feature. In this approach, the assignment variable is treated as a covariate, and a scalar-valued confounding variable is treated as a dependent variable (which may be a multivariate confounder score). Then, over the model's posterior distribution of locally-randomized subjects that cluster around the cutoff of the assignment variable, inference for causal effects are made within this random cluster, via two-group statistical comparisons of treatment outcomes and non-treatment outcomes.   We illustrate the Bayesian nonparametric approach through the analysis of a real educational data set, to investigate the causal link between basic skills and teaching ability.

</details>

<details>

<summary>2014-02-09 15:38:57 - Better Optimism By Bayes: Adaptive Planning with Rich Models</summary>

- *Arthur Guez, David Silver, Peter Dayan*

- `1402.1958v1` - [abs](http://arxiv.org/abs/1402.1958v1) - [pdf](http://arxiv.org/pdf/1402.1958v1)

> The computational costs of inference and planning have confined Bayesian model-based reinforcement learning to one of two dismal fates: powerful Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian non-parametric models but using simple, myopic planning strategies such as Thompson sampling. We ask whether it is feasible and truly beneficial to combine rich probabilistic models with a closer approximation to fully Bayesian planning. First, we use a collection of counterexamples to show formal problems with the over-optimism inherent in Thompson sampling. Then we leverage state-of-the-art techniques in efficient Bayes-adaptive planning and non-parametric Bayesian methods to perform qualitatively better than both existing conventional algorithms and Thompson sampling on two contextual bandit-like problems.

</details>

<details>

<summary>2014-02-10 12:47:47 - Bayesian analysis for a class of beta mixed models</summary>

- *Wagner Hugo Bonat, Paulo Justiniano Ribeiro Jr, Silvia emiko Shimakura*

- `1401.2957v2` - [abs](http://arxiv.org/abs/1401.2957v2) - [pdf](http://arxiv.org/pdf/1401.2957v2)

> Generalized linear mixed models (GLMM) encompass large class of statistical models, with a vast range of applications areas. GLMM extends the linear mixed models allowing for different types of response variable. Three most common data types are continuous, counts and binary and standard distributions for these types of response variables are Gaussian, Poisson and Binomial, respectively. Despite that flexibility, there are situations where the response variable is continuous, but bounded, such as rates, percentages, indexes and proportions. In such situations the usual GLMM's are not adequate because bounds are ignored and the beta distribution can be used. Likelihood and Bayesian inference for beta mixed models are not straightforward demanding a computational overhead. Recently, a new algorithm for Bayesian inference called INLA (Integrated Nested Laplace Approximation) was proposed.INLA allows computation of many Bayesian GLMMs in a reasonable amount time allowing extensive comparison among models. We explore Bayesian inference for beta mixed models by INLA. We discuss the choice of prior distributions, sensitivity analysis and model selection measures through a real data set. The results obtained from INLA are compared with those obtained by an MCMC algorithm and likelihood analysis. We analyze data from an study on a life quality index of industry workers collected according to a hierarchical sampling scheme. Results show that the INLA approach is suitable and faster to fit the proposed beta mixed models producing results similar to alternative algorithms and with easier handling of modeling alternatives. Sensitivity analysis, measures of goodness of fit and model choice are discussed.

</details>

<details>

<summary>2014-02-10 14:49:22 - The Spatial Sensitivity Function of a Light Sensor</summary>

- *N. K. Malakar, A. J. Mesiti, K. H. Knuth*

- `1402.2169v1` - [abs](http://arxiv.org/abs/1402.2169v1) - [pdf](http://arxiv.org/pdf/1402.2169v1)

> The Spatial Sensitivity Function (SSF) is used to quantify a detector's sensitivity to a spatially-distributed input signal. By weighting the incoming signal with the SSF and integrating, the overall scalar response of the detector can be estimated. This project focuses on estimating the SSF of a light intensity sensor consisting of a photodiode. This light sensor has been used previously in the Knuth Cyberphysics Laboratory on a robotic arm that performs its own experiments to locate a white circle in a dark field (Knuth et al., 2007). To use the light sensor to learn about its surroundings, the robot's inference software must be able to model and predict the light sensor's response to a hypothesized stimulus. Previous models of the light sensor treated it as a point sensor and ignored its spatial characteristics. Here we propose a parametric approach where the SSF is described by a mixture of Gaussians (MOG). By performing controlled calibration experiments with known stimulus inputs, we used nested sampling to estimate the SSF of the light sensor using an MOG model with the number of Gaussians ranging from one to five. By comparing the evidence computed for each MOG model, we found that one Gaussian is sufficient to describe the SSF to the accuracy we require. Future work will involve incorporating this more accurate SSF into the Bayesian machine learning software for the robotic system and studying how this detailed information about the properties of the light sensor will improve robot's ability to learn.

</details>

<details>

<summary>2014-02-11 14:13:17 - Risk Margin Quantile Function Via Parametric and Non-Parametric Bayesian Quantile Regression</summary>

- *Alice X. D. Dong, Jennifer S. K. Chan, Gareth W. Peters*

- `1402.2492v1` - [abs](http://arxiv.org/abs/1402.2492v1) - [pdf](http://arxiv.org/pdf/1402.2492v1)

> We develop quantile regression models in order to derive risk margin and to evaluate capital in non-life insurance applications. By utilizing the entire range of conditional quantile functions, especially higher quantile levels, we detail how quantile regression is capable of providing an accurate estimation of risk margin and an overview of implied capital based on the historical volatility of a general insurers loss portfolio. Two modelling frameworks are considered based around parametric and nonparametric quantile regression models which we develop specifically in this insurance setting.   In the parametric quantile regression framework, several models including the flexible generalized beta distribution family, asymmetric Laplace (AL) distribution and power Pareto distribution are considered under a Bayesian regression framework. The Bayesian posterior quantile regression models in each case are studied via Markov chain Monte Carlo (MCMC) sampling strategies.   In the nonparametric quantile regression framework, that we contrast to the parametric Bayesian models, we adopted an AL distribution as a proxy and together with the parametric AL model, we expressed the solution as a scale mixture of uniform distributions to facilitate implementation. The models are extended to adopt dynamic mean, variance and skewness and applied to analyze two real loss reserve data sets to perform inference and discuss interesting features of quantile regression for risk margin calculations.

</details>

<details>

<summary>2014-02-11 23:34:52 - Conditional inferential models: combining information for prior-free probabilistic inference</summary>

- *Ryan Martin, Chuanhai Liu*

- `1211.1530v5` - [abs](http://arxiv.org/abs/1211.1530v5) - [pdf](http://arxiv.org/pdf/1211.1530v5)

> The inferential model (IM) framework provides valid prior-free probabilistic inference by focusing on predicting unobserved auxiliary variables. But, efficient IM-based inference can be challenging when the auxiliary variable is of higher dimension than the parameter. Here we show that features of the auxiliary variable are often fully observed and, in such cases, a simultaneous dimension reduction and information aggregation can be achieved by conditioning. This proposed conditioning strategy leads to efficient IM inference, and casts new light on Fisher's notions of sufficiency, conditioning, and also Bayesian inference. A differential equation-driven selection of a conditional association is developed, and validity of the conditional IM is proved under some conditions. For problems that do not admit a valid conditional IM of the standard form, we propose a more flexible class of conditional IMs based on localization. Examples of local conditional IMs in a bivariate normal model and a normal variance components model are also given.

</details>

<details>

<summary>2014-02-12 01:52:11 - MCMC algorithms for Bayesian variable selection in the logistic regression model for large-scale genomic applications</summary>

- *Manuela Zucknick, Sylvia Richardson*

- `1402.2713v1` - [abs](http://arxiv.org/abs/1402.2713v1) - [pdf](http://arxiv.org/pdf/1402.2713v1)

> In large-scale genomic applications vast numbers of molecular features are scanned in order to find a small number of candidates which are linked to a particular disease or phenotype. This is a variable selection problem in the "large p, small n" paradigm where many more variables than samples are available. Additionally, a complex dependence structure is often observed among the markers/genes due to their joint involvement in biological processes and pathways. Bayesian variable selection methods that introduce sparseness through additional priors on the model size are well suited to the problem. However, the model space is very large and standard Markov chain Monte Carlo (MCMC) algorithms such as a Gibbs sampler sweeping over all p variables in each iteration are often computationally infeasible. We propose to employ the dependence structure in the data to decide which variables should always be updated together and which are nearly conditionally independent and hence do not need to be considered together. Here, we focus on binary classification applications. We follow the implementation of the Bayesian probit regression model by Albert and Chib (1993) and the Bayesian logistic regression model by Holmes and Held (2006) which both lead to marginal Gaussian distributions. We in- vestigate several MCMC samplers using the dependence structure in different ways. The mixing and convergence performances of the resulting Markov chains are evaluated and compared to standard samplers in two simulation studies and in an application to a real gene expression data set.

</details>

<details>

<summary>2014-02-12 06:31:12 - Bayesian Inference with Posterior Regularization and applications to Infinite Latent SVMs</summary>

- *Jun Zhu, Ning Chen, Eric P. Xing*

- `1210.1766v3` - [abs](http://arxiv.org/abs/1210.1766v3) - [pdf](http://arxiv.org/pdf/1210.1766v3)

> Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors can affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks whose Bayesian formulation results in hybrid chain graph models. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results were not available until now, and contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.

</details>

<details>

<summary>2014-02-12 22:01:18 - Prediction with Missing Data via Bayesian Additive Regression Trees</summary>

- *Adam Kapelner, Justin Bleich*

- `1306.0618v3` - [abs](http://arxiv.org/abs/1306.0618v3) - [pdf](http://arxiv.org/pdf/1306.0618v3)

> We present a method for incorporating missing data in non-parametric statistical learning without the need for imputation. We focus on a tree-based method, Bayesian Additive Regression Trees (BART), enhanced with "Missingness Incorporated in Attributes," an approach recently proposed incorporating missingness into decision trees (Twala, 2008). This procedure takes advantage of the partitioning mechanisms found in tree-based models. Simulations on generated models and real data indicate that our proposed method can forecast well on complicated missing-at-random and not-missing-at-random models as well as models where missingness itself influences the response. Our procedure has higher predictive performance and is more stable than competitors in many cases. We also illustrate BART's abilities to incorporate missingness into uncertainty intervals and to detect the influence of missingness on the model fit.

</details>

<details>

<summary>2014-02-13 10:48:46 - Gaussian Process Volatility Model</summary>

- *Yue Wu, Jose Miguel Hernandez Lobato, Zoubin Ghahramani*

- `1402.3085v1` - [abs](http://arxiv.org/abs/1402.3085v1) - [pdf](http://arxiv.org/pdf/1402.3085v1)

> The accurate prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the variances. Moreover, function parameters are usually learned using maximum likelihood, which can lead to overfitting. To address these problems we introduce a novel model for time-changing variances using Gaussian Processes. A Gaussian Process (GP) defines a distribution over functions, which allows us to capture highly flexible functional relationships for the variances. In addition, we develop an online algorithm to perform inference. The algorithm has two main advantages. First, it takes a Bayesian approach, thereby avoiding overfitting. Second, it is much quicker than current offline inference procedures. Finally, our new model was evaluated on financial data and showed significant improvement in predictive performance over current standard models.

</details>

<details>

<summary>2014-02-14 07:42:15 - Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget</summary>

- *Anoop Korattikara, Yutian Chen, Max Welling*

- `1304.5299v4` - [abs](http://arxiv.org/abs/1304.5299v4) - [pdf](http://arxiv.org/pdf/1304.5299v4)

> Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.

</details>

<details>

<summary>2014-02-14 15:15:43 - Generative Modelling for Unsupervised Score Calibration</summary>

- *Niko BrÃ¼mmer, Daniel Garcia-Romero*

- `1311.0707v3` - [abs](http://arxiv.org/abs/1311.0707v3) - [pdf](http://arxiv.org/pdf/1311.0707v3)

> Score calibration enables automatic speaker recognizers to make cost-effective accept / reject decisions. Traditional calibration requires supervised data, which is an expensive resource. We propose a 2-component GMM for unsupervised calibration and demonstrate good performance relative to a supervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstrates that the uncertainty associated with the unsupervised calibration parameter estimates is surprisingly small.

</details>

<details>

<summary>2014-02-17 05:39:50 - Bayesian Model-Averaged Benchmark Dose Analysis Via Reparameterized Quantal-Response Models</summary>

- *Qijun Fang, Walter W. Piegorsch, Susan J. Simmons, Xiaosong Li, Cuixian Chen, Yishi Wang*

- `1402.3896v1` - [abs](http://arxiv.org/abs/1402.3896v1) - [pdf](http://arxiv.org/pdf/1402.3896v1)

> An important objective in biomedical risk assessment is estimation of minimum exposure levels that induce a pre-specified adverse response in a target population. The exposure/dose points in such settings are known as Benchmark Doses (BMDs). Recently, parametric Bayesian estimation for finding BMDs has become popular. A large variety of candidate dose-response models is available for applying these methods, however, leading to questions of model adequacy and uncertainty. Here we enhance the Bayesian estimation technique for BMD analysis by applying Bayesian model averaging to produce point estimates and (lower) credible bounds. We include reparameterizations of traditional dose-response models that allow for more-focused use of elicited prior information when building the Bayesian hierarchy. Performance of the method is evaluated via a short simulation study. An example from carcinogenicity testing illustrates the calculations.

</details>

<details>

<summary>2014-02-17 07:01:12 - A Bayesian approach to estimate changes in condom use from limited HIV prevalence data</summary>

- *Joseph Dureau, Konstantinos Kalogeropoulos, Peter Vickerman, Michael Pickles, Marie-Claude Boily*

- `1211.5472v2` - [abs](http://arxiv.org/abs/1211.5472v2) - [pdf](http://arxiv.org/pdf/1211.5472v2)

> Evaluation of HIV large scale interventions programme is becoming increasingly important, but impact estimates frequently hinge on knowledge of changes in behaviour such as the frequency of condom use (CU) over time, or other self-reported behaviour changes, for which we generally have limited or potentially biased data. We employ a Bayesian inference methodology that incorporates a dynamic HIV transmission dynamics model to estimate CU time trends from HIV prevalence data. Estimation is implemented via particle Markov Chain Monte Carlo methods, applied for the first time in this context. The preliminary choice of the formulation for the time varying parameter reflecting the proportion of CU is critical in the context studied, due to the very limited amount of CU and HIV data available We consider various novel formulations to explore the trajectory of CU in time, based on diffusion-driven trajectories and smooth sigmoid curves. Extensive series of numerical simulations indicate that informative results can be obtained regarding the amplitude of the increase in CU during an intervention, with good levels of sensitivity and specificity performance in effectively detecting changes. The application of this method to a real life problem illustrates how it can help evaluate HIV intervention from few observational studies and suggests that these methods can potentially be applied in many different contexts.

</details>

<details>

<summary>2014-02-17 18:08:16 - Fast Hamiltonian Monte Carlo Using GPU Computing</summary>

- *Andrew L. Beam, Sujit K. Ghosh, Jon Doyle*

- `1402.4089v1` - [abs](http://arxiv.org/abs/1402.4089v1) - [pdf](http://arxiv.org/pdf/1402.4089v1)

> In recent years, the Hamiltonian Monte Carlo (HMC) algorithm has been found to work more efficiently compared to other popular Markov Chain Monte Carlo (MCMC) methods (such as random walk Metropolis-Hastings) in generating samples from a posterior distribution. A general framework for HMC based on the use of graphical processing units (GPUs) is shown to greatly reduce the computing time needed for Bayesian inference. The most expensive computational tasks in HMC are the evaluation of the posterior kernel and computing its gradient with respect to the parameters of interest. One of primary goals of this article to show that by expressing each of these tasks in terms of simple matrix or element-wise operations and maintaining persistent objects in GPU memory, the computational time can be drastically reduced. By using GPU objects to perform the entire HMC simulation, most of the latency penalties associated with transferring data from main to GPU memory can be avoided. Thus, the proposed computational framework is conceptually very simple, but also is general enough to be applied to most problems that use HMC sampling. For clarity of exposition, the effectiveness of the proposed approach is demonstrated in the high-dimensional setting on a standard statistical model - multinomial regression. Using GPUs, analyses of data sets that were previously intractable for fully Bayesian approaches due to the prohibitively high computational cost are now feasible.

</details>

<details>

<summary>2014-02-18 09:44:23 - Group-sparse Embeddings in Collective Matrix Factorization</summary>

- *Arto Klami, Guillaume Bouchard, Abhishek Tripathi*

- `1312.5921v2` - [abs](http://arxiv.org/abs/1312.5921v2) - [pdf](http://arxiv.org/pdf/1312.5921v2)

> CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.

</details>

<details>

<summary>2014-02-18 10:38:14 - Bayesian and Maximum Likelihood Estimation for Gaussian Processes on an Incomplete Lattice</summary>

- *Jonathan R. Stroud, Michael L. Stein, Shaun Lysen*

- `1402.4281v1` - [abs](http://arxiv.org/abs/1402.4281v1) - [pdf](http://arxiv.org/pdf/1402.4281v1)

> This paper proposes a new approach for Bayesian and maximum likelihood parameter estimation for stationary Gaussian processes observed on a large lattice with missing values. We propose an MCMC approach for Bayesian inference, and a Monte Carlo EM algorithm for maximum likelihood inference. Our approach uses data augmentation and circulant embedding of the covariance matrix, and provides exact inference for the parameters and the missing data. Using simulated data and an application to satellite sea surface temperatures in the Pacific Ocean, we show that our method provides accurate inference on lattices of sizes up to 512 x 512, and outperforms two popular methods: composite likelihood and spectral approximations.

</details>

<details>

<summary>2014-02-19 10:35:51 - Markov Switching Component ARCH Model: Stability and Forecasting</summary>

- *N. Alemohammad, S. Rezakhah, S. H. Alizadeh*

- `1303.5525v2` - [abs](http://arxiv.org/abs/1303.5525v2) - [pdf](http://arxiv.org/pdf/1303.5525v2)

> This paper introduces an extension of the Markov switching GARCH model where the volatility in each state is a convex combination of two different GARCH components with time varying weights. This model has the dynamic behavior to capture the variants of shocks. The asymptotic behavior of the second moment is investigated and an appropriate upper bound for it is evaluated. The estimation of the parameters by using the Bayesian method via Gibbs sampling algorithm is studied. Finally we illustrate the efficiency of the model by simulation and empirical analysis. We show that this model provides a much better forecast of the volatility than the Markov switching GARCH model.

</details>

<details>

<summary>2014-02-19 10:49:16 - Student-t Processes as Alternatives to Gaussian Processes</summary>

- *Amar Shah, Andrew Gordon Wilson, Zoubin Ghahramani*

- `1402.4306v2` - [abs](http://arxiv.org/abs/1402.4306v2) - [pdf](http://arxiv.org/pdf/1402.4306v2)

> We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.

</details>

<details>

<summary>2014-02-19 11:18:32 - Sparse Quantile Huber Regression for Efficient and Robust Estimation</summary>

- *Aleksandr Y. Aravkin, Anju Kambadur, Aurelie C. Lozano, Ronny Luss*

- `1402.4624v1` - [abs](http://arxiv.org/abs/1402.4624v1) - [pdf](http://arxiv.org/pdf/1402.4624v1)

> We consider new formulations and methods for sparse quantile regression in the high-dimensional setting. Quantile regression plays an important role in many applications, including outlier-robust exploratory analysis in gene selection. In addition, the sparsity consideration in quantile regression enables the exploration of the entire conditional distribution of the response variable given the predictors and therefore yields a more comprehensive view of the important predictors. We propose a generalized OMP algorithm for variable selection, taking the misfit loss to be either the traditional quantile loss or a smooth version we call quantile Huber, and compare the resulting greedy approaches with convex sparsity-regularized formulations. We apply a recently proposed interior point methodology to efficiently solve all convex formulations as well as convex subproblems in the generalized OMP setting, pro- vide theoretical guarantees of consistent estimation, and demonstrate the performance of our approach using empirical studies of simulated and genomic datasets.

</details>

<details>

<summary>2014-02-20 01:54:37 - Learning the Parameters of Determinantal Point Process Kernels</summary>

- *Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, Ben Taskar*

- `1402.4862v1` - [abs](http://arxiv.org/abs/1402.4862v1) - [pdf](http://arxiv.org/pdf/1402.4862v1)

> Determinantal point processes (DPPs) are well-suited for modeling repulsion and have proven useful in many applications where diversity is desired. While DPPs have many appealing properties, such as efficient sampling, learning the parameters of a DPP is still considered a difficult problem due to the non-convex nature of the likelihood function. In this paper, we propose using Bayesian methods to learn the DPP kernel parameters. These methods are applicable in large-scale and continuous DPP settings even when the exact form of the eigendecomposition is unknown. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on spatial distribution of nerve fibers, and in studying human perception of diversity in images.

</details>

<details>

<summary>2014-02-20 07:17:03 - Building fast Bayesian computing machines out of intentionally stochastic, digital parts</summary>

- *Vikash Mansinghka, Eric Jonas*

- `1402.4914v1` - [abs](http://arxiv.org/abs/1402.4914v1) - [pdf](http://arxiv.org/pdf/1402.4914v1)

> The brain interprets ambiguous sensory information faster and more reliably than modern computers, using neurons that are slower and less reliable than logic gates. But Bayesian inference, which underpins many computational models of perception and cognition, appears computationally challenging even given modern transistor speeds and energy budgets. The computational principles and structures needed to narrow this gap are unknown. Here we show how to build fast Bayesian computing machines using intentionally stochastic, digital parts, narrowing this efficiency gap by multiple orders of magnitude. We find that by connecting stochastic digital components according to simple mathematical rules, one can build massively parallel, low precision circuits that solve Bayesian inference problems and are compatible with the Poisson firing statistics of cortical neurons. We evaluate circuits for depth and motion perception, perceptual learning and causal reasoning, each performing inference over 10,000+ latent variables in real time - a 1,000x speed advantage over commodity microprocessors. These results suggest a new role for randomness in the engineering and reverse-engineering of intelligent computation.

</details>

<details>

<summary>2014-02-20 10:19:16 - Imprecise Dirichlet Process with application to the hypothesis test on the probability that X< Y</summary>

- *Alessio Benavoli, Francesca Mangili, Fabrizio Ruggeri, Marco Zaffalon*

- `1402.2755v2` - [abs](http://arxiv.org/abs/1402.2755v2) - [pdf](http://arxiv.org/pdf/1402.2755v2)

> The Dirichlet process (DP) is one of the most popular Bayesian nonparametric models. An open problem with the DP is how to choose its infinite dimensional parameter (base measure) in case of lack of prior information. In this work we present the Imprecise DP (IDP) -- a prior near-ignorance DP-based model that does not require any choice of this probability measure. It consists of a class of DPs obtained by letting the normalized base measure of the DP vary in the set of all probability measures. We discuss the tight connections of this approach with Bayesian robustness and in particular prior near-ignorance modeling via sets of probabilities. We use this model to perform a Bayesian hypothesis test on the probability P(X<Y). We study the theoretical properties of the IDP test (e.g., asymptotic consistency), and compare it with the frequentist Mann-Whitney-Wilcoxon rank test that is commonly employed as a test on P(X< Y). In particular we will show that our method is more robust, in the sense that it is able to isolate instances in which the aforementioned test is virtually guessing at random.

</details>

<details>

<summary>2014-02-21 12:52:19 - Bayesian Inference in Nonparametric Dynamic State-Space Models</summary>

- *Anurag Ghosh, Soumalya Mukhopadhyay, Sandipan Roy, Sourabh Bhattacharya*

- `1108.3262v5` - [abs](http://arxiv.org/abs/1108.3262v5) - [pdf](http://arxiv.org/pdf/1108.3262v5)

> We introduce state-space models where the functionals of the observational and the evolutionary equations are unknown, and treated as random functions evolving with time. Thus, our model is nonparametric and generalizes the traditional parametric state-space models. This random function approach also frees us from the restrictive assumption that the functional forms, although time-dependent, are of fixed forms. The traditional approach of assuming known, parametric functional forms is questionable, particularly in state-space models, since the validation of the assumptions require data on both the observed time series and the latent states; however, data on the latter are not available in state-space models.   We specify Gaussian processes as priors of the random functions and exploit the "look-up table approach" of \ctn{Bhattacharya07} to efficiently handle the dynamic structure of the model. We consider both univariate and multivariate situations, using the Markov chain Monte Carlo (MCMC) approach for studying the posterior distributions of interest. In the case of challenging multivariate situations we demonstrate that the newly developed Transformation-based MCMC (TMCMC) of \ctn{Dutta11} provides interesting and efficient alternatives to the usual proposal distributions. We illustrate our methods with a challenging multivariate simulated data set, where the true observational and the evolutionary equations are highly non-linear, and treated as unknown. The results we obtain are quite encouraging. Moreover, using our Gaussian process approach we analysed a real data set, which has also been analysed by \ctn{Shumway82} and \ctn{Carlin92} using the linearity assumption. Our analyses show that towards the end of the time series, the linearity assumption of the previous authors breaks down.

</details>

<details>

<summary>2014-02-21 19:58:59 - Bayesian Additive Regression Trees With Parametric Models of Heteroskedasticity</summary>

- *Justin Bleich, Adam Kapelner*

- `1402.5397v1` - [abs](http://arxiv.org/abs/1402.5397v1) - [pdf](http://arxiv.org/pdf/1402.5397v1)

> We incorporate heteroskedasticity into Bayesian Additive Regression Trees (BART) by modeling the log of the error variance parameter as a linear function of prespecified covariates. Under this scheme, the Gibbs sampling procedure for the original sum-of- trees model is easily modified, and the parameters for the variance model are updated via a Metropolis-Hastings step. We demonstrate the promise of our approach by providing more appropriate posterior predictive intervals than homoskedastic BART in heteroskedastic settings and demonstrating the model's resistance to overfitting. Our implementation will be offered in an upcoming release of the R package bartMachine.

</details>

<details>

<summary>2014-02-22 03:44:04 - Scaling Nonparametric Bayesian Inference via Subsample-Annealing</summary>

- *Fritz Obermeyer, Jonathan Glidden, Eric Jonas*

- `1402.5473v1` - [abs](http://arxiv.org/abs/1402.5473v1) - [pdf](http://arxiv.org/pdf/1402.5473v1)

> We describe an adaptation of the simulated annealing algorithm to nonparametric clustering and related probabilistic models. This new algorithm learns nonparametric latent structure over a growing and constantly churning subsample of training data, where the portion of data subsampled can be interpreted as the inverse temperature beta(t) in an annealing schedule. Gibbs sampling at high temperature (i.e., with a very small subsample) can more quickly explore sketches of the final latent state by (a) making longer jumps around latent space (as in block Gibbs) and (b) lowering energy barriers (as in simulated annealing). We prove subsample annealing speeds up mixing time N^2 -> N in a simple clustering model and exp(N) -> N in another class of models, where N is data size. Empirically subsample-annealing outperforms naive Gibbs sampling in accuracy-per-wallclock time, and can scale to larger datasets and deeper hierarchical models. We demonstrate improved inference on million-row subsamples of US Census data and network log data and a 307-row hospital rating dataset, using a Pitman-Yor generalization of the Cross Categorization model.

</details>

<details>

<summary>2014-02-23 17:48:16 - Joint Structure Learning of Multiple Non-Exchangeable Networks</summary>

- *Chris J. Oates, Sach Mukherjee*

- `1402.5640v1` - [abs](http://arxiv.org/abs/1402.5640v1) - [pdf](http://arxiv.org/pdf/1402.5640v1)

> Several methods have recently been developed for joint structure learning of multiple (related) graphical models or networks. These methods treat individual networks as exchangeable, such that each pair of networks are equally encouraged to have similar structures. However, in many practical applications, exchangeability in this sense may not hold, as some pairs of networks may be more closely related than others, for example due to group and sub-group structure in the data. Here we present a novel Bayesian formulation that generalises joint structure learning beyond the exchangeable case. In addition to a general framework for joint learning, we (i) provide a novel default prior over the joint structure space that requires no user input; (ii) allow for latent networks; (iii) give an efficient, exact algorithm for the case of time series data and dynamic Bayesian networks. We present empirical results on non-exchangeable populations, including a real data example from biology, where cell-line-specific networks are related according to genomic features.

</details>

<details>

<summary>2014-02-23 20:29:36 - Accelerating ABC methods using Gaussian processes</summary>

- *Richard D Wilkinson*

- `1401.1436v2` - [abs](http://arxiv.org/abs/1401.1436v2) - [pdf](http://arxiv.org/pdf/1401.1436v2)

> Approximate Bayesian computation (ABC) methods are used to approximate posterior distributions using simulation rather than likelihood calculations. We introduce Gaussian process (GP) accelerated ABC, which we show can significantly reduce the number of simulations required. As computational resource is usually the main determinant of accuracy in ABC, GP-accelerated methods can thus enable more accurate inference in some models. GP models of the unknown log-likelihood function are used to exploit continuity and smoothness, reducing the required computation. We use a sequence of models that increase in accuracy, using intermediate models to rule out regions of the parameter space as implausible. The methods will not be suitable for all problems, but when they can be used, can result in significant computational savings. For the Ricker model, we are able to achieve accurate approximations to the posterior distribution using a factor of 100 fewer simulator evaluations than comparable Monte Carlo approaches, and for a population genetics model we are able to approximate the exact posterior for the first time.

</details>

<details>

<summary>2014-02-24 02:24:00 - Bayesian Inference for NMR Spectroscopy with Applications to Chemical Quantification</summary>

- *Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin, Mick D. Mantle, Lynn F. Gladden, Andrew Blake*

- `1402.3580v2` - [abs](http://arxiv.org/abs/1402.3580v2) - [pdf](http://arxiv.org/pdf/1402.3580v2)

> Nuclear magnetic resonance (NMR) spectroscopy exploits the magnetic properties of atomic nuclei to discover the structure, reaction state and chemical environment of molecules. We propose a probabilistic generative model and inference procedures for NMR spectroscopy. Specifically, we use a weighted sum of trigonometric functions undergoing exponential decay to model free induction decay (FID) signals. We discuss the challenges in estimating the components of this general model -- amplitudes, phase shifts, frequencies, decay rates, and noise variances -- and offer practical solutions. We compare with conventional Fourier transform spectroscopy for estimating the relative concentrations of chemicals in a mixture, using synthetic and experimentally acquired FID signals. We find the proposed model is particularly robust to low signal to noise ratios (SNR), and overlapping peaks in the Fourier transform of the FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) which are not possible with conventional spectroscopy (5% sensitivity).

</details>

<details>

<summary>2014-02-24 05:12:20 - Model selection criteria for nonlinear mixed effects modeling</summary>

- *Hidetoshi Matsui*

- `1402.5724v1` - [abs](http://arxiv.org/abs/1402.5724v1) - [pdf](http://arxiv.org/pdf/1402.5724v1)

> We consider constructing model selection criteria for evaluating nonlinear mixed effects models via basis expansions. Mean functions and random functions in the mixed effects model are expressed by basis expansions, then they are estimated by the maximum likelihood method. In order to select numbers of basis we derive a Bayesian model selection criterion for evaluating nonlinear mixed effects models estimated by the maximum likelihood method. Simulation results shows the effectiveness of the mixed effects modeling.

</details>

<details>

<summary>2014-02-25 02:49:13 - Annealed Important Sampling for Models with Latent Variables</summary>

- *M. -N. Tran, C. Strickland, M. K. Pitt, R. Kohn*

- `1402.6035v1` - [abs](http://arxiv.org/abs/1402.6035v1) - [pdf](http://arxiv.org/pdf/1402.6035v1)

> This paper is concerned with Bayesian inference when the likelihood is analytically intractable but can be unbiasedly estimated. We propose an annealed importance sampling procedure for estimating expectations with respect to the posterior. The proposed algorithm is useful in cases where finding a good proposal density is challenging, and when estimates of the marginal likelihood are required. The effect of likelihood estimation is investigated, and the results provide guidelines on how to set up the precision of the likelihood estimation in order to optimally implement the procedure. The methodological results are empirically demonstrated in several simulated and real data examples.

</details>

<details>

<summary>2014-02-25 11:11:28 - Bayesian Sample Size Determination of Vibration Signals in Machine Learning Approach to Fault Diagnosis of Roller Bearings</summary>

- *Siddhant Sahu, V. Sugumaran*

- `1402.6133v1` - [abs](http://arxiv.org/abs/1402.6133v1) - [pdf](http://arxiv.org/pdf/1402.6133v1)

> Sample size determination for a data set is an important statistical process for analyzing the data to an optimum level of accuracy and using minimum computational work. The applications of this process are credible in every domain which deals with large data sets and high computational work. This study uses Bayesian analysis for determination of minimum sample size of vibration signals to be considered for fault diagnosis of a bearing using pre-defined parameters such as the inverse standard probability and the acceptable margin of error. Thus an analytical formula for sample size determination is introduced. The fault diagnosis of the bearing is done using a machine learning approach using an entropy-based J48 algorithm. The following method will help researchers involved in fault diagnosis to determine minimum sample size of data for analysis for a good statistical stability and precision.

</details>

<details>

<summary>2014-02-26 16:38:15 - Bayesian Inference for Hybrid Discrete-Continuous Stochastic Kinetic Models</summary>

- *Chris Sherlock, Andrew Golightly, Colin Gillespie*

- `1402.6602v1` - [abs](http://arxiv.org/abs/1402.6602v1) - [pdf](http://arxiv.org/pdf/1402.6602v1)

> We consider the problem of efficiently performing simulation and inference for stochastic kinetic models. Whilst it is possible to work directly with the resulting Markov jump process, computational cost can be prohibitive for networks of realistic size and complexity. In this paper, we consider an inference scheme based on a novel hybrid simulator that classifies reactions as either "fast" or "slow" with fast reactions evolving as a continuous Markov process whilst the remaining slow reaction occurrences are modelled through a Markov jump process with time dependent hazards. A linear noise approximation (LNA) of fast reaction dynamics is employed and slow reaction events are captured by exploiting the ability to solve the stochastic differential equation driving the LNA. This simulation procedure is used as a proposal mechanism inside a particle MCMC scheme, thus allowing Bayesian inference for the model parameters. We apply the scheme to a simple application and compare the output with an existing hybrid approach and also a scheme for performing inference for the underlying discrete stochastic model.

</details>

<details>

<summary>2014-02-26 20:14:20 - An analytics approach to designing patient centered medical homes</summary>

- *Saeede Ajorlou, Issac Shams, Kai Yang*

- `1402.6666v1` - [abs](http://arxiv.org/abs/1402.6666v1) - [pdf](http://arxiv.org/pdf/1402.6666v1)

> Recently the patient-centered medical home (PCMH) model has become a popular team-based approach focused on delivering more streamlined care to patients. In current practices of medical homes, a clinical-based prediction frame is recommended because it can help match the portfolio capacity of PCMH teams with the actual load generated by a set of patients. Without such balances in clinical supply and demand, issues such as excessive under and over utilization of physicians, long waiting time for receiving the appropriate treatment, and non-continuity of care will eliminate many advantages of the medical home strategy. In this paper, by extending the hierarchical generalized linear model to include multivariate responses, we develop a clinical workload prediction model for care portfolio demands in a Bayesian framework. The model allows for heterogeneous variances and unstructured covariance matrices for nested random effects that arise through complex hierarchical care systems. We show that using a multivariate approach substantially enhances the precision of workload predictions at both primary and non-primary care levels. We also demonstrate that care demands depend not only on patient demographics but also on other utilization factors, such as length of stay. Our analyses of a recent data from Veteran Health Administration further indicate that risk adjustment for patient health conditions can considerably improve the prediction power of the model.

</details>

<details>

<summary>2014-02-27 18:38:02 - Bayesian Multi-Scale Optimistic Optimization</summary>

- *Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas*

- `1402.7005v1` - [abs](http://arxiv.org/abs/1402.7005v1) - [pdf](http://arxiv.org/pdf/1402.7005v1)

> Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.

</details>


## 2014-03

<details>

<summary>2014-03-02 14:21:20 - SMERED: A Bayesian Approach to Graphical Record Linkage and De-duplication</summary>

- *Rebecca C. Steorts, Rob Hall, Stephen E. Fienberg*

- `1403.0211v1` - [abs](http://arxiv.org/abs/1403.0211v1) - [pdf](http://arxiv.org/pdf/1403.0211v1)

> We propose a novel unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation is to represent the pattern of links between records as a {\em bipartite} graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible new representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate $k$-way posterior probabilities of matches across records, and propagate the uncertainty of record linkage into later analyses. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously proposed methods of record linkage, despite the high dimensional parameter space. We assess our results on real and simulated data.

</details>

<details>

<summary>2014-03-03 11:04:20 - Approximate Integrated Likelihood via ABC methods</summary>

- *Clara Grazian, Brunero Liseo*

- `1403.0387v1` - [abs](http://arxiv.org/abs/1403.0387v1) - [pdf](http://arxiv.org/pdf/1403.0387v1)

> We propose a novel use of a recent new computational tool for Bayesian inference, namely the Approximate Bayesian Computation (ABC) methodology. ABC is a way to handle models for which the likelihood function may be intractable or even unavailable and/or too costly to evaluate; in particular, we consider the problem of eliminating the nuisance parameters from a complex statistical model in order to produce a likelihood function depending on the quantity of interest only. Given a proper prior for the entire vector parameter, we propose to approximate the integrated likelihood by the ratio of kernel estimators of the marginal posterior and prior for the quantity of interest. We present several examples.

</details>

<details>

<summary>2014-03-03 18:22:01 - Bayesian Density Estimation via Multiple Sequential Inversions of 2-D Images with Application in Electron Microscopy</summary>

- *Dalia Chakrabarty, Fabio Rigat, Nare Gabrielyan, Richard Beanland, Shashi Paul*

- `1403.0510v1` - [abs](http://arxiv.org/abs/1403.0510v1) - [pdf](http://arxiv.org/pdf/1403.0510v1)

> We present a new Bayesian methodology to learn the unknown material density of a given sample by inverting its two-dimensional images that are taken with a Scanning Electron Microscope. An image results from a sequence of projections of the convolution of the density function with the unknown microscopy correction function that we also learn from the data. We invoke a novel design of experiment, involving imaging at multiple values of the parameter that controls the sub-surface depth from which information about the density structure is carried, to result in the image. Real-life material density functions are characterised by high density contrasts and typically are highly discontinuous, implying that they exhibit correlation structures that do not vary smoothly. In the absence of training data, modelling such correlation structures of real material density functions is not possible. So we discretise the material sample and treat values of the density function at chosen locations inside it as independent and distribution-free parameters. Resolution of the available image dictates the discretisation length of the model; three models pertaining to distinct resolution classes are developed. We develop priors on the material density, such that these priors adapt to the sparsity inherent in the density function. The likelihood is defined in terms of the distance between the convolution of the unknown functions and the image data. The posterior probability density of the unknowns given the data is expressed using the developed priors on the density and priors on the microscopy correction function as elicitated from the Microscopy literature. We achieve posterior samples using an adaptive Metropolis-within-Gibbs inference scheme. The method is applied to learn the material density of a 3-D sample of a real nano-structure and of simulated alloy samples.

</details>

<details>

<summary>2014-03-03 21:45:23 - Bayesian estimation in differential equation models</summary>

- *Prithwish Bhaumik, Subhashis Ghosal*

- `1403.0609v1` - [abs](http://arxiv.org/abs/1403.0609v1) - [pdf](http://arxiv.org/pdf/1403.0609v1)

> Ordinary differential equations (ODEs) are used to model dynamic systems appearing in engineering, physics, biomedical sciences and many other fields. These equations contain unknown parameters, say $\theta$ of physical significance which have to be estimated from the noisy data. Often there is no closed form analytic solution of the equations and hence we cannot use the usual non-linear least squares technique to estimate the unknown parameters. There is a two step approach to solve this problem, where the first step involves fitting the data nonparametrically. In the second step the parameter is estimated by minimizing the distance between the nonparametrically estimated derivative and the derivative suggested by the system of ODEs. The statistical aspects of this approach have been studied under the frequentist framework. We consider this two step estimation under the Bayesian framework. The response variable is allowed to be multidimensional and the true mean function of it is not assumed to be in the model. We induce a prior on the regression function using a random series based on the B-spline basis functions. We establish the Bernstein-von Mises theorem for the posterior distribution of the parameter of interest. Interestingly, even though the posterior distribution of the regression function based on splines converges at a rate slower than $n^{-1/2}$, the parameter vector $\theta$ is nevertheless estimated at $n^{-1/2}$ rate.

</details>

<details>

<summary>2014-03-03 22:01:43 - A Semiparametric Bayesian Model for Detecting Synchrony Among Multiple Neurons</summary>

- *Babak Shahbaba, Bo Zhou, Shiwei Lan, Hernando Ombao, David Moorman, Sam Behseta*

- `1306.6103v2` - [abs](http://arxiv.org/abs/1306.6103v2) - [pdf](http://arxiv.org/pdf/1306.6103v2)

> We propose a scalable semiparametric Bayesian model to capture dependencies among multiple neurons by detecting their co-firing (possibly with some lag time) patterns over time. After discretizing time so there is at most one spike at each interval, the resulting sequence of 1's (spike) and 0's (silence) for each neuron is modeled using the logistic function of a continuous latent variable with a Gaussian process prior. For multiple neurons, the corresponding marginal distributions are coupled to their joint probability distribution using a parametric copula model. The advantages of our approach are as follows: the nonparametric component (i.e., the Gaussian process model) provides a flexible framework for modeling the underlying firing rates; the parametric component (i.e., the copula model) allows us to make inference regarding both contemporaneous and lagged relationships among neurons; using the copula model, we construct multivariate probabilistic models by separating the modeling of univariate marginal distributions from the modeling of dependence structure among variables; our method is easy to implement using a computationally efficient sampling algorithm that can be easily extended to high dimensional problems. Using simulated data, we show that our approach could correctly capture temporal dependencies in firing rates and identify synchronous neurons. We also apply our model to spike train data obtained from prefrontal cortical areas in rat's brain.

</details>

<details>

<summary>2014-03-04 03:09:37 - Wormhole Hamiltonian Monte Carlo</summary>

- *Shiwei Lan, Jeffrey Streets, Babak Shahbaba*

- `1306.0063v2` - [abs](http://arxiv.org/abs/1306.0063v2) - [pdf](http://arxiv.org/pdf/1306.0063v2)

> In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create \emph{wormholes} connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a \emph{residual energy} function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.

</details>

<details>

<summary>2014-03-04 03:42:34 - A multivariate hierarchical Bayesian framework for healthcare predictions with application to medical home study in the Department of Veteran Affairs</summary>

- *Issac Shams, Saeede Ajorlou, Kai Yang*

- `1403.0674v1` - [abs](http://arxiv.org/abs/1403.0674v1) - [pdf](http://arxiv.org/pdf/1403.0674v1)

> Recently the patient centered medical home (PCMH) model has become a popular approach to deliver better care to patients. Current research shows that the most important key for succession of this method is to make balance between healthcare supply and demand. Without such balance in clinical supply and demand, issues such as excessive under and over utilization of physicians, long waiting time for receiving the appropriate treatment, and non continuity of care will eliminate many advantages of the medical home strategy. To reach this end we need to have information about both supply and demand in healthcare system. Healthcare supply can be calculated easily based on head counts and available hours which is offered by professionals for a specific time period while healthcare demand is not easy to calculate, and it is affected by some healthcare, diagnostic and demographic attributes. In this paper, by extending the hierarchical generalized linear model to include multivariate responses, we develop a clinical workload prediction model for care portfolio demands in a Bayesian framework. Our analyses of a recent data from Veteran Health Administration indicate that our prediction model works for clinical data with high performance.

</details>

<details>

<summary>2014-03-05 20:53:47 - Generalized information criterion for model selection in penalized graphical models</summary>

- *Antonino Abbruzzo, Ivan VujaÄiÄ, Ernst Wit, Angelo M. Mineo*

- `1403.1249v1` - [abs](http://arxiv.org/abs/1403.1249v1) - [pdf](http://arxiv.org/pdf/1403.1249v1)

> This paper introduces an estimator of the relative directed distance between an estimated model and the true model, based on the Kulback-Leibler divergence and is motivated by the generalized information criterion proposed by Konishi and Kitagawa. This estimator can be used to select model in penalized Gaussian copula graphical models. The use of this estimator is not feasible for high-dimensional cases. However, we derive an efficient way to compute this estimator which is feasible for the latter class of problems. Moreover, this estimator is, generally, appropriate for several penalties such as lasso, adaptive lasso and smoothly clipped absolute deviation penalty. Simulations show that the method performs similarly to KL oracle estimator and it also improves BIC performance in terms of support recovery of the graph. Specifically, we compare our method with Akaike information criterion, Bayesian information criterion and cross validation for band, sparse and dense network structures.

</details>

<details>

<summary>2014-03-06 04:57:38 - Minimax Optimal Bayesian Aggregation</summary>

- *Yun Yang, David B. Dunson*

- `1403.1345v1` - [abs](http://arxiv.org/abs/1403.1345v1) - [pdf](http://arxiv.org/pdf/1403.1345v1)

> It is generally believed that ensemble approaches, which combine multiple algorithms or models, can outperform any single algorithm at machine learning tasks, such as prediction. In this paper, we propose Bayesian convex and linear aggregation approaches motivated by regression applications. We show that the proposed approach is minimax optimal when the true data-generating model is a convex or linear combination of models in the list. Moreover, the method can adapt to sparsity structure in which certain models should receive zero weights, and the method is tuning parameter free unlike competitors. More generally, under an M-open view when the truth falls outside the space of all convex/linear combinations, our theory suggests that the posterior measure tends to concentrate on the best approximation of the truth at the minimax rate. We illustrate the method through simulation studies and several applications.

</details>

<details>

<summary>2014-03-07 14:27:20 - Simple simulation of diffusion bridges with application to likelihood inference for diffusions</summary>

- *Mogens Bladt, Michael SÃ¸rensen*

- `1403.1762v1` - [abs](http://arxiv.org/abs/1403.1762v1) - [pdf](http://arxiv.org/pdf/1403.1762v1)

> With a view to statistical inference for discretely observed diffusion models, we propose simple methods of simulating diffusion bridges, approximately and exactly. Diffusion bridge simulation plays a fundamental role in likelihood and Bayesian inference for diffusion processes. First a simple method of simulating approximate diffusion bridges is proposed and studied. Then these approximate bridges are used as proposal for an easily implemented Metropolis-Hastings algorithm that produces exact diffusion bridges. The new method utilizes time-reversibility properties of one-dimensional diffusions and is applicable to all one-dimensional diffusion processes with finite speed-measure. One advantage of the new approach is that simple simulation methods like the Milstein scheme can be applied to bridge simulation. Another advantage over previous bridge simulation methods is that the proposed method works well for diffusion bridges in long intervals because the computational complexity of the method is linear in the length of the interval. For $\rho$-mixing diffusions the approximate method is shown to be particularly accurate for long time intervals. In a simulation study, we investigate the accuracy and efficiency of the approximate method and compare it to exact simulation methods. In the study, our method provides a very good approximation to the distribution of a diffusion bridge for bridges that are likely to occur in applications to statistical inference. To illustrate the usefulness of the new method, we present an EM-algorithm for a discretely observed diffusion process.

</details>

<details>

<summary>2014-03-07 15:37:59 - Bayesian spatio-temporal epidemic models with applications to sheep pox</summary>

- *C. Malesios, N. Demiris, K. Kalogeropoulos, I. Ntzoufras*

- `1403.1783v1` - [abs](http://arxiv.org/abs/1403.1783v1) - [pdf](http://arxiv.org/pdf/1403.1783v1)

> Epidemic data often possess certain characteristics, such as the presence of many zeros, the spatial nature of the disease spread mechanism or environmental noise. This paper addresses these issues via suitable Bayesian modelling. In doing so we utilise stochastic regression models appropriate for spatio-temporal count data with an excess number of zeros. The developed regression framework can incorporate serial correlation and time varying covariates through an Ornstein Uhlenbeck process formulation. In addition, we explore the effect of different priors, including default options and techniques based upon variations of mixtures of $g$-priors. The effect of different distance kernels for the epidemic model component is investigated. We proceed by developing branching process-based methods for testing scenarios for disease control, thus linking traditional spatio-temporal models with epidemic processes, useful in policy-focused decision making. The approach is illustrated with an application to a sheep pox dataset from the Evros region, Greece.

</details>

<details>

<summary>2014-03-07 16:56:35 - Bayesian regression discontinuity designs: Incorporating clinical knowledge in the causal analysis of primary care data</summary>

- *Sara Geneletti, Aidan G. O'Keeffe, Linda D. Sharples, Sylvia Richardson, Gianluca Baio*

- `1403.1806v1` - [abs](http://arxiv.org/abs/1403.1806v1) - [pdf](http://arxiv.org/pdf/1403.1806v1)

> The regression discontinuity (RD) design is a quasi-experimental design that estimates the causal effects of a treatment by exploiting naturally occurring treatment rules. It can be applied in any context where a particular treatment or intervention is administered according to a pre-specified rule linked to a continuous variable. Such thresholds are common in primary care drug prescription where the RD design can be used to estimate the causal effect of medication in the general population. Such results can then be contrasted to those obtained from randomised controlled trials (RCTs) and inform prescription policy and guidelines based on a more realistic and less expensive context. In this paper we focus on statins, a class of cholesterol-lowering drugs, however, the methodology can be applied to many other drugs provided these are prescribed in accordance to pre-determined guidelines. NHS guidelines state that statins should be prescribed to patients with 10 year cardiovascular disease risk scores in excess of 20%. If we consider patients whose scores are close to this threshold we find that there is an element of random variation in both the risk score itself and its measurement. We can thus consider the threshold a randomising device assigning the prescription to units just above the threshold and withholds it from those just below. Thus we are effectively replicating the conditions of an RCT in the area around the threshold, removing or at least mitigating confounding. We frame the RD design in the language of conditional independence which clarifies the assumptions necessary to apply it to data, and which makes the links with instrumental variables clear. We also have context specific knowledge about the expected sizes of the effects of statin prescription and are thus able to incorporate this into Bayesian models by formulating informative priors on our causal parameters.

</details>

<details>

<summary>2014-03-08 01:30:41 - Bayesian bandwidth estimation for a nonparametric functional regression model with mixed types of regressors and unknown error density</summary>

- *Han Lin Shang*

- `1403.1913v1` - [abs](http://arxiv.org/abs/1403.1913v1) - [pdf](http://arxiv.org/pdf/1403.1913v1)

> We investigate the issue of bandwidth estimation in a nonparametric functional regression model with function-valued, continuous real-valued and discrete-valued regressors under the framework of unknown error density. Extending from the recent work of Shang (2013, Computational Statistics & Data Analysis), we approximate the unknown error density by a kernel density estimator of residuals, where the regression function is estimated by the functional Nadaraya-Watson estimator that admits mixed types of regressors. We derive a kernel likelihood and posterior density for the bandwidth parameters under the kernel-form error density, and put forward a Bayesian bandwidth estimation approach that can simultaneously estimate the bandwidths. Simulation studies demonstrated the estimation accuracy of the regression function and error density for the proposed Bayesian approach. Illustrated by a spectroscopy data set in the food quality control, we applied the proposed Bayesian approach to select the optimal bandwidths in a nonparametric functional regression model with mixed types of regressors.

</details>

<details>

<summary>2014-03-10 05:09:31 - Relabelling Algorithms for Large Dataset Mixture Models</summary>

- *Wanchuang Zhu, Yanan Fan*

- `1403.2137v1` - [abs](http://arxiv.org/abs/1403.2137v1) - [pdf](http://arxiv.org/pdf/1403.2137v1)

> Mixture models are flexible tools in density estimation and classification problems. Bayesian estimation of such models typically relies on sampling from the posterior distribution using Markov chain Monte Carlo. Label switching arises because the posterior is invariant to permutations of the component parameters. Methods for dealing with label switching have been studied fairly extensively in the literature, with the most popular approaches being those based on loss functions. However, many of these algorithms turn out to be too slow in practice, and can be infeasible as the size and dimension of the data grow. In this article, we review earlier solutions which can scale up well for large data sets, and compare their performances on simulated and real datasets. In addition, we propose a new, and computationally efficient algorithm based on a loss function interpretation, and show that it can scale up well in larger problems. We conclude with some discussions and recommendations of all the methods studied.

</details>

<details>

<summary>2014-03-10 16:17:24 - Bayesian dynamic financial networks with time-varying predictors</summary>

- *Daniele Durante, David B. Dunson*

- `1403.2272v1` - [abs](http://arxiv.org/abs/1403.2272v1) - [pdf](http://arxiv.org/pdf/1403.2272v1)

> We propose a Bayesian nonparametric model including time-varying predictors in dynamic network inference. The model is applied to infer the dependence structure among financial markets during the global financial crisis, estimating effects of verbal and material cooperation efforts. We interestingly learn contagion effects, with increasing influence of verbal relations during the financial crisis and opposite results during the United States housing bubble.

</details>

<details>

<summary>2014-03-10 22:55:11 - Generalised Mixability, Constant Regret, and Bayesian Updating</summary>

- *Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson*

- `1403.2433v1` - [abs](http://arxiv.org/abs/1403.2433v1) - [pdf](http://arxiv.org/pdf/1403.2433v1)

> Mixability of a loss is known to characterise when constant regret bounds are achievable in games of prediction with expert advice through the use of Vovk's aggregating algorithm. We provide a new interpretation of mixability via convex analysis that highlights the role of the Kullback-Leibler divergence in its definition. This naturally generalises to what we call $\Phi$-mixability where the Bregman divergence $D_\Phi$ replaces the KL divergence. We prove that losses that are $\Phi$-mixable also enjoy constant regret bounds via a generalised aggregating algorithm that is similar to mirror descent.

</details>

<details>

<summary>2014-03-13 07:43:12 - Self-similar prior and wavelet bases for hidden incompressible turbulent motion</summary>

- *Patrick HÃ©as, FrÃ©dÃ©ric Lavancier, Souleymane Kadri-Harouna*

- `1302.5554v2` - [abs](http://arxiv.org/abs/1302.5554v2) - [pdf](http://arxiv.org/pdf/1302.5554v2)

> This work is concerned with the ill-posed inverse problem of estimating turbulent flows from the observation of an image sequence. From a Bayesian perspective, a divergence-free isotropic fractional Brownian motion (fBm) is chosen as a prior model for instantaneous turbulent velocity fields. This self-similar prior characterizes accurately second-order statistics of velocity fields in incompressible isotropic turbulence. Nevertheless, the associated maximum a posteriori involves a fractional Laplacian operator which is delicate to implement in practice. To deal with this issue, we propose to decompose the divergent-free fBm on well-chosen wavelet bases. As a first alternative, we propose to design wavelets as whitening filters. We show that these filters are fractional Laplacian wavelets composed with the Leray projector. As a second alternative, we use a divergence-free wavelet basis, which takes implicitly into account the incompressibility constraint arising from physics. Although the latter decomposition involves correlated wavelet coefficients, we are able to handle this dependence in practice. Based on these two wavelet decompositions, we finally provide effective and efficient algorithms to approach the maximum a posteriori. An intensive numerical evaluation proves the relevance of the proposed wavelet-based self-similar priors.

</details>

<details>

<summary>2014-03-13 14:31:10 - Lower bounds to the accuracy of inference on heavy tails</summary>

- *S. Y. Novak*

- `1403.3278v1` - [abs](http://arxiv.org/abs/1403.3278v1) - [pdf](http://arxiv.org/pdf/1403.3278v1)

> The paper suggests a simple method of deriving minimax lower bounds to the accuracy of statistical inference on heavy tails. A well-known result by Hall and Welsh (Ann. Statist. 12 (1984) 1079-1084) states that if $\hat{\alpha}_n$ is an estimator of the tail index $\alpha_P$ and $\{z_n\}$ is a sequence of positive numbers such that $\sup_{P\in{\mathcal{D}}_r}\mathbb{P}(|\hat{\alpha}_n-\alpha_P|\ge z_n)\to0$, where ${\mathcal{D}}_r$ is a certain class of heavy-tailed distributions, then $z_n\gg n^{-r}$. The paper presents a non-asymptotic lower bound to the probabilities $\mathbb{P}(|\hat{\alpha}_n-\alpha_P|\ge z_n)$. We also establish non-uniform lower bounds to the accuracy of tail constant and extreme quantiles estimation. The results reveal that normalising sequences of robust estimators should depend in a specific way on the tail index and the tail constant.

</details>

<details>

<summary>2014-03-14 02:33:34 - Quantile Regression with Censoring and Endogeneity</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Amanda Kowalski*

- `1104.4580v3` - [abs](http://arxiv.org/abs/1104.4580v3) - [pdf](http://arxiv.org/pdf/1104.4580v3)

> In this paper, we develop a new censored quantile instrumental variable (CQIV) estimator and describe its properties and computation. The CQIV estimator combines Powell (1986) censored quantile regression (CQR) to deal with censoring, with a control variable approach to incorporate endogenous regressors. The CQIV estimator is obtained in two stages that are non-additive in the unobservables. The first stage estimates a non-additive model with infinite dimensional parameters for the control variable, such as a quantile or distribution regression model. The second stage estimates a non-additive censored quantile regression model for the response variable of interest, including the estimated control variable to deal with endogeneity. For computation, we extend the algorithm for CQR developed by Chernozhukov and Hong (2002) to incorporate the estimation of the control variable. We give generic regularity conditions for asymptotic normality of the CQIV estimator and for the validity of resampling methods to approximate its asymptotic distribution. We verify these conditions for quantile and distribution regression estimation of the control variable. Our analysis covers two-stage (uncensored) quantile regression with non-additive first stage as an important special case. We illustrate the computation and applicability of the CQIV estimator with a Monte-Carlo numerical example and an empirical application on estimation of Engel curves for alcohol.

</details>

<details>

<summary>2014-03-16 15:10:35 - A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Arbitrary Parametric Model and Model Prior</summary>

- *Trong Nghia Hoang, Kian Hsiang Low*

- `1304.2024v3` - [abs](http://arxiv.org/abs/1304.2024v3) - [pdf](http://arxiv.org/pdf/1304.2024v3)

> Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which FDM's independence and modeling assumptions do not hold. As a result, FDM does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. To overcome these practical limitations of FDM, we propose a generalization of BRL to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. Empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms.

</details>

<details>

<summary>2014-03-17 09:19:50 - Metropolis-type algorithms for Continuous Time Bayesian Networks</summary>

- *Blazej Miasojedow, Wojciech Niemiro, John Noble, Krzysztof Opalski*

- `1403.4035v1` - [abs](http://arxiv.org/abs/1403.4035v1) - [pdf](http://arxiv.org/pdf/1403.4035v1)

> We present a Metropolis-Hastings Markov chain Monte Carlo (MCMC) algorithm for detecting hidden variables in a continuous time Bayesian network (CTBN), which uses reversible jumps in the sense defined by (Green 1995). In common with several Monte Carlo algorithms, one of the most recent and important by (Rao and Teh 2013), our algorithm exploits uniformization techniques under which a continuous time Markov process can be represented as a marked Poisson process. We exploit this in a novel way. We show that our MCMC algorithm can be more efficient than those of likelihood weighting type, as in (Nodelman et al. 2003) and (Fan et al. 2010) and that our algorithm broadens the class of important examples that can be treated effectively.

</details>

<details>

<summary>2014-03-18 12:25:12 - Des specificites de l'approche bayesienne et de ses justifications en statistique inferentielle</summary>

- *Christian P. Robert*

- `1403.4429v1` - [abs](http://arxiv.org/abs/1403.4429v1) - [pdf](http://arxiv.org/pdf/1403.4429v1)

> This book chapter (written in French) is a review of the foundations of the Bayesian approach to statistical inference, relating to its historical roots and some philosophical arguments, as well as a short presentation of its practical implementation.

</details>

<details>

<summary>2014-03-18 15:28:53 - Bayesian Decision-theoretic Methods for Parameter Ensembles with Application to Epidemiology</summary>

- *Cedric E. Ginestet*

- `1105.5004v6` - [abs](http://arxiv.org/abs/1105.5004v6) - [pdf](http://arxiv.org/pdf/1105.5004v6)

> Parameter ensembles or sets of random effects constitute one of the cornerstones of modern statistical practice. This is especially the case in Bayesian hierarchical models, where several decision theoretic frameworks can be deployed. The estimation of these parameter ensembles may substantially vary depending on which inferential goals are prioritised by the modeller. Since one may wish to satisfy a range of desiderata, it is therefore of interest to investigate whether some sets of point estimates can simultaneously meet several inferential objectives. In this thesis, we will be especially concerned with identifying ensembles of point estimates that produce good approximations of (i) the true empirical quantiles and empirical quartile ratio (QR) and (ii) provide an accurate classification of the ensemble's elements above and below a given threshold. For this purpose, we review various decision-theoretic frameworks, which have been proposed in the literature in relation to the optimisation of different aspects of the empirical distribution of a parameter ensemble. This includes the constrained Bayes (CB), weighted-rank squared error loss (WRSEL), and triple-goal (GR) ensembles of point estimates. In addition, we also consider the set of maximum likelihood estimates (MLEs) and the ensemble of posterior means --the latter being optimal under the summed squared error loss (SSEL). Firstly, we test the performance of these different sets of point estimates as plug-in estimators for the empirical quantiles and empirical QR under a range of synthetic scenarios encompassing both spatial and non-spatial simulated data sets. Performance evaluation is here conducted using the posterior regret. Secondly, two threshold classification losses (TCLs) --weighted and unweighted-- are formulated and formally optimised. The performance of these decision-theoretic tools is also evaluated on real data sets.

</details>

<details>

<summary>2014-03-18 21:40:13 - Bayesian Source Separation Applied to Identifying Complex Organic Molecules in Space</summary>

- *Kevin H. Knuth, Man Kit Tse, Joshua Choinsky, Haley A. Maunu, Duane F. Carbon*

- `1403.4626v1` - [abs](http://arxiv.org/abs/1403.4626v1) - [pdf](http://arxiv.org/pdf/1403.4626v1)

> Emission from a class of benzene-based molecules known as Polycyclic Aromatic Hydrocarbons (PAHs) dominates the infrared spectrum of star-forming regions. The observed emission appears to arise from the combined emission of numerous PAH species, each with its unique spectrum. Linear superposition of the PAH spectra identifies this problem as a source separation problem. It is, however, of a formidable class of source separation problems given that different PAH sources potentially number in the hundreds, even thousands, and there is only one measured spectral signal for a given astrophysical site. Fortunately, the source spectra of the PAHs are known, but the signal is also contaminated by other spectral sources. We describe our ongoing work in developing Bayesian source separation techniques relying on nested sampling in conjunction with an ON/OFF mechanism enabling simultaneous estimation of the probability that a particular PAH species is present and its contribution to the spectrum.

</details>

<details>

<summary>2014-03-19 08:44:18 - Stronger findings from mass spectral data through multi-peak modeling</summary>

- *Tommi Suvitaival, Simon Rogers, Samuel Kaski*

- `1403.4732v1` - [abs](http://arxiv.org/abs/1403.4732v1) - [pdf](http://arxiv.org/pdf/1403.4732v1)

> Mass spectrometry-based metabolomic analysis depends upon the identification of spectral peaks by their mass and retention time. Statistical analysis that follows the identification currently relies on one main peak of each compound. However, a compound present in the sample typically produces several spectral peaks due to its isotopic properties and the ionization process of the mass spectrometer device. In this work, we investigate the extent to which these additional peaks can be used to increase the statistical strength of differential analysis.   We present a Bayesian approach for integrating data of multiple detected peaks that come from one compound. We demonstrate the approach through a simulated experiment and validate it on ultra performance liquid chromatography-mass spectrometry (UPLC-MS) experiments for metabolomics and lipidomics. Peaks that are likely to be associated with one compound can be clustered by the similarity of their chromatographic shape. Changes of concentration between sample groups can be inferred more accurately when multiple peaks are available.   When the sample-size is limited, the proposed multi-peak approach improves the accuracy at inferring covariate effects. An R implementation, data and the supplementary material are available at http://research.ics.aalto.fi/mi/software/peakANOVA/ .

</details>

<details>

<summary>2014-03-19 18:04:21 - Asymptotics of Bayesian Error Probability and Rotating-PSF-Based Source Super-Localization in Three Dimensions</summary>

- *Sudhakar Prasad*

- `1403.4897v1` - [abs](http://arxiv.org/abs/1403.4897v1) - [pdf](http://arxiv.org/pdf/1403.4897v1)

> We present an asymptotic analysis of the minimum probability of error (MPE) in inferring the correct hypothesis in a Bayesian multi-hypothesis testing (MHT) formalism using many pixels of data that are corrupted by signal dependent shot noise, sensor read noise, and background illumination. We perform this error analysis for a variety of combined noise and background statistics, including a pseudo-Gaussian distribution that can be employed to treat approximately the photon-counting statistics of signal and background as well as purely Gaussian sensor read-out noise and more general, exponentially peaked distributions. We subsequently apply the MPE asymptotics to characterize the minimum conditions needed to localize a point source in three dimensions by means of a rotating-PSF imager and compare its performance with that of a conventional imager in the presence of background and sensor-noise fluctuations. In a separate paper, we apply the formalism to the related but qualitatively different problem of 2D super-resolution imaging of a closely spaced pair of point sources in the plane of best focus.

</details>

<details>

<summary>2014-03-19 19:24:44 - Asymptotics of Bayesian Error Probability and 2D Pair Superresolution</summary>

- *Sudhakar Prasad*

- `1403.4919v1` - [abs](http://arxiv.org/abs/1403.4919v1) - [pdf](http://arxiv.org/pdf/1403.4919v1)

> This paper employs a recently developed asymptotic Bayesian multi-hypothesis testing (MHT) error analysis to treat the problem of superresolution imaging of a pair of closely spaced, equally bright point sources. The analysis exploits the notion of the minimum probability of error (MPE) in discriminating between two competing equi-probable hypotheses, a single point source of a certain brightness at the origin vs. a pair of point sources, each of half the brightness of the single source and located symmetrically about the origin, as the distance between the source pair is changed. For a Gaussian point-spread function (PSF), the analysis makes predictions on the scaling of the minimum source strength, expressed in units of photon number, required to disambiguate the pair as a function of their separation, in both the signal-dominated and background-dominated regimes. Certain logarithmic corrections to the quartic scaling of the minimum source strength with respect to the degree of superresolution characterize the signal-dominated regime, while the scaling is purely quadratic in the background-dominated regime. For the Gaussian PSF, general results for arbitrary strengths of the signal, background, and sensor noise levels are also presented.

</details>

<details>

<summary>2014-03-20 08:37:14 - Data augmentation in Rician noise model and Bayesian Diffusion Tensor Imaging</summary>

- *Dario Gasbarra, Jia Liu, Juha Railavo*

- `1403.5065v1` - [abs](http://arxiv.org/abs/1403.5065v1) - [pdf](http://arxiv.org/pdf/1403.5065v1)

> Mapping white matter tracts is an essential step towards understanding brain function. Diffusion Magnetic Resonance Imaging (dMRI) is the only noninvasive technique which can detect in vivo anisotropies in the 3-dimensional diffusion of water molecules, which correspond to nervous fibers in the living brain. In this process, spectral data from the displacement distribution of water molecules is collected by a magnetic resonance scanner. From the statistical point of view, inverting the Fourier transform from such sparse and noisy spectral measurements leads to a non-linear regression problem. Diffusion tensor imaging (DTI) is the simplest modeling approach postulating a Gaussian displacement distribution at each volume element (voxel). Typically the inference is based on a linearized log-normal regression model that can fit the spectral data at low frequencies. However such approximation fails to fit the high frequency measurements which contain information about the details of the displacement distribution but have a low signal to noise ratio. In this paper, we directly work with the Rice noise model and cover the full range of $b$-values. Using data augmentation to represent the likelihood, we reduce the non-linear regression problem to the framework of generalized linear models. Then we construct a Bayesian hierarchical model in order to perform simultaneously estimation and regularization of the tensor field. Finally the Bayesian paradigm is implemented by using Markov chain Monte Carlo.

</details>

<details>

<summary>2014-03-21 04:39:52 - Parameter Estimation of Social Forces in Crowd Dynamics Models via a Probabilistic Method</summary>

- *Alessandro Corbetta, Adrian Muntean, Federico Toschi, Kiamars Vafayi*

- `1403.5361v1` - [abs](http://arxiv.org/abs/1403.5361v1) - [pdf](http://arxiv.org/pdf/1403.5361v1)

> Focusing on a specific crowd dynamics situation, including real life experiments and measurements, our paper targets a twofold aim: (1) we present a Bayesian probabilistic method to estimate the value and the uncertainty (in the form of a probability density function) of parameters in crowd dynamic models from the experimental data; and (2) we introduce a fitness measure for the models to classify a couple of model structures (forces) according to their fitness to the experimental data, preparing the stage for a more general model-selection and validation strategy inspired by probabilistic data analysis. Finally, we review the essential aspects of our experimental setup and measurement technique.

</details>

<details>

<summary>2014-03-21 05:23:17 - Using n-grams models for visual semantic place recognition</summary>

- *Mathieu Dubois, Frenoux Emmanuelle, Philippe Tarroux*

- `1403.5370v1` - [abs](http://arxiv.org/abs/1403.5370v1) - [pdf](http://arxiv.org/pdf/1403.5370v1)

> The aim of this paper is to present a new method for visual place recognition. Our system combines global image characterization and visual words, which allows to use efficient Bayesian filtering methods to integrate several images. More precisely, we extend the classical HMM model with techniques inspired by the field of Natural Language Processing. This paper presents our system and the Bayesian filtering algorithm. The performance of our system and the influence of the main parameters are evaluated on a standard database. The discussion highlights the interest of using such models and proposes improvements.

</details>

<details>

<summary>2014-03-21 14:19:47 - Adaptive robust variable selection</summary>

- *Jianqing Fan, Yingying Fan, Emre Barut*

- `1205.4795v3` - [abs](http://arxiv.org/abs/1205.4795v3) - [pdf](http://arxiv.org/pdf/1205.4795v3)

> Heavy-tailed high-dimensional data are commonly encountered in various scientific fields and pose great challenges to modern statistical analysis. A natural procedure to address this problem is to use penalized quantile regression with weighted $L_1$-penalty, called weighted robust Lasso (WR-Lasso), in which weights are introduced to ameliorate the bias problem induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the dimensionality can grow exponentially with the sample size, we investigate the model selection oracle property and establish the asymptotic normality of the WR-Lasso. We show that only mild conditions on the model error distribution are needed. Our theoretical results also reveal that adaptive choice of the weight vector is essential for the WR-Lasso to enjoy these nice asymptotic properties. To make the WR-Lasso practically feasible, we propose a two-step procedure, called adaptive robust Lasso (AR-Lasso), in which the weight vector in the second step is constructed based on the $L_1$-penalized quantile regression estimate from the first step. This two-step procedure is justified theoretically to possess the oracle property and the asymptotic normality. Numerical studies demonstrate the favorable finite-sample performance of the AR-Lasso.

</details>

<details>

<summary>2014-03-21 14:47:39 - Anisotropic function estimation using multi-bandwidth Gaussian processes</summary>

- *Anirban Bhattacharya, Debdeep Pati, David Dunson*

- `1111.1044v4` - [abs](http://arxiv.org/abs/1111.1044v4) - [pdf](http://arxiv.org/pdf/1111.1044v4)

> In nonparametric regression problems involving multiple predictors, there is typically interest in estimating an anisotropic multivariate regression surface in the important predictors while discarding the unimportant ones. Our focus is on defining a Bayesian procedure that leads to the minimax optimal rate of posterior contraction (up to a log factor) adapting to the unknown dimension and anisotropic smoothness of the true surface. We propose such an approach based on a Gaussian process prior with dimension-specific scalings, which are assigned carefully-chosen hyperpriors. We additionally show that using a homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate in anisotropic cases.

</details>

<details>

<summary>2014-03-21 14:59:05 - On efficient dimension reduction with respect to a statistical functional of interest</summary>

- *Wei Luo, Bing Li, Xiangrong Yin*

- `1403.5483v1` - [abs](http://arxiv.org/abs/1403.5483v1) - [pdf](http://arxiv.org/pdf/1403.5483v1)

> We introduce a new sufficient dimension reduction framework that targets a statistical functional of interest, and propose an efficient estimator for the semiparametric estimation problems of this type. The statistical functional covers a wide range of applications, such as conditional mean, conditional variance and conditional quantile. We derive the general forms of the efficient score and efficient information as well as their specific forms for three important statistical functionals: the linear functional, the composite linear functional and the implicit functional. In conjunction with our theoretical analysis, we also propose a class of one-step Newton-Raphson estimators and show by simulations that they substantially outperform existing methods. Finally, we apply the new method to construct the central mean and central variance subspaces for a data set involving the physical measurements and age of abalones, which exhibits a strong pattern of heteroscedasticity.

</details>

<details>

<summary>2014-03-21 18:40:26 - A practical sequential stopping rule for high-dimensional MCMC and its application to spatial-temporal Bayesian models</summary>

- *Lei Gong, James M. Flegal*

- `1403.5536v1` - [abs](http://arxiv.org/abs/1403.5536v1) - [pdf](http://arxiv.org/pdf/1403.5536v1)

> A current challenge for many Bayesian analyses is determining when to terminate high-dimensional Markov chain Monte Carlo simulations. To this end, we propose using an automated sequential stopping procedure that terminates the simulation when the computational uncertainty is small relative to the posterior uncertainty. Such a stopping rule has previously been shown to work well in settings with posteriors of moderate dimension. In this paper, we illustrate its utility in high-dimensional simulations while overcoming some current computational issues. Further, we investigate the relationship between the stopping rule and effective sample size. As examples, we consider two complex Bayesian analyses on spatially and temporally correlated datasets. The first involves a dynamic space-time model on weather station data and the second a spatial variable selection model on fMRI brain imaging data. Our results show the sequential stopping rule is easy to implement, provides uncertainty estimates, and performs well in high-dimensional settings.

</details>

<details>

<summary>2014-03-22 03:35:00 - Bayesian Optimization with Unknown Constraints</summary>

- *Michael A. Gelbart, Jasper Snoek, Ryan P. Adams*

- `1403.5607v1` - [abs](http://arxiv.org/abs/1403.5607v1) - [pdf](http://arxiv.org/pdf/1403.5607v1)

> Recent work on Bayesian optimization has shown its effectiveness in global optimization of difficult black-box objective functions. Many real-world optimization problems of interest also have constraints which are unknown a priori. In this paper, we study Bayesian optimization for constrained problems in the general case that noise may be present in the constraint functions, and the objective and constraints may be evaluated independently. We provide motivating practical examples, and present a general framework to solve such problems. We demonstrate the effectiveness of our approach on optimizing the performance of online latent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given test-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed time, subject to passing standard convergence diagnostics.

</details>

<details>

<summary>2014-03-22 18:21:29 - Firefly Monte Carlo: Exact MCMC with Subsets of Data</summary>

- *Dougal Maclaurin, Ryan P. Adams*

- `1403.5693v1` - [abs](http://arxiv.org/abs/1403.5693v1) - [pdf](http://arxiv.org/pdf/1403.5693v1)

> Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose tool for Bayesian inference. However, MCMC cannot be practically applied to large data sets because of the prohibitive cost of evaluating every likelihood term at every iteration. Here we present Firefly Monte Carlo (FlyMC) an auxiliary variable MCMC algorithm that only queries the likelihoods of a potentially small subset of the data at each iteration yet simulates from the exact posterior distribution, in contrast to recent proposals that are approximate even in the asymptotic limit. FlyMC is compatible with a wide variety of modern MCMC algorithms, and only requires a lower bound on the per-datum likelihood factors. In experiments, we find that FlyMC generates samples from the posterior more than an order of magnitude faster than regular MCMC, opening up MCMC methods to larger datasets than were previously considered feasible.

</details>

<details>

<summary>2014-03-24 15:40:41 - What is the `relevant population' in Bayesian forensic inference?</summary>

- *Niko BrÃ¼mmer, Edward de Villiers*

- `1403.6008v1` - [abs](http://arxiv.org/abs/1403.6008v1) - [pdf](http://arxiv.org/pdf/1403.6008v1)

> In works discussing the Bayesian paradigm for presenting forensic evidence in court, the concept of a `relevant population' is often mentioned, without a clear definition of what is meant, and without recommendations of how to select such populations. This note is to try to better understand this concept. Our analysis is intended to be general enough to be applicable to different forensic technologies and we shall consider both DNA profiling and speaker recognition as examples.

</details>

<details>

<summary>2014-03-25 18:36:23 - Construction of an informative hierarchical prior for a small sample with the help of historical data and application to electricity load forecasting</summary>

- *Tristan Launay, Anne Philippe, Sophie Lamarche*

- `1109.4533v5` - [abs](http://arxiv.org/abs/1109.4533v5) - [pdf](http://arxiv.org/pdf/1109.4533v5)

> We are interested in the estimation and prediction of a parametric model on a short dataset upon which it is expected to overfit and perform badly. To overcome the lack of data (relatively to the dimension of the model) we propose the construction of an informative hierarchical Bayesian prior based upon another longer dataset which is assumed to share some similarities with the original, short dataset. We illustrate the performance of our prior on simulated dataset from three standard models. Then we apply the methodology to a working model for the electricity load forecasting on real datasets, where it leads to a substantial improvement of the quality of the predictions.

</details>

<details>

<summary>2014-03-25 21:57:19 - Variance bounding and geometric ergodicity of Markov chain Monte Carlo kernels for approximate Bayesian computation</summary>

- *Anthony Lee, Krzysztof Latuszynski*

- `1210.6703v3` - [abs](http://arxiv.org/abs/1210.6703v3) - [pdf](http://arxiv.org/pdf/1210.6703v3)

> Approximate Bayesian computation has emerged as a standard computational tool when dealing with the increasingly common scenario of completely intractable likelihood functions in Bayesian inference. We show that many common Markov chain Monte Carlo kernels used to facilitate inference in this setting can fail to be variance bounding, and hence geometrically ergodic, which can have consequences for the reliability of estimates in practice. This phenomenon is typically independent of the choice of tolerance in the approximation. We then prove that a recently introduced Markov kernel in this setting can inherit variance bounding and geometric ergodicity from its intractable Metropolis--Hastings counterpart, under reasonably weak and manageable conditions. We show that the computational cost of this alternative kernel is bounded whenever the prior is proper, and present indicative results on an example where spectral gaps and asymptotic variances can be computed, as well as an example involving inference for a partially and discretely observed, time-homogeneous, pure jump Markov process. We also supply two general theorems, one of which provides a simple sufficient condition for lack of variance bounding for reversible kernels and the other provides a positive result concerning inheritance of variance bounding and geometric ergodicity for mixtures of reversible kernels.

</details>

<details>

<summary>2014-03-26 10:45:44 - Analyzing genome-wide association studies with an FDR controlling modification of the Bayesian information criterion</summary>

- *Erich Dolejsi, Bernhard Bodenstorfer, Florian Frommlet*

- `1403.6623v1` - [abs](http://arxiv.org/abs/1403.6623v1) - [pdf](http://arxiv.org/pdf/1403.6623v1)

> The prevailing method of analyzing GWAS data is still to test each marker individually, although from a statistical point of view it is quite obvious that in case of complex traits such single marker tests are not ideal. Recently several model selection approaches for GWAS have been suggested, most of them based on LASSO-type procedures. Here we will discuss an alternative model selection approach which is based on a modification of the Bayesian Information Criterion (mBIC2) which was previously shown to have certain asymptotic optimality properties in terms of minimizing the misclassification error. Heuristic search strategies are introduced which attempt to find the model which minimizes mBIC2, and which are efficient enough to allow the analysis of GWAS data.   Our approach is implemented in a software package called MOSGWA. Its performance in case control GWAS is compared with the two algorithms HLASSO and GWASelect, as well as with single marker tests, where we performed a simulation study based on real SNP data from the POPRES sample. Our results show that MOSGWA performs slightly better than HLASSO, whereas according to our simulations GWASelect does not control the type I error when used to automatically determine the number of important SNPs. We also reanalyze the GWAS data from the Wellcome Trust Case-Control Consortium (WTCCC) and compare the findings of the different procedures.

</details>

<details>

<summary>2014-03-26 15:16:56 - Beyond L2-Loss Functions for Learning Sparse Models</summary>

- *Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan*

- `1403.6706v1` - [abs](http://arxiv.org/abs/1403.6706v1) - [pdf](http://arxiv.org/pdf/1403.6706v1)

> Incorporating sparsity priors in learning tasks can give rise to simple, and interpretable models for complex high dimensional data. Sparse models have found widespread use in structure discovery, recovering data from corruptions, and a variety of large scale unsupervised and supervised learning problems. Assuming the availability of sufficient data, these methods infer dictionaries for sparse representations by optimizing for high-fidelity reconstruction. In most scenarios, the reconstruction quality is measured using the squared Euclidean distance, and efficient algorithms have been developed for both batch and online learning cases. However, new application domains motivate looking beyond conventional loss functions. For example, robust loss functions such as $\ell_1$ and Huber are useful in learning outlier-resilient models, and the quantile loss is beneficial in discovering structures that are the representative of a particular quantile. These new applications motivate our work in generalizing sparse learning to a broad class of convex loss functions. In particular, we consider the class of piecewise linear quadratic (PLQ) cost functions that includes Huber, as well as $\ell_1$, quantile, Vapnik, hinge loss, and smoothed variants of these penalties. We propose an algorithm to learn dictionaries and obtain sparse codes when the data reconstruction fidelity is measured using any smooth PLQ cost function. We provide convergence guarantees for the proposed algorithm, and demonstrate the convergence behavior using empirical experiments. Furthermore, we present three case studies that require the use of PLQ cost functions: (i) robust image modeling, (ii) tag refinement for image annotation and retrieval and (iii) computing empirical confidence limits for subspace clustering.

</details>

<details>

<summary>2014-03-28 01:28:52 - Accelerating MCMC via Parallel Predictive Prefetching</summary>

- *Elaine Angelino, Eddie Kohler, Amos Waterland, Margo Seltzer, Ryan P. Adams*

- `1403.7265v1` - [abs](http://arxiv.org/abs/1403.7265v1) - [pdf](http://arxiv.org/pdf/1403.7265v1)

> We present a general framework for accelerating a large class of widely used Markov chain Monte Carlo (MCMC) algorithms. Our approach exploits fast, iterative approximations to the target density to speculatively evaluate many potential future steps of the chain in parallel. The approach can accelerate computation of the target distribution of a Bayesian inference problem, without compromising exactness, by exploiting subsets of data. It takes advantage of whatever parallel resources are available, but produces results exactly equivalent to standard serial execution. In the initial burn-in phase of chain evaluation, it achieves speedup over serial evaluation that is close to linear in the number of available cores.

</details>

<details>

<summary>2014-03-28 07:46:14 - A stochastic variational framework for fitting and diagnosing generalized linear mixed models</summary>

- *Linda S. L. Tan, David J. Nott*

- `1208.4949v4` - [abs](http://arxiv.org/abs/1208.4949v4) - [pdf](http://arxiv.org/pdf/1208.4949v4)

> In stochastic variational inference, the variational Bayes objective function is optimized using stochastic gradient approximation, where gradients computed on small random subsets of data are used to approximate the true gradient over the whole data set. This enables complex models to be fit to large data sets as data can be processed in mini-batches. In this article, we extend stochastic variational inference for conjugate-exponential models to nonconjugate models and present a stochastic nonconjugate variational message passing algorithm for fitting generalized linear mixed models that is scalable to large data sets. In addition, we show that diagnostics for prior-likelihood conflict, which are useful for Bayesian model criticism, can be obtained from nonconjugate variational message passing automatically, as an alternative to simulation-based Markov chain Monte Carlo methods. Finally, we demonstrate that for moderate-sized data sets, convergence can be accelerated by using the stochastic version of nonconjugate variational message passing in the initial stage of optimization before switching to the standard version.

</details>

<details>

<summary>2014-03-28 14:06:11 - A unified approach to marginal equivalence in the general framework of group invariance</summary>

- *Hidehiko Kamiya*

- `1403.7379v1` - [abs](http://arxiv.org/abs/1403.7379v1) - [pdf](http://arxiv.org/pdf/1403.7379v1)

> Two Bayesian models with different sampling densities are said to be marginally equivalent if the joint distribution of observables and the parameter of interest is the same for both models. We discuss marginal equivalence in the general framework of group invariance. We introduce a class of sampling models and establish marginal equivalence when the prior for the nuisance parameter is relatively invariant. We also obtain some robustness properties of invariant statistics under our sampling models. Besides the prototypical example of $v$-spherical distributions, we apply our general results to two examples---analysis of affine shapes and principal component analysis.

</details>

<details>

<summary>2014-03-28 21:14:26 - Evidence and Bayes factor estimation for Gibbs random fields</summary>

- *Nial Friel*

- `1301.2917v3` - [abs](http://arxiv.org/abs/1301.2917v3) - [pdf](http://arxiv.org/pdf/1301.2917v3)

> Gibbs random fields play an important role in statistics. However they are complicated to work with due to an intractability of the likelihood function and there has been much work devoted to finding computational algorithms to allow Bayesian inference to be conducted for such so-called doubly intractable distributions. This paper extends this work and addresses the issue of estimating the evidence and Bayes factor for such models. The approach which we develop is shown to yield good performance.

</details>

<details>

<summary>2014-03-31 02:37:07 - A Latent Gaussian Process Model with Application to Monitoring Clinical Trials</summary>

- *Yanxun Xu, Yuan Ji*

- `1403.7853v1` - [abs](http://arxiv.org/abs/1403.7853v1) - [pdf](http://arxiv.org/pdf/1403.7853v1)

> In many clinical trials treatments need to be repeatedly applied as diseases relapse frequently after remission over a long period of time (e.g., 35 weeks). Most research in statistics focuses on the overall trial design, such as sample size and power calculation, or on the data analysis after trials are completed. Little is done to improve the efficiency of trial monitoring, such as early termination of trials due to futility. The challenge faced in such trial monitoring is mostly caused by the need to properly model repeated outcomes from patients. We propose a Bayesian trial monitoring scheme for clinical trials with repeated and potentially cyclic binary outcomes. We construct a latent Gaussian process (LGP) to model discrete longitudinal data in those trials. LGP describes the underlying latent process that gives rise to the observed longitudinal binary outcomes. The posterior consistency property of the proposed model is studied. Posterior inference is conducted with a hybrid Monte Carlo algorithm. Simulation studies are conducted under various clinical scenarios, and a case study is reported based on a real-life trial.

</details>

<details>

<summary>2014-03-31 13:12:28 - Coherence and elicitability</summary>

- *Johanna F. Ziegel*

- `1303.1690v3` - [abs](http://arxiv.org/abs/1303.1690v3) - [pdf](http://arxiv.org/pdf/1303.1690v3)

> The risk of a financial position is usually summarized by a risk measure. As this risk measure has to be estimated from historical data, it is important to be able to verify and compare competing estimation procedures. In statistical decision theory, risk measures for which such verification and comparison is possible, are called elicitable. It is known that quantile based risk measures such as value at risk are elicitable. In this paper we show that law-invariant spectral risk measures such as expected shortfall are not elicitable unless they reduce to minus the expected value. Hence, it is unclear how to perform forecast verification or comparison. However, the class of elicitable law-invariant coherent risk measures does not reduce to minus the expected value. We show that it consists of certain expectiles.

</details>

<details>

<summary>2014-03-31 21:14:29 - Monte Carlo error in the Bayesian estimation of risk ratios using log-binomial regression models: an efficient MCMC method</summary>

- *Diego SalmerÃ³n, Juan Antonio Cano*

- `1404.0042v1` - [abs](http://arxiv.org/abs/1404.0042v1) - [pdf](http://arxiv.org/pdf/1404.0042v1)

> In cohort studies binary outcomes are very often analyzed by logistic regression. However, it is well-known that when the goal is to estimate a risk ratio, the logistic regression is inappropriate if the outcome is common. In these cases, a log-binomial regression model is preferable. On the other hand, the estimation of the regression coefficients of the log-binomial model is difficult due to the constraints that must be imposed on these coefficients. Bayesian methods allow a straightforward approach for log-binomial regression models, produce smaller mean squared errors and the posterior inferences can be obtained using the software WinBUGS. However, the Markov chain Monte Carlo (MCMC) methods implemented in WinBUGS can lead to a high Monte Carlo error. To avoid this drawback we propose an MCMC algorithm that uses a reparameterization based on a Poisson approximation and has been designed to efficiently explore the constrained parameter space.

</details>


## 2014-04

<details>

<summary>2014-04-01 13:38:35 - Sparse additive regression on a regular lattice</summary>

- *Felix Abramovich, Tal Lahav*

- `1307.5992v2` - [abs](http://arxiv.org/abs/1307.5992v2) - [pdf](http://arxiv.org/pdf/1307.5992v2)

> We consider estimation in a sparse additive regression model with the design points on a regular lattice. We establish the minimax convergence rates over Sobolev classes and propose a Fourier-based rate-optimal estimator which is adaptive to the unknown sparsity and smoothness of the response function. The estimator is derived within Bayesian formalism but can be naturally viewed as a penalized maximum likelihood estimator with the complexity penalties on the number of nonzero univariate additive components of the response and on the numbers of the nonzero coefficients of their Fourer expansions. We compare it with several existing counterparts and perform a short simulation study to demonstrate its performance.

</details>

<details>

<summary>2014-04-01 15:52:02 - Stochastic processes and feedback-linearisation for online identification and Bayesian adaptive control of fully-actuated mechanical systems</summary>

- *Jan-Peter Calliess, Antonis Papachristodoulou, Stephen J. Roberts*

- `1311.4468v3` - [abs](http://arxiv.org/abs/1311.4468v3) - [pdf](http://arxiv.org/pdf/1311.4468v3)

> This work proposes a new method for simultaneous probabilistic identification and control of an observable, fully-actuated mechanical system. Identification is achieved by conditioning stochastic process priors on observations of configurations and noisy estimates of configuration derivatives. In contrast to previous work that has used stochastic processes for identification, we leverage the structural knowledge afforded by Lagrangian mechanics and learn the drift and control input matrix functions of the control-affine system separately. We utilise feedback-linearisation to reduce, in expectation, the uncertain nonlinear control problem to one that is easy to regulate in a desired manner. Thereby, our method combines the flexibility of nonparametric Bayesian learning with epistemological guarantees on the expected closed-loop trajectory. We illustrate our method in the context of torque-actuated pendula where the dynamics are learned with a combination of normal and log-normal processes.

</details>

<details>

<summary>2014-04-01 21:05:22 - Tensor decompositions and sparse log-linear models</summary>

- *James E. Johndrow, Anirban Battacharya, David B. Dunson*

- `1404.0396v1` - [abs](http://arxiv.org/abs/1404.0396v1) - [pdf](http://arxiv.org/pdf/1404.0396v1)

> Contingency table analysis routinely relies on log linear models, with latent structure analysis providing a common alternative. Latent structure models lead to a low rank tensor factorization of the probability mass function for multivariate categorical data, while log linear models achieve dimensionality reduction through sparsity. Little is known about the relationship between these notions of dimensionality reduction in the two paradigms. We derive several results relating the support of a log-linear model to the nonnegative rank of the associated probability tensor. Motivated by these findings, we propose a new collapsed Tucker class of tensor decompositions, which bridge existing PARAFAC and Tucker decompositions, providing a more flexible framework for parsimoniously characterizing multivariate categorical data. Taking a Bayesian approach to inference, we illustrate advantages of the new decompositions in simulations and an application to functional disability data.

</details>

<details>

<summary>2014-04-02 09:29:46 - Weighted quantile correlation test for the logistic family</summary>

- *Ferenc Balogh, Eva Krauczi*

- `1402.0369v2` - [abs](http://arxiv.org/abs/1402.0369v2) - [pdf](http://arxiv.org/pdf/1402.0369v2)

> We summarize the results of investigating the asymptotic behavior of the weighted quantile correlation tests for the location-scale family associated to the logistic distribution. Explicit representations of the limiting distribution are given in terms of integrals of weighted Brownian bridges or alternatively as infinite series of independent Gaussian random variables. The power of this test and the test for the location logistic family against some alternatives are demonstrated by numerical simulations.

</details>

<details>

<summary>2014-04-02 12:19:03 - Constrained speaker linking</summary>

- *David A. van Leeuwen, Niko BrÃ¼mmer*

- `1403.7084v2` - [abs](http://arxiv.org/abs/1403.7084v2) - [pdf](http://arxiv.org/pdf/1403.7084v2)

> In this paper we study speaker linking (a.k.a.\ partitioning) given constraints of the distribution of speaker identities over speech recordings. Specifically, we show that the intractable partitioning problem becomes tractable when the constraints pre-partition the data in smaller cliques with non-overlapping speakers. The surprisingly common case where speakers in telephone conversations are known, but the assignment of channels to identities is unspecified, is treated in a Bayesian way. We show that for the Dutch CGN database, where this channel assignment task is at hand, a lightweight speaker recognition system can quite effectively solve the channel assignment problem, with 93% of the cliques solved. We further show that the posterior distribution over channel assignment configurations is well calibrated.

</details>

<details>

<summary>2014-04-03 03:15:26 - An Efficient Search Strategy for Aggregation and Discretization of Attributes of Bayesian Networks Using Minimum Description Length</summary>

- *Jem Corcoran, Daniel Tran, Nicholas Levine*

- `1404.0752v1` - [abs](http://arxiv.org/abs/1404.0752v1) - [pdf](http://arxiv.org/pdf/1404.0752v1)

> Bayesian networks are convenient graphical expressions for high dimensional probability distributions representing complex relationships between a large number of random variables. They have been employed extensively in areas such as bioinformatics, artificial intelligence, diagnosis, and risk management. The recovery of the structure of a network from data is of prime importance for the purposes of modeling, analysis, and prediction. Most recovery algorithms in the literature assume either discrete of continuous but Gaussian data. For general continuous data, discretization is usually employed but often destroys the very structure one is out to recover. Friedman and Goldszmidt suggest an approach based on the minimum description length principle that chooses a discretization which preserves the information in the original data set, however it is one which is difficult, if not impossible, to implement for even moderately sized networks. In this paper we provide an extremely efficient search strategy which allows one to use the Friedman and Goldszmidt discretization in practice.

</details>

<details>

<summary>2014-04-04 08:00:11 - Longitudinal quantile regression in presence of informative drop-out through longitudinal-survival joint modeling</summary>

- *Alessio Farcomeni, Sara Viviani*

- `1404.1175v1` - [abs](http://arxiv.org/abs/1404.1175v1) - [pdf](http://arxiv.org/pdf/1404.1175v1)

> We propose a joint model for a time-to-event outcome and a quantile of a continuous response repeatedly measured over time. The quantile and survival processes are associated via shared latent and manifest variables. Our joint model provides a flexible approach to handle informative drop-out in quantile regression. A general Monte Carlo Expectation Maximization strategy based on importance sampling is proposed, which is directly applicable under any distributional assumption for the longitudinal outcome and random effects, and parametric and non-parametric assumptions for the baseline hazard. Model properties are illustrated through a simulation study and an application to an original data set about dilated cardiomyopathies.

</details>

<details>

<summary>2014-04-04 19:02:04 - Equivalence between hybrid CLs and bayesian methods for limit setting</summary>

- *Emmanuel Busato*

- `1404.1340v1` - [abs](http://arxiv.org/abs/1404.1340v1) - [pdf](http://arxiv.org/pdf/1404.1340v1)

> The relation between hybrid CLs and bayesian methods used for limit setting is discussed. It is shown that the two methods are equivalent in the single channel case even when the background yield is not perfectly known. Only counting experiments are considered in this document.

</details>

<details>

<summary>2014-04-04 19:20:49 - Bayesian estimation for a parametric Markov Renewal model applied to seismic data</summary>

- *I. Epifani, L. Ladelli, A. Pievatolo*

- `1301.6494v3` - [abs](http://arxiv.org/abs/1301.6494v3) - [pdf](http://arxiv.org/pdf/1301.6494v3)

> This paper presents a complete methodology for Bayesian inference on a semi-Markov process, from the elicitation of the prior distribution, to the computation of posterior summaries, including a guidance for its JAGS implementation. The holding times (conditional on the transition between two given states) are assumed to be Weibull-distributed. We examine the elicitation of the joint prior density of the shape and scale parameters of the Weibull distributions, deriving a specific class of priors in a natural way, along with a method for the determination of hyperparameters based on ``learning data'' and moment existence conditions. This framework is applied to data of earthquakes of three types of severity (low, medium and high size) that occurred in the central Northern Apennines in Italy and collected by the \cite{CPTI04} catalogue. Assumptions on two types of energy accumulation and release mechanisms are evaluated.

</details>

<details>

<summary>2014-04-07 01:45:34 - Bayesian estimation of a sparse precision matrix</summary>

- *Sayantan Banerjee, Subhashis Ghosal*

- `1309.1754v2` - [abs](http://arxiv.org/abs/1309.1754v2) - [pdf](http://arxiv.org/pdf/1309.1754v2)

> We consider the problem of estimating a sparse precision matrix of a multivariate Gaussian distribution, including the case where the dimension $p$ is large. Gaussian graphical models provide an important tool in describing conditional independence through presence or absence of the edges in the underlying graph. A popular non-Bayesian method of estimating a graphical structure is given by the graphical lasso. In this paper, we consider a Bayesian approach to the problem. We use priors which put a mixture of a point mass at zero and certain absolutely continuous distribution on off-diagonal elements of the precision matrix. Hence the resulting posterior distribution can be used for graphical structure learning. The posterior convergence rate of the precision matrix is obtained. The posterior distribution on the model space is extremely cumbersome to compute. We propose a fast computational method for approximating the posterior probabilities of various graphs using the Laplace approximation approach by expanding the posterior density around the posterior mode, which is the graphical lasso by our choice of the prior distribution. We also provide estimates of the accuracy in the approximation.

</details>

<details>

<summary>2014-04-07 09:42:58 - Pseudo-Marginal Bayesian Inference for Gaussian Processes</summary>

- *Maurizio Filippone, Mark Girolami*

- `1310.0740v4` - [abs](http://arxiv.org/abs/1310.0740v4) - [pdf](http://arxiv.org/pdf/1310.0740v4)

> The main challenges that arise when adopting Gaussian Process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses both of these issues. The results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out full Bayesian inference of Gaussian Process based hierarchic statistical models in general. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. Extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion.

</details>

<details>

<summary>2014-04-07 09:44:35 - A Bayesian Hierarchical Model for Comparative Evaluation of Teaching Quality Indicators in Higher Education</summary>

- *Dimitris Fouskakis, George Petrakos, Ioannis Vavouras*

- `1404.1710v1` - [abs](http://arxiv.org/abs/1404.1710v1) - [pdf](http://arxiv.org/pdf/1404.1710v1)

> The problem motivating the paper is the quantification of students' preferences regarding teaching/coursework quality, under certain numerical restrictions, in order to build a model for identifying, assessing and monitoring the major components of the overall academic quality. After reviewing the strengths and limitations of conjoint analysis and of the random coefficient regression model used in similar problems in the past, we propose a Bayesian beta regression model with a Dirichlet prior on the model coefficients. This approach not only allows for the incorporation of informative prior when it is available but also provides user friendly interfaces and direct probability interpretations for all quantities. Furthermore, it is a natural way to implement the usual constraints for the model weights/coefficients. This model was applied to data collected in 2009 and 2013 from undergraduate students in Panteion University, Athens, Greece and besides the construction of an instrument for the assessment and monitoring of teaching quality, it gave some input for a preliminary discussion on the association of the differences in students preferences between the two time periods with the current Greek economic and financial crisis.

</details>

<details>

<summary>2014-04-08 04:44:59 - A Permutation Approach for Selecting the Penalty Parameter in Penalized Model Selection</summary>

- *Jeremy Sabourin, William Valdar, Andrew Nobel*

- `1404.2007v1` - [abs](http://arxiv.org/abs/1404.2007v1) - [pdf](http://arxiv.org/pdf/1404.2007v1)

> We describe a simple, efficient, permutation based procedure for selecting the penalty parameter in the LASSO. The procedure, which is intended for applications where variable selection is the primary focus, can be applied in a variety of structural settings, including generalized linear models. We briefly discuss connections between permutation selection and existing theory for the LASSO. In addition, we present a simulation study and an analysis of three real data sets in which permutation selection is compared with cross-validation (CV), the Bayesian information criterion (BIC), and a selection method based on recently developed testing procedures for the LASSO.

</details>

<details>

<summary>2014-04-08 09:11:26 - Bayesian DEJD model and detection of asymmetric jumps</summary>

- *Maciej Kostrzewski*

- `1404.2050v1` - [abs](http://arxiv.org/abs/1404.2050v1) - [pdf](http://arxiv.org/pdf/1404.2050v1)

> News might trigger jump arrivals in financial time series. The "bad" and "good" news seems to have distinct impact. In the research, a double exponential jump distribution is applied to model downward and upward jumps. Bayesian double exponential jump-diffusion model is proposed. Theorems stated in the paper enable estimation of the model's parameters, detection of jumps and analysis of jump frequency. The methodology, founded upon the idea of latent variables, is illustrated with two empirical studies, employing both simulated and real-world data (the KGHM index). News might trigger jump arrivals in financial time series. The "bad" and "good" news seems to have distinct impact. In the research, a double exponential jump distribution is applied to model downward and upward jumps. Bayesian double exponential jump-diffusion model is proposed. Theorems stated in the paper enable estimation of the model's parameters, detection of jumps and analysis of jump frequency. The methodology, founded upon the idea of latent variables, is illustrated with two empirical studies, employing both simulated and real-world data (the KGHM index).

</details>

<details>

<summary>2014-04-08 10:49:08 - Efficiency of conformalized ridge regression</summary>

- *Evgeny Burnaev, Vladimir Vovk*

- `1404.2083v1` - [abs](http://arxiv.org/abs/1404.2083v1) - [pdf](http://arxiv.org/pdf/1404.2083v1)

> Conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms. The method has a guaranteed coverage probability under the standard IID assumption regardless of whether the assumptions (often considerably more restrictive) of the underlying algorithm are satisfied. However, for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied, the conformal predictor loses little in efficiency as compared with the underlying algorithm (whereas being a conformal predictor, it has the stronger guarantee of validity). In this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of Bayesian ridge regression; we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard Bayesian assumptions are satisfied.

</details>

<details>

<summary>2014-04-08 15:51:10 - Data mining for censored time-to-event data: A Bayesian network model for predicting cardiovascular risk from electronic health record data</summary>

- *Sunayan Bandyopadhyay, Julian Wolfson, David M. Vock, Gabriela Vazquez-Benitez, Gediminas Adomavicius, Mohamed Elidrisi, Paul E. Johnson, Patrick J. O'Connor*

- `1404.2189v1` - [abs](http://arxiv.org/abs/1404.2189v1) - [pdf](http://arxiv.org/pdf/1404.2189v1)

> Models for predicting the risk of cardiovascular events based on individual patient characteristics are important tools for managing patient care. Most current and commonly used risk prediction models have been built from carefully selected epidemiological cohorts. However, the homogeneity and limited size of such cohorts restricts the predictive power and generalizability of these risk models to other populations. Electronic health data (EHD) from large health care systems provide access to data on large, heterogeneous, and contemporaneous patient populations. The unique features and challenges of EHD, including missing risk factor information, non-linear relationships between risk factors and cardiovascular event outcomes, and differing effects from different patient subgroups, demand novel machine learning approaches to risk model development. In this paper, we present a machine learning approach based on Bayesian networks trained on EHD to predict the probability of having a cardiovascular event within five years. In such data, event status may be unknown for some individuals as the event time is right-censored due to disenrollment and incomplete follow-up. Since many traditional data mining methods are not well-suited for such data, we describe how to modify both modelling and assessment techniques to account for censored observation times. We show that our approach can lead to better predictive performance than the Cox proportional hazards model (i.e., a regression-based approach commonly used for censored, time-to-event data) or a Bayesian network with {\em{ad hoc}} approaches to right-censoring. Our techniques are motivated by and illustrated on data from a large U.S. Midwestern health care system.

</details>

<details>

<summary>2014-04-08 19:36:01 - The central role of Bayes theorem for joint estimation of causal effects and propensity scores</summary>

- *Corwin M. Zigler*

- `1308.5595v3` - [abs](http://arxiv.org/abs/1308.5595v3) - [pdf](http://arxiv.org/pdf/1308.5595v3)

> Although propensity scores have been central to the estimation of causal effects for over 30 years, only recently has the statistical literature begun to consider in detail methods for Bayesian estimation of propensity scores and causal effects. Underlying this recent body of literature on Bayesian propensity score estimation is an implicit discordance between the goal of the propensity score and the use of Bayes theorem. The propensity score condenses multivariate covariate information into a scalar to allow estimation of causal effects without specifying a model for how each covariate relates to the outcome. Avoiding specification of a detailed model for the outcome response surface is valuable for robust estimation of causal effects, but this strategy is at odds with the use of Bayes theorem, which presupposes a full probability model for the observed data. The goal of this paper is to explicate this fundamental feature of Bayesian estimation of causal effects with propensity scores in order to provide context for the existing literature and for future work on this important topic.

</details>

<details>

<summary>2014-04-10 10:38:25 - An external field prior for the hidden Potts model, with application to cone-beam computed tomography</summary>

- *Matthew T. Moores, Catriona E. Hargrave, Fiona Harden, Kerrie Mengersen*

- `1404.2764v1` - [abs](http://arxiv.org/abs/1404.2764v1) - [pdf](http://arxiv.org/pdf/1404.2764v1)

> In images with low contrast-to-noise ratio (CNR), the information gain from the observed pixel values can be insufficient to distinguish foreground objects. A Bayesian approach to this problem is to incorporate prior information about the objects into a statistical model. This paper introduces a method for representing spatial prior information as an external field in a hidden Potts model of the image lattice. The prior distribution of the latent pixel labels is a mixture of Gaussian fields, centred on the positions of the objects at a previous point in time. This model is particularly applicable in longitudinal imaging studies, where the manual segmentation of one image can be used as a prior for automatic segmentation of subsequent images. The model is demonstrated by application to cone-beam computed tomography (CT), an imaging modality that exhibits distortions in pixel values due to X-ray scatter. The external field prior results in a substantial improvement in segmentation accuracy, reducing the mean pixel misclassification rate on our test images from 87% to 6%.

</details>

<details>

<summary>2014-04-11 12:52:25 - A nonparametric model-based estimator for the cumulative distribution function of a right censored variable in a finite population</summary>

- *Sandrine Casanova, Eve Leconte*

- `1310.5927v2` - [abs](http://arxiv.org/abs/1310.5927v2) - [pdf](http://arxiv.org/pdf/1310.5927v2)

> In survey analysis, the estimation of the cumulative distribution function (cdf) is of great interest: it allows for instance to derive quantiles estimators or other non linear parameters derived from the cdf. We consider the case where the response variable is a right censored duration variable. In this framework, the classical estimator of the cdf is the Kaplan-Meier estimator. As an alternative, we propose a nonparametric model-based estimator of the cdf in a finite population. The new estimator uses auxiliary information brought by a continuous covariate and is based on nonparametric median regression adapted to the censored case. The bias and variance of the prediction error of the estimator are estimated by a bootstrap procedure adapted to censoring. The new estimator is compared by model-based simulations to the Kaplan-Meier estimator computed with the sampled individuals: a significant gain in precision is brought by the new method whatever the size of the sample and the censoring rate. Welfare duration data are used to illustrate the new methodology.

</details>

<details>

<summary>2014-04-11 15:40:14 - A computational framework for infinite-dimensional Bayesian inverse problems: Part II. Stochastic Newton MCMC with application to ice sheet flow inverse problems</summary>

- *Noemi Petra, James Martin, Georg Stadler, Omar Ghattas*

- `1308.6221v2` - [abs](http://arxiv.org/abs/1308.6221v2) - [pdf](http://arxiv.org/pdf/1308.6221v2)

> We address the numerical solution of infinite-dimensional inverse problems in the framework of Bayesian inference. In the Part I companion to this paper (arXiv.org:1308.1313), we considered the linearized infinite-dimensional inverse problem. Here in Part II, we relax the linearization assumption and consider the fully nonlinear infinite-dimensional inverse problem using a Markov chain Monte Carlo (MCMC) sampling method. To address the challenges of sampling high-dimensional pdfs arising from Bayesian inverse problems governed by PDEs, we build on the stochastic Newton MCMC method. This method exploits problem structure by taking as a proposal density a local Gaussian approximation of the posterior pdf, whose construction is made tractable by invoking a low-rank approximation of its data misfit component of the Hessian. Here we introduce an approximation of the stochastic Newton proposal in which we compute the low-rank-based Hessian at just the MAP point, and then reuse this Hessian at each MCMC step. We compare the performance of the proposed method to the original stochastic Newton MCMC method and to an independence sampler. The comparison of the three methods is conducted on a synthetic ice sheet inverse problem. For this problem, the stochastic Newton MCMC method with a MAP-based Hessian converges at least as rapidly as the original stochastic Newton MCMC method, but is far cheaper since it avoids recomputing the Hessian at each step. On the other hand, it is more expensive per sample than the independence sampler; however, its convergence is significantly more rapid, and thus overall it is much cheaper. Finally, we present extensive analysis and interpretation of the posterior distribution, and classify directions in parameter space based on the extent to which they are informed by the prior or the observations.

</details>

<details>

<summary>2014-04-11 22:32:20 - Generalized Method of Moments Estimator Based On Semiparametric Quantile Regression Imputation</summary>

- *Senniang Chen, Cindy L Yu*

- `1404.3239v1` - [abs](http://arxiv.org/abs/1404.3239v1) - [pdf](http://arxiv.org/pdf/1404.3239v1)

> In this article, we consider an imputation method to handle missing response values based on semiparametric quantile regression estimation. In the proposed method, the missing response values are generated using the estimated conditional quantile regression function at given values of covariates. We adopt the generalized method of moments for estimation of parameters defined through a general estimation equation. We demonstrate that the proposed estimator, which combines both semiparametric quantile regression imputation and generalized method of moments, has competitive edge against some of the most widely used parametric and non-parametric imputation estimators. The consistency and the asymptotic normality of our estimator are established and variance estimation is provided. Results from a limited simulation study and an empirical study are presented to show the adequacy of the proposed method.

</details>

<details>

<summary>2014-04-14 08:04:46 - Fast nonparametric clustering of structured time-series</summary>

- *James Hensman, Magnus Rattray, Neil D. Lawrence*

- `1401.1605v2` - [abs](http://arxiv.org/abs/1401.1605v2) - [pdf](http://arxiv.org/pdf/1401.1605v2)

> In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.

</details>

<details>

<summary>2014-04-14 09:49:56 - Concentration rate and consistency of the posterior under monotonicity constraints</summary>

- *Jean-Bernard Salomond*

- `1301.1898v3` - [abs](http://arxiv.org/abs/1301.1898v3) - [pdf](http://arxiv.org/pdf/1301.1898v3)

> In this paper, we consider the well known problem of estimating a density function under qualitative assumptions. More precisely, we estimate monotone non increasing densities in a Bayesian setting and derive concentration rate for the posterior distribution for a Dirichlet process and finite mixture prior. We prove that the posterior distribution based on both priors concentrates at the rate $(n/\log(n))^{-1/3}$, which is the minimax rate of estimation up to a \log(n)$ factor. We also study the behaviour of the posterior for the point-wise loss at any fixed point of the support the density and for the sup norm. We prove that the posterior is consistent for both losses.

</details>

<details>

<summary>2014-04-14 12:23:57 - Separable factor analysis with applications to mortality data</summary>

- *Bailey K. Fosdick, Peter D. Hoff*

- `1211.3813v2` - [abs](http://arxiv.org/abs/1211.3813v2) - [pdf](http://arxiv.org/pdf/1211.3813v2)

> Human mortality data sets can be expressed as multiway data arrays, the dimensions of which correspond to categories by which mortality rates are reported, such as age, sex, country and year. Regression models for such data typically assume an independent error distribution or an error model that allows for dependence along at most one or two dimensions of the data array. However, failing to account for other dependencies can lead to inefficient estimates of regression parameters, inaccurate standard errors and poor predictions. An alternative to assuming independent errors is to allow for dependence along each dimension of the array using a separable covariance model. However, the number of parameters in this model increases rapidly with the dimensions of the array and, for many arrays, maximum likelihood estimates of the covariance parameters do not exist. In this paper, we propose a submodel of the separable covariance model that estimates the covariance matrix for each dimension as having factor analytic structure. This model can be viewed as an extension of factor analysis to array-valued data, as it uses a factor model to estimate the covariance along each dimension of the array. We discuss properties of this model as they relate to ordinary factor analysis, describe maximum likelihood and Bayesian estimation methods, and provide a likelihood ratio testing procedure for selecting the factor model ranks. We apply this methodology to the analysis of data from the Human Mortality Database, and show in a cross-validation experiment how it outperforms simpler methods. Additionally, we use this model to impute mortality rates for countries that have no mortality data for several years. Unlike other approaches, our methodology is able to estimate similarities between the mortality rates of countries, time periods and sexes, and use this information to assist with the imputations.

</details>

<details>

<summary>2014-04-14 12:49:00 - A hierarchical Bayesian model for inference of copy number variants and their association to gene expression</summary>

- *Alberto Cassese, Michele Guindani, Mahlet G. Tadesse, Francesco Falciani, Marina Vannucci*

- `1404.3560v1` - [abs](http://arxiv.org/abs/1404.3560v1) - [pdf](http://arxiv.org/pdf/1404.3560v1)

> A number of statistical models have been successfully developed for the analysis of high-throughput data from a single source, but few methods are available for integrating data from different sources. Here we focus on integrating gene expression levels with comparative genomic hybridization (CGH) array measurements collected on the same subjects. We specify a measurement error model that relates the gene expression levels to latent copy number states which, in turn, are related to the observed surrogate CGH measurements via a hidden Markov model. We employ selection priors that exploit the dependencies across adjacent copy number states and investigate MCMC stochastic search techniques for posterior inference. Our approach results in a unified modeling framework for simultaneously inferring copy number variants (CNV) and identifying their significant associations with mRNA transcripts abundance. We show performance on simulated data and illustrate an application to data from a genomic study on human cancer cell lines.

</details>

<details>

<summary>2014-04-14 13:42:20 - Bayesian methods for genetic association analysis with heterogeneous subgroups: From meta-analyses to gene-environment interactions</summary>

- *Xiaoquan Wen, Matthew Stephens*

- `1111.1210v3` - [abs](http://arxiv.org/abs/1111.1210v3) - [pdf](http://arxiv.org/pdf/1111.1210v3)

> Genetic association analyses often involve data from multiple potentially-heterogeneous subgroups. The expected amount of heterogeneity can vary from modest (e.g., a typical meta-analysis) to large (e.g., a strong gene--environment interaction). However, existing statistical tools are limited in their ability to address such heterogeneity. Indeed, most genetic association meta-analyses use a "fixed effects" analysis, which assumes no heterogeneity. Here we develop and apply Bayesian association methods to address this problem. These methods are easy to apply (in the simplest case, requiring only a point estimate for the genetic effect and its standard error, from each subgroup) and effectively include standard frequentist meta-analysis methods, including the usual "fixed effects" analysis, as special cases. We apply these tools to two large genetic association studies: one a meta-analysis of genome-wide association studies from the Global Lipids consortium, and the second a cross-population analysis for expression quantitative trait loci (eQTLs). In the Global Lipids data we find, perhaps surprisingly, that effects are generally quite homogeneous across studies. In the eQTL study we find that eQTLs are generally shared among different continental groups, and discuss consequences of this for study design.

</details>

<details>

<summary>2014-04-14 18:24:00 - Joint probabilistic forecasting of wind speed and temperature using Bayesian model averaging</summary>

- *SÃ¡ndor Baran, Annette MÃ¶ller*

- `1404.3681v1` - [abs](http://arxiv.org/abs/1404.3681v1) - [pdf](http://arxiv.org/pdf/1404.3681v1)

> Ensembles of forecasts are typically employed to account for the forecast uncertainties inherent in predictions of future weather states. However, biases and dispersion errors often present in forecast ensembles require statistical post-processing. Univariate post-processing models such as Bayesian model averaging (BMA) have been successfully applied for various weather quantities. Nonetheless, BMA and many other standard post-processing procedures are designed for a single weather variable, thus ignoring possible dependencies among weather quantities. In line with recently upcoming research to develop multivariate post-processing procedures, e.g., BMA for bivariate wind vectors, or flexible procedures applicable for multiple weather quantities of different types, a bivariate BMA model for joint calibration of wind speed and temperature forecasts is proposed based on the bivariate truncated normal distribution. It extends the univariate truncated normal BMA model designed for post-processing ensemble forecast of wind speed by adding a normally distributed temperature component with a covariance structure representing the dependency among the two weather quantities.   The method is applied to wind speed and temperature forecasts of the eight-member University of Washington mesoscale ensemble and of the eleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service and its predictive performance is compared to that of the general Gaussian copula method. The results indicate improved calibration of probabilistic and accuracy of point forecasts in comparison to the raw ensemble and the overall performance of this model is able to keep up with that of the Gaussian copula method.

</details>

<details>

<summary>2014-04-16 00:44:21 - Bayesian Neural Networks for Genetic Association Studies of Complex Disease</summary>

- *Andrew L. Beam, Alison Motsinger-Reif, Jon Doyle*

- `1404.3989v2` - [abs](http://arxiv.org/abs/1404.3989v2) - [pdf](http://arxiv.org/pdf/1404.3989v2)

> Discovering causal genetic variants from large genetic association studies poses many difficult challenges. Assessing which genetic markers are involved in determining trait status is a computationally demanding task, especially in the presence of gene-gene interactions. A non-parametric Bayesian approach in the form of a Bayesian neural network is proposed for use in analyzing genetic association studies. Demonstrations on synthetic and real data reveal they are able to efficiently and accurately determine which variants are involved in determining case-control status. Using graphics processing units (GPUs) the time needed to build these models is decreased by several orders of magnitude. In comparison with commonly used approaches for detecting genetic interactions, Bayesian neural networks perform very well across a broad spectrum of possible genetic relationships while having the computational efficiency needed to handle large datasets.

</details>

<details>

<summary>2014-04-16 14:44:53 - Fast and exact implementation of 3-dimensional Tukey depth regions</summary>

- *Xiaohui Liu*

- `1404.4272v1` - [abs](http://arxiv.org/abs/1404.4272v1) - [pdf](http://arxiv.org/pdf/1404.4272v1)

> Tukey depth regions are important notions in nonparametric multivariate data analysis. A $\tau$-th Tukey depth region $\mathcal{D}_{\tau}$ is the set of all points that have at least depth $\tau$. While the Tukey depth regions are easily defined and interpreted as $p$-variate quantiles, their practical applications is impeded by the lack of efficient computational procedures in dimensions with $p > 2$. Feasible algorithms are available, but practically very slow. In this paper we present a new exact algorithm for 3-dimensional data. An efficient implementation is also provided. Data examples indicate that the proposed algorithm runs much faster than the existing ones.

</details>

<details>

<summary>2014-04-16 15:50:48 - Communication Communities in MOOCs</summary>

- *Nabeel Gillani, Rebecca Eynon, Michael Osborne, Isis Hjorth, Stephen Roberts*

- `1403.4640v2` - [abs](http://arxiv.org/abs/1403.4640v2) - [pdf](http://arxiv.org/pdf/1403.4640v2)

> Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.

</details>

<details>

<summary>2014-04-16 19:12:47 - Stable Graphical Models</summary>

- *Navodit Misra, Ercan E. Kuruoglu*

- `1404.4351v1` - [abs](http://arxiv.org/abs/1404.4351v1) - [pdf](http://arxiv.org/pdf/1404.4351v1)

> Stable random variables are motivated by the central limit theorem for densities with (potentially) unbounded variance and can be thought of as natural generalizations of the Gaussian distribution to skewed and heavy-tailed phenomenon. In this paper, we introduce stable graphical (SG) models, a class of multivariate stable densities that can also be represented as Bayesian networks whose edges encode linear dependencies between random variables. One major hurdle to the extensive use of stable distributions is the lack of a closed-form analytical expression for their densities. This makes penalized maximum-likelihood based learning computationally demanding. We establish theoretically that the Bayesian information criterion (BIC) can asymptotically be reduced to the computationally more tractable minimum dispersion criterion (MDC) and develop StabLe, a structure learning algorithm based on MDC. We use simulated datasets for five benchmark network topologies to empirically demonstrate how StabLe improves upon ordinary least squares (OLS) regression. We also apply StabLe to microarray gene expression data for lymphoblastoid cells from 727 individuals belonging to eight global population groups. We establish that StabLe improves test set performance relative to OLS via ten-fold cross-validation. Finally, we develop SGEX, a method for quantifying differential expression of genes between different population groups.

</details>

<details>

<summary>2014-04-17 08:38:27 - Bias-correction of the maximum likelihood estimator for the $Î±$-Brownian bridge</summary>

- *Maik GÃ¶rgens, MÃ¥ns Thulin*

- `1404.4452v1` - [abs](http://arxiv.org/abs/1404.4452v1) - [pdf](http://arxiv.org/pdf/1404.4452v1)

> The $\alpha$-Brownian bridge, or scaled Brownian bridge, is a generalization of the Brownian bridge with a scaling parameter that determines how strong the force that pulls the process back to 0 is. The bias of the maximum likelihood estimator of the parameter $\alpha$ is derived and a bias-correction that improves the estimator substantially is proposed. The properties of the bias-corrected estimator and four Bayesian estimators based on non-informative priors are evaluated in a simulation study.

</details>

<details>

<summary>2014-04-19 01:35:59 - Real-time On and Off Road GPS Tracking</summary>

- *Brandon Willard*

- `1303.1883v2` - [abs](http://arxiv.org/abs/1303.1883v2) - [pdf](http://arxiv.org/pdf/1303.1883v2)

> This document describes a GPS-based tracking model for position and velocity states on and off of a road network and it enables parallel, online learning of state-dependent parameters, such as GPS error, acceleration error, and road transition probabilities. More specifically, the conditionally linear tracking model of Ulmke and Koch (2006) is adapted to the Particle Learning framework of H. F. Lopes, et. al. (2011), which provides a foundation for further hierarchical Bayesian extensions. The filter is shown to perform well on a real city road network while sufficiently estimating on and off road transition probabilities. The model in this paper is also backed by an open-source Java project.

</details>

<details>

<summary>2014-04-21 04:45:55 - Compressed Sensing for Energy-Efficient Wireless Telemonitoring: Challenges and Opportunities</summary>

- *Zhilin Zhang, Bhaskar D. Rao, Tzyy-Ping Jung*

- `1311.3995v2` - [abs](http://arxiv.org/abs/1311.3995v2) - [pdf](http://arxiv.org/pdf/1311.3995v2)

> As a lossy compression framework, compressed sensing has drawn much attention in wireless telemonitoring of biosignals due to its ability to reduce energy consumption and make possible the design of low-power devices. However, the non-sparseness of biosignals presents a major challenge to compressed sensing. This study proposes and evaluates a spatio-temporal sparse Bayesian learning algorithm, which has the desired ability to recover such non-sparse biosignals. It exploits both temporal correlation in each individual biosignal and inter-channel correlation among biosignals from different channels. The proposed algorithm was used for compressed sensing of multichannel electroencephalographic (EEG) signals for estimating vehicle drivers' drowsiness. Results showed that the drowsiness estimation was almost unaffected even if raw EEG signals (containing various artifacts) were compressed by 90%.

</details>

<details>

<summary>2014-04-21 15:26:30 - A comparison of nonlinear population Monte Carlo and particle Markov chain Monte Carlo algorithms for Bayesian inference in stochastic kinetic models</summary>

- *Eugenia Koblents, JoaquÃ­n MÃ­guez*

- `1404.5218v1` - [abs](http://arxiv.org/abs/1404.5218v1) - [pdf](http://arxiv.org/pdf/1404.5218v1)

> In this paper we address the problem of Monte Carlo approximation of posterior probability distributions in stochastic kinetic models (SKMs). SKMs are multivariate Markov jump processes that model the interactions among species in biochemical systems according to a set of uncertain parameters. Markov chain Monte Carlo (MCMC) methods have been typically preferred for this Bayesian inference problem. Specifically, the particle MCMC (pMCMC) method has been recently shown to be an effective, while computationally demanding, method applicable to this problem. Within the pMCMC framework, importance sampling (IS) has been used only as the basis of the sequential Monte Carlo (SMC) approximation of the acceptance ratio in the Metropolis-Hastings kernel. However, the recently proposed nonlinear population Monte Carlo (NPMC) algorithm, based on an iterative IS scheme, has also been shown to be effective as a Bayesian inference tool for low dimensional (predator-prey) SKMs. In this paper, we provide an extensive performance comparison of pMCMC versus NPMC, when applied to the challenging prokaryotic autoregulatory network. We show how the NPMC method can greatly outperform the pMCMC algorithm in this scenario, with an overall moderate computational effort. We complement the numerical comparison of the two techniques with an asymptotic convergence analysis of the nonlinear IS scheme at the core of the proposed method when the importance weights can only be computed approximately.

</details>

<details>

<summary>2014-04-23 12:11:56 - Honest Bayesian confidence sets for the L2-norm</summary>

- *Botond Szabo, Aad van der Vaart, Harry van Zanten*

- `1311.7474v2` - [abs](http://arxiv.org/abs/1311.7474v2) - [pdf](http://arxiv.org/pdf/1311.7474v2)

> We investigate the problem of constructing Bayesian credible sets that are honest and adaptive for the L2-loss over a scale of Sobolev classes with regularity ranging between [D; 2D], for some given D in the context of the signal-in-white-noise model. We consider a scale of prior distributions indexed by a regularity hyper-parameter and choose the hyper-parameter both by marginal likelihood empirical Bayes and by hierarchical Bayes method, respectively. Next we consider a ball centered around the corresponding posterior mean with prescribed posterior probability. We show by theory and examples that both the empirical Bayes and the hierarchical Bayes credible sets give misleading, overconfident uncertainty quantification for certain oddly behaving truth. Then we construct a new empirical Bayes method based on risk estimation, which provides the correct uncertainty quantification and optimal size.

</details>

<details>

<summary>2014-04-24 08:52:28 - Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling</summary>

- *Xi Chen, Qihang Lin, Dengyong Zhou*

- `1403.3080v2` - [abs](http://arxiv.org/abs/1403.3080v2) - [pdf](http://arxiv.org/pdf/1403.3080v2)

> In crowd labeling, a large amount of unlabeled data instances are outsourced to a crowd of workers. Workers will be paid for each label they provide, but the labeling requester usually has only a limited amount of the budget. Since data instances have different levels of labeling difficulty and workers have different reliability, it is desirable to have an optimal policy to allocate the budget among all instance-worker pairs such that the overall labeling accuracy is maximized. We consider categorical labeling tasks and formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. Using the dynamic programming (DP) recurrence, one can obtain the optimal allocation policy. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy, called optimistic knowledge gradient policy. Our MDP is a quite general framework, which applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that the proposed policy achieves a higher labeling accuracy than other existing policies at the same budget level.

</details>

<details>

<summary>2014-04-24 18:55:55 - Approximate Bayesian Computation by Subset Simulation</summary>

- *Manuel Chiachio, James L. Beck, Juan Chiachio, Guillermo Rus*

- `1404.6225v1` - [abs](http://arxiv.org/abs/1404.6225v1) - [pdf](http://arxiv.org/pdf/1404.6225v1)

> A new Approximate Bayesian Computation (ABC) algorithm for Bayesian updating of model parameters is proposed in this paper, which combines the ABC principles with the technique of Subset Simulation for efficient rare-event simulation, first developed in S.K. Au and J.L. Beck [1]. It has been named ABC- SubSim. The idea is to choose the nested decreasing sequence of regions in Subset Simulation as the regions that correspond to increasingly closer approximations of the actual data vector in observation space. The efficiency of the algorithm is demonstrated in two examples that illustrate some of the challenges faced in real-world applications of ABC. We show that the proposed algorithm outperforms other recent sequential ABC algorithms in terms of computational efficiency while achieving the same, or better, measure of ac- curacy in the posterior distribution. We also show that ABC-SubSim readily provides an estimate of the evidence (marginal likelihood) for posterior model class assessment, as a by-product.

</details>

<details>

<summary>2014-04-25 10:02:09 - Exploring a New Class of Non-stationary Spatial Gaussian Random Fields with Varying Local Anisotropy</summary>

- *Geir-Arne Fuglstad, Finn Lindgren, Daniel Simpson, HÃ¥vard Rue*

- `1304.6949v2` - [abs](http://arxiv.org/abs/1304.6949v2) - [pdf](http://arxiv.org/pdf/1304.6949v2)

> Gaussian random fields (GRFs) constitute an important part of spatial modelling, but can be computationally infeasible for general covariance structures. An efficient approach is to specify GRFs via stochastic partial differential equations (SPDEs) and derive Gaussian Markov random field (GMRF) approximations of the solutions. We consider the construction of a class of non-stationary GRFs with varying local anisotropy, where the local anisotropy is introduced by allowing the coefficients in the SPDE to vary with position. This is done by using a form of diffusion equation driven by Gaussian white noise with a spatially varying diffusion matrix. This allows for the introduction of parameters that control the GRF by parametrizing the diffusion matrix. These parameters and the GRF may be considered to be part of a hierarchical model and the parameters estimated in a Bayesian framework. The results show that the use of an SPDE with non-constant coefficients is a promising way of creating non-stationary spatial GMRFs that allow for physical interpretability of the parameters, although there are several remaining challenges that would need to be solved before these models can be put to general practical use.

</details>

<details>

<summary>2014-04-25 10:29:50 - PReMiuM: An R Package for Profile Regression Mixture Models using Dirichlet Processes</summary>

- *Silvia Liverani, David I. Hastie, Lamiae Azizi, Michail Papathomas, Sylvia Richardson*

- `1303.2836v3` - [abs](http://arxiv.org/abs/1303.2836v3) - [pdf](http://arxiv.org/pdf/1303.2836v3)

> PReMiuM is a recently developed R package for Bayesian clustering using a Dirichlet process mixture model. This model is an alternative to regression models, non-parametrically linking a response vector to covariate data through cluster membership. The package allows Bernoulli, Binomial, Poisson, Normal and categorical response, as well as Normal and discrete covariates. Additionally, predictions may be made for the response, and missing values for the covariates are handled. Several samplers and label switching moves are implemented along with diagnostic tools to assess convergence. A number of R functions for post-processing of the output are also provided. In addition to fitting mixtures, it may additionally be of interest to determine which covariates actively drive the mixture components. This is implemented in the package as variable selection.

</details>

<details>

<summary>2014-04-25 14:38:45 - Bayes Sensitivity with Fisher-Rao Metric</summary>

- *Sebastian Kurtek, Karthik Bharath*

- `1403.5150v2` - [abs](http://arxiv.org/abs/1403.5150v2) - [pdf](http://arxiv.org/pdf/1403.5150v2)

> We propose a geometric framework to assess sensitivity of Bayesian procedures to modeling assumptions based on the nonparametric Fisher-Rao metric. While the framework is general in spirit, the focus of this article is restricted to metric-based diagnosis under two settings: assessing local and global robustness in Bayesian procedures to perturbations of the likelihood and prior, and identification of influential observations. The approach is based on the square-root representation of densities which enables one to compute geodesics and geodesic distances in analytical form, facilitating the definition of naturally calibrated local and global discrepancy measures. An important feature of our approach is the definition of a geometric $\epsilon$-contamination class of sampling distributions and priors via intrinsic analysis on the space of probability density functions. We showcase the applicability of our framework on several simulated toy datasets as well as in real data settings for generalized mixed effects models, directional data and shape data.

</details>

<details>

<summary>2014-04-26 17:57:40 - Statistical handling of reproduction data for exposure-response modelling</summary>

- *Marie Laure Delignette-Muller, Christelle Lopes, Philippe Veber, Sandrine Charles*

- `1310.2733v3` - [abs](http://arxiv.org/abs/1310.2733v3) - [pdf](http://arxiv.org/pdf/1310.2733v3)

> Reproduction data collected through standard bioassays are classically analyzed by regression in order to fit exposure-response curves and estimate ECx values (x% Effective Concentration). But regression is often misused on such data, ignoring statistical issues related to i) the special nature of reproduction data (count data), ii) a potential inter-replicate variability and iii) a possible concomitant mortality. This paper offers new insights in dealing with those issues. Concerning mortality, particular attention was paid not to waste any valuable data - by dropping all the replicates with mortality - or to bias ECx values. For that purpose we defined a new covariate summing the observation periods during which each individual contributes to the reproduction process. This covariate was then used to quantify reproduction - for each replicate at each concentration - as a number of offspring per individual-day. We formulated three exposure-response models differing by their stochastic part. Those models were fitted to four datasets and compared using a Bayesian framework. The individual-day unit proved to be a suitable approach to use all the available data and prevent bias in the estimation of ECx values. Furthermore, a non-classical negative-binomial model was shown to correctly describe the inter-replicate variability observed in the studied datasets.

</details>

<details>

<summary>2014-04-28 06:29:02 - On the log quantile difference of the temporal aggregation of a stable moving average process</summary>

- *Adrian W. Barker*

- `1404.6875v1` - [abs](http://arxiv.org/abs/1404.6875v1) - [pdf](http://arxiv.org/pdf/1404.6875v1)

> A formula is derived for the log quantile difference of the temporal aggregation of some types of stable moving average processes, MA(q). The shape of the log quantile difference as a function of the aggregation level is examined and shown to be dependent on the parameters of the moving average process but not the quantile levels. The classes of invertible, stable MA(1) and MA(2) processes are examined in more detail.

</details>

<details>

<summary>2014-04-29 10:08:34 - A semi-parametric Bayesian model of inter- and intra-examiner agreement for periodontal probing depth</summary>

- *E. G. Hill, E. H. Slate*

- `1404.7295v1` - [abs](http://arxiv.org/abs/1404.7295v1) - [pdf](http://arxiv.org/pdf/1404.7295v1)

> Periodontal probing depth is a measure of periodontitis severity. We develop a Bayesian hierarchical model linking true pocket depth to both observed and recorded values of periodontal probing depth, while permitting correlation among measures obtained from the same mouth and between duplicate examiners' measures obtained at the same periodontal site. Periodontal site-specific examiner effects are modeled as arising from a Dirichlet process mixture, facilitating identification of classes of sites that are measured with similar bias. Using simulated data, we demonstrate the model's ability to recover examiner site-specific bias and variance heterogeneity and to provide cluster-adjusted point and interval agreement estimates. We conclude with an analysis of data from a probing depth calibration training exercise.

</details>

<details>

<summary>2014-04-30 05:43:03 - A Quantile Regression Model for Failure-Time Data with Time-Dependent Covariates</summary>

- *Malka Gorfine, Yair Goldberg, Yaacov Ritov*

- `1404.7595v1` - [abs](http://arxiv.org/abs/1404.7595v1) - [pdf](http://arxiv.org/pdf/1404.7595v1)

> Since survival data occur over time, often important covariates that we wish to consider also change over time. Such covariates are referred as time-dependent covariates. Quantile regression offers flexible modeling of survival data by allowing the covariates to vary with quantiles. This paper provides a novel quantile regression model accommodating time-dependent covariates, for analyzing survival data subject to right censoring. Our simple estimation technique assumes the existence of instrumental variables. In addition, we present a doubly-robust estimator in the sense of Robins and Rotnitzky (1992). The asymptotic properties of the estimators are rigorously studied. Finite-sample properties are demonstrated by a simulation study. The utility of the proposed methodology is demonstrated using the Stanford heart transplant dataset.

</details>

<details>

<summary>2014-04-30 08:19:50 - The R Package JMbayes for Fitting Joint Models for Longitudinal and Time-to-Event Data using MCMC</summary>

- *Dimitris Rizopoulos*

- `1404.7625v1` - [abs](http://arxiv.org/abs/1404.7625v1) - [pdf](http://arxiv.org/pdf/1404.7625v1)

> Joint models for longitudinal and time-to-event data constitute an attractive modeling framework that has received a lot of interest in the recent years. This paper presents the capabilities of the R package JMbayes for fitting these models under a Bayesian approach using Markon chain Monte Carlo algorithms. JMbayes can fit a wide range of joint models, including among others joint models for continuous and categorical longitudinal responses, and provides several options for modeling the association structure between the two outcomes. In addition, this package can be used to derive dynamic predictions for both outcomes, and offers several tools to validate these predictions in terms of discrimination and calibration. All these features are illustrated using a real data example on patients with primary biliary cirrhosis.

</details>

<details>

<summary>2014-04-30 09:44:35 - The role of the information set for forecasting - with applications to risk management</summary>

- *Hajo Holzmann, Matthias Eulert*

- `1404.7653v1` - [abs](http://arxiv.org/abs/1404.7653v1) - [pdf](http://arxiv.org/pdf/1404.7653v1)

> Predictions are issued on the basis of certain information. If the forecasting mechanisms are correctly specified, a larger amount of available information should lead to better forecasts. For point forecasts, we show how the effect of increasing the information set can be quantified by using strictly consistent scoring functions, where it results in smaller average scores. Further, we show that the classical Diebold-Mariano test, based on strictly consistent scoring functions and asymptotically ideal forecasts, is a consistent test for the effect of an increase in a sequence of information sets on $h$-step point forecasts. For the value at risk (VaR), we show that the average score, which corresponds to the average quantile risk, directly relates to the expected shortfall. Thus, increasing the information set will result in VaR forecasts which lead on average to smaller expected shortfalls. We illustrate our results in simulations and applications to stock returns for unconditional versus conditional risk management as well as univariate modeling of portfolio returns versus multivariate modeling of individual risk factors. The role of the information set for evaluating probabilistic forecasts by using strictly proper scoring rules is also discussed.

</details>

<details>

<summary>2014-04-30 13:06:53 - Identification of Outlying Observations with Quantile Regression for Censored Data</summary>

- *Soo-Heang Eo, Seung-Mo Hong, HyungJun Cho*

- `1404.7710v1` - [abs](http://arxiv.org/abs/1404.7710v1) - [pdf](http://arxiv.org/pdf/1404.7710v1)

> Outlying observations, which significantly deviate from other measurements, may distort the conclusions of data analysis. Therefore, identifying outliers is one of the important problems that should be solved to obtain reliable results. While there are many statistical outlier detection algorithms and software programs for uncensored data, few are available for censored data. In this article, we propose three outlier detection algorithms based on censored quantile regression, two of which are modified versions of existing algorithms for uncensored or censored data, while the third is a newly developed algorithm to overcome the demerits of previous approaches. The performance of the three algorithms was investigated in simulation studies. In addition, real data from SEER database, which contains a variety of data sets related to various cancers, is illustrated to show the usefulness of our methodology. The algorithms are implemented into an R package OutlierDC which can be conveniently employed in the \proglang{R} environment and freely obtained from CRAN.

</details>

<details>

<summary>2014-04-30 23:42:52 - Latent Self-Exciting Point Process Model for Spatial-Temporal Networks</summary>

- *Yoon-Sik Cho, Aram Galstyan, P. Jeffrey Brantingham, George Tita*

- `1302.2671v3` - [abs](http://arxiv.org/abs/1302.2671v3) - [pdf](http://arxiv.org/pdf/1302.2671v3)

> We propose a latent self-exciting point process model that describes geographically distributed interactions between pairs of entities. In contrast to most existing approaches that assume fully observable interactions, here we consider a scenario where certain interaction events lack information about participants. Instead, this information needs to be inferred from the available observations. We develop an efficient approximate algorithm based on variational expectation-maximization to infer unknown participants in an event given the location and the time of the event. We validate the model on synthetic as well as real-world data, and obtain very promising results on the identity-inference task. We also use our model to predict the timing and participants of future events, and demonstrate that it compares favorably with baseline approaches.

</details>


## 2014-05

<details>

<summary>2014-05-01 10:39:05 - The Controlled Thermodynamic Integral for Bayesian Model Comparison</summary>

- *Chris J. Oates, Theodore Papamarkou, Mark Girolami*

- `1404.5053v2` - [abs](http://arxiv.org/abs/1404.5053v2) - [pdf](http://arxiv.org/pdf/1404.5053v2)

> Bayesian model comparison relies upon the model evidence, yet for many models of interest the model evidence is unavailable in closed form and must be approximated. Many of the estimators for evidence that have been proposed in the Monte Carlo literature suffer from high variability. This paper considers the reduction of variance that can be achieved by exploiting control variates in this setting. Our methodology is based on thermodynamic integration and applies whenever the gradient of both the log-likelihood and the log-prior with respect to the parameters can be efficiently evaluated. Results obtained on regression models and popular benchmark datasets demonstrate a significant and sometimes dramatic reduction in estimator variance and provide insight into the wider applicability of control variates to Bayesian model comparison.

</details>

<details>

<summary>2014-05-02 09:44:45 - Cover Tree Bayesian Reinforcement Learning</summary>

- *Nikolaos Tziortziotis, Christos Dimitrakakis, Konstantinos Blekas*

- `1305.1809v2` - [abs](http://arxiv.org/abs/1305.1809v2) - [pdf](http://arxiv.org/pdf/1305.1809v2)

> This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with least squares policy iteration.

</details>

<details>

<summary>2014-05-02 10:27:58 - Estimation of stable distribution parameters from a dependent sample</summary>

- *Adrian W. Barker*

- `1405.0374v1` - [abs](http://arxiv.org/abs/1405.0374v1) - [pdf](http://arxiv.org/pdf/1405.0374v1)

> Existing methods for the estimation of stable distribution parameters, such as those based on sample quantiles, sample characteristic functions or maximum likelihood generally assume an independent sample. Little attention has been paid to estimation from a dependent sample. In this paper, a method for the estimation of stable distribution parameters from a dependent sample is proposed based on the sample quantiles. The estimates are shown to be asymptotically normal. The asymptotic variance is calculated for stable moving average processes. Simulations from stable moving average processes are used to demonstrate these estimators.

</details>

<details>

<summary>2014-05-02 16:36:57 - Nested Hierarchical Dirichlet Processes</summary>

- *John Paisley, Chong Wang, David M. Blei, Michael I. Jordan*

- `1210.6738v4` - [abs](http://arxiv.org/abs/1210.6738v4) - [pdf](http://arxiv.org/pdf/1210.6738v4)

> We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We derive a stochastic variational inference algorithm for the model, in addition to a greedy subtree selection method for each document, which allows for efficient inference using massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 3.3 million documents from Wikipedia.

</details>

<details>

<summary>2014-05-07 12:02:40 - PAC-Bayes Mini-tutorial: A Continuous Union Bound</summary>

- *Tim van Erven*

- `1405.1580v1` - [abs](http://arxiv.org/abs/1405.1580v1) - [pdf](http://arxiv.org/pdf/1405.1580v1)

> When I first encountered PAC-Bayesian concentration inequalities they seemed to me to be rather disconnected from good old-fashioned results like Hoeffding's and Bernstein's inequalities. But, at least for one flavour of the PAC-Bayesian bounds, there is actually a very close relation, and the main innovation is a continuous version of the union bound, along with some ingenious applications. Here's the gist of what's going on, presented from a machine learning perspective.

</details>

<details>

<summary>2014-05-07 17:17:46 - When Data do not Bring Information: A Case Study in Markov Random Fields Estimation</summary>

- *J. Gimenez, A. C. Frery, Ana Georgina Flesia*

- `1402.1734v2` - [abs](http://arxiv.org/abs/1402.1734v2) - [pdf](http://arxiv.org/pdf/1402.1734v2)

> The Potts model is frequently used to describe the behavior of image classes, since it allows to incorporate contextual information linking neighboring pixels in a simple way. Its isotropic version has only one real parameter beta, known as smoothness parameter or inverse temperature, which regulates the classes map homogeneity. The classes are unavailable, and estimating them is central in important image processing procedures as, for instance, image classification. Methods for estimating the classes which stem from a Bayesian approach under the Potts model require to adequately specify a value for beta. The estimation of such parameter can be efficiently made solving the Pseudo Maximum likelihood (PML) equations in two different schemes, using the prior or the posterior model. Having only radiometric data available, the first scheme needs the computation of an initial segmentation, while the second uses both the segmentation and the radiometric data to make the estimation. In this paper, we compare these two PML estimators by computing the mean square error (MSE), bias, and sensitivity to deviations from the hypothesis of the model. We conclude that the use of extra data does not improve the accuracy of the PML, moreover, under gross deviations from the model, this extra information introduces unpredictable distortions and bias.

</details>

<details>

<summary>2014-05-08 15:35:45 - Technical Note: Approximate Bayesian parameterization of a process-based tropical forest model</summary>

- *Florian Hartig, Claudia Dislich, Thorsten Wiegand, Andreas Huth*

- `1401.8205v2` - [abs](http://arxiv.org/abs/1401.8205v2) - [pdf](http://arxiv.org/pdf/1401.8205v2)

> Inverse parameter estimation of process-based models is a long-standing problem in many scientific disciplines. A key question for inverse parameter estimation is how to define the metric that quantifies how well model predictions fit to the data. This metric can be expressed by general cost or objective functions, but statistical inversion methods require a particular metric, the probability of observing the data given the model parameters, known as the likelihood.   For technical and computational reasons, likelihoods for process-based stochastic models are usually based on general assumptions about variability in the observed data, and not on the stochasticity generated by the model. Only in recent years have new methods become available that allow the generation of likelihoods directly from stochastic simulations. Previous applications of these approximate Bayesian methods have concentrated on relatively simple models. Here, we report on the application of a simulation-based likelihood approximation for FORMIND, a parameter-rich individual-based model of tropical forest dynamics.   We show that approximate Bayesian inference, based on a parametric likelihood approximation placed in a conventional Markov chain Monte Carlo (MCMC) sampler, performs well in retrieving known parameter values from virtual inventory data generated by the forest model. We analyze the results of the parameter estimation, examine its sensitivity to the choice and aggregation of model outputs and observed data (summary statistics), and demonstrate the application of this method by fitting the FORMIND model to field data from an Ecuadorian tropical forest. Finally, we discuss how this approach differs from approximate Bayesian computation (ABC), another method commonly used to generate simulation-based likelihood approximations.   Our results demonstrate that simulation-based inference, [...]

</details>

<details>

<summary>2014-05-09 14:38:44 - Change Point Analysis of Histone Modifications Reveals Epigenetic Blocks Linking to Physical Domains</summary>

- *Mengjie Chen, Haifan Lin, Hongyu Zhao*

- `1309.5337v2` - [abs](http://arxiv.org/abs/1309.5337v2) - [pdf](http://arxiv.org/pdf/1309.5337v2)

> Histone modification is a vital epigenetic mechanism for transcriptional control in eukaryotes. High-throughput techniques have enabled whole-genome analysis of histone modifications in recent years. However, most studies assume one combination of histone modification invariantly translates to one transcriptional output regardless of local chromatin environment. In this study we hypothesize that, the genome is organized into local domains that manifest similar enrichment pattern of histone modification, which leads to orchestrated regulation of expression of genes with relevant bio- logical functions. We propose a multivariate Bayesian Change Point (BCP) model to segment the Drosophila melanogaster genome into consecutive blocks on the basis of combinatorial patterns of histone marks. By modeling the sparse distribution of histone marks across the chromosome with a zero-inflated Gaussian mixture, our partitions capture local BLOCKs that manifest relatively homogeneous enrichment pattern of histone modifications. We further characterized BLOCKs by their transcription levels, distribution of genes, degree of co-regulation and GO enrichment. Our results demonstrate that these BLOCKs, although inferred merely from histone modifications, reveal strong relevance with physical domains, which suggest their important roles in chromatin organization and coordinated gene regulation.

</details>

<details>

<summary>2014-05-10 02:03:22 - A Hybrid Monte Carlo Architecture for Parameter Optimization</summary>

- *James Brofos*

- `1405.2377v1` - [abs](http://arxiv.org/abs/1405.2377v1) - [pdf](http://arxiv.org/pdf/1405.2377v1)

> Much recent research has been conducted in the area of Bayesian learning, particularly with regard to the optimization of hyper-parameters via Gaussian process regression. The methodologies rely chiefly on the method of maximizing the expected improvement of a score function with respect to adjustments in the hyper-parameters. In this work, we present a novel algorithm that exploits notions of confidence intervals and uncertainties to enable the discovery of the best optimal within a targeted region of the parameter space. We demonstrate the efficacy of our algorithm with respect to machine learning problems and show cases where our algorithm is competitive with the method of maximizing expected improvement.

</details>

<details>

<summary>2014-05-10 10:45:51 - A PAC-Bayesian bound for Lifelong Learning</summary>

- *Anastasia Pentina, Christoph H. Lampert*

- `1311.2838v2` - [abs](http://arxiv.org/abs/1311.2838v2) - [pdf](http://arxiv.org/pdf/1311.2838v2)

> Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.

</details>

<details>

<summary>2014-05-12 06:38:21 - Stochastic Gradient Hamiltonian Monte Carlo</summary>

- *Tianqi Chen, Emily B. Fox, Carlos Guestrin*

- `1402.4102v2` - [abs](http://arxiv.org/abs/1402.4102v2) - [pdf](http://arxiv.org/pdf/1402.4102v2)

> Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.

</details>

<details>

<summary>2014-05-12 07:52:38 - Bayesian Nonparametric Estimation for Dynamic Treatment Regimes with Sequential Transition Times</summary>

- *Yanxun Xu, Peter Mueller, Abdus S. Wahed, Peter F. Thall*

- `1405.2656v1` - [abs](http://arxiv.org/abs/1405.2656v1) - [pdf](http://arxiv.org/pdf/1405.2656v1)

> Dynamic treatment regimes in oncology and other disease areas often can be characterized by an alternating sequence of treatments or other actions and transition times between disease states. The sequence of transition states may vary substantially from patient to patient, depending on how the regime plays out, and in practice there often are many possible counterfactual outcome sequences. For evaluating the regimes, the mean final overall time may be expressed as a weighted average of the means of all possible sums of successive transitions times. A common example arises in cancer therapies where the transition times between various sequences of treatments, disease remission, disease progression, and death characterize overall survival time. For the general setting, we propose estimating mean overall outcome time by assuming a Bayesian nonparametric regression model for the logarithm of each transition time. A dependent Dirichlet process prior with Gaussian process base measure (DDP-GP) is assumed, and a joint posterior is obtained by Markov chain Monte Carlo (MCMC) sampling. We provide general guidelines for constructing a prior using empirical Bayes methods. We compare the proposed approach with inverse probability of treatment weighting. These comparisons are done by simulation studies of both single-stage and multi-stage regimes, with treatment assignment depending on baseline covariates. The method is applied to analyze a dataset arising from a clinical trial involving multi-stage chemotherapy regimes for acute leukemia. An R program for implementing the DDP-GP-based Bayesian nonparametric analysis is freely available at https://www.ma.utexas.edu/users/yxu/.

</details>

<details>

<summary>2014-05-12 08:43:56 - Particle MCMC for Bayesian Microwave Control</summary>

- *P. Minvielle, A. Todeschini, F. Caron, P. Del Moral*

- `1405.2673v1` - [abs](http://arxiv.org/abs/1405.2673v1) - [pdf](http://arxiv.org/pdf/1405.2673v1)

> We consider the problem of local radioelectric property estimation from global electromagnetic scattering measurements. This challenging ill-posed high dimensional inverse problem can be explored by intensive computations of a parallel Maxwell solver on a petaflopic supercomputer. Then, it is shown how Bayesian inference can be perfomed with a Particle Marginal Metropolis-Hastings (PMMH) approach, which includes a Rao-Blackwellised Sequential Monte Carlo algorithm with interacting Kalman filters. Material properties, including a multiple components "Debye relaxation"/"Lorenzian resonant" material model, are estimated; it is illustrated on synthetic data. Eventually, we propose different ways to deal with higher dimensional problems, from parallelization to the original introduction of efficient sequential data assimilation techniques, widely used in weather forecasting, oceanography, geophysics, etc.

</details>

<details>

<summary>2014-05-12 12:10:08 - Model Selection in Overlapping Stochastic Block Models</summary>

- *P. Latouche, E. BirmelÃ©, C. Ambroise*

- `1405.2722v1` - [abs](http://arxiv.org/abs/1405.2722v1) - [pdf](http://arxiv.org/pdf/1405.2722v1)

> Networks are a commonly used mathematical model to describe the rich set of interactions between objects of interest. Many clustering methods have been developed in order to partition such structures, among which several rely on underlying probabilistic models, typically mixture models. The relevant hidden structure may however show overlapping groups in several applications. The Overlapping Stochastic Block Model (2011) has been developed to take this phenomenon into account. Nevertheless, the problem of the choice of the number of classes in the inference step is still open. To tackle this issue, we consider the proposed model in a Bayesian framework and develop a new criterion based on a non asymptotic approximation of the marginal log-likelihood. We describe how the criterion can be computed through a variational Bayes EM algorithm, and demonstrate its efficiency by running it on both simulated and real data.

</details>

<details>

<summary>2014-05-12 14:36:45 - Conditional quantile estimation through optimal quantization</summary>

- *Isabelle Charlier, Davy Paindaveine, JÃ©rÃ´me Saracco*

- `1405.2781v1` - [abs](http://arxiv.org/abs/1405.2781v1) - [pdf](http://arxiv.org/pdf/1405.2781v1)

> In this paper, we use quantization to construct a nonparametric estimator of conditional quantiles of a scalar response $Y$ given a d-dimensional vector of covariates $X$. First we focus on the population level and show how optimal quantization of $X$, which consists in discretizing $X$ by projecting it on an appropriate grid of $N$ points, allows to approximate conditional quantiles of $Y$ given $X$. We show that this is approximation is arbitrarily good as $N$ goes to infinity and provide a rate of convergence for the approximation error. Then we turn to the sample case and define an estimator of conditional quantiles based on quantization ideas. We prove that this estimator is consistent for its fixed-$N$ population counterpart. The results are illustrated on a numerical example. Dominance of our estimators over local constant/linear ones and nearest neighbor ones is demonstrated through extensive simulations in the companion paper Charlier et al.(2014b).

</details>

<details>

<summary>2014-05-12 15:24:53 - Moving Particles: a parallel optimal Multilevel Splitting method with application in quantiles estimation and meta-model based algorithms</summary>

- *ClÃ©ment Walter*

- `1405.2800v1` - [abs](http://arxiv.org/abs/1405.2800v1) - [pdf](http://arxiv.org/pdf/1405.2800v1)

> Considering the issue of estimating small probabilities p, ie. measuring a rare domain F = {x | g(x) > q} with respect to the distribution of a random vector X, Multilevel Splitting strategies (also called Subset Simulation) aim at writing F as an intersection of less rare events (nested subsets) such that their measures are conditionally easily computable. However the definition of an appropriate sequence of nested subsets remains an open issue.   We introduce here a new approach to Multilevel Splitting methods in terms of a move of particles in the input space. This allows us to derive two main results: (1) the number of samples required to get a realisation of X in F is drastically reduced, following a Poisson law with parameter log 1/p (to be compared with 1/p for naive Monte-Carlo); and (2) we get a parallel optimal Multilevel Splitting algorithm where there is indeed no subset to define any more.   We also apply result (1) in quantile estimation producing a new parallel algorithm and derive a new strategy for the construction of first Design Of Experiments in meta-model based algorithms.

</details>

<details>

<summary>2014-05-12 16:08:35 - Objective Bayesian Model Discrimination in Follow-up Experimental Designs</summary>

- *Guido Consonni, Laura Deldossi*

- `1405.2818v1` - [abs](http://arxiv.org/abs/1405.2818v1) - [pdf](http://arxiv.org/pdf/1405.2818v1)

> An initial screening experiment may lead to ambiguous conclusions regarding the factors which are active in explaining the variation of an outcome variable: thus adding follow-up runs becomes necessary. We propose a fully Bayes objective approach to follow-up designs, using prior distributions suitably tailored to model selection. We adopt a model criterion based on a weighted average of Kullback-Leibler divergences between predictive distributions for all possible pairs of models. When applied to real data, our method produces results which compare favorably to previous analyses based on subjective weakly informative priors.

</details>

<details>

<summary>2014-05-13 20:27:13 - How to combine correlated data sets -- A Bayesian hyperparameter matrix method</summary>

- *Yin-Zhe Ma, Aaron Berndsen*

- `1309.3271v2` - [abs](http://arxiv.org/abs/1309.3271v2) - [pdf](http://arxiv.org/pdf/1309.3271v2)

> We construct a "hyperparameter matrix" statistical method for performing the joint analyses of multiple correlated astronomical data sets, in which the weights of data sets are determined by their own statistical properties. This method is a generalization of the hyperparameter method constructed by Lahav et al. (2000) and Hobson, Bridle, & Lahav (2002) which was designed to combine independent data sets. The advantage of our method is to treat correlations between multiple data sets and gives appropriate relevant weights of multiple data sets with mutual correlations. We define a new "element-wise" product, which greatly simplifies the likelihood function with hyperparameter matrix. We rigorously prove the simplified formula of the joint likelihood and show that it recovers the original hyperparameter method in the limit of no covariance between data sets. We then illustrate the method by applying it to a demonstrative toy model of fitting a straight line to two sets of data. We show that the hyperparameter matrix method can detect unaccounted systematic errors or underestimated errors in the data sets. Additionally, the ratio of Bayes' factors provides a distinct indicator of the necessity of including hyperparameters. Our example shows that the likelihood we construct for joint analyses of correlated data sets can be widely applied to many astrophysical systems.

</details>

<details>

<summary>2014-05-14 06:50:08 - Learning rates for the risk of kernel based quantile regression estimators in additive models</summary>

- *Andreas Christmann, Ding-Xuan Zhou*

- `1405.3379v1` - [abs](http://arxiv.org/abs/1405.3379v1) - [pdf](http://arxiv.org/pdf/1405.3379v1)

> Additive models play an important role in semiparametric statistics. This paper gives learning rates for regularized kernel based methods for additive models. These learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the Gaussian radial basis function kernel, provided the assumption of an additive model is valid. Additionally, a concrete example is presented to show that a Gaussian function depending only on one variable lies in a reproducing kernel Hilbert space generated by an additive Gaussian kernel, but does not belong to the reproducing kernel Hilbert space generated by the multivariate Gaussian kernel of the same variance.

</details>

<details>

<summary>2014-05-14 14:11:59 - Flexible estimation in cure survival models using Bayesian P-splines</summary>

- *Vincent Bremhorst, Philippe Lambert*

- `1312.2369v2` - [abs](http://arxiv.org/abs/1312.2369v2) - [pdf](http://arxiv.org/pdf/1312.2369v2)

> In the analysis of survival data, it is usually assumed that any unit will experience the event of interest if it is observed for a sufficient long time. However, one can explicitly assume that an unknown proportion of the population under study will never experience the monitored event. The promotion time model, which has a biological motivation, is one of the survival models taking this feature into account. The promotion time model assumes that the failure time of each subject is generated by the minimum of N latent event times which are independent with a common distribution independent of N. We propose an extension which allows the covariates to influence simultaneously the probability of being cured and the latent distribution. We estimate the latent distribution using a flexible Cox proportional hazard model where the logarithm of the baseline hazard function is specified using Bayesian P-splines. Introducing covariates in the latent distribution implies that the population hazard function might not have a proportional hazard structure. However, the use of the P-splines provides a smooth estimation of the population hazard ratio over time. We propose a restricted use of the model when the follow up of the study is not sufficiently long. A simulation study evaluating the accuracy of our methodology is presented. The proposed model is illustrated on data from the phase III Melanoma e1684 clinical trial.

</details>

<details>

<summary>2014-05-14 16:06:39 - Credal Model Averaging for classification: representing prior ignorance and expert opinions</summary>

- *Giorgio Corani, Andrea Mignatti*

- `1405.3559v1` - [abs](http://arxiv.org/abs/1405.3559v1) - [pdf](http://arxiv.org/pdf/1405.3559v1)

> Bayesian model averaging (BMA) is the state of the art approach for overcoming model uncertainty. Yet, especially on small data sets, the results yielded by BMA might be sensitive to the prior over the models. Credal Model Averaging (CMA) addresses this problem by substituting the single prior over the models by a set of priors (credal set). Such approach solves the problem of how to choose the prior over the models and automates sensitivity analysis. We discuss various CMA algorithms for building an ensemble of logistic regressors characterized by different sets of covariates. We show how CMA can be appropriately tuned to the case in which one is prior-ignorant and to the case in which instead domain knowledge is available. CMA detects prior-dependent instances, namely instances in which a different class is more probable depending on the prior over the models. On such instances CMA suspends the judgment, returning multiple classes. We thoroughly compare different BMA and CMA variants on a real case study, predicting presence of Alpine marmot burrows in an Alpine valley. We find that BMA is almost a random guesser on the instances recognized as prior-dependent by CMA.

</details>

<details>

<summary>2014-05-15 04:32:29 - Effective Bayesian Modeling of Groups of Related Count Time Series</summary>

- *Nicolas Chapados*

- `1405.3738v1` - [abs](http://arxiv.org/abs/1405.3738v1) - [pdf](http://arxiv.org/pdf/1405.3738v1)

> Time series of counts arise in a variety of forecasting applications, for which traditional models are generally inappropriate. This paper introduces a hierarchical Bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series. We derive an efficient approximate inference technique, and illustrate its performance on a number of datasets from supply chain planning.

</details>

<details>

<summary>2014-05-15 15:25:52 - Bayesian Semiparametric Hierarchical Empirical Likelihood Spatial Models</summary>

- *Aaron T. Porter, Scott H. Holan, Christopher K. Wikle*

- `1405.3880v1` - [abs](http://arxiv.org/abs/1405.3880v1) - [pdf](http://arxiv.org/pdf/1405.3880v1)

> We introduce a general hierarchical Bayesian framework that incorporates a flexible nonparametric data model specification through the use of empirical likelihood methodology, which we term semiparametric hierarchical empirical likelihood (SHEL) models. Although general dependence structures can be readily accommodated, we focus on spatial modeling, a relatively underdeveloped area in the empirical likelihood literature. Importantly, the models we develop naturally accommodate spatial association on irregular lattices and irregularly spaced point-referenced data. We illustrate our proposed framework by means of a simulation study and through three real data examples. First, we develop a spatial Fay-Herriot model in the SHEL framework and apply it to the problem of small area estimation in the American Community Survey. Next, we illustrate the SHEL model in the context of areal data (on an irregular lattice) through the North Carolina sudden infant death syndrome (SIDS) dataset. Finally, we analyze a point-referenced dataset from the North American Breeding Bird survey that considers dove counts for the state of Missouri. In all cases, we demonstrate superior performance of our model, in terms of mean squared prediction error, over standard parametric analyses.

</details>

<details>

<summary>2014-05-16 12:39:50 - Delayed acceptance particle MCMC for exact inference in stochastic kinetic models</summary>

- *Andrew Golightly, Daniel A. Henderson, Chris Sherlock*

- `1401.4369v2` - [abs](http://arxiv.org/abs/1401.4369v2) - [pdf](http://arxiv.org/pdf/1401.4369v2)

> Recently-proposed particle MCMC methods provide a flexible way of performing Bayesian inference for parameters governing stochastic kinetic models defined as Markov (jump) processes (MJPs). Each iteration of the scheme requires an estimate of the marginal likelihood calculated from the output of a sequential Monte Carlo scheme (also known as a particle filter). Consequently, the method can be extremely computationally intensive. We therefore aim to avoid most instances of the expensive likelihood calculation through use of a fast approximation. We consider two approximations: the chemical Langevin equation diffusion approximation (CLE) and the linear noise approximation (LNA). Either an estimate of the marginal likelihood under the CLE, or the tractable marginal likelihood under the LNA can be used to calculate a first step acceptance probability. Only if a proposal is accepted under the approximation do we then run a sequential Monte Carlo scheme to compute an estimate of the marginal likelihood under the true MJP and construct a second stage acceptance probability that permits exact (simulation based) inference for the MJP. We therefore avoid expensive calculations for proposals that are likely to be rejected. We illustrate the method by considering inference for parameters governing a Lotka-Volterra system, a model of gene expression and a simple epidemic process.

</details>

<details>

<summary>2014-05-16 22:19:01 - Stochastic Volatility Filtering with Intractable Likelihoods</summary>

- *Emilian Vankov, Katherine B. Ensor*

- `1405.4323v1` - [abs](http://arxiv.org/abs/1405.4323v1) - [pdf](http://arxiv.org/pdf/1405.4323v1)

> This paper is concerned with particle filtering for $\alpha$-stable stochastic volatility models. The $\alpha$-stable distribution provides a flexible framework for modeling asymmetry and heavy tails, which is useful when modeling financial returns. An issue with this distributional assumption is the lack of a closed form for the probability density function. To estimate the volatility of financial returns in this setting, we develop a novel auxiliary particle filter. The algorithm we develop can be easily applied to any hidden Markov model for which the likelihood function is intractable or computationally expensive. The approximate target distribution of our auxiliary filter is based on the idea of approximate Bayesian computation (ABC). ABC methods allow for inference on posterior quantities in situations when the likelihood of the underlying model is not available in closed form, but simulating samples from it is possible. The ABC auxiliary particle filter (ABC-APF) that we propose provides not only a good alternative to state estimation in stochastic volatility models, but it also improves on the existing ABC literature. It allows for more flexibility in state estimation while improving on the accuracy through better proposal distributions in cases when the optimal importance density of the filter is unavailable in closed form. We assess the performance of the ABC-APF on a simulated dataset from the $\alpha$-stable stochastic volatility model and compare it to other currently existing ABC filters.

</details>

<details>

<summary>2014-05-19 10:39:10 - Estimating the Distribution of Dietary Consumption Patterns</summary>

- *Raymond J. Carroll*

- `1405.4667v1` - [abs](http://arxiv.org/abs/1405.4667v1) - [pdf](http://arxiv.org/pdf/1405.4667v1)

> In the United States the preferred method of obtaining dietary intake data is the 24-hour dietary recall, yet the measure of most interest is usual or long-term average daily intake, which is impossible to measure. Thus, usual dietary intake is assessed with considerable measurement error. We were interested in estimating the population distribution of the Healthy Eating Index-2005 (HEI-2005), a multi-component dietary quality index involving ratios of interrelated dietary components to energy, among children aged 2-8 in the United States, using a national survey and incorporating survey weights. We developed a highly nonlinear, multivariate zero-inflated data model with measurement error to address this question. Standard nonlinear mixed model software such as SAS NLMIXED cannot handle this problem. We found that taking a Bayesian approach, and using MCMC, resolved the computational issues and doing so enabled us to provide a realistic distribution estimate for the HEI-2005 total score. While our computation and thinking in solving this problem was Bayesian, we relied on the well-known close relationship between Bayesian posterior means and maximum likelihood, the latter not computationally feasible, and thus were able to develop standard errors using balanced repeated replication, a survey-sampling approach.

</details>

<details>

<summary>2014-05-19 11:15:39 - Estimation of HIV Burden through Bayesian Evidence Synthesis</summary>

- *Daniela De Angelis, Anne M. Presanis, Stefano Conti, A. E. Ades*

- `1405.4679v1` - [abs](http://arxiv.org/abs/1405.4679v1) - [pdf](http://arxiv.org/pdf/1405.4679v1)

> Planning, implementation and evaluation of public health policies to control the human immunodeficiency virus (HIV) epidemic require regular monitoring of disease burden. This includes the proportion living with HIV, whether diagnosed or not, and the rate of new infections in the general population and in specific risk groups and regions. Estimation of these quantities is not straightforward: data informing them directly are not typically available, but a wealth of indirect information from surveillance systems and ad hoc studies can inform functions of these quantities. In this paper we show how the estimation problem can be successfully solved through a Bayesian evidence synthesis approach, relaxing the focus on "best available" data to which classical methods are typically restricted. This more comprehensive and flexible use of evidence has led to the adoption of our proposed approach as the official method to estimate HIV prevalence in the United Kingdom since 2005.

</details>

<details>

<summary>2014-05-19 11:32:46 - Bayesian Estimation of Population-Level Trends in Measures of Health Status</summary>

- *Mariel M. Finucane, Christopher J. Paciorek, Goodarz Danaei, Majid Ezzati*

- `1405.4682v1` - [abs](http://arxiv.org/abs/1405.4682v1) - [pdf](http://arxiv.org/pdf/1405.4682v1)

> Improving health worldwide will require rigorous quantification of population-level trends in health status. However, global-level surveys are not available, forcing researchers to rely on fragmentary country-specific data of varying quality. We present a Bayesian model that systematically combines disparate data to make country-, region- and global-level estimates of time trends in important health indicators. The model allows for time and age nonlinearity, and it borrows strength in time, age, covariates, and within and across regional country clusters to make estimates where data are sparse. The Bayesian approach allows us to account for uncertainty from the various aspects of missingness as well as sampling and parameter uncertainty. MCMC sampling allows for inference in a high-dimensional, constrained parameter space, while providing posterior draws that allow straightforward inference on the wide variety of functionals of interest. Here we use blood pressure as an example health metric. High blood pressure is the leading risk factor for cardiovascular disease, the leading cause of death worldwide. The results highlight a risk transition, with decreasing blood pressure in high-income regions and increasing levels in many lower-income regions.

</details>

<details>

<summary>2014-05-19 11:56:42 - How Bayesian Analysis Cracked the Red-State, Blue-State Problem</summary>

- *Andrew Gelman*

- `1405.4687v1` - [abs](http://arxiv.org/abs/1405.4687v1) - [pdf](http://arxiv.org/pdf/1405.4687v1)

> In the United States as in other countries, political and economic divisions cut along geographic and demographic lines. Richer people are more likely to vote for Republican candidates while poorer voters lean Democratic; this is consistent with the positions of the two parties on economic issues. At the same time, richer states on the coasts are bastions of the Democrats, while most of the generally lower-income areas in the middle of the country strongly support Republicans. During a research project lasting several years, we reconciled these patterns by fitting a series of multilevel models to perform inference on geographic and demographic subsets of the population. We were using national survey data with relatively small samples in some states, ethnic groups and income categories; this motivated the use of Bayesian inference to partially pool between fitted models and local data. Previous, non-Bayesian analyses of income and voting had failed to connect individual and state-level patterns. Now that our analysis has been done, we believe it could be replicated using non-Bayesian methods, but Bayesian inference helped us crack the problem by directly handling the uncertainty that is inherent in working with sparse data.

</details>

<details>

<summary>2014-05-19 12:15:18 - From Science to Management: Using Bayesian Networks to Learn about Lyngbya</summary>

- *Sandra Johnson, Eva Abal, Kathleen Ahern, Grant Hamilton*

- `1405.4692v1` - [abs](http://arxiv.org/abs/1405.4692v1) - [pdf](http://arxiv.org/pdf/1405.4692v1)

> Toxic blooms of Lyngbya majuscula occur in coastal areas worldwide and have major ecological, health and economic consequences. The exact causes and combinations of factors which lead to these blooms are not clearly understood. Lyngbya experts and stakeholders are a particularly diverse group, including ecologists, scientists, state and local government representatives, community organisations, catchment industry groups and local fishermen. An integrated Bayesian network approach was developed to better understand and model this complex environmental problem, identify knowledge gaps, prioritise future research and evaluate management options.

</details>

<details>

<summary>2014-05-19 12:36:46 - Experiences in Bayesian Inference in Baltic Salmon Management</summary>

- *Sakari Kuikka, Jarno Vanhatalo, Henni Pulkkinen, Samu MÃ¤ntyniemi, Jukka Corander*

- `1405.4696v1` - [abs](http://arxiv.org/abs/1405.4696v1) - [pdf](http://arxiv.org/pdf/1405.4696v1)

> We review a success story regarding Bayesian inference in fisheries management in the Baltic Sea. The management of salmon fisheries is currently based on the results of a complex Bayesian population dynamic model, and managers and stakeholders use the probabilities in their discussions. We also discuss the technical and human challenges in using Bayesian modeling to give practical advice to the public and to government officials and suggest future areas in which it can be applied. In particular, large databases in fisheries science offer flexible ways to use hierarchical models to learn the population dynamics parameters for those by-catch species that do not have similar large stock-specific data sets like those that exist for many target species. This information is required if we are to understand the future ecosystem risks of fisheries.

</details>

<details>

<summary>2014-05-19 12:54:03 - Finding the Most Distant Quasars Using Bayesian Selection Methods</summary>

- *Daniel Mortlock*

- `1405.4701v1` - [abs](http://arxiv.org/abs/1405.4701v1) - [pdf](http://arxiv.org/pdf/1405.4701v1)

> Quasars, the brightly glowing disks of material that can form around the super-massive black holes at the centres of large galaxies, are amongst the most luminous astronomical objects known and so can be seen at great distances. The most distant known quasars are seen as they were when the Universe was less than a billion years old (i.e., $\sim\!7%$ of its current age). Such distant quasars are, however, very rare, and so are difficult to distinguish from the billions of other comparably-bright sources in the night sky. In searching for the most distant quasars in a recent astronomical sky survey (the UKIRT Infrared Deep Sky Survey, UKIDSS), there were $\sim\!10^3$ apparently plausible candidates for each expected quasar, far too many to reobserve with other telescopes. The solution to this problem was to apply Bayesian model comparison, making models of the quasar population and the dominant contaminating population (Galactic stars) to utilise the information content in the survey measurements. The result was an extremely efficient selection procedure that was used to quickly identify the most promising UKIDSS candidates, one of which was subsequently confirmed as the most distant quasar known to date.

</details>

<details>

<summary>2014-05-19 13:08:26 - Bayesian Population Projections for the United Nations</summary>

- *Adrian E. Raftery, Leontine Alkema, Patrick Gerland*

- `1405.4708v1` - [abs](http://arxiv.org/abs/1405.4708v1) - [pdf](http://arxiv.org/pdf/1405.4708v1)

> The United Nations regularly publishes projections of the populations of all the world's countries broken down by age and sex. These projections are the de facto standard and are widely used by international organizations, governments and researchers. Like almost all other population projections, they are produced using the standard deterministic cohort-component projection method and do not yield statements of uncertainty. We describe a Bayesian method for producing probabilistic population projections for most countries which are projections that the United Nations could use. It has at its core Bayesian hierarchical models for the total fertility rate and life expectancy at birth. We illustrate the method and show how it can be extended to address concerns about the UN's current assumptions about the long-term distribution of fertility. The method is implemented in the R packages bayesTFR, bayesLife, bayesPop and bayesDem.

</details>

<details>

<summary>2014-05-19 13:44:53 - Search for the Wreckage of Air France Flight AF 447</summary>

- *Lawrence D. Stone, Colleen M. Keller, Thomas M. Kratzke, Johan P. Strumpfer*

- `1405.4720v1` - [abs](http://arxiv.org/abs/1405.4720v1) - [pdf](http://arxiv.org/pdf/1405.4720v1)

> In the early morning hours of June 1, 2009, during a flight from Rio de Janeiro to Paris, Air France Flight AF 447 disappeared during stormy weather over a remote part of the Atlantic carrying 228 passengers and crew to their deaths. After two years of unsuccessful search, the authors were asked by the French Bureau d'Enqu\^{e}tes et d'Analyses pour la s\'{e}curit\'{e} de l'aviation to develop a probability distribution for the location of the wreckage that accounted for all information about the crash location as well as for previous search efforts. We used a Bayesian procedure developed for search planning to produce the posterior target location distribution. This distribution was used to guide the search in the third year, and the wreckage was found with one week of undersea search. In this paper we discuss why Bayesian analysis is ideally suited to solving this problem, review previous non-Bayesian efforts, and describe the methodology used to produce the posterior probability distribution for the location of the wreck.

</details>

<details>

<summary>2014-05-19 16:48:07 - Objective Bayesian Comparison of Constrained Analysis of Variance Models</summary>

- *Guido Consonni, Roberta Paroli*

- `1405.4801v1` - [abs](http://arxiv.org/abs/1405.4801v1) - [pdf](http://arxiv.org/pdf/1405.4801v1)

> In the social sciences we are often interested in comparing models specified by parametric equality or inequality constraints. For instance, when examining three group means $\{ \mu_1, \mu_2, \mu_3\}$ through an analysis of variance (ANOVA), a model may specify that $\mu_1<\mu_2<\mu_3$, while another one may state that $\{ \mu_1=\mu_3\} <\mu_2$, and finally a third model may instead suggest that all means are unrestricted. This is a challenging problem, because it involves a combination of non-nested models, as well as nested models having the same dimension. We adopt an objective Bayesian approach, and derive the posterior probability of each model under consideration. Our method is based on the intrinsic prior methodology, with suitably modifications to accommodate equality and inequality constraints. Focussing on normal ANOVA models, a comparative assessment is carried out through simulation studies, showing that correct model identification is possible even in situations where frequentist power is low. We also present an application to real data collected in a psychological experiment.

</details>

<details>

<summary>2014-05-19 21:03:15 - Bayesian inference of Gaussian mixture models with noninformative priors</summary>

- *Colin J. Stoneking*

- `1405.4895v1` - [abs](http://arxiv.org/abs/1405.4895v1) - [pdf](http://arxiv.org/pdf/1405.4895v1)

> This paper deals with Bayesian inference of a mixture of Gaussian distributions. A novel formulation of the mixture model is introduced, which includes the prior constraint that each Gaussian component is always assigned a minimal number of data points. This enables noninformative improper priors such as the Jeffreys prior to be placed on the component parameters. We demonstrate difficulties involved in specifying a prior for the standard Gaussian mixture model, and show how the new model can be used to overcome these. MCMC methods are given for efficient sampling from the posterior of this model.

</details>

<details>

<summary>2014-05-20 00:38:08 - Bayesian estimation of possible causal direction in the presence of latent confounders using a linear non-Gaussian acyclic structural equation model with individual-specific effects</summary>

- *Shohei Shimizu, Kenneth Bollen*

- `1310.6778v2` - [abs](http://arxiv.org/abs/1310.6778v2) - [pdf](http://arxiv.org/pdf/1310.6778v2)

> We consider learning the possible causal direction of two observed variables in the presence of latent confounding variables. Several existing methods have been shown to consistently estimate causal direction assuming linear or some type of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is actually violated. In this paper, we first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that allows latent confounders to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data.

</details>

<details>

<summary>2014-05-20 07:26:11 - Galaxy Formation: Bayesian History Matching for the Observable Universe</summary>

- *Ian Vernon, Michael Goldstein, Richard Bower*

- `1405.4976v1` - [abs](http://arxiv.org/abs/1405.4976v1) - [pdf](http://arxiv.org/pdf/1405.4976v1)

> Cosmologists at the Institute of Computational Cosmology, Durham University, have developed a state of the art model of galaxy formation known as Galform, intended to contribute to our understanding of the formation, growth and subsequent evolution of galaxies in the presence of dark matter. Galform requires the specification of many input parameters and takes a significant time to complete one simulation, making comparison between the model's output and real observations of the Universe extremely challenging. This paper concerns the analysis of this problem using Bayesian emulation within an iterative history matching strategy, and represents the most detailed uncertainty analysis of a galaxy formation simulation yet performed.

</details>

<details>

<summary>2014-05-20 07:36:19 - Discussion of Big Bayes Stories and BayesBag</summary>

- *Peter BÃ¼hlmann*

- `1405.4977v1` - [abs](http://arxiv.org/abs/1405.4977v1) - [pdf](http://arxiv.org/pdf/1405.4977v1)

> I congratulate all the authors for their insightful papers with wide-ranging contributions. The articles demonstrate the power and elegance of the Bayesian inference paradigm. In particular, it allows to incorporate prior knowledge as well as hierarchical model building in a convincing way. Regarding the latter, the contribution by Raftery, Alkema and German is a very fascinating piece, as it addresses a set of problems of great public interest and presents predictions for the world populations and other interesting quantities with uncertainty regions. Their approach is based on a hierarchical model, taking various characteristics into account (e.g., fertility projections). It would have been very difficult to come up with a "better" solution which would be as clear in terms of interpretation (in contrast to a "black-box machine") and which would provide (model-based) uncertainties for the predictions into the future.

</details>

<details>

<summary>2014-05-20 08:15:31 - Wonderful Examples, but Let's not Close Our Eyes</summary>

- *David J. Hand*

- `1405.4986v1` - [abs](http://arxiv.org/abs/1405.4986v1) - [pdf](http://arxiv.org/pdf/1405.4986v1)

> The papers in this collection are superb illustrations of the power of modern Bayesian methods. They give examples of problems which are well suited to being tackled using such methods, but one must not lose sight of the merits of having multiple different strategies and tools in one's inferential armoury.

</details>

<details>

<summary>2014-05-20 10:05:39 - Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models</summary>

- *Dimitris Fouskakis, Ioannis Ntzoufras, David Draper*

- `1307.2442v2` - [abs](http://arxiv.org/abs/1307.2442v2) - [pdf](http://arxiv.org/pdf/1307.2442v2)

> In the context of the expected-posterior prior (EPP) approach to Bayesian variable selection in linear models, we combine ideas from power-prior and unit-information-prior methodologies to simultaneously produce a minimally-informative prior and diminish the effect of training samples. The result is that in practice our power-expected-posterior (PEP) methodology is sufficiently insensitive to the size n* of the training sample, due to PEP's unit-information construction, that one may take n* equal to the full-data sample size n and dispense with training samples altogether. In this paper we focus on Gaussian linear models and develop our method under two different baseline prior choices: the independence Jeffreys (or reference) prior, yielding the J-PEP posterior, and the Zellner g-prior, leading to Z-PEP. We find that, under the reference baseline prior, the asymptotics of PEP Bayes factors are equivalent to those of Schwartz's BIC criterion, ensuring consistency of the PEP approach to model selection. We compare the performance of our method, in simulation studies and a real example involving prediction of air-pollutant concentrations from meteorological covariates, with that of a variety of previously-defined variants on Bayes factors for objective variable selection. Our prior, due to its unit-information structure, leads to a variable-selection procedure that (1) is systematically more parsimonious than the basic EPP with minimal training sample, while sacrificing no desirable performance characteristics to achieve this parsimony; (2) is robust to the size of the training sample, thus enjoying the advantages described above arising from the avoidance of training samples altogether; and (3) identifies maximum-a-posteriori models that achieve good out-of-sample predictive performance.

</details>

<details>

<summary>2014-05-20 12:11:27 - Selecting a Biased-Coin Design</summary>

- *Anthony C. Atkinson*

- `1405.5051v1` - [abs](http://arxiv.org/abs/1405.5051v1) - [pdf](http://arxiv.org/pdf/1405.5051v1)

> Biased-coin designs are used in clinical trials to allocate treatments with some randomness while maintaining approximately equal allocation. More recent rules are compared with Efron's [Biometrika 58 (1971) 403-417] biased-coin rule and extended to allow balance over covariates. The main properties are loss of information, due to imbalance, and selection bias. Theoretical results, mostly large sample, are assembled and assessed by small-sample simulations. The properties of the rules fall into three clear categories. A Bayesian rule is shown to have appealing properties; at the cost of slight imbalance, bias is virtually eliminated for large samples.

</details>

<details>

<summary>2014-05-20 19:19:30 - Scalable Recommendation with Poisson Factorization</summary>

- *Prem Gopalan, Jake M. Hofman, David M. Blei*

- `1311.1704v3` - [abs](http://arxiv.org/abs/1311.1704v3) - [pdf](http://arxiv.org/pdf/1311.1704v3)

> We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods.

</details>

<details>

<summary>2014-05-21 06:29:26 - A moderate deviation principle for empirical bootstrap measure</summary>

- *Mikhail Ermakov*

- `1206.1459v2` - [abs](http://arxiv.org/abs/1206.1459v2) - [pdf](http://arxiv.org/pdf/1206.1459v2)

> We prove two Large deviations principles (LDP) in the zone of moderate deviation probabilities. First we establish LDP for the conditional distributions of moderate deviations of empirical bootstrap measures given empirical probability measures. Second we establish LDP for the joint distributions of empirical measure and bootstrap empirical measures. Using these LDPs, similar LDPs for statistical differentiable functionals can be established. The LDPs for moderate deviations of empirical quantile processes and empirical bootstrap copula function are provided as illustration of these results.

</details>

<details>

<summary>2014-05-21 10:57:41 - An adaptive truncation method for inference in Bayesian nonparametric models</summary>

- *Jim E. Griffin*

- `1308.2045v2` - [abs](http://arxiv.org/abs/1308.2045v2) - [pdf](http://arxiv.org/pdf/1308.2045v2)

> Many exact Markov chain Monte Carlo algorithms have been developed for posterior inference in Bayesian nonparametric models which involve infinite-dimensional priors. However, these methods are not generic and special methodology must be developed for different classes of prior or different models. Alternatively, the infinite-dimensional prior can be truncated and standard Markov chain Monte Carlo methods used for inference. However, the error in approximating the infinite-dimensional posterior can be hard to control for many models. This paper describes an adaptive truncation method which allows the level of the truncation to be decided by the algorithm and so can avoid large errors in approximating the posterior. A sequence of truncated priors is constructed which are sampled using Markov chain Monte Carlo methods embedded in a sequential Monte Carlo algorithm. Implementational details for infinite mixture models with stick-breaking priors and normalized random measures with independent increments priors are discussed. The methodology is illustrated on infinite mixture models, a semiparametric linear mixed model and a nonparametric time series model.

</details>

<details>

<summary>2014-05-22 17:58:37 - Parameter Estimates of General Failure Rate Model: A Bayesian Approach</summary>

- *Asok K. Nanda, Sudhansu S. Maiti, Chanchal Kundu, Amarjit Kundu*

- `1405.5841v1` - [abs](http://arxiv.org/abs/1405.5841v1) - [pdf](http://arxiv.org/pdf/1405.5841v1)

> The failure rate function plays an important role in studying the lifetime distributions in reliability theory and life testing models. A study of the general failure rate model $r(t)=a+bt^{\theta-1}$, under squared error loss function taking $a$ and $b$ independent exponential random variables has been analyzed in the literature. In this article, we consider $a$ and $b$ not necessarily independent. The estimates of the parameters $a$ and $b$ under squared error loss, linex loss and entropy loss functions are obtained here.

</details>

<details>

<summary>2014-05-23 17:27:15 - Bayesian multivariate mixed-scale density estimation</summary>

- *Antonio Canale, David B. Dunson*

- `1110.1265v4` - [abs](http://arxiv.org/abs/1110.1265v4) - [pdf](http://arxiv.org/pdf/1110.1265v4)

> Although continuous density estimation has received abundant attention in the Bayesian nonparametrics literature, there is limited theory on multivariate mixed scale density estimation. In this note, we consider a general framework to jointly model continuous, count and categorical variables under a nonparametric prior, which is induced through rounding latent variables having an unknown density with respect to Lebesgue measure. For the proposed class of priors, we provide sufficient conditions for large support, strong consistency and rates of posterior contraction. These conditions allow one to convert sufficient conditions obtained in the setting of multivariate continuous density estimation to the mixed scale case. To illustrate the procedure a rounded multivariate nonparametric mixture of Gaussians is introduced and applied to a crime and communities dataset.

</details>

<details>

<summary>2014-05-24 05:08:09 - The inverse Lindley distribution: A stress-strength reliability model</summary>

- *Vikas Kumar Sharma, Sanjay Kumar Singh, Umesh Singh, Varun Agiwal*

- `1405.6268v1` - [abs](http://arxiv.org/abs/1405.6268v1) - [pdf](http://arxiv.org/pdf/1405.6268v1)

> In this article, we proposed an inverse Lindley distribution and studied its fundamental properties such as quantiles, mode, stochastic ordering and entropy measure. The proposed distribution is observed to be a heavy-tailed distribution and has a upside-down bathtub shape for its failure rate. Further, we proposed its applicability as a stress-strength reliability model for survival data analysis. The estimation of stress-strength parameters and $R=P[X>Y]$, the stress-strength reliability has been approached by both classical and Bayesian paradigms. Under Bayesian set-up, non-informative (Jeffrey) and informative (gamma) priors are considered under a symmetric (squared error) and a asymmetric (entropy) loss functions, and a Lindley-approximation technique is used for Bayesian computation. The proposed estimators are compared in terms of their mean squared errors through a simulation study. Two real data sets representing survival of Head and Neck cancer patients are fitted using the inverse Lindley distribution and used to estimate the stress-strength parameters and reliability.

</details>

<details>

<summary>2014-05-24 13:54:50 - Inadmissibility of the best equivariant predictive density in the unknown variance case</summary>

- *AurÃ©lie Boisbunon, Yuzo Maruyama*

- `1308.2765v3` - [abs](http://arxiv.org/abs/1308.2765v3) - [pdf](http://arxiv.org/pdf/1308.2765v3)

> In this work, we are concerned with the estimation of the predictive density of a Gaussian random vector where both the mean and the variance are unknown. In such a context, we prove the inadmissibility of the best equivariant predictive density under the Kullback-Leibler risk in a nonasymptotic framework. Our result stands whatever the dimension d of the vector is, even when d<=2, which can be somewhat surprising compared to the known variance setting. We also propose a class of priors leading to a Bayesian predictive density that dominates the best equivariant one. Throughout the article, we give several elements that we believe are useful for establishing the parallel between the prediction and the estimation problems, as it was done in the known variance framework.

</details>

<details>

<summary>2014-05-25 14:29:51 - Parameter estimation of beta-geometric model with application to human fecundability data</summary>

- *B. P. Singh, P. S. Pudir, Sonam Maheshwari*

- `1405.6392v1` - [abs](http://arxiv.org/abs/1405.6392v1) - [pdf](http://arxiv.org/pdf/1405.6392v1)

> The present study deals with the estimation of the mean value of fecundability by fitting a theoretical distribution from the observed month of first conception of the married women who did not use any contraceptive method before their first conception. It is assumed that fecundability is fixed for a given couple, but across couples it varies according to a specified distribution. Under the classical approach, methods of moment and maximum likelihood are used while for Bayesian approach we use the above two estimates as prior for fecundability parameter. A real data analysis from the third National Family Health Survey (NFHS-III) is analyzed as an application of model. Finally, a simulation study is performed to access the performance of the several of methods used in this paper

</details>

<details>

<summary>2014-05-25 18:46:47 - Parallelizing MCMC via Weierstrass Sampler</summary>

- *Xiangyu Wang, David B. Dunson*

- `1312.4605v2` - [abs](http://arxiv.org/abs/1312.4605v2) - [pdf](http://arxiv.org/pdf/1312.4605v2)

> With the rapidly growing scales of statistical problems, subset based communication-free parallel MCMC methods are a promising future for large scale Bayesian analysis. In this article, we propose a new Weierstrass sampler for parallel MCMC based on independent subsets. The new sampler approximates the full data posterior samples via combining the posterior draws from independent subset MCMC chains, and thus enjoys a higher computational efficiency. We show that the approximation error for the Weierstrass sampler is bounded by some tuning parameters and provide suggestions for choice of the values. Simulation study shows the Weierstrass sampler is very competitive compared to other methods for combining MCMC chains generated for subsets, including averaging and kernel smoothing.

</details>

<details>

<summary>2014-05-26 05:05:54 - Bayesian likelihood-free localisation of a biochemical source using multiple dispersion models</summary>

- *Branko Ristic, Ajith Gunatilaka, Ralph Gailis, Alex Skvortsov*

- `1405.6460v1` - [abs](http://arxiv.org/abs/1405.6460v1) - [pdf](http://arxiv.org/pdf/1405.6460v1)

> Localisation of a source of a toxic release of biochemical aerosols in the atmosphere is a problem of great importance for public safety. Two main practical difficulties are encountered in this problem: the lack of knowledge of the likelihood function of measurements collected by biochemical sensors, and the plethora of candidate dispersion models, developed under various assumptions (e.g. meteorological conditions, terrain). Aiming to overcome these two difficulties, the paper proposes a likelihood-free approximate Bayesian computation method, which simultaneously uses a set of candidate dispersion models, to localise the source. This estimation framework is implemented via the Monte Carlo method and tested using two experimental datasets.

</details>

<details>

<summary>2014-05-26 07:56:10 - Bayesian hierarchical modeling of extreme hourly precipitation in Norway</summary>

- *Anita V. Dyrrdal, Alex Lenkoski, Thordis L. Thorarinsdottir, Frode Stordal*

- `1309.6111v2` - [abs](http://arxiv.org/abs/1309.6111v2) - [pdf](http://arxiv.org/pdf/1309.6111v2)

> Spatial maps of extreme precipitation are a critical component of flood estimation in hydrological modeling, as well as in the planning and design of important infrastructure. This is particularly relevant in countries such as Norway that have a high density of hydrological power generating facilities and are exposed to significant risk of infrastructure damage due to flooding. In this work, we estimate a spatially coherent map of the distribution of extreme hourly precipitation in Norway, in terms of return levels, by linking generalized extreme value (GEV) distributions with latent Gaussian fields in a Bayesian hierarchical model. Generalized linear models on the parameters of the GEV distribution are able to incorporate location-specific geographic and meteorological information and thereby accommodate these effects on extreme precipitation. A Gaussian field on the GEV parameters captures additional unexplained spatial heterogeneity and overcomes the sparse grid on which observations are collected. We conduct an extensive analysis of the factors that affect the GEV parameters and show that our combination is able to appropriately characterize both the spatial variability of the distribution of extreme hourly precipitation in Norway, and the associated uncertainty in these estimates.

</details>

<details>

<summary>2014-05-26 11:24:26 - Bayesian variable selection with shrinking and diffusing priors</summary>

- *Naveen Naidu Narisetty, Xuming He*

- `1405.6545v1` - [abs](http://arxiv.org/abs/1405.6545v1) - [pdf](http://arxiv.org/pdf/1405.6545v1)

> We consider a Bayesian approach to variable selection in the presence of high dimensional covariates based on a hierarchical model that places prior distributions on the regression coefficients as well as on the model space. We adopt the well-known spike and slab Gaussian priors with a distinct feature, that is, the prior variances depend on the sample size through which appropriate shrinkage can be achieved. We show the strong selection consistency of the proposed method in the sense that the posterior probability of the true model converges to one even when the number of covariates grows nearly exponentially with the sample size. This is arguably the strongest selection consistency result that has been available in the Bayesian variable selection literature; yet the proposed method can be carried out through posterior sampling with a simple Gibbs sampler. Furthermore, we argue that the proposed method is asymptotically similar to model selection with the $L_0$ penalty. We also demonstrate through empirical work the fine performance of the proposed approach relative to some state of the art alternatives.

</details>

<details>

<summary>2014-05-26 14:59:29 - Bayesian Inference for Gaussian Process Classifiers with Annealing and Pseudo-Marginal MCMC</summary>

- *Maurizio Filippone*

- `1311.7320v2` - [abs](http://arxiv.org/abs/1311.7320v2) - [pdf](http://arxiv.org/pdf/1311.7320v2)

> Kernel methods have revolutionized the fields of pattern recognition and machine learning. Their success, however, critically depends on the choice of kernel parameters. Using Gaussian process (GP) classification as a working example, this paper focuses on Bayesian inference of covariance (kernel) parameters using Markov chain Monte Carlo (MCMC) methods. The motivation is that, compared to standard optimization of kernel parameters, they have been systematically demonstrated to be superior in quantifying uncertainty in predictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a practical inference tool for GP models. In particular, it amounts in replacing the analytically intractable marginal likelihood by an unbiased estimate obtainable by approximate methods and importance sampling. After discussing the potential drawbacks in employing importance sampling, this paper proposes the application of annealed importance sampling. The results empirically demonstrate that compared to importance sampling, annealed importance sampling can reduce the variance of the estimate of the marginal likelihood exponentially in the number of data at a computational cost that scales only polynomially. The results on real data demonstrate that employing annealed importance sampling in the Pseudo-Marginal MCMC approach represents a step forward in the development of fully automated exact inference engines for GP models.

</details>

<details>

<summary>2014-05-27 14:02:45 - Supervised Dictionary Learning by a Variational Bayesian Group Sparse Nonnegative Matrix Factorization</summary>

- *Ivan Ivek*

- `1405.6914v1` - [abs](http://arxiv.org/abs/1405.6914v1) - [pdf](http://arxiv.org/pdf/1405.6914v1)

> Nonnegative matrix factorization (NMF) with group sparsity constraints is formulated as a probabilistic graphical model and, assuming some observed data have been generated by the model, a feasible variational Bayesian algorithm is derived for learning model parameters. When used in a supervised learning scenario, NMF is most often utilized as an unsupervised feature extractor followed by classification in the obtained feature subspace. Having mapped the class labels to a more general concept of groups which underlie sparsity of the coefficients, what the proposed group sparse NMF model allows is incorporating class label information to find low dimensional label-driven dictionaries which not only aim to represent the data faithfully, but are also suitable for class discrimination. Experiments performed in face recognition and facial expression recognition domains point to advantages of classification in such label-driven feature subspaces over classification in feature subspaces obtained in an unsupervised manner.

</details>

<details>

<summary>2014-05-27 15:18:01 - Computationally efficient spatial modeling of annual maximum 24 hour precipitation. An application to data from Iceland</summary>

- *Ãli PÃ¡ll Geirsson, Birgir Hrafnkelsson, Daniel Simpson*

- `1405.6947v1` - [abs](http://arxiv.org/abs/1405.6947v1) - [pdf](http://arxiv.org/pdf/1405.6947v1)

> We propose a computationally efficient statistical method to obtain distributional properties of annual maximum 24 hour precipitation on a 1 km by 1 km regular grid over Iceland. A latent Gaussian model is built which takes into account observations, spatial variations and outputs from a local meteorological model. A covariate based on the meteorological model is constructed at each observational site and each grid point in order to assimilate available scientific knowledge about precipitation into the statistical model. The model is applied to two data sets on extreme precipitation, one uncorrected data set and one data set that is corrected for phase and wind. The observations are assumed to follow the generalized extreme value distribution. At the latent level, we implement SPDE spatial models for both the location and scale parameters of the likelihood. An efficient MCMC sampler which exploits the model structure is constructed, which yields fast continuous spatial predictions for spatially varying model parameters and quantiles.

</details>

<details>

<summary>2014-05-27 22:52:32 - A-optimal design of experiments for infinite-dimensional Bayesian linear inverse problems with regularized $\ell_0$-sparsification</summary>

- *Alen Alexanderian, Noemi Petra, Georg Stadler, Omar Ghattas*

- `1308.4084v2` - [abs](http://arxiv.org/abs/1308.4084v2) - [pdf](http://arxiv.org/pdf/1308.4084v2)

> We present an efficient method for computing A-optimal experimental designs for infinite-dimensional Bayesian linear inverse problems governed by partial differential equations (PDEs). Specifically, we address the problem of optimizing the location of sensors (at which observational data are collected) to minimize the uncertainty in the parameters estimated by solving the inverse problem, where the uncertainty is expressed by the trace of the posterior covariance. Computing optimal experimental designs (OEDs) is particularly challenging for inverse problems governed by computationally expensive PDE models with infinite-dimensional (or, after discretization, high-dimensional) parameters. To alleviate the computational cost, we exploit the problem structure and build a low-rank approximation of the parameter-to-observable map, preconditioned with the square root of the prior covariance operator. This relieves our method from expensive PDE solves when evaluating the optimal experimental design objective function and its derivatives. Moreover, we employ a randomized trace estimator for efficient evaluation of the OED objective function. We control the sparsity of the sensor configuration by employing a sequence of penalty functions that successively approximate the $\ell_0$-"norm"; this results in binary designs that characterize optimal sensor locations. We present numerical results for inference of the initial condition from spatio-temporal observations in a time-dependent advection-diffusion problem in two and three space dimensions. We find that an optimal design can be computed at a cost, measured in number of forward PDE solves, that is independent of the parameter and sensor dimensions. We demonstrate numerically that $\ell_0$-sparsified experimental designs obtained via a continuation method outperform $\ell_1$-sparsified designs.

</details>

<details>

<summary>2014-05-27 23:49:48 - Bayesian hierarchical modelling for inferring genetic interactions in yeast</summary>

- *Jonathan Heydari*

- `1405.7091v1` - [abs](http://arxiv.org/abs/1405.7091v1) - [pdf](http://arxiv.org/pdf/1405.7091v1)

> Identifying genetic interactions for a given microorganism such as yeast is difficult. Quantitative Fitness Analysis (QFA) is a high-throughput experimental and computational methodology for quantifying the fitness of microbial cultures. QFA can be used to compare between fitness observations for different genotypes and thereby infer genetic interaction strengths. Current "naive" frequentist statistical approaches used in QFA do not model between-genotype variation or difference in genotype variation under different conditions. In this thesis, a Bayesian approach is introduced to evaluate hierarchical models that better reflect the structure or design of QFA experiments. First, a two-stage approach is presented: a hierarchical logistic model is fitted to microbial culture growth curves and then a hierarchical interaction model is fitted to fitness summaries inferred for each genotype. Next, a one-stage Bayesian approach is presented: a joint hierarchical model which does not require a univariate summary of fitness, used to pass information between models. The new hierarchical approaches are then compared using a dataset examining the effect of telomere defects on yeast. By better describing the experimental structure, new evidence is found for genes and complexes which interact with the telomere cap. Various extensions of these models, including models for data transformation, batch effects, and intrinsically stochastic growth models are also considered.

</details>

<details>

<summary>2014-05-28 09:25:13 - Nonparametric maximum likelihood approach to multiple change-point problems</summary>

- *Changliang Zou, Guosheng Yin, Long Feng, Zhaojun Wang*

- `1405.7173v1` - [abs](http://arxiv.org/abs/1405.7173v1) - [pdf](http://arxiv.org/pdf/1405.7173v1)

> In multiple change-point problems, different data segments often follow different distributions, for which the changes may occur in the mean, scale or the entire distribution from one segment to another. Without the need to know the number of change-points in advance, we propose a nonparametric maximum likelihood approach to detecting multiple change-points. Our method does not impose any parametric assumption on the underlying distributions of the data sequence, which is thus suitable for detection of any changes in the distributions. The number of change-points is determined by the Bayesian information criterion and the locations of the change-points can be estimated via the dynamic programming algorithm and the use of the intrinsic order structure of the likelihood function. Under some mild conditions, we show that the new method provides consistent estimation with an optimal rate. We also suggest a prescreening procedure to exclude most of the irrelevant points prior to the implementation of the nonparametric likelihood method. Simulation studies show that the proposed method has satisfactory performance of identifying multiple change-points in terms of estimation accuracy and computation time.

</details>

<details>

<summary>2014-05-29 03:04:31 - Horizontal resolution in a nested-domain WRF simulation: a Bayesian analysis approach</summary>

- *Michel d. S. Mesquita, BjÃ¸rn Ãdlandsvik, Cindy BruyÃ¨re, Anne D. Sandvik*

- `1405.7447v1` - [abs](http://arxiv.org/abs/1405.7447v1) - [pdf](http://arxiv.org/pdf/1405.7447v1)

> The fast-paced development of state-of-the-art limited area models and faster computational resources have made it possible to create simulations at increasing horizontal resolution. This has led to a ubiquitous demand for even higher resolutions from users of various disciplines. This study revisits one of the simulations used in marine ecosystem projects at the Bjerknes Centre. We present a fresh perspective on the assessment of these data, related more specifically to: a) the value added by increased horizontal resolution; and b) a new method for comparing sensitivity studies. The assessment is made using a Bayesian framework for the distribution of mean surface temperature in the Hardanger fjord region in Norway. Population estimates are calculated based on samples from the joint posterior distribution generated using a Monte Carlo procedure. The Bayesian statistical model is applied to output data from the Weather Research and Forecasting (WRF) model at three horizontal resolutions (9, 3 and 1 km) and the ERA Interim Reanalysis. The period considered in this study is from 2007 to 2009, for the months of April, May and June.

</details>

<details>

<summary>2014-05-29 07:30:38 - Merging and testing opinions</summary>

- *Luciano Pomatto, Nabil Al-Najjar, Alvaro Sandroni*

- `1405.7481v1` - [abs](http://arxiv.org/abs/1405.7481v1) - [pdf](http://arxiv.org/pdf/1405.7481v1)

> We study the merging and the testing of opinions in the context of a prediction model. In the absence of incentive problems, opinions can be tested and rejected, regardless of whether or not data produces consensus among Bayesian agents. In contrast, in the presence of incentive problems, opinions can only be tested and rejected when data produces consensus among Bayesian agents. These results show a strong connection between the testing and the merging of opinions. They also relate the literature on Bayesian learning and the literature on testing strategic experts.

</details>

<details>

<summary>2014-05-29 14:41:42 - Multi-resolution two-sample comparison through the divide-merge Markov tree</summary>

- *Jacopo Soriano, Li Ma*

- `1404.3753v2` - [abs](http://arxiv.org/abs/1404.3753v2) - [pdf](http://arxiv.org/pdf/1404.3753v2)

> We introduce a probabilistic framework for two-sample comparison based on a nonparametric process taking the form of a Markov model that transitions between a "divide" and a "merge" state on a multi-resolution partition tree of the sample space. Multi-scale two-sample comparison is achieved through inferring the underlying state of the process along the partition tree. The Markov design allows the process to incorporate spatial clustering of differential structures, which is commonly observed in two-sample problems but ignored by existing methods. Inference is carried out under the Bayesian paradigm through recursive propagation algorithms. We demonstrate the work of our method through simulated data and a real flow cytometry data set, and show that it substantially outperforms other state-of-the-art two-sample tests in several settings.

</details>

<details>

<summary>2014-05-29 20:00:14 - Astrophysical data analysis with information field theory</summary>

- *Torsten EnÃlin*

- `1405.7701v1` - [abs](http://arxiv.org/abs/1405.7701v1) - [pdf](http://arxiv.org/pdf/1405.7701v1)

> Non-parametric imaging and data analysis in astrophysics and cosmology can be addressed by information field theory (IFT), a means of Bayesian, data based inference on spatially distributed signal fields. IFT is a statistical field theory, which permits the construction of optimal signal recovery algorithms. It exploits spatial correlations of the signal fields even for nonlinear and non-Gaussian signal inference problems. The alleviation of a perception threshold for recovering signals of unknown correlation structure by using IFT will be discussed in particular as well as a novel improvement on instrumental self-calibration schemes. IFT can be applied to many areas. Here, applications in in cosmology (cosmic microwave background, large-scale structure) and astrophysics (galactic magnetism, radio interferometry) are presented.

</details>

<details>

<summary>2014-05-29 21:06:02 - Simulation of multivariate diffusion bridge</summary>

- *Mogens Bladt, Samuel Finch, Michael SÃ¸rensen*

- `1405.7728v1` - [abs](http://arxiv.org/abs/1405.7728v1) - [pdf](http://arxiv.org/pdf/1405.7728v1)

> We propose simple methods for multivariate diffusion bridge simulation, which plays a fundamental role in simulation-based likelihood and Bayesian inference for stochastic differential equations. By a novel application of classical coupling methods, the new approach generalizes a previously proposed simulation method for one-dimensional bridges to the multi-variate setting. First a method of simulating approximate, but often very accurate, diffusion bridges is proposed. These approximate bridges are used as proposal for easily implementable MCMC algorithms that produce exact diffusion bridges. The new method is much more generally applicable than previous methods. Another advantage is that the new method works well for diffusion bridges in long intervals because the computational complexity of the method is linear in the length of the interval. In a simulation study the new method performs well, and its usefulness is illustrated by an application to Bayesian estimation for the multivariate hyperbolic diffusion model.

</details>

<details>

<summary>2014-05-30 05:27:39 - Survey data and Bayesian analysis: a cost-efficient way to estimate customer equity</summary>

- *Juha Karvanen, Ari Rantanen, Lasse Luoma*

- `1304.5380v3` - [abs](http://arxiv.org/abs/1304.5380v3) - [pdf](http://arxiv.org/pdf/1304.5380v3)

> We present a Bayesian framework for estimating the customer lifetime value (CLV) and the customer equity (CE) based on the purchasing behavior deducible from the market surveys on customer purchasing behavior. The proposed framework systematically addresses the challenges faced when the future value of customers is estimated based on survey data. The scarcity of the survey data and the sampling variance are countered by utilizing the prior information and quantifying the uncertainty of the CE and CLV estimates by posterior distributions. Furthermore, information on the purchase behavior of the customers of competitors available in the survey data is integrated to the framework. The introduced approach is directly applicable in the domains where a customer relationship can be thought to be monogamous.   As an example on the use of the framework, we analyze a consumer survey on mobile phones carried out in Finland in February 2013. The survey data contains consumer given information on the current and previous brand of the phone and the times of the last two purchases.

</details>

<details>

<summary>2014-05-30 10:00:36 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models</summary>

- *Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra*

- `1401.4082v3` - [abs](http://arxiv.org/abs/1401.4082v3) - [pdf](http://arxiv.org/pdf/1401.4082v3)

> We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.

</details>

<details>

<summary>2014-05-30 19:02:45 - The Infinite Degree Corrected Stochastic Block Model</summary>

- *Tue Herlau, Mikkel N. Schmidt, Morten MÃ¸rup*

- `1311.2520v3` - [abs](http://arxiv.org/abs/1311.2520v3) - [pdf](http://arxiv.org/pdf/1311.2520v3)

> In Stochastic blockmodels, which are among the most prominent statistical models for cluster analysis of complex networks, clusters are defined as groups of nodes with statistically similar link probabilities within and between groups. A recent extension by Karrer and Newman incorporates a node degree correction to model degree heterogeneity within each group. Although this demonstrably leads to better performance on several networks it is not obvious whether modelling node degree is always appropriate or necessary. We formulate the degree corrected stochastic blockmodel as a non-parametric Bayesian model, incorporating a parameter to control the amount of degree correction which can then be inferred from data. Additionally, our formulation yields principled ways of inferring the number of groups as well as predicting missing links in the network which can be used to quantify the model's predictive performance. On synthetic data we demonstrate that including the degree correction yields better performance both on recovering the true group structure and predicting missing links when degree heterogeneity is present, whereas performance is on par for data with no degree heterogeneity within clusters. On seven real networks (with no ground truth group structure available) we show that predictive performance is about equal whether or not degree correction is included; however, for some networks significantly fewer clusters are discovered when correcting for degree indicating that the data can be more compactly explained by clusters of heterogenous degree nodes.

</details>

<details>

<summary>2014-05-31 10:19:56 - Causal network inference using biochemical kinetics</summary>

- *C. J. Oates, F. Dondelinger, N. Bayani, J. Korola, J. W. Gray, S. Mukherjee*

- `1406.0063v1` - [abs](http://arxiv.org/abs/1406.0063v1) - [pdf](http://arxiv.org/pdf/1406.0063v1)

> Network models are widely used as structural summaries of biochemical systems. Statistical estimation of networks is usually based on linear or discrete models. However, the dynamics of these systems are generally nonlinear, suggesting that suitable nonlinear formulations may offer gains with respect to network inference and associated prediction problems. We present a general framework for both network inference and dynamical prediction that is rooted in nonlinear biochemical kinetics. This is done by considering a dynamical system based on a chemical reaction graph and associated kinetics parameters. Inference regarding both parameters and the reaction graph itself is carried out within a fully Bayesian framework. Prediction of dynamical behavior is achieved by averaging over both parameters and reaction graphs, allowing prediction even when the underlying reactions themselves are unknown or uncertain. Results, based on (i) data simulated from a mechanistic model of mitogen-activated protein kinase signaling and (ii) phosphoproteomic data from cancer cell lines, demonstrate that nonlinear formulations can yield gains in network inference and permit dynamical prediction in the challenging setting where the reaction graph is unknown.

</details>

<details>

<summary>2014-05-31 11:50:03 - Adaptive Reconfiguration Moves for Dirichlet Mixtures</summary>

- *Tue Herlau, Morten MÃ¸rup, Yee Whye Teh, Mikkel N. Schmidt*

- `1406.0071v1` - [abs](http://arxiv.org/abs/1406.0071v1) - [pdf](http://arxiv.org/pdf/1406.0071v1)

> Bayesian mixture models are widely applied for unsupervised learning and exploratory data analysis. Markov chain Monte Carlo based on Gibbs sampling and split-merge moves are widely used for inference in these models. However, both methods are restricted to limited types of transitions and suffer from torpid mixing and low accept rates even for problems of modest size. We propose a method that considers a broader range of transitions that are close to equilibrium by exploiting multiple chains in parallel and using the past states adaptively to inform the proposal distribution. The method significantly improves on Gibbs and split-merge sampling as quantified using convergence diagnostics and acceptance rates. Adaptive MCMC methods which use past states to inform the proposal distribution has given rise to many ingenious sampling schemes for continuous problems and the present work can be seen as an important first step in bringing these benefits to partition-based problems

</details>


## 2014-06

<details>

<summary>2014-06-01 16:07:15 - Nonparametric Bayesian testing for monotonicity</summary>

- *James G. Scott, Thomas S. Shively, Stephen G. Walker*

- `1304.3378v2` - [abs](http://arxiv.org/abs/1304.3378v2) - [pdf](http://arxiv.org/pdf/1304.3378v2)

> This paper studies the problem of testing whether a function is monotone from a nonparametric Bayesian perspective. Two new families of tests are constructed. The first uses constrained smoothing splines, together with a hierarchical stochastic-process prior that explicitly controls the prior probability of monotonicity. The second uses regression splines, together with two proposals for the prior over the regression coefficients. The finite-sample performance of the tests is shown via simulation to improve upon existing frequentist and Bayesian methods. The asymptotic properties of the Bayes factor for comparing monotone versus non-monotone regression functions in a Gaussian model are also studied. Our results significantly extend those currently available, which chiefly focus on determining the dimension of a parametric linear model.

</details>

<details>

<summary>2014-06-01 19:09:14 - Inference of Sparse Networks with Unobserved Variables. Application to Gene Regulatory Networks</summary>

- *Nikolai Slavov*

- `1406.0193v1` - [abs](http://arxiv.org/abs/1406.0193v1) - [pdf](http://arxiv.org/pdf/1406.0193v1)

> Networks are a unifying framework for modeling complex systems and network inference problems are frequently encountered in many fields. Here, I develop and apply a generative approach to network inference (RCweb) for the case when the network is sparse and the latent (not observed) variables affect the observed ones. From all possible factor analysis (FA) decompositions explaining the variance in the data, RCweb selects the FA decomposition that is consistent with a sparse underlying network. The sparsity constraint is imposed by a novel method that significantly outperforms (in terms of accuracy, robustness to noise, complexity scaling, and computational efficiency) Bayesian methods and MLE methods using l1 norm relaxation such as K-SVD and l1--based sparse principle component analysis (PCA). Results from simulated models demonstrate that RCweb recovers exactly the model structures for sparsity as low (as non-sparse) as 50% and with ratio of unobserved to observed variables as high as 2. RCweb is robust to noise, with gradual decrease in the parameter ranges as the noise level increases.

</details>

<details>

<summary>2014-06-02 07:52:28 - Posterior contraction in sparse Bayesian factor models for massive covariance matrices</summary>

- *Debdeep Pati, Anirban Bhattacharya, Natesh S. Pillai, David Dunson*

- `1206.3627v4` - [abs](http://arxiv.org/abs/1206.3627v4) - [pdf](http://arxiv.org/pdf/1206.3627v4)

> Sparse Bayesian factor models are routinely implemented for parsimonious dependence modeling and dimensionality reduction in high-dimensional applications. We provide theoretical understanding of such Bayesian procedures in terms of posterior convergence rates in inferring high-dimensional covariance matrices where the dimension can be larger than the sample size. Under relevant sparsity assumptions on the true covariance matrix, we show that commonly-used point mass mixture priors on the factor loadings lead to consistent estimation in the operator norm even when $p\gg n$. One of our major contributions is to develop a new class of continuous shrinkage priors and provide insights into their concentration around sparse vectors. Using such priors for the factor loadings, we obtain similar rate of convergence as obtained with point mass mixture priors. To obtain the convergence rates, we construct test functions to separate points in the space of high-dimensional covariance matrices using insights from random matrix theory; the tools developed may be of independent interest. We also derive minimax rates and show that the Bayesian posterior rates of convergence coincide with the minimax rates upto a $\sqrt{\log n}$ term.

</details>

<details>

<summary>2014-06-02 08:26:04 - Maximum-A-Posteriori Estimates in Linear Inverse Problems with Log-concave Priors are Proper Bayes Estimators</summary>

- *Martin Burger, Felix Lucka*

- `1402.5297v2` - [abs](http://arxiv.org/abs/1402.5297v2) - [pdf](http://arxiv.org/pdf/1402.5297v2)

> A frequent matter of debate in Bayesian inversion is the question, which of the two principle point-estimators, the maximum-a-posteriori (MAP) or the conditional mean (CM) estimate is to be preferred. As the MAP estimate corresponds to the solution given by variational regularization techniques, this is also a constant matter of debate between the two research areas. Following a theoretical argument - the Bayes cost formalism - the CM estimate is classically preferred for being the Bayes estimator for the mean squared error cost while the MAP estimate is classically discredited for being only asymptotically the Bayes estimator for the uniform cost function. In this article we present recent theoretical and computational observations that challenge this point of view, in particular for high-dimensional sparsity-promoting Bayesian inversion. Using Bregman distances, we present new, proper convex Bayes cost functions for which the MAP estimator is the Bayes estimator. We complement this finding by results that correct further common misconceptions about MAP estimates. In total, we aim to rehabilitate MAP estimates in linear inverse problems with log-concave priors as proper Bayes estimators.

</details>

<details>

<summary>2014-06-02 21:49:14 - Parameter identifiability of discrete Bayesian networks with hidden variables</summary>

- *Elizabeth S. Allman, John A. Rhodes, Elena Stanghellini, Marco Valtorta*

- `1406.0541v1` - [abs](http://arxiv.org/abs/1406.0541v1) - [pdf](http://arxiv.org/pdf/1406.0541v1)

> Identifiability of parameters is an essential property for a statistical model to be useful in most settings. However, establishing parameter identifiability for Bayesian networks with hidden variables remains challenging. In the context of finite state spaces, we give algebraic arguments establishing identifiability of some special models on small DAGs. We also establish that, for fixed state spaces, generic identifiability of parameters depends only on the Markov equivalence class of the DAG. To illustrate the use of these results, we investigate identifiability for all binary Bayesian networks with up to five variables, one of which is hidden and parental to all observable ones. Surprisingly, some of these models have parameterizations that are generically 4-to-one, and not 2-to-one as label swapping of the hidden states would suggest. This leads to interesting difficulties in interpreting causal effects.

</details>

<details>

<summary>2014-06-03 12:05:07 - RMCMC: A System for Updating Bayesian Models</summary>

- *F. Din-Houn Lau, Axel Gandy*

- `1307.0742v7` - [abs](http://arxiv.org/abs/1307.0742v7) - [pdf](http://arxiv.org/pdf/1307.0742v7)

> A system to update estimates from a sequence of probability distributions is presented. The aim of the system is to quickly produce estimates with a user-specified bound on the Monte Carlo error. The estimates are based upon weighted samples stored in a database. The stored samples are maintained such that the accuracy of the estimates and quality of the samples is satisfactory. This maintenance involves varying the number of samples in the database and updating their weights. New samples are generated, when required, by a Markov chain Monte Carlo algorithm. The system is demonstrated using a football league model that is used to predict the end of season table. Correctness of the estimates and their accuracy is shown in a simulation using a linear Gaussian model.

</details>

<details>

<summary>2014-06-03 19:20:27 - Learning Latent Block Structure in Weighted Networks</summary>

- *Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset*

- `1404.0431v2` - [abs](http://arxiv.org/abs/1404.0431v2) - [pdf](http://arxiv.org/pdf/1404.0431v2)

> Community detection is an important task in network analysis, in which we aim to learn a network partition that groups together vertices with similar community-level connectivity patterns. By finding such groups of vertices with similar structural roles, we extract a compact representation of the network's large-scale structure, which can facilitate its scientific interpretation and the prediction of unknown or future interactions. Popular approaches, including the stochastic block model, assume edges are unweighted, which limits their utility by throwing away potentially useful information. We introduce the `weighted stochastic block model' (WSBM), which generalizes the stochastic block model to networks with edge weights drawn from any exponential family distribution. This model learns from both the presence and weight of edges, allowing it to discover structure that would otherwise be hidden when weights are discarded or thresholded. We describe a Bayesian variational algorithm for efficiently approximating this model's posterior distribution over latent block structures. We then evaluate the WSBM's performance on both edge-existence and edge-weight prediction tasks for a set of real-world weighted networks. In all cases, the WSBM performs as well or better than the best alternatives on these tasks.

</details>

<details>

<summary>2014-06-04 12:20:21 - Generalizations related to hypothesis testing with the Posterior distribution of the Likelihood Ratio</summary>

- *I. Smith, A. Ferrari*

- `1406.1023v1` - [abs](http://arxiv.org/abs/1406.1023v1) - [pdf](http://arxiv.org/pdf/1406.1023v1)

> The Posterior distribution of the Likelihood Ratio (PLR) is proposed by Dempster in 1974 for significance testing in the simple vs composite hypotheses case. In this hypotheses test case, classical frequentist and Bayesian hypotheses tests are irreconcilable, as emphasized by Lindley's paradox, Berger & Selke in 1987 and many others. However, Dempster shows that the PLR (with inner threshold 1) is equal to the frequentist p-value in the simple Gaussian case. In 1997, Aitkin extends this result by adding a nuisance parameter and showing its asymptotic validity under more general distributions. Here we extend the reconciliation between the PLR and a frequentist p-value for a finite sample, through a framework analogous to the Stein's theorem frame in which a credible (Bayesian) domain is equal to a confidence (frequentist) domain.   This general reconciliation result only concerns simple vs composite hypotheses testing. The measures proposed by Aitkin in 2010 and Evans in 1997 have interesting properties and extend Dempster's PLR but only by adding a nuisance parameter. Here we propose two extensions of the PLR concept to the general composite vs composite hypotheses test. The first extension can be defined for improper priors as soon as the posterior is proper. The second extension appears from a new Bayesian-type Neyman-Pearson lemma and emphasizes, from a Bayesian perspective, the role of the LR as a discrepancy variable for hypothesis testing.

</details>

<details>

<summary>2014-06-05 10:47:16 - An Adaptive LASSO-Penalized BIC</summary>

- *Sakyajit Bhattacharya, Paul D. McNicholas*

- `1406.1332v1` - [abs](http://arxiv.org/abs/1406.1332v1) - [pdf](http://arxiv.org/pdf/1406.1332v1)

> Mixture models are becoming a popular tool for the clustering and classification of high-dimensional data. In such high dimensional applications, model selection is problematic. The Bayesian information criterion, which is popular in lower dimensional applications, tends to underestimate the true number of components in high dimensions. We introduce an adaptive LASSO-penalized BIC (ALPBIC) to mitigate this problem. This efficacy of the ALPBIC is illustrated via applications of parsimonious mixtures of factor analyzers. The selection of the best model by ALPBIC is shown to be consistent with increasing numbers of observations based on simulated and real data analyses.

</details>

<details>

<summary>2014-06-06 19:51:07 - Advances in Learning Bayesian Networks of Bounded Treewidth</summary>

- *Siqi Nie, Denis Deratani Maua, Cassio Polpo de Campos, Qiang Ji*

- `1406.1411v2` - [abs](http://arxiv.org/abs/1406.1411v2) - [pdf](http://arxiv.org/pdf/1406.1411v2)

> This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that $k$-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate.

</details>

<details>

<summary>2014-06-07 10:23:17 - Bayesian density regression for count data</summary>

- *Charalampos Chanialidis, Ludger Evers, Tereza Neocleous*

- `1406.1882v1` - [abs](http://arxiv.org/abs/1406.1882v1) - [pdf](http://arxiv.org/pdf/1406.1882v1)

> Despite the increasing popularity of quantile regression models for continuous responses, models for count data have so far received little attention. The main quantile regression technique for count data involves adding uniform random noise or "jittering", thus overcoming the problem that the conditional quantile function is not a continuous function of the parameters of interest. Although jittering allows estimating the conditional quantiles, it has the drawback that, for small values of the response variable $Y,$ the added noise can have a large influence on the estimated quantiles. In addition, quantile regression can lead to "crossing" quantiles. We propose a Bayesian Dirichlet process (DP)-based approach to quantile regression for count data. The approach is based on an adaptive DP mixture (DPM) of COM-Poisson regression models and determines the quantiles by estimating the density of the data, thus eliminating all the aforementioned problems. Taking advantage of the exchange algorithm, the proposed MCMC algorithm can be applied to distributions on which the likelihood can only be computed up to a normalising constant.

</details>

<details>

<summary>2014-06-09 07:45:17 - Determinantal Point Process Priors for Bayesian Variable Selection in Linear Regression</summary>

- *Mutsuki Kojima, Fumiyasu Komaki*

- `1406.2100v1` - [abs](http://arxiv.org/abs/1406.2100v1) - [pdf](http://arxiv.org/pdf/1406.2100v1)

> We propose discrete determinantal point processes (DPPs) for priors on the model parameter in Bayesian variable selection. By our variable selection method, collinear predictors are less likely to be selected simultaneously because of the repulsion property of discrete DPPs. Three types of DPP priors are proposed. We show the efficiency of the proposed priors through numerical experiments and applications to collinear datasets.

</details>

<details>

<summary>2014-06-10 00:50:05 - ExpertBayes: Automatically refining manually built Bayesian networks</summary>

- *Ezilda Almeida, Pedro Ferreira, Tiago Vinhoza, InÃªs Dutra, Jingwei Li, Yirong Wu, Elizabeth Burnside*

- `1406.2395v1` - [abs](http://arxiv.org/abs/1406.2395v1) - [pdf](http://arxiv.org/pdf/1406.2395v1)

> Bayesian network structures are usually built using only the data and starting from an empty network or from a naive Bayes structure. Very often, in some domains, like medicine, a prior structure knowledge is already known. This structure can be automatically or manually refined in search for better performance models. In this work, we take Bayesian networks built by specialists and show that minor perturbations to this original network can yield better classifiers with a very small computational cost, while maintaining most of the intended meaning of the original model.

</details>

<details>

<summary>2014-06-10 08:18:06 - Bayesian calibration for forensic evidence reporting</summary>

- *Niko BrÃ¼mmer, Albert Swart*

- `1403.5997v3` - [abs](http://arxiv.org/abs/1403.5997v3) - [pdf](http://arxiv.org/pdf/1403.5997v3)

> We introduce a Bayesian solution for the problem in forensic speaker recognition, where there may be very little background material for estimating score calibration parameters. We work within the Bayesian paradigm of evidence reporting and develop a principled probabilistic treatment of the problem, which results in a Bayesian likelihood-ratio as the vehicle for reporting weight of evidence. We show in contrast, that reporting a likelihood-ratio distribution does not solve this problem. Our solution is experimentally exercised on a simulated forensic scenario, using NIST SRE'12 scores, which demonstrates a clear advantage for the proposed method compared to the traditional plugin calibration recipe.

</details>

<details>

<summary>2014-06-10 13:29:09 - Predictive Entropy Search for Efficient Global Optimization of Black-box Functions</summary>

- *JosÃ© Miguel HernÃ¡ndez-Lobato, Matthew W. Hoffman, Zoubin Ghahramani*

- `1406.2541v1` - [abs](http://arxiv.org/abs/1406.2541v1) - [pdf](http://arxiv.org/pdf/1406.2541v1)

> We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.

</details>

<details>

<summary>2014-06-10 19:53:09 - Augur: a Modeling Language for Data-Parallel Probabilistic Inference</summary>

- *Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock, Stephen J. Green, Guy L. Steele Jr*

- `1312.3613v2` - [abs](http://arxiv.org/abs/1312.3613v2) - [pdf](http://arxiv.org/pdf/1312.3613v2)

> It is time-consuming and error-prone to implement inference procedures for each new probabilistic model. Probabilistic programming addresses this problem by allowing a user to specify the model and having a compiler automatically generate an inference procedure for it. For this approach to be practical, it is important to generate inference code that has reasonable performance. In this paper, we present a probabilistic programming language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. Our language is fully integrated within the Scala programming language and benefits from tools such as IDE support, type-checking, and code completion. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.

</details>

<details>

<summary>2014-06-10 20:17:39 - Bayesian Analysis of Simple Random Densities</summary>

- *Paulo C. Marques F., Carlos A. de B. Pereira*

- `1209.4947v2` - [abs](http://arxiv.org/abs/1209.4947v2) - [pdf](http://arxiv.org/pdf/1209.4947v2)

> A tractable nonparametric prior over densities is introduced which is closed under sampling and exhibits proper posterior asymptotics.

</details>

<details>

<summary>2014-06-10 20:25:47 - Predictive analysis of microarray data</summary>

- *Paulo C. Marques F., Carlos A. de B. Pereira*

- `1312.2291v2` - [abs](http://arxiv.org/abs/1312.2291v2) - [pdf](http://arxiv.org/pdf/1312.2291v2)

> Microarray gene expression data are analyzed by means of a Bayesian nonparametric model, with emphasis on prediction of future observables, yielding a method for selection of differentially expressed genes and a classifier.

</details>

<details>

<summary>2014-06-10 21:17:30 - Bayesian Benchmark Dose Analysis</summary>

- *Qijun Fang, Walter W. Piegorsch, Katherine Y. Barnes*

- `1406.2725v1` - [abs](http://arxiv.org/abs/1406.2725v1) - [pdf](http://arxiv.org/pdf/1406.2725v1)

> An important objective in environmental risk assessment is estimation of minimum exposure levels, called Benchmark Doses (BMDs) that induce a pre-specified Benchmark Response (BMR) in a target population. Established inferential approaches for BMD analysis typically involve one-sided, frequentist confidence limits, leading in practice to what are called Benchmark Dose Lower Limits (BMDLs). Appeal to Bayesian modeling and credible limits for building BMDLs is far less developed, however. Indeed, for the few existing forms of Bayesian BMDs, informative prior information is seldom incorporated. We develop reparameterized quantal-response models that explicitly describe the BMD as a target parameter. Our goal is to obtain an improved estimation and calculation archetype for the BMD and for the BMDL, by employing quantifiable prior belief to represent parameter uncertainty in the statistical model. Implementation is facilitated via a Monte Carlo-based adaptive Metropolis (AM) algorithm to approximate the posterior distribution. An example from environmental carcinogenicity testing illustrates the calculations.

</details>

<details>

<summary>2014-06-11 05:15:59 - An R Implementation of the Polya-Aeppli Distribution</summary>

- *Conrad J. Burden*

- `1406.2780v1` - [abs](http://arxiv.org/abs/1406.2780v1) - [pdf](http://arxiv.org/pdf/1406.2780v1)

> An efficient implementation of the Polya-Aeppli, or geometirc compound Poisson, distribution in the statistical programming language R is presented. The implementation is available as the package polyaAeppli and consists of functions for the mass function, cumulative distribution function, quantile function and random variate generation with those parameters conventionally provided for standard univatiate probability distributions in the stats package in R

</details>

<details>

<summary>2014-06-11 20:32:11 - Input Warping for Bayesian Optimization of Non-stationary Functions</summary>

- *Jasper Snoek, Kevin Swersky, Richard S. Zemel, Ryan P. Adams*

- `1402.0929v3` - [abs](http://arxiv.org/abs/1402.0929v3) - [pdf](http://arxiv.org/pdf/1402.0929v3)

> Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions which can be queried efficiently, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in "log-space," to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.

</details>

<details>

<summary>2014-06-12 08:41:03 - Powerful nonparametric checks for quantile regression</summary>

- *Samuel Maistre, Pascal Lavergne, Valentin Patilea*

- `1404.0216v2` - [abs](http://arxiv.org/abs/1404.0216v2) - [pdf](http://arxiv.org/pdf/1404.0216v2)

> We address the issue of lack-of-fit testing for a parametric quantile regression. We propose a simple test that involves one-dimensional kernel smoothing, so that the rate at which it detects local alternatives is independent of the number of covariates. The test has asymptotically gaussian critical values, and wild bootstrap can be applied to obtain more accurate ones in small samples. Our procedure appears to be competitive with existing ones in simulations. We illustrate the usefulness of our test on birthweight data.

</details>

<details>

<summary>2014-06-14 08:50:53 - Monotone Function Estimation for Computer Experiments</summary>

- *Shirin Golchi, Derek R. Bingham, Hugh Chipman, David A. Campbell*

- `1309.3802v2` - [abs](http://arxiv.org/abs/1309.3802v2) - [pdf](http://arxiv.org/pdf/1309.3802v2)

> In statistical modeling of computer experiments sometimes prior information is available about the underlying function. For example, the physical system simulated by the computer code may be known to be monotone with respect to some or all inputs. We develop a Bayesian approach to Gaussian process modelling capable of incorporating monotonicity information for computer model emulation. Markov chain Monte Carlo methods are used to sample from the posterior distribution of the process given the simulator output and monotonicity information. The performance of the proposed approach in terms of predictive accuracy and uncertainty quantification is demonstrated in a number of simulated examples as well as a real queueing system application.

</details>

<details>

<summary>2014-06-14 10:21:03 - Dimensionality reduction for time series data</summary>

- *Diego Vidaurre, Iead Rezek, Samuel L. Harrison, Stephen S. Smith, Mark Woolrich*

- `1406.3711v1` - [abs](http://arxiv.org/abs/1406.3711v1) - [pdf](http://arxiv.org/pdf/1406.3711v1)

> Despite the fact that they do not consider the temporal nature of data, classic dimensionality reduction techniques, such as PCA, are widely applied to time series data. In this paper, we introduce a factor decomposition specific for time series that builds upon the Bayesian multivariate autoregressive model and hence evades the assumption that data points are mutually independent. The key is to find a low-rank estimation of the autoregressive matrices. As in the probabilistic version of other factor models, this induces a latent low-dimensional representation of the original data. We discuss some possible generalisations and alternatives, with the most relevant being a technique for simultaneous smoothing and dimensionality reduction. To illustrate the potential applications, we apply the model on a synthetic data set and different types of neuroimaging data (EEG and ECoG).

</details>

<details>

<summary>2014-06-15 10:30:13 - Finite Sample Bernstein -- von Mises Theorem for Semiparametric Problems</summary>

- *Maxim Panov, Vladimir Spokoiny*

- `1310.7796v2` - [abs](http://arxiv.org/abs/1310.7796v2) - [pdf](http://arxiv.org/pdf/1310.7796v2)

> The classical parametric and semiparametric Bernstein -- von Mises (BvM) results are reconsidered in a non-classical setup allowing finite samples and model misspecification. In the case of a finite dimensional nuisance parameter we obtain an upper bound on the error of Gaussian approximation of the posterior distribution for the target parameter which is explicit in the dimension of the nuisance and target parameters. This helps to identify the so called \emph{critical dimension} $ p $ of the full parameter for which the BvM result is applicable. In the important i.i.d. case, we show that the condition "$ p^{3} / n $ is small" is sufficient for BvM result to be valid under general assumptions on the model. We also provide an example of a model with the phase transition effect: the statement of the BvM theorem fails when the dimension $ p $ approaches $ n^{1/3} $. The results are extended to the case of infinite dimensional parameters with the nuisance parameter from a Sobolev class. In particular we show near normality of the posterior if the smoothness parameter $s$ exceeds 3/2.

</details>

<details>

<summary>2014-06-15 19:03:46 - Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models</summary>

- *Yichuan Zhang, Charles Sutton*

- `1406.3843v1` - [abs](http://arxiv.org/abs/1406.3843v1) - [pdf](http://arxiv.org/pdf/1406.3843v1)

> Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.

</details>

<details>

<summary>2014-06-16 01:32:26 - Bayesian Manifold Regression</summary>

- *Yun Yang, David B. Dunson*

- `1305.0617v2` - [abs](http://arxiv.org/abs/1305.0617v2) - [pdf](http://arxiv.org/pdf/1305.0617v2)

> There is increasing interest in the problem of nonparametric regression with high-dimensional predictors. When the number of predictors $D$ is large, one encounters a daunting problem in attempting to estimate a $D$-dimensional surface based on limited data. Fortunately, in many applications, the support of the data is concentrated on a $d$-dimensional subspace with $d \ll D$. Manifold learning attempts to estimate this subspace. Our focus is on developing computationally tractable and theoretically supported Bayesian nonparametric regression methods in this context. When the subspace corresponds to a locally-Euclidean compact Riemannian manifold, we show that a Gaussian process regression approach can be applied that leads to the minimax optimal adaptive rate in estimating the regression function under some conditions. The proposed model bypasses the need to estimate the manifold, and can be implemented using standard algorithms for posterior computation in Gaussian processes. Finite sample performance is illustrated in an example data analysis.

</details>

<details>

<summary>2014-06-16 03:43:20 - Freeze-Thaw Bayesian Optimization</summary>

- *Kevin Swersky, Jasper Snoek, Ryan Prescott Adams*

- `1406.3896v1` - [abs](http://arxiv.org/abs/1406.3896v1) - [pdf](http://arxiv.org/pdf/1406.3896v1)

> In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.

</details>

<details>

<summary>2014-06-16 07:24:34 - Well-Posed Bayesian Geometric Inverse Problems Arising in Subsurface Flow</summary>

- *Marco A. Iglesias, Kui Lin, Andrew M. Stuart*

- `1401.5571v2` - [abs](http://arxiv.org/abs/1401.5571v2) - [pdf](http://arxiv.org/pdf/1401.5571v2)

> In this paper, we consider the inverse problem of determining the permeability of the subsurface from hydraulic head measurements, within the framework of a steady Darcy model of groundwater flow. We study geometrically defined prior permeability fields, which admit layered, fault and channel structures, in order to mimic realistic subsurface features; within each layer we adopt either constant or continuous function representation of the permeability. This prior model leads to a parameter identification problem for a finite number of unknown parameters determining the geometry, together with either a finite number of permeability values (in the constant case) or a finite number of fields (in the continuous function case). We adopt a Bayesian framework showing existence and well-posedness of the posterior distribution. We also introduce novel Markov Chain-Monte Carlo (MCMC) methods, which exploit the different character of the geometric and permeability parameters, and build on recent advances in function space MCMC. These algorithms provide rigorous estimates of the permeability, as well as the uncertainty associated with it, and only require forward model evaluations. No adjoint solvers are required and hence the methodology is applicable to black-box forward models. We then use these methods to explore the posterior and to illustrate the methodology with numerical experiments.

</details>

<details>

<summary>2014-06-16 08:04:42 - Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy Posterior Sampling Algorithm</summary>

- *Yasin Abbasi-Yadkori, Csaba Szepesvari*

- `1406.3926v1` - [abs](http://arxiv.org/abs/1406.3926v1) - [pdf](http://arxiv.org/pdf/1406.3926v1)

> We study Bayesian optimal control of a general class of smoothly parameterized Markov decision problems. Since computing the optimal control is computationally expensive, we design an algorithm that trades off performance for computational efficiency. The algorithm is a lazy posterior sampling method that maintains a distribution over the unknown parameter. The algorithm changes its policy only when the variance of the distribution is reduced sufficiently. Importantly, we analyze the algorithm and show the precise nature of the performance vs. computation tradeoff. Finally, we show the effectiveness of the method on a web server control application.

</details>

<details>

<summary>2014-06-17 10:32:30 - Toward an enhanced Bayesian estimation framework for multiphase flow soft-sensing</summary>

- *Xiaodong Luo, Rolf J. Lorentzen, Andreas S. Stordal, Geir NÃ¦vdal*

- `1406.4306v1` - [abs](http://arxiv.org/abs/1406.4306v1) - [pdf](http://arxiv.org/pdf/1406.4306v1)

> In this work the authors study the multiphase flow soft-sensing problem based on a previously established framework. There are three functional modules in this framework, namely, a transient well flow model that describes the response of certain physical variables in a well, for instance, temperature, velocity and pressure, to the flow rates entering and leaving the well zones; a Markov jump process that is designed to capture the potential abrupt changes in the flow rates; and an estimation method that is adopted to estimate the underlying flow rates based on the measurements from the physical sensors installed in the well.   In the previous studies, the variances of the flow rates in the Markov jump process are chosen manually. To fill this gap, in the current work two automatic approaches are proposed in order to optimize the variance estimation. Through a numerical example, we show that, when the estimation framework is used in conjunction with these two proposed variance-estimation approaches, it can achieve reasonable performance in terms of matching both the measurements of the physical sensors and the true underlying flow rates.

</details>

<details>

<summary>2014-06-17 21:36:20 - Calibration of Computational Models with Categorical Parameters and Correlated Outputs via Bayesian Smoothing Spline ANOVA</summary>

- *Curtis B. Storlie, William A. Lane, Emily M. Ryan, James R. Gattiker, David M. Higdon*

- `1405.5297v2` - [abs](http://arxiv.org/abs/1405.5297v2) - [pdf](http://arxiv.org/pdf/1405.5297v2)

> It has become commonplace to use complex computer models to predict outcomes in regions where data does not exist. Typically these models need to be calibrated and validated using some experimental data, which often consists of multiple correlated outcomes. In addition, some of the model parameters may be categorical in nature, such as a pointer variable to alternate models (or submodels) for some of the physics of the system. Here we present a general approach for calibration in such situations where an emulator of the computationally demanding models and a discrepancy term from the model to reality are represented within a Bayesian Smoothing Spline (BSS) ANOVA framework. The BSS-ANOVA framework has several advantages over the traditional Gaussian Process, including ease of handling categorical inputs and correlated outputs, and improved computational efficiency. Finally this framework is then applied to the problem that motivated its design; a calibration of a computational fluid dynamics model of a bubbling fluidized which is used as an absorber in a CO2 capture system.

</details>

<details>

<summary>2014-06-17 21:53:41 - Semiparametric stochastic volatility modelling using penalized splines</summary>

- *Roland Langrock, ThÃ©o Michelot, Alexander Sohn, Thomas Kneib*

- `1308.5836v3` - [abs](http://arxiv.org/abs/1308.5836v3) - [pdf](http://arxiv.org/pdf/1308.5836v3)

> Stochastic volatility (SV) models mimic many of the stylized facts attributed to time series of asset returns, while maintaining conceptual simplicity. The commonly made assumption of conditionally normally distributed or Student-t-distributed returns, given the volatility, has however been questioned. In this manuscript, we introduce a novel maximum penalized likelihood approach for estimating the conditional distribution in an SV model in a nonparametric way, thus avoiding any potentially critical assumptions on the shape. The considered framework exploits the strengths both of the powerful hidden Markov model machinery and of penalized B-splines, and constitutes a powerful and flexible alternative to recently developed Bayesian approaches to semiparametric SV modelling. We demonstrate the feasibility of the approach in a simulation study before outlining its potential in applications to three series of returns on stocks and one series of stock index returns.

</details>

<details>

<summary>2014-06-18 03:41:36 - Estimation of Causal Invertible VARMA Models</summary>

- *Anindya Roy, Tucker S. McElroy, Peter Linton*

- `1406.4584v1` - [abs](http://arxiv.org/abs/1406.4584v1) - [pdf](http://arxiv.org/pdf/1406.4584v1)

> We present a re-parameterization of vector autoregressive moving average (VARMA) models that allows estimation of parameters under the constraints of causality and invertibility. The parameter constraints associated with a causal invertible VARMA model are highly complex. Currently there are no procedures that can maintain the constraints in the estimated VARMA process, except in the special case of a vector autoregression (VAR), where some moment based causal estimators are available. Even in the VAR case, the available likelihood based estimators are not causal. The maximum likelihood estimator based on the full likelihood that does not condition on the initial observations by definition satisfies the causal invertible constraints but optimization of the likelihood under the complex constraints is an intractable problem. The commonly used Bayesian procedure for VAR often has posterior mass outside the causal set because the priors are not constrained to the causal set of parameters.   We provide an exact mathematical solution to this problem. An $m$-variate VARMA$(p, q)$ process contains $(p+ q) m^2 + \binom{m+1}{2}$ parameters, which must be constrained to a subset of Euclidean space in order to guarantee causality and invertibility. This space is implicitly described in this paper, through the device of parameterizing the entire space of block Toeplitz matrices in terms of positive definite matrices and orthogonal matrices. The parameterization has connection to Schur- stability of polynomials and the associated Stein transformation that are often used in dynamical systems literature. As an important by-product of our investigation, we generalize a classical result in dynamical systems to provide a characterization of Schur stable matrix polynomials.

</details>

<details>

<summary>2014-06-18 10:40:43 - Active Learning Via Sequential Design and Uncertainty Sampling</summary>

- *Jing Wang, Eunsik Park, Yuan-chin Ivan Chang*

- `1406.4676v1` - [abs](http://arxiv.org/abs/1406.4676v1) - [pdf](http://arxiv.org/pdf/1406.4676v1)

> Classification is an important task in many fields including biomedical research and machine learning. Traditionally, a classification rule is constructed based a bunch of labeled data. Recently, due to technological innovation and automatic data collection schemes, we easily encounter with data sets containing large amounts of unlabeled samples. Because to label each of them is usually costly and inefficient, how to utilize these unlabeled data in a classifier construction process becomes an important problem. In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure. In this paper, we study sequential learning procedures for building efficient and effective classifiers, where only the selected subjects are labeled and included in its learning stage. The proposed algorithm combines the ideas of Bayesian sequential optimal design and uncertainty sampling. Computational issues of the algorithm are discussed. Numerical results using both synthesized data and real examples are reported.

</details>

<details>

<summary>2014-06-18 13:13:17 - Semisupervised hyperspectral image unmixing using a variational Bayes algorithm</summary>

- *Konstantinos E. Themelis, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas*

- `1406.4705v1` - [abs](http://arxiv.org/abs/1406.4705v1) - [pdf](http://arxiv.org/pdf/1406.4705v1)

> This technical report presents a variational Bayes algorithm for semisupervised hyperspectral image unmixing. The presented Bayesian model employs a heavy tailed, nonnegatively truncated Laplace prior over the abundance coefficients. This prior imposes both the sparsity assumption and the nonnegativity constraint on the abundance coefficients. Experimental results conducted on the Aviris Cuprite data set are presented that demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2014-06-19 08:06:24 - Majority Vote of Diverse Classifiers for Late Fusion</summary>

- *Emilie Morvant, Amaury Habrard, StÃ©phane Ayache*

- `1404.7796v2` - [abs](http://arxiv.org/abs/1404.7796v2) - [pdf](http://arxiv.org/pdf/1404.7796v2)

> In the past few years, a lot of attention has been devoted to multimedia indexing by fusing multimodal informations. Two kinds of fusion schemes are generally considered: The early fusion and the late fusion. We focus on late classifier fusion, where one combines the scores of each modality at the decision level. To tackle this problem, we investigate a recent and elegant well-founded quadratic program named MinCq coming from the machine learning PAC-Bayesian theory. MinCq looks for the weighted combination, over a set of real-valued functions seen as voters, leading to the lowest misclassification rate, while maximizing the voters' diversity. We propose an extension of MinCq tailored to multimedia indexing. Our method is based on an order-preserving pairwise loss adapted to ranking that allows us to improve Mean Averaged Precision measure while taking into account the diversity of the voters that we want to fuse. We provide evidence that this method is naturally adapted to late fusion procedures and confirm the good behavior of our approach on the challenging PASCAL VOC'07 benchmark.

</details>

<details>

<summary>2014-06-19 14:32:48 - Overcoming computational inability to predict clinical outcome from high-dimensional patient data using Bayesian methods</summary>

- *A Shalabi, A C C Coolen, E de Rinaldis*

- `1406.5062v1` - [abs](http://arxiv.org/abs/1406.5062v1) - [pdf](http://arxiv.org/pdf/1406.5062v1)

> Clinical outcome prediction from high-dimensional data is problematic in the common setting where there is only a relatively small number of samples. The imbalance causes data overfitting, and outcome prediction becomes computationally expensive or even impossible. We propose a Bayesian outcome prediction method that can be applied to data of arbitrary dimension d, from 2 outcome classes, and reduces overfitting without any approximations at parameter level. This is achieved by avoiding numerical integration or approximation, and solving the Bayesian integrals analytically. We thereby reduce the dimension of numerical integrals from 2d dimensions to 4, for any d. For large d, this is reduced further to 3, and we obtain a simple outcome prediction formula without integrals in leading order for very large d. We compare our method to the mclustDA method (Fraley and Raftery 2002), using simulated and real data sets. Our method perform as well as or better than mclustDA in low dimensions d. In large dimensions d, mclustDA breaks down due to computational limitations, while our method provides a feasible and computationally efficient alternative.

</details>

<details>

<summary>2014-06-19 15:06:02 - Unsupervised Unmixing of Hyperspectral Images Accounting for Endmember Variability</summary>

- *Abderrahim Halimi, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1406.5071v1` - [abs](http://arxiv.org/abs/1406.5071v1) - [pdf](http://arxiv.org/pdf/1406.5071v1)

> This paper presents an unsupervised Bayesian algorithm for hyperspectral image unmixing accounting for endmember variability. The pixels are modeled by a linear combination of endmembers weighted by their corresponding abundances. However, the endmembers are assumed random to take into account their variability in the image. An additive noise is also considered in the proposed model generalizing the normal compositional model. The proposed algorithm exploits the whole image to provide spectral and spatial information. It estimates both the mean and the covariance matrix of each endmember in the image. This allows the behavior of each material to be analyzed and its variability to be quantified in the scene. A spatial segmentation is also obtained based on the estimated abundances. In order to estimate the parameters associated with the proposed Bayesian model, we propose to use a Hamiltonian Monte Carlo algorithm. The performance of the resulting unmixing strategy is evaluated via simulations conducted on both synthetic and real data.

</details>

<details>

<summary>2014-06-23 10:22:49 - A Fast Algorithm for Sampling from the Posterior of a von Mises distribution</summary>

- *Peter G. M. Forbes, Kanti V. Mardia*

- `1402.3569v2` - [abs](http://arxiv.org/abs/1402.3569v2) - [pdf](http://arxiv.org/pdf/1402.3569v2)

> Motivated by molecular biology, there has been an upsurge of research activities in directional statistics in general and its Bayesian aspect in particular. The central distribution for the circular case is von Mises distribution which has two parameters (mean and concentration) akin to the univariate normal distribution. However, there has been a challenge to sample efficiently from the posterior distribution of the concentration parameter. We describe a novel, highly efficient algorithm to sample from the posterior distribution and fill this long-standing gap.

</details>

<details>

<summary>2014-06-24 10:05:02 - Iterative regularization for ensemble data assimilation in reservoir models</summary>

- *Marco A. Iglesias*

- `1401.5375v2` - [abs](http://arxiv.org/abs/1401.5375v2) - [pdf](http://arxiv.org/pdf/1401.5375v2)

> We propose the application of iterative regularization for the development of ensemble methods for solving Bayesian inverse problems. In concrete, we construct (i) a variational iterative regularizing ensemble Levenberg-Marquardt method (IR-enLM) and (ii) a derivative-free iterative ensemble Kalman smoother (IR-ES). The aim of these methods is to provide a robust ensemble approximation of the Bayesian posterior. The proposed methods are based on fundamental ideas from iterative regularization methods that have been widely used for the solution of deterministic inverse problems [21]. In this work we are interested in the application of the proposed ensemble methods for the solution of Bayesian inverse problems that arise in reservoir modeling applications. The proposed ensemble methods use key aspects of the regularizing Levenberg-Marquardt scheme developed by Hanke [16] and that we recently applied for history matching in [18].   In the case where the forward operator is linear and the prior is Gaussian, we show that the proposed IR-enLM and IR-ES coincide with standard randomized maximum likelihood (RML) and the ensemble smoother (ES) respectively. For the general nonlinear case, we develop a numerical framework to assess the performance of the proposed ensemble methods at capturing the posterior. This framework consists of using a state-of-the art MCMC method for resolving the Bayesian posterior from synthetic experiments. The resolved posterior via MCMC then provides a gold standard against to which compare the proposed IR-enLM and IR-ES. We show that for the careful selection of regularization parameters, robust approximations of the posterior can be accomplished in terms of mean and variance. Our numerical experiments showcase the advantage of using iterative regularization for obtaining more robust and stable approximation of the posterior than standard unregularized methods.

</details>

<details>

<summary>2014-06-24 10:56:13 - Combining predictions from linear models when training and test inputs differ</summary>

- *Thijs van Ommen*

- `1406.6200v1` - [abs](http://arxiv.org/abs/1406.6200v1) - [pdf](http://arxiv.org/pdf/1406.6200v1)

> Methods for combining predictions from different models in a supervised learning setting must somehow estimate/predict the quality of a model's predictions at unknown future inputs. Many of these methods (often implicitly) make the assumption that the test inputs are identical to the training inputs, which is seldom reasonable. By failing to take into account that prediction will generally be harder for test inputs that did not occur in the training set, this leads to the selection of too complex models. Based on a novel, unbiased expression for KL divergence, we propose XAIC and its special case FAIC as versions of AIC intended for prediction that use different degrees of knowledge of the test inputs. Both methods substantially differ from and may outperform all the known versions of AIC even when the training and test inputs are iid, and are especially useful for deterministic inputs and under covariate shift. Our experiments on linear models suggest that if the test and training inputs differ substantially, then XAIC and FAIC predictively outperform AIC, BIC and several other methods including Bayesian model averaging.

</details>

<details>

<summary>2014-06-24 20:00:20 - What we talk about when we talk about fields</summary>

- *Ewan Cameron*

- `1406.6371v1` - [abs](http://arxiv.org/abs/1406.6371v1) - [pdf](http://arxiv.org/pdf/1406.6371v1)

> In astronomical and cosmological studies one often wishes to infer some properties of an infinite-dimensional field indexed within a finite-dimensional metric space given only a finite collection of noisy observational data. Bayesian inference offers an increasingly-popular strategy to overcome the inherent ill-posedness of this signal reconstruction challenge. However, there remains a great deal of confusion within the astronomical community regarding the appropriate mathematical devices for framing such analyses and the diversity of available computational procedures for recovering posterior functionals. In this brief research note I will attempt to clarify both these issues from an "applied statistics" perpective, with insights garnered from my post-astronomy experiences as a computational Bayesian / epidemiological geostatistician.

</details>

<details>

<summary>2014-06-25 19:07:52 - Learning the ergodic decomposition</summary>

- *Nabil Al-Najjar, Eran Shmaya*

- `1406.6670v1` - [abs](http://arxiv.org/abs/1406.6670v1) - [pdf](http://arxiv.org/pdf/1406.6670v1)

> A Bayesian agent learns about the structure of a stationary process from ob- serving past outcomes. We prove that his predictions about the near future become ap- proximately those he would have made if he knew the long run empirical frequencies of the process.

</details>

<details>

<summary>2014-06-25 22:40:49 - Bayesian Survival Modelling of University Outcomes</summary>

- *Catalina A. Vallejos, Mark F. J. Steel*

- `1406.6728v1` - [abs](http://arxiv.org/abs/1406.6728v1) - [pdf](http://arxiv.org/pdf/1406.6728v1)

> The aim of this paper is to model the length of registration at university and its associated academic outcome for undergraduate students at the Pontificia Universidad Cat\'olica de Chile. Survival time is defined as the time until the end of the enrollment period, which can relate to different reasons - graduation or two types of dropout - that are driven by different processes. Hence, a competing risks model is employed for the analysis. The issue of separation of the outcomes (which precludes maximum likelihood estimation) is handled through the use of Bayesian inference with an appropriately chosen prior. We are interested in identifying important determinants of university outcomes and the associ- ated model uncertainty is formally addressed through Bayesian model averaging. The methodology introduced for modelling university outcomes is applied to three selected degree programmes, which are particularly affected by dropout and late graduation.

</details>

<details>

<summary>2014-06-26 12:21:51 - Coping with area price risk in electricity markets: Forecasting Contracts for Difference in the Nordic power market</summary>

- *Egil Ferkingstad, Anders LÃ¸land*

- `1406.6862v1` - [abs](http://arxiv.org/abs/1406.6862v1) - [pdf](http://arxiv.org/pdf/1406.6862v1)

> Contracts for Difference (CfDs) are forwards on the spread between an area price and the system price. Together with the system price forwards, these products are used to hedge the area price risk in the Nordic electricity market. The CfDs are typically available for the next two months, three quarters and three years. This is fine, except that CfDs are not traded at NASDAQ OMX Commodities for every Nord Pool Spot price area. We therefore ask the hypothetical question: What would the CfD market price have been, say in the price area NO2, if it had been traded? We build regression models for each observable price area, and use Bayesian elicitation techniques to obtain prior information on how similar the different price areas are to forecast the price in an area where CfDs are not traded.

</details>

<details>

<summary>2014-06-29 00:48:35 - Semiparametric Bayesian Density Estimation with Disparate Data Sources: A Meta-Analysis of Global Childhood Undernutrition</summary>

- *Mariel M. Finucane, Christopher J. Paciorek, Gretchen A. Stevens, Majid Ezzati*

- `1301.5390v3` - [abs](http://arxiv.org/abs/1301.5390v3) - [pdf](http://arxiv.org/pdf/1301.5390v3)

> Undernutrition, resulting in restricted growth, and quantified here using height-for-age z-scores, is an important contributor to childhood morbidity and mortality. Since all levels of mild, moderate and severe undernutrition are of clinical and public health importance, it is of interest to estimate the shape of the z-scores' distributions.   We present a finite normal mixture model that uses data on 4.3 million children to make annual country-specific estimates of these distributions for under-5-year-old children in the world's 141 low- and middle-income countries between 1985 and 2011. We incorporate both individual-level data when available, as well as aggregated summary statistics from studies whose individual-level data could not be obtained. We place a hierarchical Bayesian probit stick-breaking model on the mixture weights. The model allows for nonlinear changes in time, and it borrows strength in time, in covariates, and within and across regional country clusters to make estimates where data are uncertain, sparse, or missing.   This work addresses three important problems that often arise in the fields of public health surveillance and global health monitoring. First, data are always incomplete. Second, different data sources commonly use different reporting metrics. Last, distributions, and especially their tails, are often of substantive interest.

</details>

<details>

<summary>2014-06-30 10:16:51 - Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data</summary>

- *Mingjun Zhong, Rong Liu, Bo Liu*

- `1210.3456v2` - [abs](http://arxiv.org/abs/1210.3456v2) - [pdf](http://arxiv.org/pdf/1210.3456v2)

> MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play important regulatory roles in post-transcriptional gene regulation by inhibiting the translation of the mRNA into proteins or otherwise cleaving the target mRNA. Inferring miRNA targets provides useful information for understanding the roles of miRNA in biological processes that are potentially involved in complex diseases. Statistical methodologies for point estimation, such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm, have been proposed to identify the interactions of miRNA and mRNA based on sequence and expression data. In this paper, we propose using the Bayesian LASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the interactions between miRNA and mRNA using expression data. The proposed Bayesian methods explore the posterior distributions for those parameters required to model the miRNA-mRNA interactions. These approaches can be used to observe the inferred effects of the miRNAs on the targets by plotting the posterior distributions of those parameters. For comparison purposes, the Least Squares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO (nLASSO), and the proposed Bayesian approaches were applied to four public datasets. We concluded that nLASSO and nBLASSO perform best in terms of sensitivity and specificity. Compared to the point estimate algorithms, which only provide single estimates for those parameters, the Bayesian methods are more meaningful and provide credible intervals, which take into account the uncertainty of the inferred interactions of the miRNA and mRNA. Furthermore, Bayesian methods naturally provide statistical significance to select convincing inferred interactions, while point estimate algorithms require a manually chosen threshold, which is less meaningful, to choose the possible interactions.

</details>

<details>

<summary>2014-06-30 12:19:17 - Relevance Singular Vector Machine for low-rank matrix sensing</summary>

- *Martin Sundin, Saikat Chatterjee, Magnus Jansson, Cristian R. Rojas*

- `1407.0013v1` - [abs](http://arxiv.org/abs/1407.0013v1) - [pdf](http://arxiv.org/pdf/1407.0013v1)

> In this paper we develop a new Bayesian inference method for low rank matrix reconstruction. We call the new method the Relevance Singular Vector Machine (RSVM) where appropriate priors are defined on the singular vectors of the underlying matrix to promote low rank. To accelerate computations, a numerically efficient approximation is developed. The proposed algorithms are applied to matrix completion and matrix reconstruction problems and their performance is studied numerically.

</details>

<details>

<summary>2014-06-30 14:35:58 - Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters</summary>

- *Ziyu Wang, Nando de Freitas*

- `1406.7758v1` - [abs](http://arxiv.org/abs/1406.7758v1) - [pdf](http://arxiv.org/pdf/1406.7758v1)

> Bayesian optimisation has gained great popularity as a tool for optimising the parameters of machine learning algorithms and models. Somewhat ironically, setting up the hyper-parameters of Bayesian optimisation methods is notoriously hard. While reasonable practical solutions have been advanced, they can often fail to find the best optima. Surprisingly, there is little theoretical analysis of this crucial problem in the literature. To address this, we derive a cumulative regret bound for Bayesian optimisation with Gaussian processes and unknown kernel hyper-parameters in the stochastic setting. The bound, which applies to the expected improvement acquisition function and sub-Gaussian observation noise, provides us with guidelines on how to design hyper-parameter estimation methods. A simple simulation demonstrates the importance of following these guidelines.

</details>

<details>

<summary>2014-06-30 20:18:18 - Infinite Structured Hidden Semi-Markov Models</summary>

- *Jonathan H. Huggins, Frank Wood*

- `1407.0044v1` - [abs](http://arxiv.org/abs/1407.0044v1) - [pdf](http://arxiv.org/pdf/1407.0044v1)

> This paper reviews recent advances in Bayesian nonparametric techniques for constructing and performing inference in infinite hidden Markov models. We focus on variants of Bayesian nonparametric hidden Markov models that enhance a posteriori state-persistence in particular. This paper also introduces a new Bayesian nonparametric framework for generating left-to-right and other structured, explicit-duration infinite hidden Markov models that we call the infinite structured hidden semi-Markov model.

</details>

<details>

<summary>2014-06-30 20:36:54 - Posterior predictive checks to quantify lack-of-fit in admixture models of latent population structure</summary>

- *David Mimno, David M Blei, Barbara E Engelhardt*

- `1407.0050v1` - [abs](http://arxiv.org/abs/1407.0050v1) - [pdf](http://arxiv.org/pdf/1407.0050v1)

> Admixture models are a ubiquitous approach to capture latent population structure in genetic samples. Despite the widespread application of admixture models, little thought has been devoted to the quality of the model fit or the accuracy of the estimates of parameters of interest for a particular study. Here we develop methods for validating admixture models based on posterior predictive checks (PPCs), a Bayesian method for assessing the quality of a statistical model. We develop PPCs for five population-level statistics of interest: within-population genetic variation, background linkage disequilibrium, number of ancestral populations, between-population genetic variation, and the downstream use of admixture parameters to correct for population structure in association studies. Using PPCs, we evaluate the quality of the model estimates for four qualitatively different population genetic data sets: the POPRES European individuals, the HapMap phase 3 individuals, continental Indians, and African American individuals. We found that the same model fitted to different genomic studies resulted in highly study-specific results when evaluated using PPCs, illustrating the utility of PPCs for model-based analyses in large genomic studies.

</details>

<details>

<summary>2014-06-30 21:19:37 - Spatial postprocessing of ensemble forecasts for temperature using nonhomogeneous Gaussian regression</summary>

- *Kira Feldmann, Michael Scheuerer, Thordis L. Thorarinsdottir*

- `1407.0058v1` - [abs](http://arxiv.org/abs/1407.0058v1) - [pdf](http://arxiv.org/pdf/1407.0058v1)

> Statistical postprocessing techniques are commonly used to improve the skill of ensembles of numerical weather forecasts. This paper considers spatial extensions of the well-established nonhomogeneous Gaussian regression (NGR) postprocessing technique for surface temperature and a recent modification thereof in which the local climatology is included in the regression model for a locally adaptive postprocessing. In a comparative study employing 21 h forecasts from the COSMO-DE ensemble predictive system over Germany, two approaches for modeling spatial forecast error correlations are considered: A parametric Gaussian random field model and the ensemble copula coupling approach which utilizes the spatial rank correlation structure of the raw ensemble. Additionally, the NGR methods are compared to both univariate and spatial versions of the ensemble Bayesian model averaging (BMA) postprocessing technique.

</details>


## 2014-07

<details>

<summary>2014-07-02 06:42:55 - Single step and multiple step forecasting in one dimensional single chirp signal using MCMC based Bayesian analysis</summary>

- *Satyaki Mazumder*

- `1406.7143v3` - [abs](http://arxiv.org/abs/1406.7143v3) - [pdf](http://arxiv.org/pdf/1406.7143v3)

> Chirp signals are frequently used in different areas of science and engineering. MCMC based Bayesian inference is done here for purpose of one step and multiple step prediction in case of one dimensional single chirp signal with i.\ i.\ d.\ error structure as well as dependent error structure with exponentially decaying covariances. We use Gibbs sampling technique and random walk MCMC to update the parameters. We perform total five simulation studies for illustration purpose. We also do some real data analysis to show how the method is working in practice.

</details>

<details>

<summary>2014-07-02 08:42:33 - Bayesian inference with dependent normalized completely random measures</summary>

- *Antonio Lijoi, Bernardo Nipoti, Igor PrÃ¼nster*

- `1407.0482v1` - [abs](http://arxiv.org/abs/1407.0482v1) - [pdf](http://arxiv.org/pdf/1407.0482v1)

> The proposal and study of dependent prior processes has been a major research focus in the recent Bayesian nonparametric literature. In this paper, we introduce a flexible class of dependent nonparametric priors, investigate their properties and derive a suitable sampling scheme which allows their concrete implementation. The proposed class is obtained by normalizing dependent completely random measures, where the dependence arises by virtue of a suitable construction of the Poisson random measures underlying the completely random measures. We first provide general distributional results for the whole class of dependent completely random measures and then we specialize them to two specific priors, which represent the natural candidates for concrete implementation due to their analytic tractability: the bivariate Dirichlet and normalized $\sigma$-stable processes. Our analytical results, and in particular the partially exchangeable partition probability function, form also the basis for the determination of a Markov Chain Monte Carlo algorithm for drawing posterior inferences, which reduces to the well-known Blackwell--MacQueen P\'{o}lya urn scheme in the univariate case. Such an algorithm can be used for density estimation and for analyzing the clustering structure of the data and is illustrated through a real two-sample dataset example.

</details>

<details>

<summary>2014-07-02 13:55:55 - Bayesian Regression Analysis of Data with Random Effects Covariates from Nonlinear Longitudinal Measurements</summary>

- *Rolando De la Cruz, Cristian Meza, Ana Arribas-Gil, Raymond J. Carroll*

- `1310.8176v2` - [abs](http://arxiv.org/abs/1310.8176v2) - [pdf](http://arxiv.org/pdf/1310.8176v2)

> Joint models for a wide class of response variables and longitudinal measurements consist on a mixed-effects model to fit longitudinal trajectories whose random effects enter as covariates in a generalized linear model for the primary response. They provide a useful way to asses association between these two kinds of data, which in clinical studies are often collected jointly on a series of individuals and may help understanding, for instance, the mechanisms of recovery of a certain disease or the efficacy of a given therapy. The most common joint model in this framework is based on a linear mixed model for the longitudinal data. However, for complex datasets the linearity assumption may be too restrictive. Some works have considered generalizing this setting with the use of a nonlinear mixed-effects model for the longitudinal trajectories but the proposed estimation procedures based on likelihood approximations have been shown De la Cruz et al. (2011) to exhibit some computational efficiency problems. In this article we propose an MCMC-based estimation procedure in the joint model with a nonlinear mixed-effects model for the longitudinal data and a generalized linear model for the primary response. Moreover, we consider that the errors in the longitudinal model may be correlated. We apply our method to the analysis of hormone levels measured at the early stages of pregnancy that can be used to predict normal versus abnormal pregnancy outcomes. We also conduct a simulation study to asses the importance of modelling correlated errors and quantify the consequences of model misspecification.

</details>

<details>

<summary>2014-07-02 15:32:10 - Nonparametric Hierarchical Clustering of Functional Data</summary>

- *Marc BoullÃ©, Romain GuigourÃ¨s, Fabrice Rossi*

- `1407.0612v1` - [abs](http://arxiv.org/abs/1407.0612v1) - [pdf](http://arxiv.org/pdf/1407.0612v1)

> In this paper, we deal with the problem of curves clustering. We propose a nonparametric method which partitions the curves into clusters and discretizes the dimensions of the curve points into intervals. The cross-product of these partitions forms a data-grid which is obtained using a Bayesian model selection approach while making no assumptions regarding the curves. Finally, a post-processing technique, aiming at reducing the number of clusters in order to improve the interpretability of the clustering, is proposed. It consists in optimally merging the clusters step by step, which corresponds to an agglomerative hierarchical classification whose dissimilarity measure is the variation of the criterion. Interestingly this measure is none other than the sum of the Kullback-Leibler divergences between clusters distributions before and after the merges. The practical interest of the approach for functional data exploratory analysis is presented and compared with an alternative approach on an artificial and a real world data set.

</details>

<details>

<summary>2014-07-02 23:03:44 - The Beta-Gompertz Distribution</summary>

- *Ali Akbar Jafari, Saeid Tahmasebi, Morad Alizadeh*

- `1407.0743v1` - [abs](http://arxiv.org/abs/1407.0743v1) - [pdf](http://arxiv.org/pdf/1407.0743v1)

> In this paper, we introduce a new four-parameter generalized version of the Gompertz model which is called Beta-Gompertz (BG) distribution. It includes some well-known lifetime distributions such as beta-exponential and generalized Gompertz distributions as special sub-models. This new distribution is quite flexible and can be used effectively in modeling survival data and reliability problems. It can have a decreasing, increasing, and bathtub-shaped failure rate function depending on its parameters. Some mathematical properties of the new distribution, such as closed-form expressions for the density, cumulative distribution, hazard rate function, the $k$th order moment, moment generating function, Shannon entropy, and the quantile measure are provided. We discuss maximum likelihood estimation of the BG parameters from one observed sample and derive the observed Fisher's information matrix. A simulation study is performed in order to investigate this proposed estimator for parameters. At the end, in order to show the BG distribution flexibility, an application using a real data set is presented.

</details>

<details>

<summary>2014-07-03 05:31:20 - The spatial distribution in infinite dimensional spaces and related quantiles and depths</summary>

- *Anirvan Chakraborty, Probal Chaudhuri*

- `1402.3480v2` - [abs](http://arxiv.org/abs/1402.3480v2) - [pdf](http://arxiv.org/pdf/1402.3480v2)

> The spatial distribution has been widely used to develop various nonparametric procedures for finite dimensional multivariate data. In this paper, we investigate the concept of spatial distribution for data in infinite dimensional Banach spaces. Many technical difficulties are encountered in such spaces that are primarily due to the noncompactness of the closed unit ball. In this work, we prove some Glivenko-Cantelli and Donsker-type results for the empirical spatial distribution process in infinite dimensional spaces. The spatial quantiles in such spaces can be obtained by inverting the spatial distribution function. A Bahadur-type asymptotic linear representation and the associated weak convergence results for the sample spatial quantiles in infinite dimensional spaces are derived. A study of the asymptotic efficiency of the sample spatial median relative to the sample mean is carried out for some standard probability distributions in function spaces. The spatial distribution can be used to define the spatial depth in infinite dimensional Banach spaces, and we study the asymptotic properties of the empirical spatial depth in such spaces. We also demonstrate the spatial quantiles and the spatial depth using some real and simulated functional data.

</details>

<details>

<summary>2014-07-03 15:01:24 - Bayesian nonparametric inference on the Stiefel manifold</summary>

- *Lizhen Lin, Vinayak Rao, David B. Dunson*

- `1311.0907v2` - [abs](http://arxiv.org/abs/1311.0907v2) - [pdf](http://arxiv.org/pdf/1311.0907v2)

> The Stiefel manifold $V_{p,d}$ is the space of all $d \times p$ orthonormal matrices, with the $d-1$ hypersphere and the space of all orthogonal matrices constituting special cases. In modeling data lying on the Stiefel manifold, parametric distributions such as the matrix Langevin distribution are often used; however, model misspecification is a concern and it is desirable to have nonparametric alternatives. Current nonparametric methods are Fr\'echet mean based. We take a fully generative nonparametric approach, which relies on mixing parametric kernels such as the matrix Langevin. The proposed kernel mixtures can approximate a large class of distributions on the Stiefel manifold, and we develop theory showing posterior consistency. While there exists work developing general posterior consistency results, extending these results to this particular manifold requires substantial new theory. Posterior inference is illustrated on a real-world dataset of near-Earth objects.

</details>

<details>

<summary>2014-07-04 13:03:27 - Comparison of multivariate distributions using quantile-quantile plots and related tests</summary>

- *Subhra Sankar Dhar, Biman Chakraborty, Probal Chaudhuri*

- `1407.1212v1` - [abs](http://arxiv.org/abs/1407.1212v1) - [pdf](http://arxiv.org/pdf/1407.1212v1)

> The univariate quantile-quantile (Q-Q) plot is a well-known graphical tool for examining whether two data sets are generated from the same distribution or not. It is also used to determine how well a specified probability distribution fits a given sample. In this article, we develop and study a multivariate version of the Q-Q plot based on the spatial quantile. The usefulness of the proposed graphical device is illustrated on different real and simulated data, some of which have fairly large dimensions. We also develop certain statistical tests that are related to the proposed multivariate Q-Q plot and study their asymptotic properties. The performance of those tests are compared with that of some other well-known tests for multivariate distributions available in the literature.

</details>

<details>

<summary>2014-07-04 13:32:04 - Asymptotics of nonparametric L-1 regression models with dependent data</summary>

- *Zhibiao Zhao, Ying Wei, Dennis K. J. Lin*

- `1407.1225v1` - [abs](http://arxiv.org/abs/1407.1225v1) - [pdf](http://arxiv.org/pdf/1407.1225v1)

> We investigate asymptotic properties of least-absolute-deviation or median quantile estimates of the location and scale functions in nonparametric regression models with dependent data from multiple subjects. Under a general dependence structure that allows for longitudinal data and some spatially correlated data, we establish uniform Bahadur representations for the proposed median quantile estimates. The obtained Bahadur representations provide deep insights into the asymptotic behavior of the estimates. Our main theoretical development is based on studying the modulus of continuity of kernel weighted empirical process through a coupling argument. Progesterone data is used for an illustration.

</details>

<details>

<summary>2014-07-06 17:45:11 - Solving Large-Scale PDE-constrained Bayesian Inverse Problems with Riemann Manifold Hamiltonian Monte Carlo</summary>

- *Tan Bui-Thanh, Mark Girolami*

- `1407.1517v1` - [abs](http://arxiv.org/abs/1407.1517v1) - [pdf](http://arxiv.org/pdf/1407.1517v1)

> We consider the Riemann manifold Hamiltonian Monte Carlo (RMHMC) method for solving statistical inverse problems governed by partial differential equations (PDEs). The power of the RMHMC method is that it exploits the geometric structure induced by the PDE constraints of the underlying inverse problem. Consequently, each RMHMC posterior sample is almost independent from the others providing statistically efficient Markov chain simulation. We reduce the cost of forming the Fisher information matrix by using a low rank approximation via a randomized singular value decomposition technique. This is efficient since a small number of Hessian-vector products are required. The Hessian-vector product in turn requires only two extra PDE solves using the adjoint technique. The results suggest RMHMC as a highly efficient simulation scheme for sampling from PDE induced posterior measures.

</details>

<details>

<summary>2014-07-08 01:31:46 - Efficient Bayesian inference for long memory processes</summary>

- *Timothy Graves, Robert B. Gramacy, Christian Franzke, Nicholas Watkins*

- `1403.2940v2` - [abs](http://arxiv.org/abs/1403.2940v2) - [pdf](http://arxiv.org/pdf/1403.2940v2)

> In forecasting problems it is important to know whether or not recent events represent a regime change (low long-term predictive potential), or rather a local manifestation of longer term effects (potentially higher predictive potential). Mathematically, a key question is about whether the underlying stochastic process exhibits "memory", and if so whether the memory is "long" in a precise sense. Being able to detect or rule out such effects can have a profound impact on speculative investment (e.g., in financial markets) and inform public policy (e.g., characterising the size and timescales of the earth system's response to the anthropogenic CO2 perturbation). Most previous work on inference of long memory effects is frequentist in nature. Here we provide a systematic treatment of Bayesian inference for long memory processes via the Autoregressive Fractional Integrated Moving Average (ARFIMA) model. In particular, we provide a new approximate likelihood for efficient parameter inference, and show how nuisance parameters (e.g., short memory effects) can be integrated over in order to focus on long memory parameters and hypothesis testing more directly than ever before. We illustrate our new methodology on both synthetic and observational data, with favorable comparison to the standard estimators.

</details>

<details>

<summary>2014-07-08 19:50:25 - Bayesian Structured Sparsity from Gaussian Fields</summary>

- *Barbara E. Engelhardt, Ryan P. Adams*

- `1407.2235v1` - [abs](http://arxiv.org/abs/1407.2235v1) - [pdf](http://arxiv.org/pdf/1407.2235v1)

> Substantial research on structured sparsity has contributed to analysis of many different applications. However, there have been few Bayesian procedures among this work. Here, we develop a Bayesian model for structured sparsity that uses a Gaussian process (GP) to share parameters of the sparsity-inducing prior in proportion to feature similarity as defined by an arbitrary positive definite kernel. For linear regression, this sparsity-inducing prior on regression coefficients is a relaxation of the canonical spike-and-slab prior that flattens the mixture model into a scale mixture of normals. This prior retains the explicit posterior probability on inclusion parameters---now with GP probit prior distributions---but enables tractable computation via elliptical slice sampling for the latent Gaussian field. We motivate development of this prior using the genomic application of association mapping, or identifying genetic variants associated with a continuous trait. Our Bayesian structured sparsity model produced sparse results with substantially improved sensitivity and precision relative to comparable methods. Through simulations, we show that three properties are key to this improvement: i) modeling structure in the covariates, ii) significance testing using the posterior probabilities of inclusion, and iii) model averaging. We present results from applying this model to a large genomic dataset to demonstrate computational tractability.

</details>

<details>

<summary>2014-07-08 20:00:17 - Inferring latent structures via information inequalities</summary>

- *R. Chaves, L. Luft, T. O. Maciel, D. Gross, D. Janzing, B. SchÃ¶lkopf*

- `1407.2256v1` - [abs](http://arxiv.org/abs/1407.2256v1) - [pdf](http://arxiv.org/pdf/1407.2256v1)

> One of the goals of probabilistic inference is to decide whether an empirically observed distribution is compatible with a candidate Bayesian network. However, Bayesian networks with hidden variables give rise to highly non-trivial constraints on the observed distribution. Here, we propose an information-theoretic approach, based on the insight that conditions on entropies of Bayesian networks take the form of simple linear inequalities. We describe an algorithm for deriving entropic tests for latent structures. The well-known conditional independence tests appear as a special case. While the approach applies for generic Bayesian networks, we presently adopt the causal view, and show the versatility of the framework by treating several relevant problems from that domain: detecting common ancestors, quantifying the strength of causal influence, and inferring the direction of causation from two-variable marginals.

</details>

<details>

<summary>2014-07-11 16:11:58 - Bayesian Model for Multiple Change-points Detection in Multivariate Time Series</summary>

- *Flore HarlÃ©, Florent Chatelain, CÃ©dric Gouy-Pailler, Sophie Achard*

- `1407.3206v1` - [abs](http://arxiv.org/abs/1407.3206v1) - [pdf](http://arxiv.org/pdf/1407.3206v1)

> This paper addresses the issue of detecting change-points in multivariate time series. The proposed approach differs from existing counterparts by making only weak assumptions on both the change-points structure across series, and the statistical signal distributions. Specifically change-points are not assumed to occur at simultaneous time instants across series, and no specific distribution is assumed on the individual signals. It relies on the combination of a local robust statistical test acting on individual time segments, with a global Bayesian framework able to optimize configurations from multiple local statistics (from segments of a unique time series or multiple time series). Using an extensive experimental set-up, our algorithm is shown to perform well on Gaussian data, with the same results in term of recall and precision as classical approaches, such as the fused lasso and the Bernoulli Gaussian model. Furthermore, it outperforms the reference models in the case of non normal data with outliers. The control of the False Discovery Rate by an acceptance level is confirmed. In the case of multivariate data, the probabilities that simultaneous change-points are shared by some specific time series are learned. We finally illustrate our algorithm with real datasets from energy monitoring and genomic. Segmentations are compared to state-of-the-art approaches based on fused lasso and group fused lasso.

</details>

<details>

<summary>2014-07-12 17:23:57 - Counting Markov Blanket Structures</summary>

- *Shyam Visweswaran, Gregory F. Cooper*

- `1407.2483v2` - [abs](http://arxiv.org/abs/1407.2483v2) - [pdf](http://arxiv.org/pdf/1407.2483v2)

> Learning Markov blanket (MB) structures has proven useful in performing feature selection, learning Bayesian networks (BNs), and discovering causal relationships. We present a formula for efficiently determining the number of MB structures given a target variable and a set of other variables. As expected, the number of MB structures grows exponentially. However, we show quantitatively that there are many fewer MB structures that contain the target variable than there are BN structures that contain it. In particular, the ratio of BN structures to MB structures appears to increase exponentially in the number of variables.

</details>

<details>

<summary>2014-07-13 11:07:25 - A Dynamic Approach to Linear Statistical Calibration with an Application in Microwave Radiometry</summary>

- *Derick L. Rivers, Edward L. Boone*

- `1406.7863v4` - [abs](http://arxiv.org/abs/1406.7863v4) - [pdf](http://arxiv.org/pdf/1406.7863v4)

> The problem of statistical calibration of a measuring instrument can be framed both in a statistical context as well as in an engineering context. In the first, the problem is dealt with by distinguishing between the 'classical' approach and the 'inverse' regression approach. Both of these models are static models and are used to estimate exact measurements from measurements that are affected by error. In the engineering context, the variables of interest are considered to be taken at the time at which you observe it. The Bayesian time series analysis method of Dynamic Linear Models (DLM) can be used to monitor the evolution of the measures, thus introducing an dynamic approach to statistical calibration. The research presented employs the use of Bayesian methodology to perform statistical calibration. The DLM's framework is used to capture the time-varying parameters that maybe changing or drifting over time. Two separate DLM based models are presented in this paper. A simulation study is conducted where the two models are compared to some well known 'static' calibration approaches in the literature from both the frequentist and Bayesian perspectives. The focus of the study is to understand how well the dynamic statistical calibration methods performs under various signal-to-noise ratios, r. The posterior distributions of the estimated calibration points as well as the 95% coverage intervals are compared by statistical summaries. These dynamic methods are applied to a microwave radiometry data set.

</details>

<details>

<summary>2014-07-14 01:37:35 - Modeling the Complex Dynamics and Changing Correlations of Epileptic Events</summary>

- *Drausin F. Wulsin, Emily B. Fox, Brian Litt*

- `1402.6951v2` - [abs](http://arxiv.org/abs/1402.6951v2) - [pdf](http://arxiv.org/pdf/1402.6951v2)

> Patients with epilepsy can manifest short, sub-clinical epileptic "bursts" in addition to full-blown clinical seizures. We believe the relationship between these two classes of events---something not previously studied quantitatively---could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes. A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients. We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable number of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes. We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics and demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data. We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures.

</details>

<details>

<summary>2014-07-15 01:40:35 - Quantile and Probability Curves Without Crossing</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Alfred Galichon*

- `0704.3649v3` - [abs](http://arxiv.org/abs/0704.3649v3) - [pdf](http://arxiv.org/pdf/0704.3649v3)

> This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural quantile functions using data on Vietnam veteran status and earnings.

</details>

<details>

<summary>2014-07-15 04:02:51 - Data-Driven Model Reduction for the Bayesian Solution of Inverse Problems</summary>

- *Tiangang Cui, Youssef M. Marzouk, Karen E. Willcox*

- `1403.4290v2` - [abs](http://arxiv.org/abs/1403.4290v2) - [pdf](http://arxiv.org/pdf/1403.4290v2)

> One of the major challenges in the Bayesian solution of inverse problems governed by partial differential equations (PDEs) is the computational cost of repeatedly evaluating numerical PDE models, as required by Markov chain Monte Carlo (MCMC) methods for posterior sampling. This paper proposes a data-driven projection-based model reduction technique to reduce this computational cost. The proposed technique has two distinctive features. First, the model reduction strategy is tailored to inverse problems: the snapshots used to construct the reduced-order model are computed adaptively from the posterior distribution. Posterior exploration and model reduction are thus pursued simultaneously. Second, to avoid repeated evaluations of the full-scale numerical model as in a standard MCMC method, we couple the full-scale model and the reduced-order model together in the MCMC algorithm. This maintains accurate inference while reducing its overall computational cost. In numerical experiments considering steady-state flow in a porous medium, the data-driven reduced-order model achieves better accuracy than a reduced-order model constructed using the classical approach. It also improves posterior sampling efficiency by several orders of magnitude compared to a standard MCMC method.

</details>

<details>

<summary>2014-07-15 04:50:31 - Likelihood-informed dimension reduction for nonlinear inverse problems</summary>

- *Tiangang Cui, James Martin, Youssef M. Marzouk, Antti Solonen, Alessio Spantini*

- `1403.4680v2` - [abs](http://arxiv.org/abs/1403.4680v2) - [pdf](http://arxiv.org/pdf/1403.4680v2)

> The intrinsic dimensionality of an inverse problem is affected by prior information, the accuracy and number of observations, and the smoothing properties of the forward operator. From a Bayesian perspective, changes from the prior to the posterior may, in many problems, be confined to a relatively low-dimensional subspace of the parameter space. We present a dimension reduction approach that defines and identifies such a subspace, called the "likelihood-informed subspace" (LIS), by characterizing the relative influences of the prior and the likelihood over the support of the posterior distribution. This identification enables new and more efficient computational methods for Bayesian inference with nonlinear forward models and Gaussian priors. In particular, we approximate the posterior distribution as the product of a lower-dimensional posterior defined on the LIS and the prior distribution marginalized onto the complementary subspace. Markov chain Monte Carlo sampling can then proceed in lower dimensions, with significant gains in computational efficiency. We also introduce a Rao-Blackwellization strategy that de-randomizes Monte Carlo estimates of posterior expectations for additional variance reduction. We demonstrate the efficiency of our methods using two numerical examples: inference of permeability in a groundwater system governed by an elliptic PDE, and an atmospheric remote sensing problem based on Global Ozone Monitoring System (GOMOS) observations.

</details>

<details>

<summary>2014-07-15 08:46:27 - Item selection by Latent Class-based methods</summary>

- *Francesco Bartolucci, Giorgio E. Montanari, Silvia Pandolfi*

- `1407.3912v1` - [abs](http://arxiv.org/abs/1407.3912v1) - [pdf](http://arxiv.org/pdf/1407.3912v1)

> The evaluation of nursing homes is usually based on the administration of questionnaires made of a large number of polytomous items. In such a context, the Latent Class (LC) model represents a useful tool for clustering subjects in homogenous groups corresponding to different degrees of impairment of the health conditions. It is known that the performance of model-based clustering and the accuracy of the choice of the number of latent classes may be affected by the presence of irrelevant or noise variables. In this paper, we show the application of an item selection algorithm to real data collected within a project, named ULISSE, on the quality-of-life of elderly patients hosted in italian nursing homes. This algorithm, which is closely related to that proposed by Dean and Raftery in 2010, is aimed at finding the subset of items which provides the best clustering according to the Bayesian Information Criterion. At the same time, it allows us to select the optimal number of latent classes. Given the complexity of the ULISSE study, we perform a validation of the results by means of a sensitivity analysis to different specifications of the initial subset of items and of a resampling procedure.

</details>

<details>

<summary>2014-07-15 11:45:15 - Analysis of the Gibbs sampler for hierarchical inverse problems</summary>

- *Sergios Agapiou, Johnathan M. Bardsley, Omiros Papaspiliopoulos, Andrew M. Stuart*

- `1311.1138v3` - [abs](http://arxiv.org/abs/1311.1138v3) - [pdf](http://arxiv.org/pdf/1311.1138v3)

> Many inverse problems arising in applications come from continuum models where the unknown parameter is a field. In practice the unknown field is discretized resulting in a problem in $\mathbb{R}^N$, with an understanding that refining the discretization, that is increasing $N$, will often be desirable. In the context of Bayesian inversion this situation suggests the importance of two issues: (i) defining hyper-parameters in such a way that they are interpretable in the continuum limit $N \to \infty$ and so that their values may be compared between different discretization levels; (ii) understanding the efficiency of algorithms for probing the posterior distribution, as a function of large $N.$ Here we address these two issues in the context of linear inverse problems subject to additive Gaussian noise within a hierarchical modelling framework based on a Gaussian prior for the unknown field and an inverse-gamma prior for a hyper-parameter, namely the amplitude of the prior variance. The structure of the model is such that the Gibbs sampler can be easily implemented for probing the posterior distribution. Subscribing to the dogma that one should think infinite-dimensionally before implementing in finite dimensions, we present function space intuition and provide rigorous theory showing that as $N$ increases, the component of the Gibbs sampler for sampling the amplitude of the prior variance becomes increasingly slower. We discuss a reparametrization of the prior variance that is robust with respect to the increase in dimension; we give numerical experiments which exhibit that our reparametrization prevents the slowing down. Our intuition on the behaviour of the prior hyper-parameter, with and without reparametrization, is sufficiently general to include a broad class of nonlinear inverse problems as well as other families of hyper-priors.

</details>

<details>

<summary>2014-07-15 20:14:05 - Automatic discovery of cell types and microcircuitry from neural connectomics</summary>

- *Eric Jonas, Konrad Kording*

- `1407.4137v1` - [abs](http://arxiv.org/abs/1407.4137v1) - [pdf](http://arxiv.org/pdf/1407.4137v1)

> Neural connectomics has begun producing massive amounts of data, necessitating new analysis methods to discover the biological and computational structure. It has long been assumed that discovering neuron types and their relation to microcircuitry is crucial to understanding neural function. Here we developed a nonparametric Bayesian technique that identifies neuron types and microcircuitry patterns in connectomics data. It combines the information traditionally used by biologists, including connectivity, cell body location and the spatial distribution of synapses, in a principled and probabilistically-coherent manner. We show that the approach recovers known neuron types in the retina and enables predictions of connectivity, better than simpler algorithms. It also can reveal interesting structure in the nervous system of C. elegans, and automatically discovers the structure of a microprocessor. Our approach extracts structural meaning from connectomics, enabling new approaches of automatically deriving anatomical insights from these emerging datasets.

</details>

<details>

<summary>2014-07-16 16:20:13 - Adaptive ABC model choice and geometric summary statistics for hidden Gibbs random fields</summary>

- *Julien Stoehr, Pierre Pudlo, Lionel Cucala*

- `1402.1380v2` - [abs](http://arxiv.org/abs/1402.1380v2) - [pdf](http://arxiv.org/pdf/1402.1380v2)

> Selecting between different dependency structures of hidden Markov random field can be very challenging, due to the intractable normalizing constant in the likelihood. We answer this question with approximate Bayesian computation (ABC) which provides a model choice method in the Bayesian paradigm. This comes after the work of Grelaud et al. (2009) who exhibited sufficient statistics on directly observed Gibbs random fields. But when the random field is latent, the sufficiency falls and we complement the set with geometric summary statistics. The general approach to construct these intuitive statistics relies on a clustering analysis of the sites based on the observed colors and plausible latent graphs. The efficiency of ABC model choice based on these statistics is evaluated via a local error rate which may be of independent interest. As a byproduct we derived an ABC algorithm that adapts the dimension of the summary statistics to the dataset without distorting the model selection.

</details>

<details>

<summary>2014-07-17 18:58:46 - Confidence Statements for Ordering Quantiles</summary>

- *Carlos A. de B. Pereira, Cassio P. de Campos, Adriano Polpo*

- `1212.5405v2` - [abs](http://arxiv.org/abs/1212.5405v2) - [pdf](http://arxiv.org/pdf/1212.5405v2)

> This work proposes Quor, a simple yet effective nonparametric method to compare independent samples with respect to corresponding quantiles of their populations. The method is solely based on the order statistics of the samples, and independence is its only requirement. All computations are performed using exact distributions with no need for any asymptotic considerations, and yet can be run using a fast quadratic-time dynamic programming idea. Computational performance is essential in high-dimensional domains, such as gene expression data. We describe the approach and discuss on the most important assumptions, building a parallel with assumptions and properties of widely used techniques for the same problem. Experiments using real data from biomedical studies are performed to empirically compare Quor and other methods in a classification task over a selection of high-dimensional data sets.

</details>

<details>

<summary>2014-07-18 08:18:12 - Certainty bands for the conditional cumulative distribution function and applications</summary>

- *Sandie Ferrigno, Bernard Foliguet, Myriam Maumy-Bertrand, AurÃ©lie Muller-Gueudin*

- `1407.4909v1` - [abs](http://arxiv.org/abs/1407.4909v1) - [pdf](http://arxiv.org/pdf/1407.4909v1)

> In this paper, we establish uniform asymptotic certainty bands for the conditional cumulative distribution function. To this aim, we give exact rate of strong uniform consistency for the local linear estimator of this function. The corollaries of this result are the asymptotic certainty bands for the quantiles and the regression function. We illustrate our results with simulations and an application on fetopathologic data.

</details>

<details>

<summary>2014-07-18 12:18:37 - The Rate of Convergence for Approximate Bayesian Computation</summary>

- *Stuart Barber, Jochen Voss, Mark Webster*

- `1311.2038v3` - [abs](http://arxiv.org/abs/1311.2038v3) - [pdf](http://arxiv.org/pdf/1311.2038v3)

> Approximate Bayesian Computation (ABC) is a popular computational method for likelihood-free Bayesian inference. The term "likelihood-free" refers to problems where the likelihood is intractable to compute or estimate directly, but where it is possible to generate simulated data $X$ relatively easily given a candidate set of parameters $\theta$ simulated from a prior distribution. Parameters which generate simulated data within some tolerance $\delta$ of the observed data $x^*$ are regarded as plausible, and a collection of such $\theta$ is used to estimate the posterior distribution $\theta\,|\,X\!=\!x^*$. Suitable choice of $\delta$ is vital for ABC methods to return good approximations to $\theta$ in reasonable computational time.   While ABC methods are widely used in practice, particularly in population genetics, study of the mathematical properties of ABC estimators is still in its infancy. We prove that ABC estimates converge to the exact solution under very weak assumptions and, under slightly stronger assumptions, quantify the rate of this convergence. Our results can be used to guide the choice of the tolerance parameter $\delta$.

</details>

<details>

<summary>2014-07-18 14:51:34 - Bayesian Nonparametric Crowdsourcing</summary>

- *Pablo G. Moreno, Yee Whye Teh, Fernando Perez-Cruz, Antonio ArtÃ©s-RodrÃ­guez*

- `1407.5017v1` - [abs](http://arxiv.org/abs/1407.5017v1) - [pdf](http://arxiv.org/pdf/1407.5017v1)

> Crowdsourcing has been proven to be an effective and efficient tool to annotate large datasets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese Restaurant Process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms.

</details>

<details>

<summary>2014-07-22 09:31:31 - Reflecting about Selecting Noninformative Priors</summary>

- *Kaniav Kamary, Christian P. Robert*

- `1402.6257v3` - [abs](http://arxiv.org/abs/1402.6257v3) - [pdf](http://arxiv.org/pdf/1402.6257v3)

> Following the critical review of Seaman et al. (2012), we reflect on what is presumably the most essential aspect of Bayesian statistics, namely the selection of a prior density. In some cases, Bayesian inference remains fairly stable under a large range of noninformative prior distributions. However, as discussed by \citet{Hd}, there may also be unintended consequences of a choice of a noninformative prior and, these authors consider this problem ignored in Bayesian studies. As they based their argumentation on four examples, we reassess these examples and their Bayesian processing via different prior choices. Our conclusion is to lower the degree of worry about the impact of the prior, exhibiting an overall stability of the posterior distributions. We thus consider that the warnings of Seaman et al. (2012), while commendable, do not jeopardize the use of most noninformative priors.

</details>

<details>

<summary>2014-07-22 10:12:51 - Multi-agents adaptive estimation and coverage control using Gaussian regression</summary>

- *Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schenato, Gianluigi Pillonetto*

- `1407.5807v1` - [abs](http://arxiv.org/abs/1407.5807v1) - [pdf](http://arxiv.org/pdf/1407.5807v1)

> We consider a scenario where the aim of a group of agents is to perform the optimal coverage of a region according to a sensory function. In particular, centroidal Voronoi partitions have to be computed. The difficulty of the task is that the sensory function is unknown and has to be reconstructed on line from noisy measurements. Hence, estimation and coverage needs to be performed at the same time. We cast the problem in a Bayesian regression framework, where the sensory function is seen as a Gaussian random field. Then, we design a set of control inputs which try to well balance coverage and estimation, also discussing convergence properties of the algorithm. Numerical experiments show the effectivness of the new approach.

</details>

<details>

<summary>2014-07-22 17:55:29 - A path-integral approach to Bayesian inference for inverse problems using the semiclassical approximation</summary>

- *Joshua C Chang, Van Savage, Tom Chou*

- `1312.2974v4` - [abs](http://arxiv.org/abs/1312.2974v4) - [pdf](http://arxiv.org/pdf/1312.2974v4)

> We demonstrate how path integrals often used in problems of theoretical physics can be adapted to provide a machinery for performing Bayesian inference in function spaces. Such inference comes about naturally in the study of inverse problems of recovering continuous (infinite dimensional) coefficient functions from ordinary or partial differential equations (ODE, PDE), a problem which is typically ill-posed. Regularization of these problems using $L^2$ function spaces (Tikhonov regularization) is equivalent to Bayesian probabilistic inference, using a Gaussian prior. The Bayesian interpretation of inverse problem regularization is useful since it allows one to quantify and characterize error and degree of precision in the solution of inverse problems, as well as examine assumptions made in solving the problem -- namely whether the subjective choice of regularization is compatible with prior knowledge. Using path-integral formalism, Bayesian inference can be explored through various perturbative techniques, such as the semiclassical approximation, which we use in this manuscript. Perturbative path-integral approaches, while offering alternatives to computational approaches like Markov-Chain-Monte-Carlo (MCMC), also provide natural starting points for MCMC methods that can be used to refine approximations.   In this manuscript, we illustrate a path-integral formulation for inverse problems and demonstrate it on an inverse problem in membrane biophysics as well as inverse problems in potential theories involving the Poisson equation.

</details>

<details>

<summary>2014-07-23 14:40:31 - Frequency behaviour for multinomial counts of fisheries discards via a nested wavelet zero and N inflated binomial model</summary>

- *Andrew C. Parnell, Norman Graham, Andrew L. Jackson, Mafalda Viana*

- `1407.6242v1` - [abs](http://arxiv.org/abs/1407.6242v1) - [pdf](http://arxiv.org/pdf/1407.6242v1)

> In this paper we identify the changing frequency behaviour of multinomial counts of fish species discarded by vessels in the Irish Sea. We use a Bayesian hierarchical model which captures dynamic frequency changes via a shrinkage model applied to wavelet basis functions. Wavelets are known for capturing data features at different temporal scales; we use a recently-proposed shrinkage prior from the factor analysis literature so that features at the finest levels of detail exhibit the greatest shrinkage. Rather than using a multinomial distribution for monitoring the changes in discards over time, which can be slow to fit and inflexible, we use a nested zero-and-N inflated (ZaNI) binomial distribution which enables much faster computation with no obvious deterioration in model flexibility. Our results show that seasonal behaviour in these data are not regular and occur at different frequencies. We also show that the nested ZaNI binomial distribution is a good fit to multinomial count data of this sort when an informative nested structure is applied.

</details>

<details>

<summary>2014-07-24 06:06:11 - Inference for Quantile Measures of Kurtosis, Peakedness and Tail-weight</summary>

- *R. G Staudte*

- `1407.6461v1` - [abs](http://arxiv.org/abs/1407.6461v1) - [pdf](http://arxiv.org/pdf/1407.6461v1)

> Many measures of peakedness, heavy-tailedness and kurtosis have been proposed in the literature, mainly because kurtosis, as originally defined, is a complex combination of the other two concepts. Insight into all three concepts can be gained by studying Ruppert's ratios of interquantile ranges. They are not only monotone in Horn's measure of peakedness when applied to the central portion of the population, but also monotone in the practical tail-index of Morgenthaler and Tukey, when applied to the tails. Distribution-free confidence intervals are found for Ruppert's ratios, and sample sizes required to obtain such intervals for a pre-specified relative width and level are provided. In addition, the empirical power of distribution-free tests for peakedness and bimodality are found for symmetric beta families and mixtures of $t$ distributions. An R script that computes the confidence intervals is provided in online supplementary material.

</details>

<details>

<summary>2014-07-24 10:34:37 - Bayesian identification of protein differential expression in multi-group isobaric labelled mass spectrometry data</summary>

- *Howsun Jow, Richard J. Boys, Darren J. Wilkinson*

- `1407.6519v1` - [abs](http://arxiv.org/abs/1407.6519v1) - [pdf](http://arxiv.org/pdf/1407.6519v1)

> In this paper we develop a Bayesian statistical inference approach to the unified analysis of isobaric labelled MS/MS proteomic data across multiple experiments. An explicit probabilistic model of the log-intensity of the isobaric labels' reporter ions across multiple pre-defined groups and experiments is developed. This is then used to develop a full Bayesian statistical methodology for the identification of differentially expressed proteins, with respect to a control group, across multiple groups and experiments. This methodology is implemented and then evaluated on simulated data and on two model experimental datasets (for which the differentially expressed proteins are known) that use a TMT labelling protocol.

</details>

<details>

<summary>2014-07-25 15:57:49 - Efficient Bayesian Nonparametric Modelling of Structured Point Processes</summary>

- *Tom Gunter, Chris Lloyd, Michael A. Osborne, Stephen J. Roberts*

- `1407.6949v1` - [abs](http://arxiv.org/abs/1407.6949v1) - [pdf](http://arxiv.org/pdf/1407.6949v1)

> This paper presents a Bayesian generative model for dependent Cox point processes, alongside an efficient inference scheme which scales as if the point processes were modelled independently. We can handle missing data naturally, infer latent structure, and cope with large numbers of observed processes. A further novel contribution enables the model to work effectively in higher dimensional spaces. Using this method, we achieve vastly improved predictive performance on both 2D and 1D real data, validating our structured approach.

</details>

<details>

<summary>2014-07-26 11:23:53 - Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI</summary>

- *Yue Huang, John Paisley, Qin Lin, Xinghao Ding, Xueyang Fu, Xiao-ping Zhang*

- `1302.2712v3` - [abs](http://arxiv.org/abs/1302.2712v3) - [pdf](http://arxiv.org/pdf/1302.2712v3)

> We develop a Bayesian nonparametric model for reconstructing magnetic resonance images (MRI) from highly undersampled k-space data. We perform dictionary learning as part of the image reconstruction process. To this end, we use the beta process as a nonparametric dictionary learning prior for representing an image patch as a sparse combination of dictionary elements. The size of the dictionary and the patch-specific sparsity pattern are inferred from the data, in addition to other dictionary learning variables. Dictionary learning is performed directly on the compressed image, and so is tailored to the MRI being considered. In addition, we investigate a total variation penalty term in combination with the dictionary learning model, and show how the denoising property of dictionary learning removes dependence on regularization parameters in the noisy setting. We derive a stochastic optimization algorithm based on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use the alternating direction method of multipliers (ADMM) for efficiently performing total variation minimization. We present empirical results on several MRI, which show that the proposed regularization framework can improve reconstruction accuracy over other methods.

</details>

<details>

<summary>2014-07-26 19:42:41 - On Quantizer Design for Distributed Bayesian Estimation in Sensor Networks</summary>

- *Aditya Vempaty, Hao He, Biao Chen, Pramod K. Varshney*

- `1407.7152v1` - [abs](http://arxiv.org/abs/1407.7152v1) - [pdf](http://arxiv.org/pdf/1407.7152v1)

> We consider the problem of distributed estimation under the Bayesian criterion and explore the design of optimal quantizers in such a system. We show that, for a conditionally unbiased and efficient estimator at the fusion center and when local observations have identical distributions, it is optimal to partition the local sensors into groups, with all sensors within a group using the same quantization rule. When all the sensors use identical number of decision regions, use of identical quantizers at the sensors is optimal. When the network is constrained by the capacity of the wireless multiple access channel over which the sensors transmit their quantized observations, we show that binary quantizers at the local sensors are optimal under certain conditions. Based on these observations, we address the location parameter estimation problem and present our optimal quantizer design approach. We also derive the performance limit for distributed location parameter estimation under the Bayesian criterion and find the conditions when the widely used threshold quantizer achieves this limit. We corroborate this result using simulations. We then relax the assumption of conditionally independent observations and derive the optimality conditions of quantizers for conditionally dependent observations. Using counter-examples, we also show that the previous results do not hold in this setting of dependent observations and, therefore, identical quantizers are not optimal.

</details>

<details>

<summary>2014-07-26 20:36:39 - Mixture Model Averaging for Clustering</summary>

- *Yuhong Wei, Paul D. McNicholas*

- `1212.5760v3` - [abs](http://arxiv.org/abs/1212.5760v3) - [pdf](http://arxiv.org/pdf/1212.5760v3)

> In mixture model-based clustering applications, it is common to fit several models from a family and report clustering results from only the `best' one. In such circumstances, selection of this best model is achieved using a model selection criterion, most often the Bayesian information criterion. Rather than throw away all but the best model, we average multiple models that are in some sense close to the best one, thereby producing a weighted average of clustering results. Two (weighted) averaging approaches are considered: averaging the component membership probabilities and averaging models. In both cases, Occam's window is used to determine closeness to the best model and weights are computed within a Bayesian model averaging paradigm. In some cases, we need to merge components before averaging; we introduce a method for merging mixture components based on the adjusted Rand index. The effectiveness of our model-based clustering averaging approaches is illustrated using a family of Gaussian mixture models on real and simulated data.

</details>

<details>

<summary>2014-07-27 14:57:19 - Parameter Estimation: The Proper Way to Use Bayesian Posterior Processes with Brownian Noise</summary>

- *Asaf Cohen*

- `1310.6713v2` - [abs](http://arxiv.org/abs/1310.6713v2) - [pdf](http://arxiv.org/pdf/1310.6713v2)

> This paper studies a problem of Bayesian parameter estimation for a sequence of scaled counting processes whose weak limit is a Brownian motion with an unknown drift. The main result of the paper is that the limit of the posterior distribution processes is, in general, not equal to the posterior distribution process of the mentioned Brownian motion with the unknown drift. Instead, it is equal to the posterior distribution process associated with a Brownian motion with the same unknown drift and a different standard deviation coefficient. The difference between the two standard deviation coefficients can be arbitrarily large. The characterization of the limit of the posterior distribution processes is then applied to a family of stopping time problems. We show that the proper way to find asymptotically optimal solutions to stopping time problems w.r.t.~the scaled counting processes is by looking at the limit of the posterior distribution processes rather than by the naive approach of looking at the limit of the scaled counting processes themselves. The difference between the performances can be arbitrarily large.

</details>

<details>

<summary>2014-07-28 11:16:19 - Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression</summary>

- *Tim Salimans, David A. Knowles*

- `1206.6679v6` - [abs](http://arxiv.org/abs/1206.6679v6) - [pdf](http://arxiv.org/pdf/1206.6679v6)

> We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice.

</details>

<details>

<summary>2014-07-29 02:39:40 - A Statistical Test for Clades in Phylogenies</summary>

- *Thurston H. Y. Dang, Elchanan Mossel*

- `1407.7619v1` - [abs](http://arxiv.org/abs/1407.7619v1) - [pdf](http://arxiv.org/pdf/1407.7619v1)

> We investigated testing the likelihood of a phylogenetic tree by comparison to its subtree pruning and regrafting (SPR) neighbors, with or without re-optimizing branch lengths. This is inspired by aspects of Bayesian significance tests, and the use of SPRs for heuristically finding maximum likelihood trees. Through a number of simulations with the Jukes-Cantor model on various topologies, it is observed that the SPR tests are informative, and reasonably fast compared to searching for the maximum likelihood tree. This suggests that the SPR tests would be a useful addition to the suite of existing statistical tests, for identifying potential inaccuracies of inferred topologies.

</details>

<details>

<summary>2014-07-29 11:40:36 - Genome scans for detecting footprints of local adaptation using a Bayesian factor model</summary>

- *N. Duforet-Frebourg, E. Bazin, M. G. B. Blum*

- `1402.5321v4` - [abs](http://arxiv.org/abs/1402.5321v4) - [pdf](http://arxiv.org/pdf/1402.5321v4)

> A central part of population genomics consists of finding genomic regions implicated in local adaptation. Population genomic analyses are based on genotyping numerous molecular markers and looking for outlier loci in terms of patterns of genetic differentiation. One of the most common approach for selection scan is based on statistics that measure population differentiation such as $F_{ST}$. However they are important caveats with approaches related to $F_{ST}$ because they require grouping individuals into populations and they additionally assume a particular model of population structure. Here we implement a more flexible individual-based approach based on Bayesian factor models. Factor models capture population structure with latent variables called factors, which can describe clustering of individuals into populations or isolation-by-distance patterns. Using hierarchical Bayesian modeling, we both infer population structure and identify outlier loci that are candidates for local adaptation. As outlier loci, the hierarchical factor model searches for loci that are atypically related to population structure as measured by the latent factors. In a model of population divergence, we show that the factor model can achieve a 2-fold or more reduction of false discovery rate compared to the software BayeScan or compared to a $F_{ST}$ approach. We analyze the data of the Human Genome Diversity Panel to provide an example of how factor models can be used to detect local adaptation with a large number of SNPs. The Bayesian factor model is implemented in the open-source PCAdapt software.

</details>

<details>

<summary>2014-07-29 15:03:48 - Bayesian meta-analysis of correlation coefficients through power prior</summary>

- *Zhiyong Zhang, Kaifeng Jiang, Haiyan Liu, In-Sue Oh*

- `1401.2054v2` - [abs](http://arxiv.org/abs/1401.2054v2) - [pdf](http://arxiv.org/pdf/1401.2054v2)

> To answer the call of introducing more Bayesian techniques to organizational research (e.g., Kruschke, Aguinis, & Joo, 2012; Zyphur & Oswald, 2013), we propose a Bayesian approach for meta-analysis with power prior in this article. The primary purpose of this method is to allow meta-analytic researchers to control the contribution of each individual study to an estimated overall effect size though power prior. This is due to the consideration that not all studies included in a meta-analysis should be viewed as equally reliable, and that by assigning more weights to reliable studies with power prior, researchers may obtain an overall effect size that reflects the population effect size more accurately. We use the relationship between high-performance work systems and financial performance as an example to illustrate how to apply this method to organizational research. We also provide free online software that can be used to conduct Bayesian meta-analysis proposed in this study. Research implications and future directions are discussed.

</details>

<details>

<summary>2014-07-29 17:43:10 - Bayesian Marked Point Process Modeling for Generating Fully Synthetic Public Use Data with Point-Referenced Geography</summary>

- *Harrison Quick, Scott H. Holan, Christopher K. Wikle, Jerome P. Reiter*

- `1407.7795v1` - [abs](http://arxiv.org/abs/1407.7795v1) - [pdf](http://arxiv.org/pdf/1407.7795v1)

> Many data stewards collect confidential data that include fine geography. When sharing these data with others, data stewards strive to disseminate data that are informative for a wide range of spatial and non-spatial analyses while simultaneously protecting the confidentiality of data subjects' identities and attributes. Typically, data stewards meet this challenge by coarsening the resolution of the released geography and, as needed, perturbing the confidential attributes. When done with high intensity, these redaction strategies can result in released data with poor analytic quality. We propose an alternative dissemination approach based on fully synthetic data. We generate data using marked point process models that can maintain both the statistical properties and the spatial dependence structure of the confidential data. We illustrate the approach using data consisting of mortality records from Durham, North Carolina.

</details>

<details>

<summary>2014-07-29 19:38:32 - Bayesian Probabilistic Matrix Factorization: A User Frequency Analysis</summary>

- *Cody Severinski, Ruslan Salakhutdinov*

- `1407.7840v1` - [abs](http://arxiv.org/abs/1407.7840v1) - [pdf](http://arxiv.org/pdf/1407.7840v1)

> Matrix factorization (MF) has become a common approach to collaborative filtering, due to ease of implementation and scalability to large data sets. Two existing drawbacks of the basic model is that it does not incorporate side information on either users or items, and assumes a common variance for all users. We extend the work of constrained probabilistic matrix factorization by deriving the Gibbs updates for the side feature vectors for items (Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to the constrained PMF model outperforms simple MAP estimation. We also consider extensions to heteroskedastic precision introduced in the literature (Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tends result in overfitting for deterministic approximation algorithms (ex: Variational inference) when the observed entries in the user / item matrix are distributed in an non-uniform manner. In light of this, we propose a truncated precision model. Our experimental results suggest that this model tends to delay overfitting.

</details>

<details>

<summary>2014-07-30 06:32:01 - An extended space approach for particle Markov chain Monte Carlo methods</summary>

- *Christopher K. Carter, Eduardo F. Mendes, Robert Kohn*

- `1406.5795v2` - [abs](http://arxiv.org/abs/1406.5795v2) - [pdf](http://arxiv.org/pdf/1406.5795v2)

> In this paper we consider fully Bayesian inference in general state space models. Existing particle Markov chain Monte Carlo (MCMC) algorithms use an augmented model that takes into account all the variable sampled in a sequential Monte Carlo algorithm. This paper describes an approach that also uses sequential Monte Carlo to construct an approximation to the state space, but generates extra states using MCMC runs at each time point. We construct an augmented model for our extended space with the marginal distribution of the sampled states matching the posterior distribution of the state vector. We show how our method may be combined with particle independent Metropolis-Hastings or particle Gibbs steps to obtain a smoothing algorithm. All the Metropolis acceptance probabilities are identical to those obtained in existing approaches, so there is no extra cost in term of Metropolis-Hastings rejections when using our approach. The number of MCMC iterates at each time point is chosen by the used and our augmented model collapses back to the model in Olsson and Ryden (2011) when the number of MCMC iterations reduces. We show empirically that our approach works well on applied examples and can outperform existing methods.

</details>

<details>

<summary>2014-07-30 08:29:38 - Automated Machine Learning on Big Data using Stochastic Algorithm Tuning</summary>

- *Thomas Nickson, Michael A Osborne, Steven Reece, Stephen J Roberts*

- `1407.7969v1` - [abs](http://arxiv.org/abs/1407.7969v1) - [pdf](http://arxiv.org/pdf/1407.7969v1)

> We introduce a means of automating machine learning (ML) for big data tasks, by performing scalable stochastic Bayesian optimisation of ML algorithm parameters and hyper-parameters. More often than not, the critical tuning of ML algorithm parameters has relied on domain expertise from experts, along with laborious hand-tuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is finding increasing use in automating parameter tuning, making ML algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsification strategies for Bayesian optimisation, concluding that a Nystrom approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data.

</details>

<details>

<summary>2014-07-30 16:41:25 - Additive Models for Conditional Copulas</summary>

- *Avideh Sabeti, Mian Wei, Radu V. Craiu*

- `1407.8119v1` - [abs](http://arxiv.org/abs/1407.8119v1) - [pdf](http://arxiv.org/pdf/1407.8119v1)

> Conditional copulas are flexible statistical tools that couple joint conditional and marginal conditional distributions. In a linear regression setting with more than one covariate and two dependent outcomes, we propose the use of additive models for conditional bivariate copula models and discuss computation and model selection tools for performing Bayesian inference. The method is illustrated using simulations and a real example.

</details>

<details>

<summary>2014-07-30 20:00:14 - Fast Bayesian Feature Selection for High Dimensional Linear Regression in Genomics via the Ising Approximation</summary>

- *Charles K. Fisher, Pankaj Mehta*

- `1407.8187v1` - [abs](http://arxiv.org/abs/1407.8187v1) - [pdf](http://arxiv.org/pdf/1407.8187v1)

> Feature selection, identifying a subset of variables that are relevant for predicting a response, is an important and challenging component of many methods in statistics and machine learning. Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples, as is often the case for many genomic datasets. Here, we introduce a new approach -- the Bayesian Ising Approximation (BIA) -- to rapidly calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the regime where the regression problem is strongly regularized by the prior, we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model. Using a mean field approximation, we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems. Finally, we demonstrate the applicability of the BIA to high dimensional regression by analyzing a gene expression dataset with nearly 30,000 features.

</details>

<details>

<summary>2014-07-31 00:03:49 - A class of regression models for parallel and series systems with a random number of components</summary>

- *Alice L. Morais, Silvia L. P. Ferrari*

- `1405.7746v2` - [abs](http://arxiv.org/abs/1405.7746v2) - [pdf](http://arxiv.org/pdf/1405.7746v2)

> In this paper we extend the Weibull power series (WPS) class of distributions and named this new class as extended Weibull power series (EWPS) class of distributions. The EWPS distributions are related to series and parallel systems with a random num- ber of components, whereas the WPS distributions (Morais and Barreto-Souza, 2011) are related to series systems only. Unlike the WPS distributions, for which the Weibull is a limiting special case, the Weibull law is a particular case of the EWPS distributions. We prove that the distributions in this class are identifiable under a simple assumption. We also prove stochastic and hazard rate order results and highlight that the shapes of the EWPS distributions are markedly more flexible than the shapes of the WPS distributions. We define a regression model for the EWPS response random variable to model a scale parameter and its quantiles. We present the maximum likelihood estimator and prove its consistency and normal asymptotic distribution. Although the construction of this class was motivated by series and parallel systems, the EWPS distributions are suitable for modeling a wide range of positive data sets. To illustrate potential uses of this model, we apply it to a real data set on the tensile strength of coconut fibers and present a simple device for diagnostic purposes.

</details>

<details>

<summary>2014-07-31 10:45:38 - Fast dimension-reduced climate model calibration and the effect of data aggregation</summary>

- *Won Chang, Murali Haran, Roman Olson, Klaus Keller*

- `1303.1382v5` - [abs](http://arxiv.org/abs/1303.1382v5) - [pdf](http://arxiv.org/pdf/1303.1382v5)

> How will the climate system respond to anthropogenic forcings? One approach to this question relies on climate model projections. Current climate projections are considerably uncertain. Characterizing and, if possible, reducing this uncertainty is an area of ongoing research. We consider the problem of making projections of the North Atlantic meridional overturning circulation (AMOC). Uncertainties about climate model parameters play a key role in uncertainties in AMOC projections. When the observational data and the climate model output are high-dimensional spatial data sets, the data are typically aggregated due to computational constraints. The effects of aggregation are unclear because statistically rigorous approaches for model parameter inference have been infeasible for high-resolution data. Here we develop a flexible and computationally efficient approach using principal components and basis expansions to study the effect of spatial data aggregation on parametric and projection uncertainties. Our Bayesian reduced-dimensional calibration approach allows us to study the effect of complicated error structures and data-model discrepancies on our ability to learn about climate model parameters from high-dimensional data. Considering high-dimensional spatial observations reduces the effect of deep uncertainty associated with prior specifications for the data-model discrepancy. Also, using the unaggregated data results in sharper projections based on our climate model. Our computationally efficient approach may be widely applicable to a variety of high-dimensional computer model calibration problems.

</details>

<details>

<summary>2014-07-31 12:20:19 - Clustering South African households based on their asset status using latent variable models</summary>

- *Damien McParland, Isobel Claire Gormley, Tyler H. McCormick, Samuel J. Clark, Chodziwadziwa Whiteson Kabudula, Mark A. Collinson*

- `1401.5343v2` - [abs](http://arxiv.org/abs/1401.5343v2) - [pdf](http://arxiv.org/pdf/1401.5343v2)

> The Agincourt Health and Demographic Surveillance System has since 2001 conducted a biannual household asset survey in order to quantify household socio-economic status (SES) in a rural population living in northeast South Africa. The survey contains binary, ordinal and nominal items. In the absence of income or expenditure data, the SES landscape in the study population is explored and described by clustering the households into homogeneous groups based on their asset status. A model-based approach to clustering the Agincourt households, based on latent variable models, is proposed. In the case of modeling binary or ordinal items, item response theory models are employed. For nominal survey items, a factor analysis model, similar in nature to a multinomial probit model, is used. Both model types have an underlying latent variable structure - this similarity is exploited and the models are combined to produce a hybrid model capable of handling mixed data types. Further, a mixture of the hybrid models is considered to provide clustering capabilities within the context of mixed binary, ordinal and nominal response data. The proposed model is termed a mixture of factor analyzers for mixed data (MFA-MD). The MFA-MD model is applied to the survey data to cluster the Agincourt households into homogeneous groups. The model is estimated within the Bayesian paradigm, using a Markov chain Monte Carlo algorithm. Intuitive groupings result, providing insight to the different socio-economic strata within the Agincourt region.

</details>

<details>

<summary>2014-07-31 12:42:00 - Small area estimation of general parameters with application to poverty indicators: A hierarchical Bayes approach</summary>

- *Isabel Molina, Balgobin Nandram, J. N. K. Rao*

- `1407.8384v1` - [abs](http://arxiv.org/abs/1407.8384v1) - [pdf](http://arxiv.org/pdf/1407.8384v1)

> Poverty maps are used to aid important political decisions such as allocation of development funds by governments and international organizations. Those decisions should be based on the most accurate poverty figures. However, often reliable poverty figures are not available at fine geographical levels or for particular risk population subgroups due to the sample size limitation of current national surveys. These surveys cannot cover adequately all the desired areas or population subgroups and, therefore, models relating the different areas are needed to 'borrow strength" from area to area. In particular, the Spanish Survey on Income and Living Conditions (SILC) produces national poverty estimates but cannot provide poverty estimates by Spanish provinces due to the poor precision of direct estimates, which use only the province specific data. It also raises the ethical question of whether poverty is more severe for women than for men in a given province. We develop a hierarchical Bayes (HB) approach for poverty mapping in Spanish provinces by gender that overcomes the small province sample size problem of the SILC. The proposed approach has a wide scope of application because it can be used to estimate general nonlinear parameters. We use a Bayesian version of the nested error regression model in which Markov chain Monte Carlo procedures and the convergence monitoring therein are avoided. A simulation study reveals good frequentist properties of the HB approach. The resulting poverty maps indicate that poverty, both in frequency and intensity, is localized mostly in the southern and western provinces and it is more acute for women than for men in most of the provinces.

</details>

<details>

<summary>2014-07-31 13:05:38 - A Bayesian nonparametric mixture model for selecting genes and gene subnetworks</summary>

- *Yize Zhao, Jian Kang, Tianwei Yu*

- `1407.8399v1` - [abs](http://arxiv.org/abs/1407.8399v1) - [pdf](http://arxiv.org/pdf/1407.8399v1)

> It is very challenging to select informative features from tens of thousands of measured features in high-throughput data analysis. Recently, several parametric/regression models have been developed utilizing the gene network information to select genes or pathways strongly associated with a clinical/biological outcome. Alternatively, in this paper, we propose a nonparametric Bayesian model for gene selection incorporating network information. In addition to identifying genes that have a strong association with a clinical outcome, our model can select genes with particular expressional behavior, in which case the regression models are not directly applicable. We show that our proposed model is equivalent to an infinity mixture model for which we develop a posterior computation algorithm based on Markov chain Monte Carlo (MCMC) methods. We also propose two fast computing algorithms that approximate the posterior simulation with good accuracy but relatively low computational cost. We illustrate our methods on simulation studies and the analysis of Spellman yeast cell cycle microarray data.

</details>

<details>

<summary>2014-07-31 13:33:30 - Analysis of multiple sclerosis lesions via spatially varying coefficients</summary>

- *Tian Ge, Nicole MÃ¼ller-Lenke, Kerstin Bendfeldt, Thomas E. Nichols, Timothy D. Johnson*

- `1407.8406v1` - [abs](http://arxiv.org/abs/1407.8406v1) - [pdf](http://arxiv.org/pdf/1407.8406v1)

> Magnetic resonance imaging (MRI) plays a vital role in the scientific investigation and clinical management of multiple sclerosis. Analyses of binary multiple sclerosis lesion maps are typically "mass univariate" and conducted with standard linear models that are ill suited to the binary nature of the data and ignore the spatial dependence between nearby voxels (volume elements). Smoothing the lesion maps does not entirely eliminate the non-Gaussian nature of the data and requires an arbitrary choice of the smoothing parameter. Here we present a Bayesian spatial model to accurately model binary lesion maps and to determine if there is spatial dependence between lesion location and subject specific covariates such as MS subtype, age, gender, disease duration and disease severity measures. We apply our model to binary lesion maps derived from $T_2$-weighted MRI images from 250 multiple sclerosis patients classified into five clinical subtypes, and demonstrate unique modeling and predictive capabilities over existing methods.

</details>


## 2014-08

<details>

<summary>2014-08-01 06:34:00 - Bayesian nonparametric Plackett-Luce models for the analysis of preferences for college degree programmes</summary>

- *FranÃ§ois Caron, Yee Whye Teh, Thomas Brendan Murphy*

- `1211.5037v3` - [abs](http://arxiv.org/abs/1211.5037v3) - [pdf](http://arxiv.org/pdf/1211.5037v3)

> In this paper we propose a Bayesian nonparametric model for clustering partial ranking data. We start by developing a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a completely random measure. We characterise the posterior distribution given data, and derive a simple and effective Gibbs sampler for posterior simulation. We then develop a Dirichlet process mixture extension of our model and apply it to investigate the clustering of preferences for college degree programmes amongst Irish secondary school graduates. The existence of clusters of applicants who have similar preferences for degree programmes is established and we determine that subject matter and geographical location of the third level institution characterise these clusters.

</details>

<details>

<summary>2014-08-01 20:20:34 - Generalized Species Sampling Priors with Latent Beta reinforcements</summary>

- *Edoardo M. Airoldi, Thiago Costa, Federico Bassetti, Fabrizio Leisen, Michele Guindani*

- `1012.0866v4` - [abs](http://arxiv.org/abs/1012.0866v4) - [pdf](http://arxiv.org/pdf/1012.0866v4)

> Many popular Bayesian nonparametric priors can be characterized in terms of exchangeable species sampling sequences. However, in some applications, exchangeability may not be appropriate. We introduce a {novel and probabilistically coherent family of non-exchangeable species sampling sequences characterized by a tractable predictive probability function with weights driven by a sequence of independent Beta random variables. We compare their theoretical clustering properties with those of the Dirichlet Process and the two parameters Poisson-Dirichlet process. The proposed construction provides a complete characterization of the joint process, differently from existing work. We then propose the use of such process as prior distribution in a hierarchical Bayes modeling framework, and we describe a Markov Chain Monte Carlo sampler for posterior inference. We evaluate the performance of the prior and the robustness of the resulting inference in a simulation study, providing a comparison with popular Dirichlet Processes mixtures and Hidden Markov Models. Finally, we develop an application to the detection of chromosomal aberrations in breast cancer by leveraging array CGH data.

</details>

<details>

<summary>2014-08-02 04:31:56 - A Bayesian estimation approach to analyze non-Gaussian data-generating processes with latent classes</summary>

- *Naoki Tanaka, Shohei Shimizu, Takashi Washio*

- `1408.0337v1` - [abs](http://arxiv.org/abs/1408.0337v1) - [pdf](http://arxiv.org/pdf/1408.0337v1)

> A large amount of observational data has been accumulated in various fields in recent times, and there is a growing need to estimate the generating processes of these data. A linear non-Gaussian acyclic model (LiNGAM) based on the non-Gaussianity of external influences has been proposed to estimate the data-generating processes of variables. However, the results of the estimation can be biased if there are latent classes. In this paper, we first review LiNGAM, its extended model, as well as the estimation procedure for LiNGAM in a Bayesian framework. We then propose a new Bayesian estimation procedure that solves the problem.

</details>

<details>

<summary>2014-08-02 21:40:22 - Equivariant minimax dominators of the MLE in the array normal model</summary>

- *David Gerard, Peter Hoff*

- `1408.0424v1` - [abs](http://arxiv.org/abs/1408.0424v1) - [pdf](http://arxiv.org/pdf/1408.0424v1)

> Inference about dependencies in a multiway data array can be made using the array normal model, which corresponds to the class of multivariate normal distributions with separable covariance matrices. Maximum likelihood and Bayesian methods for inference in the array normal model have appeared in the literature, but there have not been any results concerning the optimality properties of such estimators. In this article, we obtain results for the array normal model that are analogous to some classical results concerning covariance estimation for the multivariate normal model. We show that under a lower triangular product group, a uniformly minimum risk equivariant estimator (UMREE) can be obtained via a generalized Bayes procedure. Although this UMREE is minimax and dominates the MLE, it can be improved upon via an orthogonally equivariant modification. Numerical comparisons of the risks of these estimators show that the equivariant estimators can have substantially lower risks than the MLE.

</details>

<details>

<summary>2014-08-03 06:50:05 - Shrinkage priors for linear instrumental variable models with many instruments</summary>

- *P. Richard Hahn, Hedibert Lopes*

- `1408.0462v1` - [abs](http://arxiv.org/abs/1408.0462v1) - [pdf](http://arxiv.org/pdf/1408.0462v1)

> This paper addresses the weak instruments problem in linear instrumental variable models from a Bayesian perspective. The new approach has two components. First, a novel predictor-dependent shrinkage prior is developed for the many instruments setting. The prior is constructed based on a factor model decomposition of the matrix of observed instruments, allowing many instruments to be incorporated into the analysis in a robust way.   Second, the new prior is implemented via an importance sampling scheme, which utilizes posterior Monte Carlo samples from a first-stage Bayesian regression analysis. This modular computation makes sensitivity analyses straightforward.   Two simulation studies are provided to demonstrate the advantages of the new method. As an empirical illustration, the new method is used to estimate a key parameter in macro-economic models: the elasticity of inter-temporal substitution. The empirical analysis produces substantive conclusions in line with previous studies, but certain inconsistencies of earlier analyses are resolved.

</details>

<details>

<summary>2014-08-03 07:17:31 - Decoupling shrinkage and selection in Bayesian linear models: a posterior summary perspective</summary>

- *P. Richard Hahn, Carlos M. Carvalho*

- `1408.0464v1` - [abs](http://arxiv.org/abs/1408.0464v1) - [pdf](http://arxiv.org/pdf/1408.0464v1)

> Selecting a subset of variables for linear models remains an active area of research. This paper reviews many of the recent contributions to the Bayesian model selection and shrinkage prior literature. A posterior variable selection summary is proposed, which distills a full posterior distribution over regression coefficients into a sequence of sparse linear predictors.

</details>

<details>

<summary>2014-08-04 15:50:33 - A Fully Nonparametric Modelling Approach to Binary Regression</summary>

- *Maria DeYoreo, Athanasios Kottas*

- `1404.5097v2` - [abs](http://arxiv.org/abs/1404.5097v2) - [pdf](http://arxiv.org/pdf/1404.5097v2)

> We propose a general nonparametric Bayesian framework for binary regression, which is built from modeling for the joint response-covariate distribution. The observed binary responses are assumed to arise from underlying continuous random variables through discretization, and we model the joint distribution of these latent responses and the covariates using a Dirichlet process mixture of multivariate normals. We show that the kernel of the induced mixture model for the observed data is identifiable upon a restriction on the latent variables. To allow for appropriate dependence structure while facilitating identifiability, we use a square-root-free Cholesky decomposition of the covariance matrix in the normal mixture kernel. In addition to allowing for the necessary restriction, this modeling strategy provides substantial simplifications in implementation of Markov chain Monte Carlo posterior simulation. We present two data examples taken from areas for which the methodology is especially well suited. In particular, the first example involves estimation of relationships between environmental variables, and the second develops inference for natural selection surfaces in evolutionary biology. Finally, we discuss extensions to regression settings with multivariate ordinal responses.

</details>

<details>

<summary>2014-08-04 19:51:57 - The Letac-Massam conjecture and existence of high dimensional Bayes estimators for Graphical Models</summary>

- *Emanuel Ben-David, Bala Rajaratnam*

- `1408.0788v1` - [abs](http://arxiv.org/abs/1408.0788v1) - [pdf](http://arxiv.org/pdf/1408.0788v1)

> In recent years, a variety of useful extensions of the Wishart have been proposed in the literature for the purposes of studying Markov random fields/graphical models. In particular, generalizations of the Wishart, referred to as Type I and Type II Wishart distributions, have been introduced by Letac and Massam (\emph{Annals of Statistics} 2006) and play important roles in both frequentist and Bayesian inference for Gaussian graphical models. These distributions have been especially useful in high-dimensional settings due to the flexibility offered by their multiple shape parameters. The domain of In this paper we resolve a long-standing conjecture of Letac and Massam (LM) concerning the domains of the multi-parameters of graphical Wishart type distributions. This conjecture, posed in \emph{Annals of Statistics}, also relates fundamentally to the existence of Bayes estimators corresponding to these high dimensional priors. To achieve our goal, we first develop novel theory in the context of probabilistic analysis of graphical models. Using these tools, and a recently introduced class of Wishart distributions for directed acyclic graph (DAG) models, we proceed to give counterexamples to the LM conjecture, thus completely resolving the problem. Our analysis also proceeds to give useful insights on graphical Wishart distributions with implications for Bayesian inference for such models.

</details>

<details>

<summary>2014-08-05 07:19:16 - Accounting for parameter uncertainty in two-stage designs for Phase II dose-response studies</summary>

- *Emma McCallum, BjÃ¶rn Bornkamp*

- `1408.0534v2` - [abs](http://arxiv.org/abs/1408.0534v2) - [pdf](http://arxiv.org/pdf/1408.0534v2)

> In this paper we consider two-stage adaptive dose-response study designs, where the study design is changed at an interim analysis based on the information collected so far. In a simulation study, two approaches will be compared for these type of designs; (i) updating the study design by calculating the maximum likelihood estimate for the dose-response model parameters and then calculating the design for the second stage that is locally optimal for this estimate, and (ii) using the complete posterior distribution of the model parameter at interim to calculate a Bayesian optimal design (i.e. taking into account parameter uncertainty). In particular, for an early interim analysis respecting parameter uncertainty seems more adequate, on the other hand for a Bayesian approach dependency on the prior is expected and an adequately thought-through prior is required. A computationally efficient method is proposed for calculating the Bayesian design at interim based on approximating the full posterior sample using k-means clustering. The sigmoid Emax dose-response model and the D-optimality criterion will be used in this paper.

</details>

<details>

<summary>2014-08-05 15:16:35 - Nonparametric Identification in Panels using Quantiles</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Stefan Hoderlein, Hajo Holzmann, Whitney Newey*

- `1312.4094v3` - [abs](http://arxiv.org/abs/1312.4094v3) - [pdf](http://arxiv.org/pdf/1312.4094v3)

> This paper considers identification and estimation of ceteris paribus effects of continuous regressors in nonseparable panel models with time homogeneity. The effects of interest are derivatives of the average and quantile structural functions of the model. We find that these derivatives are identified with two time periods for "stayers", i.e. for individuals with the same regressor values in two time periods. We show that the identification results carry over to models that allow location and scale time effects. We propose nonparametric series methods and a weighted bootstrap scheme to estimate and make inference on the identified effects. The bootstrap proposed allows uniform inference for function-valued parameters such as quantile effects uniformly over a region of quantile indices and/or regressor values. An empirical application to Engel curve estimation with panel data illustrates the results.

</details>

<details>

<summary>2014-08-09 05:20:28 - A Bayesian Probability Calculus for Density Matrices</summary>

- *Manfred K. Warmuth, Dima Kuzmin*

- `1408.3100v1` - [abs](http://arxiv.org/abs/1408.3100v1) - [pdf](http://arxiv.org/pdf/1408.3100v1)

> One of the main concepts in quantum physics is a density matrix, which is a symmetric positive definite matrix of trace one. Finite probability distributions are a special case where the density matrix is restricted to be diagonal. Density matrices are mixtures of dyads, where a dyad has the form uu' for any any unit column vector u. These unit vectors are the elementary events of the generalized probability space. Perhaps the simplest case to see that something unusual is going on is the case of uniform density matrix, i.e. 1/n times identity. This matrix assigns probability 1/n to every unit vector, but of course there are infinitely many of them. The new normalization rule thus says that sum of probabilities over any orthonormal basis of directions is one. We develop a probability calculus based on these more general distributions that includes definitions of joints, conditionals and formulas that relate these, i.e. analogs of the theorem of total probability, various Bayes rules for the calculation of posterior density matrices, etc. The resulting calculus parallels the familiar 'classical' probability calculus and always retains the latter as a special case when all matrices are diagonal.   Whereas the classical Bayesian methods maintain uncertainty about which model is 'best', the generalization maintains uncertainty about which unit direction has the largest variance. Surprisingly the bounds also generalize: as in the classical setting we bound the negative log likelihood of the data by the negative log likelihood of the MAP estimator.

</details>

<details>

<summary>2014-08-09 05:26:02 - Bayesian Multitask Learning with Latent Hierarchies</summary>

- *Hal Daume III*

- `1408.2032v1` - [abs](http://arxiv.org/abs/1408.2032v1) - [pdf](http://arxiv.org/pdf/1408.2032v1)

> We learn multiple hypotheses for related tasks under a latent hierarchical relationship between tasks. We exploit the intuition that for domain adaptation, we wish to share classifier structure, but for multitask learning, we wish to share covariance structure. Our hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real-world data sets.

</details>

<details>

<summary>2014-08-09 05:34:21 - A direct method for estimating a causal ordering in a linear non-Gaussian acyclic model</summary>

- *Shohei Shimizu, Aapo Hyvarinen, Yoshinobu Kawahara*

- `1408.2038v1` - [abs](http://arxiv.org/abs/1408.2038v1) - [pdf](http://arxiv.org/pdf/1408.2038v1)

> Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the datagenerating process of variables. Recently, it was shown that use of non-Gaussianity identifies a causal ordering of variables in a linear acyclic model without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model.

</details>

<details>

<summary>2014-08-09 05:39:50 - Gaussian Process Structural Equation Models with Latent Variables</summary>

- *Ricardo Silva, Robert B. Gramacy*

- `1408.2042v1` - [abs](http://arxiv.org/abs/1408.2042v1) - [pdf](http://arxiv.org/pdf/1408.2042v1)

> In a variety of disciplines such as social sciences, psychology, medicine and economics, the recorded data are considered to be noisy measurements of latent variables connected by some causal structure. This corresponds to a family of graphical models known as the structural equation model with latent variables. While linear non-Gaussian variants have been well-studied, inference in nonparametric structural equation models is still underdeveloped. We introduce a sparse Gaussian process parameterization that defines a non-linear structure connecting latent variables, unlike common formulations of Gaussian process latent variable models. The sparse parameterization is given a full Bayesian treatment without compromising Markov chain Monte Carlo efficiency. We compare the stability of the sampling procedure and the predictive ability of the model against the current practice.

</details>

<details>

<summary>2014-08-09 05:45:11 - Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior</summary>

- *Yutian Chen, Max Welling*

- `1408.2047v1` - [abs](http://arxiv.org/abs/1408.2047v1) - [pdf](http://arxiv.org/pdf/1408.2047v1)

> In recent years a number of methods have been developed for automatically learning the (sparse) connectivity structure of Markov Random Fields. These methods are mostly based on L1-regularized optimization which has a number of disadvantages such as the inability to assess model uncertainty and expensive crossvalidation to find the optimal regularization parameter. Moreover, the model's predictive performance may degrade dramatically with a suboptimal value of the regularization parameter (which is sometimes desirable to induce sparseness). We propose a fully Bayesian approach based on a "spike and slab" prior (similar to L0 regularization) that does not suffer from these shortcomings. We develop an approximate MCMC method combining Langevin dynamics and reversible jump MCMC to conduct inference in this model. Experiments show that the proposed model learns a good combination of the structure and parameter values without the need for separate hyper-parameter tuning. Moreover, the model's predictive performance is much more robust than L1-based methods with hyper-parameter settings that induce highly sparse model structures.

</details>

<details>

<summary>2014-08-09 05:52:02 - Non-Convex Rank Minimization via an Empirical Bayesian Approach</summary>

- *David Wipf*

- `1408.2054v1` - [abs](http://arxiv.org/abs/1408.2054v1) - [pdf](http://arxiv.org/pdf/1408.2054v1)

> In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candes et al., 2011) can be viewed as a special case.

</details>

<details>

<summary>2014-08-09 05:58:33 - Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations</summary>

- *Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet*

- `1408.2060v1` - [abs](http://arxiv.org/abs/1408.2060v1) - [pdf](http://arxiv.org/pdf/1408.2060v1)

> Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP.

</details>

<details>

<summary>2014-08-09 06:04:33 - One-Class Support Measure Machines for Group Anomaly Detection</summary>

- *Krikamol Muandet, Bernhard Schoelkopf*

- `1408.2064v1` - [abs](http://arxiv.org/abs/1408.2064v1) - [pdf](http://arxiv.org/pdf/1408.2064v1)

> We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications.

</details>

<details>

<summary>2014-08-09 06:07:52 - Probabilistic inverse reinforcement learning in unknown environments</summary>

- *Aristide Tossou, Christos Dimitrakakis*

- `1408.2067v1` - [abs](http://arxiv.org/abs/1408.2067v1) - [pdf](http://arxiv.org/pdf/1408.2067v1)

> We consider the problem of learning by demonstration from agents acting in unknown stochastic Markov environments or games. Our aim is to estimate agent preferences in order to construct improved policies for the same task that the agents are trying to solve. To do so, we extend previous probabilistic approaches for inverse reinforcement learning in known MDPs to the case of unknown dynamics or opponents. We do this by deriving two simplified probabilistic models of the demonstrator's policy and utility. For tractability, we use maximum a posteriori estimation rather than full Bayesian inference. Under a flat prior, this results in a convex optimisation problem. We find that the resulting algorithms are highly competitive against a variety of other methods for inverse reinforcement learning that do have knowledge of the dynamics.

</details>

<details>

<summary>2014-08-11 18:41:41 - Systematic Physics Constrained Parameter Estimation of Stochastic Differential Equations</summary>

- *Daniel Peavoy, Christian L. E. Franzke, Gareth O. Roberts*

- `1312.1881v3` - [abs](http://arxiv.org/abs/1312.1881v3) - [pdf](http://arxiv.org/pdf/1312.1881v3)

> A systematic Bayesian framework is developed for physics constrained parameter inference ofstochastic differential equations (SDE) from partial observations. The physical constraints arederived for stochastic climate models but are applicable for many fluid systems. A condition isderived for global stability of stochastic climate models based on energy conservation. Stochasticclimate models are globally stable when a quadratic form, which is related to the cubic nonlinearoperator, is negative definite. A new algorithm for the efficient sampling of such negative definite matrices is developed and also for imputing unobserved data which improve the accuracy of theparameter estimates. The performance of this framework is evaluated on two conceptual climatemodels.

</details>

<details>

<summary>2014-08-11 20:39:42 - Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction of Tumors</summary>

- *Amit G. Deshwar, Shankar Vembu, Quaid Morris*

- `1408.2552v1` - [abs](http://arxiv.org/abs/1408.2552v1) - [pdf](http://arxiv.org/pdf/1408.2552v1)

> Statistical machine learning methods, especially nonparametric Bayesian methods, have become increasingly popular to infer clonal population structure of tumors. Here we describe the treeCRP, an extension of the Chinese restaurant process (CRP), a popular construction used in nonparametric mixture models, to infer the phylogeny and genotype of major subclonal lineages represented in the population of cancer cells. We also propose new split-merge updates tailored to the subclonal reconstruction problem that improve the mixing time of Markov chains. In comparisons with the tree-structured stick breaking prior used in PhyloSub, we demonstrate superior mixing and running time using the treeCRP with our new split-merge procedures. We also show that given the same number of samples, TSSB and treeCRP have similar ability to recover the subclonal structure of a tumor.

</details>

<details>

<summary>2014-08-12 06:13:40 - On the Estimation of Homogeneous Population Size in a Complex Dual-record System</summary>

- *Kiranmoy Chatterjee, Diganta Mukherjee*

- `1408.2153v2` - [abs](http://arxiv.org/abs/1408.2153v2) - [pdf](http://arxiv.org/pdf/1408.2153v2)

> Dual-record system (DRS) (equivalently two sample Capture-recapture experiment) model with time and behavioral response variation, has attracted much attention specifically in the domain of Official Statistics and Epidemiology. The relevant model suffers from parameter identifiability problem and proper Bayesian methodologies could be helpful to overcome the situation. In this article, we have formulated the population size estimation problem in DRS as a missing data analysis under both the known and unknown directional nature of underlying behavioral response effect. Two simple empirical Bayes approaches are proposed and investigated their performances for this complex model along with a fully Bayes treatment. Extensive simulation studies are carried out to compare the performances of these competitive approaches and a real data example is also illustrated. Finally, some features of these methods and recommendations to implement them in practice are explored depending upon the availability of knowledge on the nature of behavioral response effect.

</details>

<details>

<summary>2014-08-12 15:58:36 - Bayesian Lattice Filters for Time-Varying Autoregression and Time-Frequency Analysis</summary>

- *Wen-Hsi Yang, Scott H. Holan, Christopher K. Wikle*

- `1408.2757v1` - [abs](http://arxiv.org/abs/1408.2757v1) - [pdf](http://arxiv.org/pdf/1408.2757v1)

> Modeling nonstationary processes is of paramount importance to many scientific disciplines including environmental science, ecology, and finance, among others. Consequently, flexible methodology that provides accurate estimation across a wide range of processes is a subject of ongoing interest. We propose a novel approach to model-based time-frequency estimation using time-varying autoregressive models. In this context, we take a fully Bayesian approach and allow both the autoregressive coefficients and innovation variance to vary over time. Importantly, our estimation method uses the lattice filter and is cast within the partial autocorrelation domain. The marginal posterior distributions are of standard form and, as a convenient by-product of our estimation method, our approach avoids undesirable matrix inversions. As such, estimation is extremely computationally efficient and stable. To illustrate the effectiveness of our approach, we conduct a comprehensive simulation study that compares our method with other competing methods and find that, in most cases, our approach performs superior in terms of average squared error between the estimated and true time-varying spectral density. Lastly, we demonstrate our methodology through three modeling applications; namely, insect communication signals, environmental data (wind components), and macroeconomic data (US gross domestic product (GDP) and consumption).

</details>

<details>

<summary>2014-08-13 15:20:55 - A Bayesian semiparametric model for semicontinuous data</summary>

- *Emanuela Dreassi, Emilia Rocco*

- `1408.3027v1` - [abs](http://arxiv.org/abs/1408.3027v1) - [pdf](http://arxiv.org/pdf/1408.3027v1)

> When the target variable exhibits a semicontinuous behaviour (i.e. a point mass in a single value and a continuous distribution elsewhere) parametric `two-part regression models' have been extensively used and investigated. In this paper, a semiparametric Bayesian two-part regression model for dealing with such variables is proposed. The model allows a semiparametric expression for the two part of the model by using Dirichlet processes. A motivating example (in the `small area estimation' framework) based on pseudo-real data on grapewine production in Tuscany, is used to evaluate the capabilities of the model. Results show a satisfactory performance of the suggested approach to model and predict semicontinuous data when parametric assumptions (distributional and/or relationship) are not reasonable.

</details>

<details>

<summary>2014-08-13 19:16:29 - Convergence rate of Bayesian tensor estimator: Optimal rate without restricted strong convexity</summary>

- *Taiji Suzuki*

- `1408.3092v1` - [abs](http://arxiv.org/abs/1408.3092v1) - [pdf](http://arxiv.org/pdf/1408.3092v1)

> In this paper, we investigate the statistical convergence rate of a Bayesian low-rank tensor estimator. Our problem setting is the regression problem where a tensor structure underlying the data is estimated. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a near optimal rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori.

</details>

<details>

<summary>2014-08-14 08:52:10 - An adaptive composite quantile approach to dimension reduction</summary>

- *Efang Kong, Yingcun Xia*

- `1408.3221v1` - [abs](http://arxiv.org/abs/1408.3221v1) - [pdf](http://arxiv.org/pdf/1408.3221v1)

> Sufficient dimension reduction [J. Amer. Statist. Assoc. 86 (1991) 316-342] has long been a prominent issue in multivariate nonparametric regression analysis. To uncover the central dimension reduction space, we propose in this paper an adaptive composite quantile approach. Compared to existing methods, (1) it requires minimal assumptions and is capable of revealing all dimension reduction directions; (2) it is robust against outliers and (3) it is structure-adaptive, thus more efficient. Asymptotic results are proved and numerical examples are provided, including a real data analysis.

</details>

<details>

<summary>2014-08-14 21:37:26 - Asymptotic Analysis of Distributed Bayesian Detection with Byzantine Data</summary>

- *Bhavya Kailkhura, Yunghsiang S. Han, Swastik Brahma, Pramod K. Varshney*

- `1408.3434v1` - [abs](http://arxiv.org/abs/1408.3434v1) - [pdf](http://arxiv.org/pdf/1408.3434v1)

> In this letter, we consider the problem of distributed Bayesian detection in the presence of data falsifying Byzantines in the network. The problem of distributed detection is formulated as a binary hypothesis test at the fusion center (FC) based on 1-bit data sent by the sensors. Adopting Chernoff information as our performance metric, we study the detection performance of the system under Byzantine attack in the asymptotic regime. The expression for minimum attacking power required by the Byzantines to blind the FC is obtained. More specifically, we show that above a certain fraction of Byzantine attackers in the network, the detection scheme becomes completely incapable of utilizing the sensor data for detection. When the fraction of Byzantines is not sufficient to blind the FC, we also provide closed form expressions for the optimal attacking strategies for the Byzantines that most degrade the detection performance.

</details>

<details>

<summary>2014-08-16 00:15:35 - Hierarchical sparse Bayesian learning for structural health monitoring with incomplete modal data</summary>

- *Yong Huang, James L. Beck*

- `1408.3685v1` - [abs](http://arxiv.org/abs/1408.3685v1) - [pdf](http://arxiv.org/pdf/1408.3685v1)

> For civil structures, structural damage due to severe loading events such as earthquakes, or due to long-term environmental degradation, usually occurs in localized areas of a structure. A new sparse Bayesian probabilistic framework for computing the probability of localized stiffness reductions induced by damage is presented that uses noisy incomplete modal data from before and after possible damage. This new approach employs system modal parameters of the structure as extra variables for Bayesian model updating with incomplete modal data. A specific hierarchical Bayesian model is constructed that promotes spatial sparseness in the inferred stiffness reductions in a way that is consistent with the Bayesian Ockham razor. To obtain the most plausible model of sparse stiffness reductions together with its uncertainty within a specified class of models, the method employs an optimization scheme that iterates among all uncertain parameters, including the hierarchical hyper-parameters. The approach has four important benefits: (1) it infers spatially-sparse stiffness changes based on the identified modal parameters; (2) the uncertainty in the inferred stiffness reductions is quantified; (3) no matching of model and experimental modes is needed, and (4) solving the nonlinear eigenvalue problem of a structural model is not required. The proposed method is applied to two previously-studied examples using simulated data: a ten-story shear-building and the three-dimensional braced-frame model from the Phase II Simulated Benchmark problem sponsored by the IASC-ASCE Task Group on Structural Health Monitoring. The results show that the occurrence of false-positive and false-negative damage detection is clearly reduced in the presence of modeling error. Furthermore, the identified most probable stiffness loss ratios are close to their actual values.

</details>

<details>

<summary>2014-08-18 04:45:26 - Bayesian image segmentations by Potts prior and loopy belief propagation</summary>

- *Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi, Chiou-Ting Hsu*

- `1404.3012v5` - [abs](http://arxiv.org/abs/1404.3012v5) - [pdf](http://arxiv.org/pdf/1404.3012v5)

> This paper presents a Bayesian image segmentation model based on Potts prior and loopy belief propagation. The proposed Bayesian model involves several terms, including the pairwise interactions of Potts models, and the average vectors and covariant matrices of Gauss distributions in color image modeling. These terms are often referred to as hyperparameters in statistical machine learning theory. In order to determine these hyperparameters, we propose a new scheme for hyperparameter estimation based on conditional maximization of entropy in the Potts prior. The algorithm is given based on loopy belief propagation. In addition, we compare our conditional maximum entropy framework with the conventional maximum likelihood framework, and also clarify how the first order phase transitions in LBP's for Potts models influence our hyperparameter estimation procedures.

</details>

<details>

<summary>2014-08-18 20:12:09 - BET: Bayesian Ensemble Trees for Clustering and Prediction in Heterogeneous Data</summary>

- *Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak*

- `1408.4140v1` - [abs](http://arxiv.org/abs/1408.4140v1) - [pdf](http://arxiv.org/pdf/1408.4140v1)

> We propose a novel "tree-averaging" model that utilizes the ensemble of classification and regression trees (CART). Each constituent tree is estimated with a subset of similar data. We treat this grouping of subsets as Bayesian ensemble trees (BET) and model them as an infinite mixture Dirichlet process. We show that BET adapts to data heterogeneity and accurately estimates each component. Compared with the bootstrap-aggregating approach, BET shows improved prediction performance with fewer trees. We develop an efficient estimating procedure with improved sampling strategies in both CART and mixture models. We demonstrate these advantages of BET with simulations, classification of breast cancer and regression of lung function measurement of cystic fibrosis patients.   Keywords: Bayesian CART; Dirichlet Process; Ensemble Approach; Heterogeneity; Mixture of Trees.

</details>

<details>

<summary>2014-08-20 12:16:54 - A new integral loss function for Bayesian optimization</summary>

- *Emmanuel Vazquez, Julien Bect*

- `1408.4622v1` - [abs](http://arxiv.org/abs/1408.4622v1) - [pdf](http://arxiv.org/pdf/1408.4622v1)

> We consider the problem of maximizing a real-valued continuous function $f$ using a Bayesian approach. Since the early work of Jonas Mockus and Antanas \v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by considering the loss function $\max f - M_n$ (where $M_n$ denotes the best function value observed after $n$ evaluations of $f$). This loss function puts emphasis on the value of the maximum, at the expense of the location of the maximizer. In the special case of a one-step Bayes-optimal strategy, it leads to the classical Expected Improvement (EI) sampling criterion. This is a special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk associated to a certain uncertainty measure (here, the expected loss) on the quantity of interest is minimized at each step of the algorithm. In this article, assuming that $f$ is defined over a measure space $(\mathbb{X}, \lambda)$, we propose to consider instead the integral loss function $\int_{\mathbb{X}} (f - M_n)_{+}\, d\lambda$, and we show that this leads, in the case of a Gaussian process prior, to a new numerically tractable sampling criterion that we call $\rm EI^2$ (for Expected Integrated Expected Improvement). A numerical experiment illustrates that a SUR strategy based on this new sampling criterion reduces the error on both the value and the location of the maximizer faster than the EI-based strategy.

</details>

<details>

<summary>2014-08-22 14:36:56 - Joint Hierarchical Gaussian Process Model with Application to Forecast in Medical Monitoring</summary>

- *Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak*

- `1408.4660v2` - [abs](http://arxiv.org/abs/1408.4660v2) - [pdf](http://arxiv.org/pdf/1408.4660v2)

> A novel extrapolation method is proposed for longitudinal forecasting. A hierarchical Gaussian process model is used to combine nonlinear population change and individual memory of the past to make prediction. The prediction error is minimized through the hierarchical design. The method is further extended to joint modeling of continuous measurements and survival events. The baseline hazard, covariate and joint effects are conveniently modeled in this hierarchical structure. The estimation and inference are implemented in fully Bayesian framework using the objective and shrinkage priors. In simulation studies, this model shows robustness in latent estimation, correlation detection and high accuracy in forecasting. The model is illustrated with medical monitoring data from cystic fibrosis (CF) patients. Estimation and forecasts are obtained in the measurement of lung function and records of acute respiratory events.   Keyword: Extrapolation, Joint Model, Longitudinal Model, Hierarchical Gaussian Process, Cystic Fibrosis, Medical Monitoring

</details>

<details>

<summary>2014-08-23 15:14:31 - The Jeffreys-Lindley Paradox and Discovery Criteria in High Energy Physics</summary>

- *Robert D. Cousins*

- `1310.3791v6` - [abs](http://arxiv.org/abs/1310.3791v6) - [pdf](http://arxiv.org/pdf/1310.3791v6)

> The Jeffreys-Lindley paradox displays how the use of a p-value (or number of standard deviations z) in a frequentist hypothesis test can lead to an inference that is radically different from that of a Bayesian hypothesis test in the form advocated by Harold Jeffreys in the 1930s and common today. The setting is the test of a well-specified null hypothesis (such as the Standard Model of elementary particle physics, possibly with "nuisance parameters") versus a composite alternative (such as the Standard Model plus a new force of nature of unknown strength). The p-value, as well as the ratio of the likelihood under the null hypothesis to the maximized likelihood under the alternative, can strongly disfavor the null hypothesis, while the Bayesian posterior probability for the null hypothesis can be arbitrarily large. The academic statistics literature contains many impassioned comments on this paradox, yet there is no consensus either on its relevance to scientific communication or on its correct resolution. The paradox is quite relevant to frontier research in high energy physics. This paper is an attempt to explain the situation to both physicists and statisticians, in the hope that further progress can be made.

</details>

<details>

<summary>2014-08-26 05:35:20 - Posterior consistency in misspecified models for i.n.i.d response</summary>

- *Karthik Sriram, R. V. Ramamoorthi*

- `1408.6015v1` - [abs](http://arxiv.org/abs/1408.6015v1) - [pdf](http://arxiv.org/pdf/1408.6015v1)

> We derive conditions for posterior consistency when the responses are independent but not identically distributed ($i.n.i.d$) and the model is "misspecified" to be a family of densities parametrized by a possibly infinite dimensional parameter. Our approach has connections to key ideas developed for $i.i.d$ models in Kleijn and van der Vaart(2006) and it's subsequent simplification in Ramamoorthi, et al.(2014) (unpublished manuscript). While key results in these two papers rely heavily on the convexity of the specified family of densities, parametric families are seldom convex. In this note, we take a direct approach to deriving posterior consistency with respect to natural topologies on the parameter space without having to impose conditions on the convex hull of the parametric family. We first derive our results for the case when the responses are $i.i.d$ and then extend it to the $i.n.i.d$ case. As an example, we demonstrate the applicability of the results to the Bayesian quantile estimation problem.

</details>

<details>

<summary>2014-08-26 09:34:49 - Bayesian Fusion of Multi-Band Images</summary>

- *Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1307.5996v2` - [abs](http://arxiv.org/abs/1307.5996v2) - [pdf](http://arxiv.org/pdf/1307.5996v2)

> In this paper, a Bayesian fusion technique for remotely sensed multi-band images is presented. The observed images are related to the high spectral and high spatial resolution image to be recovered through physical degradations, e.g., spatial and spectral blurring and/or subsampling defined by the sensor characteristics. The fusion problem is formulated within a Bayesian estimation framework. An appropriate prior distribution exploiting geometrical consideration is introduced. To compute the Bayesian estimator of the scene of interest from its posterior distribution, a Markov chain Monte Carlo algorithm is designed to generate samples asymptotically distributed according to the target distribution. To efficiently sample from this high-dimension distribution, a Hamiltonian Monte Carlo step is introduced in the Gibbs sampling strategy. The efficiency of the proposed fusion method is evaluated with respect to several state-of-the-art fusion techniques. In particular, low spatial resolution hyperspectral and multispectral images are fused to produce a high spatial resolution hyperspectral image.

</details>

<details>

<summary>2014-08-26 19:07:20 - Bayesian sample sizes for exploratory clinical trials comparing multiple experimental treatments with a control</summary>

- *John Whitehead, Faye Cleary, Amanda Turner*

- `1408.6211v1` - [abs](http://arxiv.org/abs/1408.6211v1) - [pdf](http://arxiv.org/pdf/1408.6211v1)

> In this paper, a Bayesian approach is developed for simultaneously comparing multiple experimental treatments with a common control treatment in an exploratory clinical trial. The sample size is set to ensure that, at the end of the study, there will be at least one treatment for which the investigators have a strong belief that it is better than control, or else they have a strong belief that none of the experimental treatments are substantially better than control. This criterion bears a direct relationship with conventional frequentist power requirements, while allowing prior opinion to feature in the analysis with a consequent reduction in sample size. If it is concluded that at least one of the experimental treatments shows promise, then it is envisaged that one or more of these promising treatments will be developed further in a definitive phase III trial. The approach is developed in the context of normally distributed responses sharing a common standard deviation regardless of treatment. To begin with, the standard deviation will be assumed known when the sample size is calculated. The final analysis will not rely upon this assumption, although the intended properties of the design may not be achieved if the anticipated standard deviation turns out to be inappropriate. Methods that formally allow for uncertainty about the standard deviation, expressed in the form of a Bayesian prior, are then explored. Illustrations of the sample sizes computed from the new method are presented, and comparisons are made with frequentist methods devised for the same situation.

</details>

<details>

<summary>2014-08-27 00:56:54 - Decreasing flow uncertainty in Bayesian inverse problems through Lagrangian drifter control</summary>

- *Damon McDougall, Chris K. R. T. Jones*

- `1408.6288v1` - [abs](http://arxiv.org/abs/1408.6288v1) - [pdf](http://arxiv.org/pdf/1408.6288v1)

> Commonplace in oceanography is the collection of ocean drifter positions. Ocean drifters are devices that sit on the surface of the ocean and move with the flow, transmitting their position via GPS to stations on land. Using drifter data, it is possible to obtain a posterior on the underlying flow. This problem, however, is highly underdetermined. Through controlling an ocean drifter, we attempt to improve our knowledge of the underlying flow. We do this by instructing the drifter to explore parts of the flow currently uncharted, thereby obtaining fresh observations. The efficacy of a control is determined by its effect on the variance of the posterior distribution. A smaller variance is interpreted as a better understanding of the flow. We show a systematic reduction in variance can be achieved by utilising controls that allow the drifter to navigate new or interesting flow structures, a good example of which are eddies.

</details>

<details>

<summary>2014-08-27 05:59:00 - A simulation approach for change-points on phylogenetic trees</summary>

- *Adam Persing, Ajay Jasra, Alexandros Beskos, David Balding, Maria De Iorio*

- `1408.6317v1` - [abs](http://arxiv.org/abs/1408.6317v1) - [pdf](http://arxiv.org/pdf/1408.6317v1)

> We observe $n$ sequences at each of $m$ sites, and assume that they have evolved from an ancestral sequence that forms the root of a binary tree of known topology and branch lengths, but the sequence states at internal nodes are unknown. The topology of the tree and branch lengths are the same for all sites, but the parameters of the evolutionary model can vary over sites. We assume a piecewise constant model for these parameters, with an unknown number of change-points and hence a trans-dimensional parameter space over which we seek to perform Bayesian inference. We propose two novel ideas to deal with the computational challenges of such inference. Firstly, we approximate the model based on the time machine principle: the top nodes of the binary tree (near the root) are replaced by an approximation of the true distribution; as more nodes are removed from the top of the tree, the cost of computing the likelihood is reduced linearly in $n$. The approach introduces a bias, which we investigate empirically. Secondly, we develop a particle marginal Metropolis-Hastings (PMMH) algorithm, that employs a sequential Monte Carlo (SMC) sampler and can use the first idea. Our time-machine PMMH algorithm copes well with one of the bottle-necks of standard computational algorithms: the trans-dimensional nature of the posterior distribution. The algorithm is implemented on simulated and real data examples, and we empirically demonstrate its potential to outperform competing methods based on approximate Bayesian computation (ABC) techniques.

</details>

<details>

<summary>2014-08-28 11:08:56 - Entropy measure for the quantification of upper quantile interdependence in multivariate distributions</summary>

- *Jhan RodrÃ­guez, AndrÃ¡s BÃ¡rdossy*

- `1408.6681v1` - [abs](http://arxiv.org/abs/1408.6681v1) - [pdf](http://arxiv.org/pdf/1408.6681v1)

> We introduce a new measure of interdependence among the components of a random vector along the main diagonal of the vector copula, i.e. along the line $u_{1}=\ldots=u_{J}$, for $\left(u_{1},\ldots,u_{J}\right)\in\left[0,1\right]^{J}$. Our measure is related to the Shannon entropy of a discrete random variable, hence we call it an "entropy index". This entropy index is invariant with respect to marginal non-decreasing transformations and can be used to quantify the intensity of the vector components association in arbitrary dimensions. We show the applicability of our entropy index by an example with real data of 4 stock prices of the DAX index. In case the random vector is in the domain of attraction of an extreme value distribution, our index is shown to have as limit the distribution's extremal coefficient, which can be interpreted as the effective number of asymptotically independent components in the vector.

</details>

<details>

<summary>2014-08-28 15:34:56 - Quantile-Based Spectral Analysis in an Object-Oriented Framework and a Reference Implementation in R: The quantspec Package</summary>

- *Tobias Kley*

- `1408.6755v1` - [abs](http://arxiv.org/abs/1408.6755v1) - [pdf](http://arxiv.org/pdf/1408.6755v1)

> Quantile-based approaches to the spectral analysis of time series have recently attracted a lot of attention. Despite a growing literature that contains various estimation proposals, no systematic methods for computing the new estimators are available to date. This paper contains two main contributions. First, an extensible framework for quantile-based spectral analysis of time series is developed and documented using object-oriented models. A comprehensive, open source, reference implementation of this framework, the R package quantspec, was recently contributed to CRAN by the author of this paper. The second contribution of the present paper is to provide a detailed tutorial, with worked examples, to this R package. A reader who is already familiar with quantile-based spectral analysis and whose primary interest is not the design of the quantspec package, but how to use it, can read the tutorial and worked examples (Sections 3 and 4) independently.

</details>

<details>

<summary>2014-08-28 19:43:18 - A Model of Consistent Node Types in Signed Directed Social Networks</summary>

- *Dongjin Song, David A. Meyer*

- `1408.6822v1` - [abs](http://arxiv.org/abs/1408.6822v1) - [pdf](http://arxiv.org/pdf/1408.6822v1)

> Signed directed social networks, in which the relationships between users can be either positive (indicating relations such as trust) or negative (indicating relations such as distrust), are increasingly common. Thus the interplay between positive and negative relationships in such networks has become an important research topic. Most recent investigations focus upon edge sign inference using structural balance theory or social status theory. Neither of these two theories, however, can explain an observed edge sign well when the two nodes connected by this edge do not share a common neighbor (e.g., common friend). In this paper we develop a novel approach to handle this situation by applying a new model for node types. Initially, we analyze the local node structure in a fully observed signed directed network, inferring underlying node types. The sign of an edge between two nodes must be consistent with their types; this explains edge signs well even when there are no common neighbors. We show, moreover, that our approach can be extended to incorporate directed triads, when they exist, just as in models based upon structural balance or social status theory. We compute Bayesian node types within empirical studies based upon partially observed Wikipedia, Slashdot, and Epinions networks in which the largest network (Epinions) has 119K nodes and 841K edges. Our approach yields better performance than state-of-the-art approaches for these three signed directed networks.

</details>


## 2014-09

<details>

<summary>2014-09-01 18:55:02 - Perturbation Detection Through Modeling of Gene Expression on a Latent Biological Pathway Network: A Bayesian hierarchical approach</summary>

- *Lisa M. Pham, Luis Carvalho, Scott Schaus, Eric D. Kolaczyk*

- `1409.0503v1` - [abs](http://arxiv.org/abs/1409.0503v1) - [pdf](http://arxiv.org/pdf/1409.0503v1)

> Cellular response to a perturbation is the result of a dynamic system of biological variables linked in a complex network. A major challenge in drug and disease studies is identifying the key factors of a biological network that are essential in determining the cell's fate.   Here our goal is the identification of perturbed pathways from high-throughput gene expression data. We develop a three-level hierarchical model, where (i) the first level captures the relationship between gene expression and biological pathways using confirmatory factor analysis, (ii) the second level models the behavior within an underlying network of pathways induced by an unknown perturbation using a conditional autoregressive model, and (iii) the third level is a spike-and-slab prior on the perturbations. We then identify perturbations through posterior-based variable selection.   We illustrate our approach using gene transcription drug perturbation profiles from the DREAM7 drug sensitivity predication challenge data set. Our proposed method identified regulatory pathways that are known to play a causative role and that were not readily resolved using gene set enrichment analysis or exploratory factor models. Simulation results are presented assessing the performance of this model relative to a network-free variant and its robustness to inaccuracies in biological databases.

</details>

<details>

<summary>2014-09-02 14:28:39 - Uniform bias study and Bahadur representation for local polynomial estimators of the conditional quantile function</summary>

- *Emmanuel Guerre, Camille Sabbah*

- `1105.5038v2` - [abs](http://arxiv.org/abs/1105.5038v2) - [pdf](http://arxiv.org/pdf/1105.5038v2)

> This paper investigates the bias and the weak Bahadur representation of a local polynomial estimator of the conditional quantile function and its derivatives. The bias and Bahadur remainder term are studied uniformly with respect to the quantile level, the covariates and the smoothing parameter. The order of the local polynomial estimator can be higher than the differentiability order of the conditional quantile function. Applications of the results deal with global optimal consistency rates of the local polynomial quantile estimator, performance of random bandwidths and estimation of the conditional quantile density function. The latter allows to obtain a simple estimator of the conditional quantile function of the private values in a first price sealed bids auctions under the independent private values paradigm and risk neutrality.

</details>

<details>

<summary>2014-09-02 19:57:25 - Bayesian dose-response analysis for epidemiological studies with complex uncertainty in dose estimation</summary>

- *Deukwoo Kwon, F. Owen Hoffman, Brian E. Moroz, Steven L. Simon*

- `1409.0849v1` - [abs](http://arxiv.org/abs/1409.0849v1) - [pdf](http://arxiv.org/pdf/1409.0849v1)

> Most conventional risk analysis methods rely on a single best estimate of exposure per person which does not allow for adjustment for exposure-related uncertainty. Here, we propose a Bayesian model averaging method to properly quantify the relationship between radiation dose and disease outcomes by accounting for shared and unshared uncertainty in estimated dose. Our Bayesian risk analysis method utilizes multiple realizations of sets (vectors) of doses generated by a two-dimensional Monte Carlo simulation method that properly separates shared and unshared errors in dose estimation. The exposure model used in this work is taken from a study of the risk of thyroid nodules among a cohort of 2,376 subjects following exposure to fallout resulting from nuclear testing in Kazakhstan. We assessed the performance of our method through an extensive series of simulation tests and comparisons against conventional regression risk analysis methods. We conclude that when estimated doses contain relatively small amounts of uncertainty, the Bayesian method using multiple realizations of possibly true dose vectors gave similar results to the conventional regression-based methods of dose-response analysis. However, when large and complex mixtures of shared and unshared uncertainties are present, the Bayesian method using multiple dose vectors had significantly lower relative bias than conventional regression-based risk analysis methods as well as a markedly increased capability to include the pre-established 'true' risk coefficient within the credible interval of the Bayesian-based risk estimate. An evaluation of the dose-response using our method is presented for an epidemiological study of thyroid disease following radiation exposure.

</details>

<details>

<summary>2014-09-03 05:18:53 - On Bayesian A- and D-optimal experimental designs in infinite dimensions</summary>

- *Alen Alexanderian, Philip Gloor, Omar Ghattas*

- `1408.6323v2` - [abs](http://arxiv.org/abs/1408.6323v2) - [pdf](http://arxiv.org/pdf/1408.6323v2)

> We consider Bayesian linear inverse problems in infinite-dimensional separable Hilbert spaces, with a Gaussian prior measure and additive Gaussian noise model, and provide an extension of the concept of Bayesian D-optimality to the infinite-dimensional case. To this end, we derive the infinite-dimensional version of the expression for the Kullback-Leibler divergence from the posterior measure to the prior measure, which is subsequently used to derive the expression for the expected information gain. We also study the notion of Bayesian A-optimality in the infinite-dimensional setting, and extend the well known (in the finite-dimensional case) equivalence of the Bayes risk of the MAP estimator with the trace of the posterior covariance, for the Gaussian linear case, to the infinite-dimensional Hilbert space case.

</details>

<details>

<summary>2014-09-03 15:56:46 - Distributed Bayesian Detection with Byzantine Data</summary>

- *Bhavya Kailkhura, Yunghsiang S. Han, Swastik Brahma, Pramod K. Varshney*

- `1307.3544v2` - [abs](http://arxiv.org/abs/1307.3544v2) - [pdf](http://arxiv.org/pdf/1307.3544v2)

> In this paper, we consider the problem of distributed Bayesian detection in the presence of Byzantines in the network. It is assumed that a fraction of the nodes in the network are compromised and reprogrammed by an adversary to transmit false information to the fusion center (FC) to degrade detection performance. The problem of distributed detection is formulated as a binary hypothesis test at the FC based on 1-bit data sent by the sensors. The expression for minimum attacking power required by the Byzantines to blind the FC is obtained. More specifically, we show that above a certain fraction of Byzantine attackers in the network, the detection scheme becomes completely incapable of utilizing the sensor data for detection. We analyze the problem under different attacking scenarios and derive results for different non-asymptotic cases. It is found that existing asymptotics-based results do not hold under several non-asymptotic scenarios. When the fraction of Byzantines is not sufficient to blind the FC, we also provide closed form expressions for the optimal attacking strategies for the Byzantines that most degrade the detection performance.

</details>

<details>

<summary>2014-09-05 17:38:55 - Pre-processing for approximate Bayesian computation in image analysis</summary>

- *Matthew T. Moores, Christopher C. Drovandi, Kerrie Mengersen, Christian P. Robert*

- `1403.4359v2` - [abs](http://arxiv.org/abs/1403.4359v2) - [pdf](http://arxiv.org/pdf/1403.4359v2)

> Most of the existing algorithms for approximate Bayesian computation (ABC) assume that it is feasible to simulate pseudo-data from the model at each iteration. However, the computational cost of these simulations can be prohibitive for high dimensional data. An important example is the Potts model, which is commonly used in image analysis. Images encountered in real world applications can have millions of pixels, therefore scalability is a major concern. We apply ABC with a synthetic likelihood to the hidden Potts model with additive Gaussian noise. Using a pre-processing step, we fit a binding function to model the relationship between the model parameters and the synthetic likelihood parameters. Our numerical experiments demonstrate that the precomputed binding function dramatically improves the scalability of ABC, reducing the average runtime required for model fitting from 71 hours to only 7 minutes. We also illustrate the method by estimating the smoothing parameter for remotely sensed satellite imagery. Without precomputation, Bayesian inference is impractical for datasets of that scale.

</details>

<details>

<summary>2014-09-05 21:45:55 - A Bayesian Beta Markov Random Field Calibration of the Term Structure of Implied Risk Neutral Densities</summary>

- *Roberto Casarin, Fabrizio Leisen, German Molina, Enrique ter Horst*

- `1409.1956v1` - [abs](http://arxiv.org/abs/1409.1956v1) - [pdf](http://arxiv.org/pdf/1409.1956v1)

> We build on the work in Fackler and King 1990, and propose a more general calibration model for implied risk neutral densities. Our model allows for the joint calibration of a set of densities at different maturities and dates through a Bayesian dynamic Beta Markov Random Field. Our approach allows for possible time dependence between densities with the same maturity, and for dependence across maturities at the same point in time. This approach to the problem encompasses model flexibility, parameter parsimony and, more importantly, information pooling across densities.

</details>

<details>

<summary>2014-09-07 06:42:51 - On the asymptotics of random forests</summary>

- *Erwan Scornet*

- `1409.2090v1` - [abs](http://arxiv.org/abs/1409.2090v1) - [pdf](http://arxiv.org/pdf/1409.2090v1)

> The last decade has witnessed a growing interest in random forest models which are recognized to exhibit good practical performance, especially in high-dimensional settings. On the theoretical side, however, their predictive power remains largely unexplained, thereby creating a gap between theory and practice. The aim of this paper is twofold. Firstly, we provide theoretical guarantees to link finite forests used in practice (with a finite number M of trees) to their asymptotic counterparts. Using empirical process theory, we prove a uniform central limit theorem for a large class of random forest estimates, which holds in particular for Breiman's original forests. Secondly, we show that infinite forest consistency implies finite forest consistency and thus, we state the consistency of several infinite forests. In particular, we prove that q quantile forests---close in spirit to Breiman's forests but easier to study---are able to combine inconsistent trees to obtain a final consistent prediction, thus highlighting the benefits of random forests compared to single trees.

</details>

<details>

<summary>2014-09-08 10:10:25 - Stratified Gaussian Graphical Models</summary>

- *Henrik Nyman, Johan Pensar, Jukka Corander*

- `1409.2262v1` - [abs](http://arxiv.org/abs/1409.2262v1) - [pdf](http://arxiv.org/pdf/1409.2262v1)

> Gaussian graphical models represent the backbone of the statistical toolbox for analyzing continuous multivariate systems. However, due to the intrinsic properties of the multivariate normal distribution, use of this model family may hide certain forms of context-specific independence that are natural to consider from an applied perspective. Such independencies have been earlier introduced to generalize discrete graphical models and Bayesian networks into more flexible model families. Here we adapt the idea of context-specific independence to Gaussian graphical models by introducing a stratification of the Euclidean space such that a conditional independence may hold in certain segments but be absent elsewhere. It is shown that the stratified models define a curved exponential family, which retains considerable tractability for parameter estimation and model selection.

</details>

<details>

<summary>2014-09-08 10:47:23 - Variational Inference for Uncertainty on the Inputs of Gaussian Process Models</summary>

- *Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence*

- `1409.2287v1` - [abs](http://arxiv.org/abs/1409.2287v1) - [pdf](http://arxiv.org/pdf/1409.2287v1)

> The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.

</details>

<details>

<summary>2014-09-08 17:14:10 - Bayesian Discovery of Threat Networks</summary>

- *Steven T. Smith, Edward K. Kao, Kenneth D. Senne, Garrett Bernstein, Scott Philips*

- `1311.5552v3` - [abs](http://arxiv.org/abs/1311.5552v3) - [pdf](http://arxiv.org/pdf/1311.5552v3)

> A novel unified Bayesian framework for network detection is developed, under which a detection algorithm is derived based on random walks on graphs. The algorithm detects threat networks using partial observations of their activity, and is proved to be optimum in the Neyman-Pearson sense. The algorithm is defined by a graph, at least one observation, and a diffusion model for threat. A link to well-known spectral detection methods is provided, and the equivalence of the random walk and harmonic solutions to the Bayesian formulation is proven. A general diffusion model is introduced that utilizes spatio-temporal relationships between vertices, and is used for a specific space-time formulation that leads to significant performance improvements on coordinated covert networks. This performance is demonstrated using a new hybrid mixed-membership blockmodel introduced to simulate random covert networks with realistic properties.

</details>

<details>

<summary>2014-09-09 05:31:50 - Iterative Posterior Inference for Bayesian Kriging</summary>

- *Zepu Zhang*

- `1409.2599v1` - [abs](http://arxiv.org/abs/1409.2599v1) - [pdf](http://arxiv.org/pdf/1409.2599v1)

> We propose a method for estimating the posterior distribution of a standard geostatistical model. After choosing the model formulation and specifying a prior, we use normal mixture densities to approximate the posterior distribution. The approximation is improved iteratively. Some difficulties in estimating the normal mixture densities, including determining tuning parameters concerning bandwidth and localization, are addressed. The method is applicable to other model formulations as long as all the parameters, or transforms thereof, are defined on the whole real line, $(-\infty, \infty)$. Ad hoc treatments in the posterior inference such as imposing bounds on an unbounded parameter or discretizing a continuous parameter are avoided. The method is illustrated by two examples, one using digital elevation data and the other using historical soil moisture data. The examples in particular examine convergence of the approximate posterior distributions in the iterations.

</details>

<details>

<summary>2014-09-09 10:57:23 - Efficient sampling of Gaussian graphical models using conditional Bayes factors</summary>

- *Max Hinne, Alex Lenkoski, Tom Heskes, Marcel van Gerven*

- `1409.2676v1` - [abs](http://arxiv.org/abs/1409.2676v1) - [pdf](http://arxiv.org/pdf/1409.2676v1)

> Bayesian estimation of Gaussian graphical models has proven to be challenging because the conjugate prior distribution on the Gaussian precision matrix, the G-Wishart distribution, has a doubly intractable partition function. Recent developments provide a direct way to sample from the G-Wishart distribution, which allows for more efficient algorithms for model selection than previously possible. Still, estimating Gaussian graphical models with more than a handful of variables remains a nearly infeasible task. Here, we propose two novel algorithms that use the direct sampler to more efficiently approximate the posterior distribution of the Gaussian graphical model. The first algorithm uses conditional Bayes factors to compare models in a Metropolis-Hastings framework. The second algorithm is based on a continuous time Markov process. We show that both algorithms are substantially faster than state-of-the-art alternatives. Finally, we show how the algorithms may be used to simultaneously estimate both structural and functional connectivity between subcortical brain regions using resting-state fMRI.

</details>

<details>

<summary>2014-09-09 15:02:29 - Posterior contraction rate for non-parametric Bayesian estimation of the dispersion coefficient of a stochastic differential equation</summary>

- *Shota Gugushvili, Peter Spreij*

- `1409.2767v1` - [abs](http://arxiv.org/abs/1409.2767v1) - [pdf](http://arxiv.org/pdf/1409.2767v1)

> We derive the posteror contraction rate for non-parametric Bayesian estimation of a deterministic dispersion coefficient of a linear stochastic differential equation.

</details>

<details>

<summary>2014-09-09 18:27:41 - Building complex networks through classical and Bayesian statistics - a comparison</summary>

- *Lina D. Thomas, Victor Fossaluza, Anatoly Yambartsev*

- `1409.2833v1` - [abs](http://arxiv.org/abs/1409.2833v1) - [pdf](http://arxiv.org/pdf/1409.2833v1)

> This research is about studying and comparing two different ways of building complex networks. The main goal of our study is to find an effective way to build networks, particularly when we have fewer observations than variables. We construct networks estimating the partial correlation coefficient on Classic Statistics (Inverse Method) and on Bayesian Statistics (Normal - Inverse Wishart conjugate prior). In this current work, in order to solve the problem of having less observations than variables, we propose a new methodology called local partial correlation, which consists of selecting, for each pair of variables, the other variables most correlated to the pair.We applied these methods on simulated data and compared them through ROC curves. The most attractive result is that, even though it has high computational costs, to use Bayesian inference on trees is better when we have less observations than variables. In other cases, both approaches present satisfactory results.

</details>

<details>

<summary>2014-09-10 09:30:37 - Scalable Bayesian Modelling of Paired Symbols</summary>

- *Ulrich Paquet, Noam Koenigstein, Ole Winther*

- `1409.2824v2` - [abs](http://arxiv.org/abs/1409.2824v2) - [pdf](http://arxiv.org/pdf/1409.2824v2)

> We present a novel, scalable and Bayesian approach to modelling the occurrence of pairs of symbols (i,j) drawn from a large vocabulary. Observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function. By basing inference on the well-founded principle of variational bounding, and using new site-independent bounds, we show how a scalable inference procedure can be obtained for large data sets. State of the art results are presented on real-world movie viewing data.

</details>

<details>

<summary>2014-09-10 12:32:26 - Revealing the Beauty behind the Sleeping Beauty Problem</summary>

- *Ioannis Mariolis*

- `1409.3803v1` - [abs](http://arxiv.org/abs/1409.3803v1) - [pdf](http://arxiv.org/pdf/1409.3803v1)

> A large number of essays address the Sleeping Beauty problem, which undermines the validity of Bayesian inference and Bas Van Fraassen's 'Reflection Principle'. In this study a straightforward analysis of the problem based on probability theory is presented. The key difference from previous works is that apart from the random experiment imposed by the problem's description, a different one is also considered, in order to negate the confusion on the involved conditional probabilities. The results of the analysis indicate that no inconsistency takes place, whereas both Bayesian inference and 'Reflection Principle' are valid.

</details>

<details>

<summary>2014-09-11 20:24:41 - Parsimonious Topic Models with Salient Word Discovery</summary>

- *Hossein Soleimani, David J. Miller*

- `1401.6169v2` - [abs](http://arxiv.org/abs/1401.6169v2) - [pdf](http://arxiv.org/pdf/1401.6169v2)

> We propose a parsimonious topic model for text corpora. In related models such as Latent Dirichlet Allocation (LDA), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics. Our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model. Further, in LDA all topics are in principle present in every document. By contrast our model gives sparse topic representation, determining the (small) subset of relevant topics for each document. We derive a Bayesian Information Criterion (BIC), balancing model complexity and goodness of fit. Here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model. We minimize BIC to jointly determine our entire model -- the topic-specific words, document-specific topics, all model parameter values, {\it and} the total number of topics -- in a wholly unsupervised fashion. Results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to LDA and to a model designed to incorporate sparsity.

</details>

<details>

<summary>2014-09-14 04:01:37 - Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces</summary>

- *Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, Michael A. Osborne*

- `1409.4011v1` - [abs](http://arxiv.org/abs/1409.4011v1) - [pdf](http://arxiv.org/pdf/1409.4011v1)

> In practical Bayesian optimization, we must often search over structures with differing numbers of parameters. For instance, we may wish to search over neural network architectures with an unknown number of layers. To relate performance data gathered for different architectures, we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels.

</details>

<details>

<summary>2014-09-14 13:05:38 - Generalised Fisher Matrices</summary>

- *A. F. Heavens, M. Seikel, B. D. Nord, M. Aich, Y. Bouffanais, B. A. Bassett, M. P. Hobson*

- `1404.2854v2` - [abs](http://arxiv.org/abs/1404.2854v2) - [pdf](http://arxiv.org/pdf/1404.2854v2)

> The Fisher Information Matrix formalism is extended to cases where the data is divided into two parts (X,Y), where the expectation value of Y depends on X according to some theoretical model, and X and Y both have errors with arbitrary covariance. In the simplest case, (X,Y) represent data pairs of abscissa and ordinate, in which case the analysis deals with the case of data pairs with errors in both coordinates, but X can be any measured quantities on which Y depends. The analysis applies for arbitrary covariance, provided all errors are gaussian, and provided the errors in X are small, both in comparison with the scale over which the expected signal Y changes, and with the width of the prior distribution. This generalises the Fisher Matrix approach, which normally only considers errors in the `ordinate' Y. In this work, we include errors in X by marginalising over latent variables, effectively employing a Bayesian hierarchical model, and deriving the Fisher Matrix for this more general case. The methods here also extend to likelihood surfaces which are not gaussian in the parameter space, and so techniques such as DALI (Derivative Approximation for Likelihoods) can be generalised straightforwardly to include arbitrary gaussian data error covariances. For simple mock data and theoretical models, we compare to Markov Chain Monte Carlo experiments, illustrating the method with cosmological supernova data. We also include the new method in the Fisher4Cast software.

</details>

<details>

<summary>2014-09-15 14:35:22 - The two envelopes paradox in non-Bayesian and Bayesian statistics</summary>

- *Shiro Ishikawa*

- `1408.4916v4` - [abs](http://arxiv.org/abs/1408.4916v4) - [pdf](http://arxiv.org/pdf/1408.4916v4)

> The purpose of this paper is to clarify the (non-Bayesian and Bayesian) two-envelope problems in terms of quantum language (or, measurement theory), which was recently proposed as a linguistic turn of quantum mechanics (with the Copenhagen interpretation). The two envelopes paradox is only a kind of high school student's probability puzzle, and it may be exaggerated to say that this is an unsolved problem. However, since we are convinced that quantum language is just statistics of the future, we believe that there is no clear answer without the description by quantum language. In this sense, the readers are to find that quantum language provides the final answer (i.e., the easiest and deepest understanding) to the two envelope-problems in both non-Bayesian and Bayesian statistics. Also, we add the discussion about St. Petersburg two-envelope paradox.

</details>

<details>

<summary>2014-09-15 18:16:45 - Bayesian inference for Markov jump processes with informative observations</summary>

- *Andrew Golightly, Darren J. Wilkinson*

- `1409.4362v1` - [abs](http://arxiv.org/abs/1409.4362v1) - [pdf](http://arxiv.org/pdf/1409.4362v1)

> In this paper we consider the problem of parameter inference for Markov jump process (MJP) representations of stochastic kinetic models. Since transition probabilities are intractable for most processes of interest yet forward simulation is straightforward, Bayesian inference typically proceeds through computationally intensive methods such as (particle) MCMC. Such methods ostensibly require the ability to simulate trajectories from the conditioned jump process. When observations are highly informative, use of the forward simulator is likely to be inefficient and may even preclude an exact (simulation based) analysis. We therefore propose three methods for improving the efficiency of simulating conditioned jump processes. A conditioned hazard is derived based on an approximation to the jump process, and used to generate end-point conditioned trajectories for use inside an importance sampling algorithm. We also adapt a recently proposed sequential Monte Carlo scheme to our problem. Essentially, trajectories are reweighted at a set of intermediate time points, with more weight assigned to trajectories that are consistent with the next observation. We consider two implementations of this approach, based on two continuous approximations of the MJP. We compare these constructs for a simple tractable jump process before using them to perform inference for a Lotka-Volterra system. The best performing construct is used to infer the parameters governing a simple model of motility regulation in Bacillus subtilis.

</details>

<details>

<summary>2014-09-16 21:46:03 - A Bayesian hierarchical model for inferring player strategy types in a number guessing game</summary>

- *P. Richard Hahn, Indranil Goswami, Carl Mela*

- `1409.4815v1` - [abs](http://arxiv.org/abs/1409.4815v1) - [pdf](http://arxiv.org/pdf/1409.4815v1)

> This paper presents an in-depth statistical analysis of an experiment designed to measure the extent to which players in a simple game behave according to a popular behavioral economic model. The p-beauty contest is a multi-player number guessing game that has been widely used to study strategic behavior. This paper describes beauty contest experiments for an audience of data analysts, with a special focus on a class of models for game play called k-step thinking models, which allow each player in the game to employ an idiosyncratic strategy. We fit a Bayesian statistical model to estimate the proportion of our player population whose game play is compatible with a k-step thinking model. Our findings put this number at approximately 25%.

</details>

<details>

<summary>2014-09-16 22:11:20 - Model-Robust Designs for Quantile Regression</summary>

- *Linglong Kong, Douglas P. Wiens*

- `1403.1638v2` - [abs](http://arxiv.org/abs/1403.1638v2) - [pdf](http://arxiv.org/pdf/1403.1638v2)

> We give methods for the construction of designs for linear models, when the purpose of the investigation is the estimation of the conditional quantile function and the estimation method is quantile regression. The designs are robust against misspecified response functions, and against unanticipated heteroscedasticity. The methods are illustrated by example, and in a case study in which they are applied to growth charts.

</details>

<details>

<summary>2014-09-17 12:59:01 - Phenotypic landscape inference reveals multiple evolutionary paths to C$_4$ photosynthesis</summary>

- *Ben P. Williams, Iain G. Johnston, Sarah Covshoff, Julian M. Hibberd*

- `1409.4978v1` - [abs](http://arxiv.org/abs/1409.4978v1) - [pdf](http://arxiv.org/pdf/1409.4978v1)

> C$_4$ photosynthesis has independently evolved from the ancestral C$_3$ pathway in at least 60 plant lineages, but, as with other complex traits, how it evolved is unclear. Here we show that the polyphyletic appearance of C$_4$ photosynthesis is associated with diverse and flexible evolutionary paths that group into four major trajectories. We conducted a meta-analysis of 18 lineages containing species that use C$_3$, C$_4$, or intermediate C$_3$-C$_4$ forms of photosynthesis to parameterise a 16-dimensional phenotypic landscape. We then developed and experimentally verified a novel Bayesian approach based on a hidden Markov model that predicts how the C$_4$ phenotype evolved. The alternative evolutionary histories underlying the appearance of C$_4$ photosynthesis were determined by ancestral lineage and initial phenotypic alterations unrelated to photosynthesis. We conclude that the order of C$_4$ trait acquisition is flexible and driven by non-photosynthetic drivers. This flexibility will have facilitated the convergent evolution of this complex trait.

</details>

<details>

<summary>2014-09-17 14:25:11 - Efficient computational strategies for doubly intractable problems with applications to Bayesian social networks</summary>

- *Alberto Caimo, Antonietta Mira*

- `1403.4402v2` - [abs](http://arxiv.org/abs/1403.4402v2) - [pdf](http://arxiv.org/pdf/1403.4402v2)

> Powerful ideas recently appeared in the literature are adjusted and combined to design improved samplers for Bayesian exponential random graph models. Different forms of adaptive Metropolis-Hastings proposals (vertical, horizontal and rectangular) are tested and combined with the Delayed rejection (DR) strategy with the aim of reducing the variance of the resulting Markov chain Monte Carlo estimators for a given computational time. In the examples treated in this paper the best combination, namely horizontal adaptation with delayed rejection, leads to a variance reduction that varies between 92% and 144% relative to the adaptive direction sampling approximate exchange algorithm of Caimo and Friel (2011). These results correspond to an increased performance which varies from 10% to 94% if we take simulation time into account. The highest improvements are obtained when highly correlated posterior distributions are considered.

</details>

<details>

<summary>2014-09-17 19:27:06 - Unsupervised Learning via Mixtures of Skewed Distributions with Hypercube Contours</summary>

- *Brian C. Franczak, Cristina Tortora, Ryan P. Browne, Paul D. McNicholas*

- `1403.2285v5` - [abs](http://arxiv.org/abs/1403.2285v5) - [pdf](http://arxiv.org/pdf/1403.2285v5)

> Mixture models whose components have skewed hypercube contours are developed via a generalization of the multivariate shifted asymmetric Laplace density. Specifically, we develop mixtures of multiple scaled shifted asymmetric Laplace distributions. The component densities have two unique features: they include a multivariate weight function, and the marginal distributions are also asymmetric Laplace. We use these mixtures of multiple scaled shifted asymmetric Laplace distributions for clustering applications, but they could equally well be used in the supervised or semi-supervised paradigms. The expectation-maximization algorithm is used for parameter estimation and the Bayesian information criterion is used for model selection. Simulated and real data sets are used to illustrate the approach and, in some cases, to visualize the skewed hypercube structure of the components.

</details>

<details>

<summary>2014-09-18 18:31:50 - SAME but Different: Fast and High-Quality Gibbs Parameter Estimation</summary>

- *Huasha Zhao, Biye Jiang, John Canny*

- `1409.5402v1` - [abs](http://arxiv.org/abs/1409.5402v1) - [pdf](http://arxiv.org/pdf/1409.5402v1)

> Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference methods. SAME (State Augmentation for Marginal Estimation) \cite{Doucet99,Doucet02} is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sampling. SAME can be viewed as cooling the posterior parameter distribution and allows annealed search for the MAP parameters, often yielding very high quality (lower loss) estimates. But it does so at the expense of additional samples per iteration and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sampling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better quality. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (online Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to implement and should be applicable to many other models.

</details>

<details>

<summary>2014-09-18 22:28:55 - Particle Metropolis-Hastings using gradient and Hessian information</summary>

- *Johan Dahlin, Fredrik Lindsten, Thomas B. SchÃ¶n*

- `1311.0686v4` - [abs](http://arxiv.org/abs/1311.0686v4) - [pdf](http://arxiv.org/pdf/1311.0686v4)

> Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference in nonlinear state space models by combining Markov chain Monte Carlo (MCMC) and particle filtering. The latter is used to estimate the intractable likelihood. In its original formulation, PMH makes use of a marginal MCMC proposal for the parameters, typically a Gaussian random walk. However, this can lead to a poor exploration of the parameter space and an inefficient use of the generated particles.   We propose a number of alternative versions of PMH that incorporate gradient and Hessian information about the posterior into the proposal. This information is more or less obtained as a byproduct of the likelihood estimation. Indeed, we show how to estimate the required information using a fixed-lag particle smoother, with a computational cost growing linearly in the number of particles. We conclude that the proposed methods can: (i) decrease the length of the burn-in phase, (ii) increase the mixing of the Markov chain at the stationary phase, and (iii) make the proposal distribution scale invariant which simplifies tuning.

</details>

<details>

<summary>2014-09-19 22:57:20 - Bayesian Prediction for The Winds of Winter</summary>

- *Richard Vale*

- `1409.5830v1` - [abs](http://arxiv.org/abs/1409.5830v1) - [pdf](http://arxiv.org/pdf/1409.5830v1)

> Predictions are made for the number of chapters told from the point of view of each character in the next two novels in George R. R. Martin's \emph{A Song of Ice and Fire} series by fitting a random effects model to a matrix of point-of-view chapters in the earlier novels using Bayesian methods. {\textbf{SPOILER WARNING: readers who have not read all five existing novels in the series should not read further, as major plot points will be spoiled.}}

</details>

<details>

<summary>2014-09-21 07:56:21 - Moment Consistency of the Exchangeably Weighted Bootstrap for Semiparametric M-Estimation</summary>

- *Guang Cheng*

- `1109.4204v2` - [abs](http://arxiv.org/abs/1109.4204v2) - [pdf](http://arxiv.org/pdf/1109.4204v2)

> The bootstrap variance estimate is widely used in semiparametric inferences. However, its theoretical validity is a well known open problem. In this paper, we provide a {\em first} theoretical study on the bootstrap moment estimates in semiparametric models. Specifically, we establish the bootstrap moment consistency of the Euclidean parameter which immediately implies the consistency of $t$-type bootstrap confidence set. It is worth pointing out that the only additional cost to achieve the bootstrap moment consistency in contrast with the distribution consistency is to simply strengthen the $L_1$ maximal inequality condition required in the latter to the $L_p$ maximal inequality condition for $p\geq 1$. The general $L_p$ multiplier inequality developed in this paper is also of independent interest. These general conclusions hold for the bootstrap methods with exchangeable bootstrap weights, e.g., nonparametric bootstrap and Bayesian bootstrap. Our general theory is illustrated in the celebrated Cox regression model.

</details>

<details>

<summary>2014-09-22 13:43:08 - Bayesian adaptation</summary>

- *Catia Scricciolo*

- `1407.2219v2` - [abs](http://arxiv.org/abs/1407.2219v2) - [pdf](http://arxiv.org/pdf/1407.2219v2)

> In the need for low assumption inferential methods in infinite-dimensional settings, Bayesian adaptive estimation via a prior distribution that does not depend on the regularity of the function to be estimated nor on the sample size is valuable. We elucidate relationships among the main approaches followed to design priors for minimax-optimal rate-adaptive estimation meanwhile shedding light on the underlying ideas.

</details>

<details>

<summary>2014-09-22 13:52:44 - A Bayesian Network View on Acoustic Model-Based Techniques for Robust Speech Recognition</summary>

- *Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann*

- `1310.3099v2` - [abs](http://arxiv.org/abs/1310.3099v2) - [pdf](http://arxiv.org/pdf/1310.3099v2)

> This article provides a unifying Bayesian network view on various approaches for acoustic model adaptation, missing feature, and uncertainty decoding that are well-known in the literature of robust automatic speech recognition. The representatives of these classes can often be deduced from a Bayesian network that extends the conventional hidden Markov models used in speech recognition. These extensions, in turn, can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors. By converting the observation models into a Bayesian network representation, we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to new formulations for certain approaches. The generic Bayesian perspective provided in this contribution thus highlights structural differences and similarities between the analyzed approaches.

</details>

<details>

<summary>2014-09-22 14:29:37 - Joint Inference of Misaligned Irregular Time Series with Application to Greenland Ice Core Data</summary>

- *Thinh K. Doan, Andrew C. Parnell, John Haslett*

- `1402.3014v3` - [abs](http://arxiv.org/abs/1402.3014v3) - [pdf](http://arxiv.org/pdf/1402.3014v3)

> Ice cores provide insight into the past climate over many millennia. Due to ice compaction, the raw data for any single core are irregular in time. Multiple cores have different irregularities; jointly these series are misaligned. After processing, such data are made available to researchers as regular time series: a data product. Typically, these cores are independently processed. In this paper, we consider a fast Bayesian method for the joint processing of multiple irregular series. This is shown to be more efficient. Further, our approach permits a realistic modelling of the impact of the multiple sources of uncertainty. The methodology is illustrated with the analysis of a pair of ice cores (GISP2 and GRIP). Our data products, in the form of marginal posterior distributions on an arbitrary temporal grid, are finite Gaussian mixtures. We can also produce sample paths from the joint posterior distribution to study non-linear functionals of interest. More generally, the concept of joint analysis via hierarchical Gaussian process model can be widely extended as the models used can be viewed within the larger context of continuous space-time processes.

</details>

<details>

<summary>2014-09-23 08:33:47 - On the propriety of the posterior of hierarchical linear mixed models with flexible random effects distributions</summary>

- *F. J. Rubio*

- `1409.6447v1` - [abs](http://arxiv.org/abs/1409.6447v1) - [pdf](http://arxiv.org/pdf/1409.6447v1)

> The use of improper priors in the context of Bayesian hierarchical linear mixed models has been studied under the assumption of normality of the random effects. We study the propriety of the posterior under more flexible distributional assumptions and general improper prior structures.

</details>

<details>

<summary>2014-09-23 11:35:15 - Preconditioning the prior to overcome saturation in Bayesian inverse problems</summary>

- *Sergios Agapiou, Peter MathÃ©*

- `1409.6496v1` - [abs](http://arxiv.org/abs/1409.6496v1) - [pdf](http://arxiv.org/pdf/1409.6496v1)

> We study Bayesian inference in statistical linear inverse problems with Gaussian noise and priors in Hilbert space. We focus our interest on the posterior contraction rate in the small noise limit. Existing results suffer from a certain saturation phenomenon, when the data generating element is too smooth compared to the smoothness inherent in the prior. We show how to overcome this saturation in an empirical Bayesian framework by using a non-centered data-dependent prior. The center is obtained from a preconditioning regularization step, which provides us with additional information to be used in the Bayesian framework. We use general techniques known from regularization theory. To highlight the significance of the findings we provide several examples. In particular, our approach allows to obtain and, using preconditioning improve after saturation, minimax rates of contraction established in previous studies. We also establish minimax contraction rates in cases which have not been considered so far.

</details>

<details>

<summary>2014-09-24 09:25:09 - One-class Collaborative Filtering with Random Graphs: Annotated Version</summary>

- *Ulrich Paquet, Noam Koenigstein*

- `1309.6786v4` - [abs](http://arxiv.org/abs/1309.6786v4) - [pdf](http://arxiv.org/pdf/1309.6786v4)

> The bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. In this paper we present a novel Bayesian generative model for implicit collaborative filtering. It forms a core component of the Xbox Live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply not considering it. The latent signal is treated as an unobserved random graph connecting users with items they might have encountered. We demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples. A fine-grained comparison is done against a state of the art baseline on real world data.

</details>

<details>

<summary>2014-09-25 05:22:17 - Bayesian Inference for Tumor Subclones Accounting for Sequencing and Structural Variants</summary>

- *Juhee Lee, Peter Mueller, Subhajit Sengupta, Kamalakar Gulukota, Yuan Ji*

- `1409.7158v1` - [abs](http://arxiv.org/abs/1409.7158v1) - [pdf](http://arxiv.org/pdf/1409.7158v1)

> Tumor samples are heterogeneous. They consist of different subclones that are characterized by differences in DNA nucleotide sequences and copy numbers on multiple loci. Heterogeneity can be measured through the identification of the subclonal copy number and sequence at a selected set of loci. Understanding that the accurate identification of variant allele fractions greatly depends on a precise determination of copy numbers, we develop a Bayesian feature allocation model for jointly calling subclonal copy numbers and the corresponding allele sequences for the same loci. The proposed method utilizes three random matrices, L, Z and w to represent subclonal copy numbers (L), numbers of subclonal variant alleles (Z) and cellular fractions of subclones in samples (w), respectively. The unknown number of subclones implies a random number of columns for these matrices. We use next-generation sequencing data to estimate the subclonal structures through inference on these three matrices. Using simulation studies and a real data analysis, we demonstrate how posterior inference on the subclonal structure is enhanced with the joint modeling of both structure and sequencing variants on subclonal genomes. Software is available at http://compgenome.org/BayClone2.

</details>

<details>

<summary>2014-09-25 20:36:36 - Nonparametric Bayes modeling with sample survey weights</summary>

- *T. Kunihama, A. H. Herring, C. T. Halpern, D. B. Dunson*

- `1409.5914v2` - [abs](http://arxiv.org/abs/1409.5914v2) - [pdf](http://arxiv.org/pdf/1409.5914v2)

> In population studies, it is standard to sample data via designs in which the population is divided into strata, with the different strata assigned different probabilities of inclusion. Although there have been some proposals for including sample survey weights into Bayesian analyses, existing methods require complex models or ignore the stratified design underlying the survey weights. We propose a simple approach based on modeling the distribution of the selected sample as a mixture, with the mixture weights appropriately adjusted, while accounting for uncertainty in the adjustment. We focus for simplicity on Dirichlet process mixtures but the proposed approach can be applied more broadly. We sketch a simple Markov chain Monte Carlo algorithm for computation, and assess the approach via simulations and an application.

</details>

<details>

<summary>2014-09-25 21:03:00 - Identifying the number of clusters in discrete mixture models</summary>

- *ClÃ¡udia Silvestre, Margarida G. M. S. Cardoso, MÃ¡rio A. T. Figueiredo*

- `1409.7419v1` - [abs](http://arxiv.org/abs/1409.7419v1) - [pdf](http://arxiv.org/pdf/1409.7419v1)

> Research on cluster analysis for categorical data continues to develop, with new clustering algorithms being proposed. However, in this context, the determination of the number of clusters is rarely addressed. In this paper, we propose a new approach in which clustering of categorical data and the estimation of the number of clusters is carried out simultaneously. Assuming that the data originate from a finite mixture of multinomial distributions, we develop a method to select the number of mixture components based on a minimum message length (MML) criterion and implement a new expectation-maximization (EM) algorithm to estimate all the model parameters. The proposed EM-MML approach, rather than selecting one among a set of pre-estimated candidate models (which requires running EM several times), seamlessly integrates estimation and model selection in a single algorithm. The performance of the proposed approach is compared with other well-known criteria (such as the Bayesian information criterion-BIC), resorting to synthetic data and to two real applications from the European Social Survey. The EM-MML computation time is a clear advantage of the proposed method. Also, the real data solutions are much more parsimonious than the solutions provided by competing methods, which reduces the risk of model order overestimation and increases interpretability.

</details>

<details>

<summary>2014-09-26 01:45:34 - Beyond Maximum Likelihood: from Theory to Practice</summary>

- *Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman*

- `1409.7458v1` - [abs](http://arxiv.org/abs/1409.7458v1) - [pdf](http://arxiv.org/pdf/1409.7458v1)

> Maximum likelihood is the most widely used statistical estimation technique. Recent work by the authors introduced a general methodology for the construction of estimators for functionals in parametric models, and demonstrated improvements - both in theory and in practice - over the maximum likelihood estimator (MLE), particularly in high dimensional scenarios involving parameter dimension comparable to or larger than the number of samples. This approach to estimation, building on results from approximation theory, is shown to yield minimax rate-optimal estimators for a wide class of functionals, implementable with modest computational requirements. In a nutshell, a message of this recent work is that, for a wide class of functionals, the performance of these essentially optimal estimators with $n$ samples is comparable to that of the MLE with $n \ln n$ samples.   In the present paper, we highlight the applicability of the aforementioned methodology to statistical problems beyond functional estimation, and show that it can yield substantial gains. For example, we demonstrate that for learning tree-structured graphical models, our approach achieves a significant reduction of the required data size compared with the classical Chow--Liu algorithm, which is an implementation of the MLE, to achieve the same accuracy. The key step in improving the Chow--Liu algorithm is to replace the empirical mutual information with the estimator for mutual information proposed by the authors. Further, applying the same replacement approach to classical Bayesian network classification, the resulting classifiers uniformly outperform the previous classifiers on 26 widely used datasets.

</details>

<details>

<summary>2014-09-26 19:18:43 - Order-invariant prior specification in Bayesian factor analysis</summary>

- *Dennis Leung, Mathias Drton*

- `1409.7672v1` - [abs](http://arxiv.org/abs/1409.7672v1) - [pdf](http://arxiv.org/pdf/1409.7672v1)

> In (exploratory) factor analysis, the loading matrix is identified only up to orthogonal rotation. For identifiability, one thus often takes the loading matrix to be lower triangular with positive diagonal entries. In Bayesian inference, a standard practice is then to specify a prior under which the loadings are independent, the off-diagonal loadings are normally distributed, and the diagonal loadings follow a truncated normal distribution. This prior specification, however, depends in an important way on how the variables and associated rows of the loading matrix are ordered. We show how a minor modification of the approach allows one to compute with the identifiable lower triangular loading matrix but maintain invariance properties under reordering of the variables.

</details>

<details>

<summary>2014-09-28 16:22:08 - parallelMCMCcombine: An R Package for Bayesian Methods for Big Data and Analytics</summary>

- *Alexey Miroshnikov, Erin Conlon*

- `1409.2546v2` - [abs](http://arxiv.org/abs/1409.2546v2) - [pdf](http://arxiv.org/pdf/1409.2546v2)

> Recent advances in big data and analytics research have provided a wealth of large data sets that are too big to be analyzed in their entirety, due to restrictions on computer memory or storage size. New Bayesian methods have been developed for large data sets that are only large due to large sample sizes; these methods partition big data sets into subsets, and perform independent Bayesian Markov chain Monte Carlo analyses on the subsets. The methods then combine the independent subset posterior samples to estimate a posterior density given the full data set. These approaches were shown to be effective for Bayesian models including logistic regression models, Gaussian mixture models and hierarchical models. Here, we introduce the R package parallelMCMCcombine which carries out four of these techniques for combining independent subset posterior samples. We illustrate each of the methods using a Bayesian logistic regression model for simulation data and a Bayesian Gamma model for real data; we also demonstrate features and capabilities of the R package. The package assumes the user has carried out the Bayesian analysis and has produced the independent subposterior samples outside of the package. The methods are primarily suited to models with unknown parameters of fixed dimension that exist in continuous parameter spaces. We envision this tool will allow researchers to explore the various methods for their specific applications, and will assist future progress in this rapidly developing field.

</details>

<details>

<summary>2014-09-29 11:45:36 - Variational Inference For Probabilistic Latent Tensor Factorization with KL Divergence</summary>

- *Beyza Ermis, Y. Kenan YÄ±lmaz, A. Taylan Cemgil, Evrim Acar*

- `1409.8083v1` - [abs](http://arxiv.org/abs/1409.8083v1) - [pdf](http://arxiv.org/pdf/1409.8083v1)

> Probabilistic Latent Tensor Factorization (PLTF) is a recently proposed probabilistic framework for modelling multi-way data. Not only the common tensor factorization models but also any arbitrary tensor factorization structure can be realized by the PLTF framework. This paper presents full Bayesian inference via variational Bayes that facilitates more powerful modelling and allows more sophisticated inference on the PLTF framework. We illustrate our approach on model order selection and link prediction.

</details>

<details>

<summary>2014-09-29 12:29:21 - A Bayesian Tensor Factorization Model via Variational Inference for Link Prediction</summary>

- *Beyza Ermis, A. Taylan Cemgil*

- `1409.8276v1` - [abs](http://arxiv.org/abs/1409.8276v1) - [pdf](http://arxiv.org/pdf/1409.8276v1)

> Probabilistic approaches for tensor factorization aim to extract meaningful structure from incomplete data by postulating low rank constraints. Recently, variational Bayesian (VB) inference techniques have successfully been applied to large scale models. This paper presents full Bayesian inference via VB on both single and coupled tensor factorization models. Our method can be run even for very large models and is easily implemented. It exhibits better prediction performance than existing approaches based on maximum likelihood on several real-world datasets for missing link prediction problem.

</details>

<details>

<summary>2014-09-29 14:48:57 - A statistical noise model for a class of Physically Unclonable Functions</summary>

- *Benjamin Hackl, Daniel Kurz, Clemens Heuberger, JÃ¼rgen Pilz, Martin Deutschmann*

- `1409.8137v1` - [abs](http://arxiv.org/abs/1409.8137v1) - [pdf](http://arxiv.org/pdf/1409.8137v1)

> The interest in "Physically Unclonable Function"-devices has increased rapidly over the last few years, as they have several interesting properties for system security related applications like, for example, the management of cryptographic keys. Unfortunately, the output provided by these devices is noisy and needs to be corrected for these applications.   Related error correcting mechanisms are typically constructed on the basis of an equal error probability for each output bit. This assumption does not hold for Physically Unclonable Functions, where varying error probabilities can be observed. This results in a generalized binomial distribution for the number of errors in the output.   The intention of this paper is to discuss a novel Bayesian statistical model for the noise of an especially wide-spread class of Physically Unclonable Functions, which properly handles the varying output stability and also reflects the different noise behaviors observed in a collection of such devices. Furthermore, we compare several different methods for estimating the model parameters and apply the proposed model to concrete measurements obtained within the CODES research project in order to evaluate typical correction and stabilization approaches.

</details>

<details>

<summary>2014-09-29 21:08:54 - Bayesian and regularization approaches to multivariable linear system identification: the role of rank penalties</summary>

- *Giulia Prando, Alessandro Chiuso, Gianluigi Pillonetto*

- `1409.8327v1` - [abs](http://arxiv.org/abs/1409.8327v1) - [pdf](http://arxiv.org/pdf/1409.8327v1)

> Recent developments in linear system identification have proposed the use of non-parameteric methods, relying on regularization strategies, to handle the so-called bias/variance trade-off. This paper introduces an impulse response estimator which relies on an $\ell_2$-type regularization including a rank-penalty derived using the log-det heuristic as a smooth approximation to the rank function. This allows to account for different properties of the estimated impulse response (e.g. smoothness and stability) while also penalizing high-complexity models. This also allows to account and enforce coupling between different input-output channels in MIMO systems. According to the Bayesian paradigm, the parameters defining the relative weight of the two regularization terms as well as the structure of the rank penalty are estimated optimizing the marginal likelihood. Once these hyperameters have been estimated, the impulse response estimate is available in closed form. Experiments show that the proposed method is superior to the estimator relying on the "classic" $\ell_2$-regularization alone as well as those based in atomic and nuclear norm.

</details>

<details>

<summary>2014-09-30 02:11:25 - Approximate Bayesian Computation in State Space Models</summary>

- *Gael M. Martin, Brendan P. M. McCabe, Worapree Maneesoonthorn, Christian P. Robert*

- `1409.8363v1` - [abs](http://arxiv.org/abs/1409.8363v1) - [pdf](http://arxiv.org/pdf/1409.8363v1)

> A new approach to inference in state space models is proposed, based on approximate Bayesian computation (ABC). ABC avoids evaluation of the likelihood function by matching observed summary statistics with statistics computed from data simulated from the true process; exact inference being feasible only if the statistics are sufficient. With finite sample sufficiency unattainable in the state space setting, we seek asymptotic sufficiency via the maximum likelihood estimator (MLE) of the parameters of an auxiliary model. We prove that this auxiliary model-based approach achieves Bayesian consistency, and that - in a precise limiting sense - the proximity to (asymptotic) sufficiency yielded by the MLE is replicated by the score. In multiple parameter settings a separate treatment of scalar parameters, based on integrated likelihood techniques, is advocated as a way of avoiding the curse of dimensionality. Some attention is given to a structure in which the state variable is driven by a continuous time process, with exact inference typically infeasible in this case as a result of intractable transitions. The ABC method is demonstrated using the unscented Kalman filter as a fast and simple way of producing an approximation in this setting, with a stochastic volatility model for financial returns used for illustration.

</details>

<details>

<summary>2014-09-30 22:44:36 - Markov Chain Monte Carlo Estimation of Quantiles</summary>

- *Charles Doss, James M. Flegal, Galin L. Jones, Ronald C. Neath*

- `1207.6432v3` - [abs](http://arxiv.org/abs/1207.6432v3) - [pdf](http://arxiv.org/pdf/1207.6432v3)

> We consider quantile estimation using Markov chain Monte Carlo and establish conditions under which the sampling distribution of the Monte Carlo error is approximately Normal. Further, we investigate techniques to estimate the associated asymptotic variance, which enables construction of an asymptotically valid interval estimator. Finally, we explore the finite sample properties of these methods through examples and provide some recommendations to practitioners.

</details>


## 2014-10

<details>

<summary>2014-10-01 08:33:37 - Efficient Coordinated Recovery of Sparse Channels in Massive MIMO</summary>

- *Mudassir Masood, Laila H. Afify, Tareq Y. Al-Naffouri*

- `1409.4671v2` - [abs](http://arxiv.org/abs/1409.4671v2) - [pdf](http://arxiv.org/pdf/1409.4671v2)

> This paper addresses the problem of estimating sparse channels in massive MIMO-OFDM systems. Most wireless channels are sparse in nature with large delay spread. In addition, these channels as observed by multiple antennas in a neighborhood have approximately common support. The sparsity and common support properties are attractive when it comes to the efficient estimation of large number of channels in massive MIMO systems. Moreover, to avoid pilot contamination and to achieve better spectral efficiency, it is important to use a small number of pilots. We present a novel channel estimation approach which utilizes the sparsity and common support properties to estimate sparse channels and require a small number of pilots. Two algorithms based on this approach have been developed which perform Bayesian estimates of sparse channels even when the prior is non-Gaussian or unknown. Neighboring antennas share among each other their beliefs about the locations of active channel taps to perform estimation. The coordinated approach improves channel estimates and also reduces the required number of pilots. Further improvement is achieved by the data-aided version of the algorithm. Extensive simulation results are provided to demonstrate the performance of the proposed algorithms.

</details>

<details>

<summary>2014-10-01 11:34:46 - The Bernstein-von Mises theorem and nonregular models</summary>

- *Natalia A. Bochkina, Peter J. Green*

- `1211.3434v5` - [abs](http://arxiv.org/abs/1211.3434v5) - [pdf](http://arxiv.org/pdf/1211.3434v5)

> We study the asymptotic behaviour of the posterior distribution in a broad class of statistical models where the "true" solution occurs on the boundary of the parameter space. We show that in this case Bayesian inference is consistent, and that the posterior distribution has not only Gaussian components as in the case of regular models (the Bernstein-von Mises theorem) but also has Gamma distribution components whose form depends on the behaviour of the prior distribution near the boundary and have a faster rate of convergence. We also demonstrate a remarkable property of Bayesian inference, that for some models, there appears to be no bound on efficiency of estimating the unknown parameter if it is on the boundary of the parameter space. We illustrate the results on a problem from emission tomography.

</details>

<details>

<summary>2014-10-01 13:28:05 - Adaptive Metropolis Algorithm Using Variational Bayesian Adaptive Kalman Filter</summary>

- *Isambi S. Mbalawata, Simo SÃ¤rkkÃ¤, Matti Vihola, Heikki Haario*

- `1308.5875v3` - [abs](http://arxiv.org/abs/1308.5875v3) - [pdf](http://arxiv.org/pdf/1308.5875v3)

> Markov chain Monte Carlo (MCMC) methods are powerful computational tools for analysis of complex statistical problems. However, their computational efficiency is highly dependent on the chosen proposal distribution, which is generally difficult to find. One way to solve this problem is to use adaptive MCMC algorithms which automatically tune the statistics of a proposal distribution during the MCMC run. A new adaptive MCMC algorithm, called the variational Bayesian adaptive Metropolis (VBAM) algorithm, is developed. The VBAM algorithm updates the proposal covariance matrix using the variational Bayesian adaptive Kalman filter (VB-AKF). A strong law of large numbers for the VBAM algorithm is proven. The empirical convergence results for three simulated examples and for two real data examples are also provided.

</details>

<details>

<summary>2014-10-02 12:04:42 - Likelihood free inference for Markov processes: a comparison</summary>

- *Jamie Owen, Darren J. Wilkinson, Colin S. Gillespie*

- `1410.0524v1` - [abs](http://arxiv.org/abs/1410.0524v1) - [pdf](http://arxiv.org/pdf/1410.0524v1)

> Approaches to Bayesian inference for problems with intractable likelihoods have become increasingly important in recent years. Approximate Bayesian computation (ABC) and "likelihood free" Markov chain Monte Carlo techniques are popular methods for tackling inference in these scenarios but such techniques are computationally expensive. In this paper we compare the two approaches to inference, with a particular focus on parameter inference for stochastic kinetic models, widely used in systems biology. Discrete time transition kernels for models of this type are intractable for all but the most trivial systems yet forward simulation is usually straightforward. We discuss the relative merits and drawbacks of each approach whilst considering the computational cost implications and efficiency of these techniques. In order to explore the properties of each approach we examine a range of observation regimes using two example models. We use a Lotka--Volterra predator prey model to explore the impact of full or partial species observations using various time course observations under the assumption of known and unknown measurement error. Further investigation into the impact of observation error is then made using a Schl\"ogl system, a test case which exhibits bi-modal state stability in some regions of parameter space.

</details>

<details>

<summary>2014-10-03 07:25:22 - Linear State-Space Model with Time-Varying Dynamics</summary>

- *Jaakko Luttinen, Tapani Raiko, Alexander Ilin*

- `1410.0555v2` - [abs](http://arxiv.org/abs/1410.0555v2) - [pdf](http://arxiv.org/pdf/1410.0555v2)

> This paper introduces a linear state-space model with time-varying dynamics. The time dependency is obtained by forming the state dynamics matrix as a time-varying linear combination of a set of matrices. The time dependency of the weights in the linear combination is modelled by another linear Gaussian dynamical model allowing the model to learn how the dynamics of the process changes. Previous approaches have used switching models which have a small set of possible state dynamics matrices and the model selects one of those matrices at each time, thus jumping between them. Our model forms the dynamics as a linear combination and the changes can be smooth and more continuous. The model is motivated by physical processes which are described by linear partial differential equations whose parameters vary in time. An example of such a process could be a temperature field whose evolution is driven by a varying wind direction. The posterior inference is performed using variational Bayesian approximation. The experiments on stochastic advection-diffusion processes and real-world weather processes show that the model with time-varying dynamics can outperform previously introduced approaches.

</details>

<details>

<summary>2014-10-03 12:05:04 - Multiscale Bernstein polynomials for densities</summary>

- *Antonio Canale, David B. Dunson*

- `1410.0827v1` - [abs](http://arxiv.org/abs/1410.0827v1) - [pdf](http://arxiv.org/pdf/1410.0827v1)

> Our focus is on constructing a multiscale nonparametric prior for densities. The Bayes density estimation literature is dominated by single scale methods, with the exception of Polya trees, which favor overly-spiky densities even when the truth is smooth. We propose a multiscale Bernstein polynomial family of priors, which produce smooth realizations that do not rely on hard partitioning of the support. At each level in an infinitely-deep binary tree, we place a beta dictionary density; within a scale the densities are equivalent to Bernstein polynomials. Using a stick-breaking characterization, stochastically decreasing weights are allocated to the finer scale dictionary elements. A slice sampler is used for posterior computation, and properties are described. The method characterizes densities with locally-varying smoothness, and can produce a sequence of coarse to fine density estimates. An extension for Bayesian testing of group differences is introduced and applied to DNA methylation array data.

</details>

<details>

<summary>2014-10-03 21:14:51 - A note on the minimax solution for the two-stage group testing problem</summary>

- *Yaakov Malinovsky, Paul S. Albert*

- `1410.0979v1` - [abs](http://arxiv.org/abs/1410.0979v1) - [pdf](http://arxiv.org/pdf/1410.0979v1)

> Group testing is an active area of current research and has important applications in medicine, biotechnology, genetics, and product testing. There have been recent advances in design and estimation, but the simple Dorfman procedure introduced by R. Dorfman in 1943 is widely used in practice. In many practical situations the exact value of the probability p of being affected is unknown. We present both minimax and Bayesian solutions for the group size problem when p is unknown. For unbounded p we show that the minimax solution for group size is 8, while using a Bayesian strategy with Jeffreys prior results in a group size of 13. We also present solutions when p is bounded from above. For the practitioner we propose strong justification for using a group size of between eight to thirteen when a constraint on p is not incorporated and provide useable code for computing the minimax group size under a constrained p.

</details>

<details>

<summary>2014-10-04 17:36:58 - Gamma Processes, Stick-Breaking, and Variational Inference</summary>

- *Anirban Roychowdhury, Brian Kulis*

- `1410.1068v1` - [abs](http://arxiv.org/abs/1410.1068v1) - [pdf](http://arxiv.org/pdf/1410.1068v1)

> While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithms on nonnegative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors.

</details>

<details>

<summary>2014-10-05 16:46:04 - Learning Topology and Dynamics of Large Recurrent Neural Networks</summary>

- *Yiyuan She, Yuejia He, Dapeng Wu*

- `1410.1174v1` - [abs](http://arxiv.org/abs/1410.1174v1) - [pdf](http://arxiv.org/pdf/1410.1174v1)

> Large-scale recurrent networks have drawn increasing attention recently because of their capabilities in modeling a large variety of real-world phenomena and physical mechanisms. This paper studies how to identify all authentic connections and estimate system parameters of a recurrent network, given a sequence of node observations. This task becomes extremely challenging in modern network applications, because the available observations are usually very noisy and limited, and the associated dynamical system is strongly nonlinear. By formulating the problem as multivariate sparse sigmoidal regression, we develop simple-to-implement network learning algorithms, with rigorous convergence guarantee in theory, for a variety of sparsity-promoting penalty forms. A quantile variant of progressive recurrent network screening is proposed for efficient computation and allows for direct cardinality control of network topology in estimation. Moreover, we investigate recurrent network stability conditions in Lyapunov's sense, and integrate such stability constraints into sparse network learning. Experiments show excellent performance of the proposed algorithms in network topology identification and forecasting.

</details>

<details>

<summary>2014-10-06 00:38:39 - Bayesian regression and Bitcoin</summary>

- *Devavrat Shah, Kang Zhang*

- `1410.1231v1` - [abs](http://arxiv.org/abs/1410.1231v1) - [pdf](http://arxiv.org/pdf/1410.1231v1)

> In this paper, we discuss the method of Bayesian regression and its efficacy for predicting price variation of Bitcoin, a recently popularized virtual, cryptographic currency. Bayesian regression refers to utilizing empirical data as proxy to perform Bayesian inference. We utilize Bayesian regression for the so-called "latent source model". The Bayesian regression for "latent source model" was introduced and discussed by Chen, Nikolov and Shah (2013) and Bresler, Chen and Shah (2014) for the purpose of binary classification. They established theoretical as well as empirical efficacy of the method for the setting of binary classification.   In this paper, instead we utilize it for predicting real-valued quantity, the price of Bitcoin. Based on this price prediction method, we devise a simple strategy for trading Bitcoin. The strategy is able to nearly double the investment in less than 60 day period when run against real data trace.

</details>

<details>

<summary>2014-10-08 10:02:06 - Bayesian tracking and parameter learning for non-linear multiple target tracking models</summary>

- *Lan Jiang, Sumeetpal S. Singh, Sinan YÄ±ldÄ±rÄ±m*

- `1410.2046v1` - [abs](http://arxiv.org/abs/1410.2046v1) - [pdf](http://arxiv.org/pdf/1410.2046v1)

> We propose a new Bayesian tracking and parameter learning algorithm for non-linear non-Gaussian multiple target tracking (MTT) models. We design a Markov chain Monte Carlo (MCMC) algorithm to sample from the posterior distribution of the target states, birth and death times, and association of observations to targets, which constitutes the solution to the tracking problem, as well as the model parameters. In the numerical section, we present performance comparisons with several competing techniques and demonstrate significant performance improvements in all cases.

</details>

<details>

<summary>2014-10-08 13:17:10 - Narrowband Interference Mitigation in SC-FDMA Using Bayesian Sparse Recovery</summary>

- *Anum Ali, Mudassir Masood, Muhammad S. Sohail, Samir Al-Ghadhban, Tareq Y. Al-Naffouri*

- `1412.6137v1` - [abs](http://arxiv.org/abs/1412.6137v1) - [pdf](http://arxiv.org/pdf/1412.6137v1)

> This paper presents a novel narrowband interference (NBI) mitigation scheme for SC-FDMA systems. The proposed NBI cancellation scheme exploits the frequency domain sparsity of the unknown signal and adopts a low complexity Bayesian sparse recovery procedure. At the transmitter, a few randomly chosen sub-carriers are kept data free to sense the NBI signal at the receiver. Further, it is noted that in practice, the sparsity of the NBI signal is destroyed by a grid mismatch between NBI sources and the system under consideration. Towards this end, first an accurate grid mismatch model is presented that is capable of assuming independent offsets for multiple NBI sources. Secondly, prior to NBI reconstruction, the sparsity of the unknown signal is restored by employing a sparsifying transform. To improve the spectral efficiency of the proposed scheme, a data-aided NBI recovery procedure is outlined that relies on adaptively selecting a subset of data carriers and uses them as additional measurements to enhance the NBI estimation. Finally, the proposed scheme is extended to single-input multi-output systems by performing a collaborative NBI support search over all antennas. Numerical results are presented that depict the suitability of the proposed scheme for NBI mitigation.

</details>

<details>

<summary>2014-10-08 14:55:40 - Robust Estimation of High-Dimensional Mean Regression</summary>

- *Jianqing Fan, Quefeng Li, Yuyan Wang*

- `1410.2150v1` - [abs](http://arxiv.org/abs/1410.2150v1) - [pdf](http://arxiv.org/pdf/1410.2150v1)

> Data subject to heavy-tailed errors are commonly encountered in various scientific fields, especially in the modern era with explosion of massive data. To address this problem, procedures based on quantile regression and Least Absolute Deviation (LAD) regression have been devel- oped in recent years. These methods essentially estimate the conditional median (or quantile) function. They can be very different from the conditional mean functions when distributions are asymmetric and heteroscedastic. How can we efficiently estimate the mean regression functions in ultra-high dimensional setting with existence of only the second moment? To solve this problem, we propose a penalized Huber loss with diverging parameter to reduce biases created by the traditional Huber loss. Such a penalized robust approximate quadratic (RA-quadratic) loss will be called RA-Lasso. In the ultra-high dimensional setting, where the dimensionality can grow exponentially with the sample size, our results reveal that the RA-lasso estimator produces a consistent estimator at the same rate as the optimal rate under the light-tail situation. We further study the computational convergence of RA-Lasso and show that the composite gradient descent algorithm indeed produces a solution that admits the same optimal rate after sufficient iterations. As a byproduct, we also establish the concentration inequality for estimat- ing population mean when there exists only the second moment. We compare RA-Lasso with other regularized robust estimators based on quantile regression and LAD regression. Extensive simulation studies demonstrate the satisfactory finite-sample performance of RA-Lasso.

</details>

<details>

<summary>2014-10-09 05:27:39 - Full Bayesian inference with hazard mixture models</summary>

- *Julyan Arbel, Antonio Lijoi, Bernardo Nipoti*

- `1405.6628v4` - [abs](http://arxiv.org/abs/1405.6628v4) - [pdf](http://arxiv.org/pdf/1405.6628v4)

> Bayesian nonparametric inferential procedures based on Markov chain Monte Carlo marginal methods typically yield point estimates in the form of posterior expectations. Though very useful and easy to implement in a variety of statistical problems, these methods may suffer from some limitations if used to estimate non-linear functionals of the posterior distribution. The main goal of the present paper is to develop a novel methodology that extends a well-established marginal procedure designed for hazard mixture models, in order to draw approximate inference on survival functions that is not limited to the posterior mean but includes, as remarkable examples, credible intervals and median survival time. Our approach relies on a characterization of the posterior moments that, in turn, is used to approximate the posterior distribution by means of a technique based on Jacobi polynomials. The inferential performance of our methodology is analyzed by means of an extensive study of simulated data and real data consisting of leukemia remission times. Although tailored to the survival analysis context, the procedure we introduce can be adapted to a range of other models for which moments of the posterior distribution can be estimated.

</details>

<details>

<summary>2014-10-09 09:48:37 - Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination</summary>

- *Qibin Zhao, Liqing Zhang, Andrzej Cichocki*

- `1401.6497v2` - [abs](http://arxiv.org/abs/1401.6497v2) - [pdf](http://arxiv.org/pdf/1401.6497v2)

> CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank. In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance.

</details>

<details>

<summary>2014-10-09 17:13:29 - A unified approach for multi-object triangulation, tracking and camera calibration</summary>

- *Jeremie Houssineau, Daniel Clark, Spela Ivekovic, Chee Sing Lee, Jose Franco*

- `1410.2535v1` - [abs](http://arxiv.org/abs/1410.2535v1) - [pdf](http://arxiv.org/pdf/1410.2535v1)

> Object triangulation, 3-D object tracking, feature correspondence, and camera calibration are key problems for estimation from camera networks. This paper addresses these problems within a unified Bayesian framework for joint multi-object tracking and sensor registration. Given that using standard filtering approaches for state estimation from cameras is problematic, an alternative parametrisation is exploited, called disparity space. The disparity space-based approach for triangulation and object tracking is shown to be more effective than non-linear versions of the Kalman filter and particle filtering for non-rectified cameras. The approach for feature correspondence is based on the Probability Hypothesis Density (PHD) filter, and hence inherits the ability to update without explicit measurement association, to initiate new targets, and to discriminate between target and clutter. The PHD filtering approach then forms the basis of a camera calibration method from static or moving objects. Results are shown on simulated data.

</details>

<details>

<summary>2014-10-09 21:58:02 - A Noninformative Bayes-like Approach to Probability-Preserving Prediction of Extremes</summary>

- *Allan McRobie*

- `1410.2639v1` - [abs](http://arxiv.org/abs/1410.2639v1) - [pdf](http://arxiv.org/pdf/1410.2639v1)

> The extrapolation of extremes to values beyond the span of stationary univariate historical data is considered from Bayesian and Frequentist perspectives. The intention is to make predictions which in some sense "preserve probability". A Frequentist approach based on a simple curve-fit estimate of the tail parameter $\xi$ of a Generalised Pareto Distribution was described in McRobie (2014) (arXiv:1408.1532). In this paper, the corresponding Bayes-like approach is described, using a plausible noninformative prior for the tail parameter. The two approaches, though philosophically different, show a reasonable degree of correspondence.

</details>

<details>

<summary>2014-10-10 03:01:06 - Convergence of the Square Root Ensemble Kalman Filter in the Large Ensemble Limit</summary>

- *Evan Kwiatkowski, Jan Mandel*

- `1404.4093v2` - [abs](http://arxiv.org/abs/1404.4093v2) - [pdf](http://arxiv.org/pdf/1404.4093v2)

> Ensemble filters implement sequential Bayesian estimation by representing the probability distribution by an ensemble mean and covariance. Unbiased square root ensemble filters use deterministic algorithms to produce an analysis (posterior) ensemble with prescribed mean and covariance, consistent with the Kalman update. This includes several filters used in practice, such as the Ensemble Transform Kalman Filter (ETKF), the Ensemble Adjustment Kalman Filter (EAKF), and a filter by Whitaker and Hamill. We show that at every time index, as the number of ensemble members increases to infinity, the mean and covariance of an unbiased ensemble square root filter converge to those of the Kalman filter, in the case a linear model and an initial distribution of which all moments exist. The convergence is in $L^{p}$ and the convergence rate does not depend on the model dimension. The result holds in the infinitely dimensional Hilbert space as well.

</details>

<details>

<summary>2014-10-10 11:34:18 - Point estimates in phylogenetic reconstructions</summary>

- *Philipp Benner, Miroslav Bacak, Pierre-Yves Bourguignon*

- `1305.3692v5` - [abs](http://arxiv.org/abs/1305.3692v5) - [pdf](http://arxiv.org/pdf/1305.3692v5)

> Motivation: The construction of statistics for summarizing posterior samples returned by a Bayesian phylogenetic study has so far been hindered by the poor geometric insights available into the space of phylogenetic trees, and ad hoc methods such as the derivation of a consensus tree makeup for the ill-definition of the usual concepts of posterior mean, while bootstrap methods mitigate the absence of a sound concept of variance. Yielding satisfactory results with sufficiently concentrated posterior distributions, such methods fall short of providing a faithful summary of posterior distributions if the data do not offer compelling evidence for a single topology.   Results: Building upon previous work of Billera et al., summary statistics such as sample mean, median and variance are defined as the geometric median, Fr\'echet mean and variance, respectively. Their computation is enabled by recently published works, and embeds an algorithm for computing shortest paths in the space of trees. Studying the phylogeny of a set of plants, where several tree topologies occur in the posterior sample, the posterior mean balances correctly the contributions from the different topologies, where a consensus tree would be biased. Comparisons of the posterior mean, median and consensus trees with the ground truth using simulated data also reveals the benefits of a sound averaging method when reconstructing phylogenetic trees.

</details>

<details>

<summary>2014-10-11 13:44:06 - Modeling Infection with Multi-agent Dynamics</summary>

- *Wen Dong, Katherine A. Heller, Alex Sandy Pentland*

- `1204.0168v2` - [abs](http://arxiv.org/abs/1204.0168v2) - [pdf](http://arxiv.org/pdf/1204.0168v2)

> Developing the ability to comprehensively study infections in small populations enables us to improve epidemic models and better advise individuals about potential risks to their health. We currently have a limited understanding of how infections spread within a small population because it has been difficult to closely track an infection within a complete community. The paper presents data closely tracking the spread of an infection centered on a student dormitory, collected by leveraging the residents' use of cellular phones. The data are based on daily symptom surveys taken over a period of four months and proximity tracking through cellular phones. We demonstrate that using a Bayesian, discrete-time multi-agent model of infection to model real-world symptom reports and proximity tracking records gives us important insights about infec-tions in small populations.

</details>

<details>

<summary>2014-10-12 21:14:27 - Sample Size Dependent Species Models</summary>

- *Mingyuan Zhou, Stephen G Walker*

- `1410.3155v1` - [abs](http://arxiv.org/abs/1410.3155v1) - [pdf](http://arxiv.org/pdf/1410.3155v1)

> Motivated by the fundamental problem of measuring species diversity, this paper introduces the concept of a cluster structure to define an exchangeable cluster probability function that governs the joint distribution of a random count and its exchangeable random partitions. A cluster structure, naturally arising from a completely random measure mixed Poisson process, allows the probability distribution of the random partitions of a subset of a sample to be dependent on the sample size, a distinct and motivated feature that differs it from a partition structure. A generalized negative binomial process model is proposed to generate a cluster structure, where in the prior the number of clusters is finite and Poisson distributed, and the cluster sizes follow a truncated negative binomial distribution. We construct a nonparametric Bayesian estimator of Simpson's index of diversity under the generalized negative binomial process. We illustrate our results through the analysis of two real sequencing count datasets.

</details>

<details>

<summary>2014-10-13 09:13:13 - PAC-Bayesian AUC classification and scoring</summary>

- *James Ridgway, Pierre Alquier, Nicolas Chopin, Feng Liang*

- `1410.1771v2` - [abs](http://arxiv.org/abs/1410.1771v2) - [pdf](http://arxiv.org/pdf/1410.1771v2)

> We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.

</details>

<details>

<summary>2014-10-13 22:54:06 - Specification tests for nonlinear dynamic models</summary>

- *Igor L. Kheifets*

- `1410.3533v1` - [abs](http://arxiv.org/abs/1410.3533v1) - [pdf](http://arxiv.org/pdf/1410.3533v1)

> We propose a new adequacy test and a graphical evaluation tool for nonlinear dynamic models. The proposed techniques can be applied in any setup where parametric conditional distribution of the data is specified, in particular to models involving conditional volatility, conditional higher moments, conditional quantiles, asymmetry, Value at Risk models, duration models, diffusion models, etc. Compared to other tests, the new test properly controls the nonlinear dynamic behavior in conditional distribution and does not rely on smoothing techniques which require a choice of several tuning parameters. The test is based on a new kind of multivariate empirical process of contemporaneous and lagged probability integral transforms. We establish weak convergence of the process under parameter uncertainty and local alternatives. We justify a parametric bootstrap approximation that accounts for parameter estimation effects often ignored in practice. Monte Carlo experiments show that the test has good finite-sample size and power properties. Using the new test and graphical tools we check the adequacy of various popular heteroscedastic models for stock exchange index data.

</details>

<details>

<summary>2014-10-14 10:17:19 - Quantum Minimax Theorem</summary>

- *Fuyuhiko Tanaka*

- `1410.3639v1` - [abs](http://arxiv.org/abs/1410.3639v1) - [pdf](http://arxiv.org/pdf/1410.3639v1)

> Recently, many fundamental and important results in statistical decision theory have been extended to the quantum system. Quantum Hunt-Stein theorem and quantum locally asymptotic normality are typical successful examples. In the present paper, we show quantum minimax theorem, which is also an extension of a well-known result, minimax theorem in statistical decision theory, first shown by Wald and generalized by LeCam. Our assertions hold for every closed convex set of measurements and for general parametric models of density operator. On the other hand, Bayesian analysis based on least favorable priors has been widely used in classical statistics and is expected to play a crucial role in quantum statistics. According to this trend, we also show the existence of least favorable priors, which seems to be new even in classical statistics.

</details>

<details>

<summary>2014-10-14 13:36:19 - On Bayesian supremum norm contraction rates</summary>

- *IsmaÃ«l Castillo*

- `1304.1761v3` - [abs](http://arxiv.org/abs/1304.1761v3) - [pdf](http://arxiv.org/pdf/1304.1761v3)

> Building on ideas from Castillo and Nickl [Ann. Statist. 41 (2013) 1999-2028], a method is provided to study nonparametric Bayesian posterior convergence rates when "strong" measures of distances, such as the sup-norm, are considered. In particular, we show that likelihood methods can achieve optimal minimax sup-norm rates in density estimation on the unit interval. The introduced methodology is used to prove that commonly used families of prior distributions on densities, namely log-density priors and dyadic random density histograms, can indeed achieve optimal sup-norm rates of convergence. New results are also derived in the Gaussian white noise model as a further illustration of the presented techniques.

</details>

<details>

<summary>2014-10-15 09:24:08 - Recursive Pathways to Marginal Likelihood Estimation with Prior-Sensitivity Analysis</summary>

- *Ewan Cameron, Anthony Pettitt*

- `1301.6450v3` - [abs](http://arxiv.org/abs/1301.6450v3) - [pdf](http://arxiv.org/pdf/1301.6450v3)

> We investigate the utility to computational Bayesian analyses of a particular family of recursive marginal likelihood estimators characterized by the (equivalent) algorithms known as "biased sampling" or "reverse logistic regression" in the statistics literature and "the density of states" in physics. Through a pair of numerical examples (including mixture modeling of the well-known galaxy data set) we highlight the remarkable diversity of sampling schemes amenable to such recursive normalization, as well as the notable efficiency of the resulting pseudo-mixture distributions for gauging prior sensitivity in the Bayesian model selection context. Our key theoretical contributions are to introduce a novel heuristic ("thermodynamic integration via importance sampling") for qualifying the role of the bridging sequence in this procedure and to reveal various connections between these recursive estimators and the nested sampling technique.

</details>

<details>

<summary>2014-10-15 21:19:19 - Bayesian nonparametric models for spatially indexed data of mixed type</summary>

- *Georgios Papageorgiou, Sylvia Richardson, Nicky Best*

- `1408.1368v3` - [abs](http://arxiv.org/abs/1408.1368v3) - [pdf](http://arxiv.org/pdf/1408.1368v3)

> We develop Bayesian nonparametric models for spatially indexed data of mixed type. Our work is motivated by challenges that occur in environmental epidemiology, where the usual presence of several confounding variables that exhibit complex interactions and high correlations makes it difficult to estimate and understand the effects of risk factors on health outcomes of interest. The modeling approach we adopt assumes that responses and confounding variables are manifestations of continuous latent variables, and uses multivariate Gaussians to jointly model these. Responses and confounding variables are not treated equally as relevant parameters of the distributions of the responses only are modeled in terms of explanatory variables or risk factors. Spatial dependence is introduced by allowing the weights of the nonparametric process priors to be location specific, obtained as probit transformations of Gaussian Markov random fields. Confounding variables and spatial configuration have a similar role in the model, in that they only influence, along with the responses, the allocation probabilities of the areas into the mixture components, thereby allowing for flexible adjustment of the effects of observed confounders, while allowing for the possibility of residual spatial structure, possibly occurring due to unmeasured or undiscovered spatially varying factors. Aspects of the model are illustrated in simulation studies and an application to a real data set.

</details>

<details>

<summary>2014-10-16 19:04:25 - Estimation of CO$_2$ flux from targeted satellite observations: a Bayesian approach</summary>

- *Graham Cox*

- `1403.3306v2` - [abs](http://arxiv.org/abs/1403.3306v2) - [pdf](http://arxiv.org/pdf/1403.3306v2)

> We consider the estimation of carbon dioxide flux at the ocean-atmosphere interface, given weighted averages of the mixing ratio in a vertical atmospheric column. In particular we examine the dependence of the posterior covariance on the weighting function used in taking observations, motivated by the fact that this function is instrument-dependent, hence one needs the ability to compare different weights. The estimation problem is considered using a variational data assimilation method, which is shown to admit an equivalent infinite-dimensional Bayesian formulation.   The main tool in our investigation is an explicit formula for the posterior covariance in terms of the prior covariance and observation operator. Using this formula, we compare weighting functions concentrated near the surface of the earth with those concentrated near the top of the atmosphere, in terms of the resulting covariance operators.   We also consider the problem of observational targeting, and ask if it is possible to reduce the covariance in a prescribed direction through an appropriate choice of weighting function. We find that this is not the case---there exist directions in which one can never gain information, regardless of the choice of weight.

</details>

<details>

<summary>2014-10-17 16:46:45 - Variational Bayes for Merging Noisy Databases</summary>

- *Tamara Broderick, Rebecca C. Steorts*

- `1410.4792v1` - [abs](http://arxiv.org/abs/1410.4792v1) - [pdf](http://arxiv.org/pdf/1410.4792v1)

> Bayesian entity resolution merges together multiple, noisy databases and returns the minimal collection of unique individuals represented, together with their true, latent record values. Bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final, resolved database. However, existing Bayesian methods for entity resolution use Markov monte Carlo method (MCMC) approximations and are too slow to run on modern databases containing millions or billions of records. Instead, we propose applying variational approximations to allow scalable Bayesian inference in these models. We derive a coordinate-ascent approximation for mean-field variational Bayes, qualitatively compare our algorithm to existing methods, note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution, and discuss directions for future work in this domain.

</details>

<details>

<summary>2014-10-21 02:34:07 - Variational Reformulation of Bayesian Inverse Problems</summary>

- *Panagiotis Tsilifis, Ilias Bilionis, Ioannis Katsounaros, Nicholas Zabaras*

- `1410.5522v1` - [abs](http://arxiv.org/abs/1410.5522v1) - [pdf](http://arxiv.org/pdf/1410.5522v1)

> The classical approach to inverse problems is based on the optimization of a misfit function. Despite its computational appeal, such an approach suffers from many shortcomings, e.g., non-uniqueness of solutions, modeling prior knowledge, etc. The Bayesian formalism to inverse problems avoids most of the difficulties encountered by the optimization approach, albeit at an increased computational cost. In this work, we use information theoretic arguments to cast the Bayesian inference problem in terms of an optimization problem. The resulting scheme combines the theoretical soundness of fully Bayesian inference with the computational efficiency of a simple optimization.

</details>

<details>

<summary>2014-10-21 05:35:33 - A Conversation with Donald B. Rubin</summary>

- *Fan Li, Fabrizia Mealli*

- `1404.1789v3` - [abs](http://arxiv.org/abs/1404.1789v3) - [pdf](http://arxiv.org/pdf/1404.1789v3)

> Donald Bruce Rubin is John L. Loeb Professor of Statistics at Harvard University. He has made fundamental contributions to statistical methods for missing data, causal inference, survey sampling, Bayesian inference, computing and applications to a wide range of disciplines, including psychology, education, policy, law, economics, epidemiology, public health and other social and biomedical sciences.

</details>

<details>

<summary>2014-10-21 15:25:56 - Adaptive LASSO model selection in a multiphase quantile regression</summary>

- *Gabriela Ciuperca*

- `1309.1262v3` - [abs](http://arxiv.org/abs/1309.1262v3) - [pdf](http://arxiv.org/pdf/1309.1262v3)

> We propose a general adaptive LASSO method for a quantile regression model. Our method is very interesting when we know nothing about the first two moments of the model error. We first prove that the obtained estimators satisfy the oracle properties, which involves the relevant variable selection without using hypothesis test. Next, we study the proposed method when the (multiphase) model changes to unknown observations called change-points. Convergence rates of the change-points and of the regression parameters estimators in each phase are found. The sparsity of the adaptive LASSO quantile estimators of the regression parameters is not affected by the change-points estimation. If the phases number is unknown, a consistent criterion is proposed. Numerical studies by Monte Carlo simulations show the performance of the proposed method, compared to other existing methods in the literature, for models with a single phase or for multiphase models. The adaptive LASSO quantile method performs better than known variable selection methods, as the least squared method with adaptive LASSO penalty, $L_1$-method with LASSO-type penalty and quantile method with SCAD penalty.

</details>

<details>

<summary>2014-10-21 19:07:49 - Receiver-based Recovery of Clipped OFDM Signals for PAPR Reduction: A Bayesian Approach</summary>

- *Anum Ali, Abdullatif Al-Rabah, Mudassir Masood, Tareq Y. Al-Naffouri*

- `1410.2457v3` - [abs](http://arxiv.org/abs/1410.2457v3) - [pdf](http://arxiv.org/pdf/1410.2457v3)

> Clipping is one of the simplest peak-to-average power ratio (PAPR) reduction schemes for orthogonal frequency division multiplexing (OFDM). Deliberately clipping the transmission signal degrades system performance, and clipping mitigation is required at the receiver for information restoration. In this work, we acknowledge the sparse nature of the clipping signal and propose a low-complexity Bayesian clipping estimation scheme. The proposed scheme utilizes a priori information about the sparsity rate and noise variance for enhanced recovery. At the same time, the proposed scheme is robust against inaccurate estimates of the clipping signal statistics. The undistorted phase property of the clipped signal, as well as the clipping likelihood, is utilized for enhanced reconstruction. Further, motivated by the nature of modern OFDM-based communication systems, we extend our clipping reconstruction approach to multiple antenna receivers, and multi-user OFDM. We also address the problem of channel estimation from pilots contaminated by the clipping distortion. Numerical findings are presented, that depict favourable results for the proposed scheme compared to the established sparse reconstruction schemes.

</details>

<details>

<summary>2014-10-22 00:14:46 - Scalable Bayesian model averaging through local information propagation</summary>

- *Li Ma*

- `1403.2397v2` - [abs](http://arxiv.org/abs/1403.2397v2) - [pdf](http://arxiv.org/pdf/1403.2397v2)

> We show that a probabilistic version of the classical forward-stepwise variable inclusion procedure can serve as a general data-augmentation scheme for model space distributions in (generalized) linear models. This latent variable representation takes the form of a Markov process, thereby allowing information propagation algorithms to be applied for sampling from model space posteriors. In particular, we propose a sequential Monte Carlo method for achieving effective unbiased Bayesian model averaging in high-dimensional problems, utilizing proposal distributions constructed using local information propagation. We illustrate our method---called LIPS for local information propagation based sampling---through real and simulated examples with dimensionality ranging from 15 to 1,000, and compare its performance in estimating posterior inclusion probabilities and in out-of-sample prediction to those of several other methods---namely, MCMC, BAS, iBMA, and LASSO. In addition, we show that the latent variable representation can also serve as a modeling tool for specifying model space priors that account for knowledge regarding model complexity and conditional inclusion relationships.

</details>

<details>

<summary>2014-10-22 09:54:36 - Dimension-Independent MCMC Sampling for Inverse Problems with Non-Gaussian Priors</summary>

- *Sebastian J. Vollmer*

- `1302.2213v3` - [abs](http://arxiv.org/abs/1302.2213v3) - [pdf](http://arxiv.org/pdf/1302.2213v3)

> The computational complexity of MCMC methods for the exploration of complex probability measures is a challenging and important problem. A challenge of particular importance arises in Bayesian inverse problems where the target distribution may be supported on an infinite dimensional space. In practice this involves the approximation of measures defined on sequences of spaces of increasing dimension. Motivated by an elliptic inverse problem with non-Gaussian prior, we study the design of proposal chains for the Metropolis-Hastings algorithm with dimension independent performance. Dimension-independent bounds on the Monte-Carlo error of MCMC sampling for Gaussian prior measures have already been established. In this paper we provide a simple recipe to obtain these bounds for non-Gaussian prior measures. To illustrate the theory we consider an elliptic inverse problem arising in groundwater flow. We explicitly construct an efficient Metropolis-Hastings proposal based on local proposals, and we provide numerical evidence which supports the theory.

</details>

<details>

<summary>2014-10-22 14:50:05 - Scalable Inference for Markov Processes with Intractable Likelihoods</summary>

- *Jamie Owen, Darren J. Wilkinson, Colin S. Gillespie*

- `1403.6886v2` - [abs](http://arxiv.org/abs/1403.6886v2) - [pdf](http://arxiv.org/pdf/1403.6886v2)

> Bayesian inference for Markov processes has become increasingly relevant in recent years. Problems of this type often have intractable likelihoods and prior knowledge about model rate parameters is often poor. Markov Chain Monte Carlo (MCMC) techniques can lead to exact inference in such models but in practice can suffer performance issues including long burn-in periods and poor mixing. On the other hand approximate Bayesian computation techniques can allow rapid exploration of a large parameter space but yield only approximate posterior distributions. Here we consider the combined use of approximate Bayesian computation (ABC) and MCMC techniques for improved computational efficiency while retaining exact inference on parallel hardware.

</details>

<details>

<summary>2014-10-22 15:34:33 - Bayesian matrix completion: prior specification</summary>

- *Pierre Alquier, Vincent Cottet, Nicolas Chopin, Judith Rousseau*

- `1406.1440v3` - [abs](http://arxiv.org/abs/1406.1440v3) - [pdf](http://arxiv.org/pdf/1406.1440v3)

> Low-rank matrix estimation from incomplete measurements recently received increased attention due to the emergence of several challenging applications, such as recommender systems; see in particular the famous Netflix challenge. While the behaviour of algorithms based on nuclear norm minimization is now well understood, an as yet unexplored avenue of research is the behaviour of Bayesian algorithms in this context. In this paper, we briefly review the priors used in the Bayesian literature for matrix completion. A standard approach is to assign an inverse gamma prior to the singular values of a certain singular value decomposition of the matrix of interest; this prior is conjugate. However, we show that two other types of priors (again for the singular values) may be conjugate for this model: a gamma prior, and a discrete prior. Conjugacy is very convenient, as it makes it possible to implement either Gibbs sampling or Variational Bayes. Interestingly enough, the maximum a posteriori for these different priors is related to the nuclear norm minimization problems. We also compare all these priors on simulated datasets, and on the classical MovieLens and Netflix datasets.

</details>

<details>

<summary>2014-10-22 19:49:43 - Small-noise analysis and symmetrization of implicit Monte Carlo samplers</summary>

- *Jonathan Goodman, Kevin K. Lin, Matthias Morzfeld*

- `1410.6151v1` - [abs](http://arxiv.org/abs/1410.6151v1) - [pdf](http://arxiv.org/pdf/1410.6151v1)

> Implicit samplers are algorithms for producing independent, weighted samples from multi-variate probability distributions. These are often applied in Bayesian data assimilation algorithms. We use Laplace asymptotic expansions to analyze two implicit samplers in the small noise regime. Our analysis suggests a symmetrization of the algo- rithms that leads to improved (implicit) sampling schemes at a rel- atively small additional cost. Computational experiments confirm the theory and show that symmetrization is effective for small noise sampling problems.

</details>

<details>

<summary>2014-10-23 21:55:30 - Non-parametric Bayesian Learning with Deep Learning Structure and Its Applications in Wireless Networks</summary>

- *Erte Pan, Zhu Han*

- `1410.4599v2` - [abs](http://arxiv.org/abs/1410.4599v2) - [pdf](http://arxiv.org/pdf/1410.4599v2)

> In this paper, we present an infinite hierarchical non-parametric Bayesian model to extract the hidden factors over observed data, where the number of hidden factors for each layer is unknown and can be potentially infinite. Moreover, the number of layers can also be infinite. We construct the model structure that allows continuous values for the hidden factors and weights, which makes the model suitable for various applications. We use the Metropolis-Hastings method to infer the model structure. Then the performance of the algorithm is evaluated by the experiments. Simulation results show that the model fits the underlying structure of simulated data.

</details>

<details>

<summary>2014-10-24 10:24:31 - About the posterior distribution in hidden Markov models with unknown number of states</summary>

- *Elisabeth Gassiat, Judith Rousseau*

- `1207.2064v2` - [abs](http://arxiv.org/abs/1207.2064v2) - [pdf](http://arxiv.org/pdf/1207.2064v2)

> We consider finite state space stationary hidden Markov models (HMMs) in the situation where the number of hidden states is unknown. We provide a frequentist asymptotic evaluation of Bayesian analysis methods. Our main result gives posterior concentration rates for the marginal densities, that is for the density of a fixed number of consecutive observations. Using conditions on the prior, we are then able to define a consistent Bayesian estimator of the number of hidden states. It is known that the likelihood ratio test statistic for overfitted HMMs has a nonstandard behaviour and is unbounded. Our conditions on the prior may be seen as a way to penalize parameters to avoid this phenomenon. Inference of parameters is a much more difficult task than inference of marginal densities, we still provide a precise description of the situation when the observations are i.i.d. and we allow for $2$ possible hidden states.

</details>

<details>

<summary>2014-10-24 14:01:31 - Brittleness of Bayesian inference and new Selberg formulas</summary>

- *Houman Owhadi, Clint Scovel*

- `1304.7046v2` - [abs](http://arxiv.org/abs/1304.7046v2) - [pdf](http://arxiv.org/pdf/1304.7046v2)

> The incorporation of priors in the Optimal Uncertainty Quantification (OUQ) framework \cite{OSSMO:2011} reveals brittleness in Bayesian inference; a model may share an arbitrarily large number of finite-dimensional marginals with, or be arbitrarily close (in Prokhorov or total variation metrics) to, the data-generating distribution and still make the largest possible prediction error after conditioning on an arbitrarily large number of samples. The initial purpose of this paper is to unwrap this brittleness mechanism by providing (i) a quantitative version of the Brittleness Theorem of \cite{BayesOUQ} and (ii) a detailed and comprehensive analysis of its application to the revealing example of estimating the mean of a random variable on the unit interval $[0,1]$ using priors that exactly capture the distribution of an arbitrarily large number of Hausdorff moments.   However, in doing so, we discovered that the free parameter associated with Markov and Kre\u{\i}n's canonical representations of truncated Hausdorff moments generates reproducing kernel identities corresponding to reproducing kernel Hilbert spaces of polynomials.   Furthermore, these reproducing identities lead to biorthogonal systems of Selberg integral formulas.   This process of discovery appears to be generic: whereas Karlin and Shapley used Selberg's integral formula to first compute the volume of the Hausdorff moment space (the polytope defined by the first $n$ moments of a probability measure on the interval $[0,1]$), we observe that the computation of that volume along with higher order moments of the uniform measure on the moment space, using different finite-dimensional representations of subsets of the infinite-dimensional set of probability measures on $[0,1]$ representing the first $n$ moments, leads to families of equalities corresponding to classical and new Selberg identities.

</details>

<details>

<summary>2014-10-24 19:07:08 - Limitations of polynomial chaos expansions in the Bayesian solution of inverse problems</summary>

- *Fei Lu, Matthias Morzfeld, Xuemin Tu, Alexandre J. Chorin*

- `1404.7188v2` - [abs](http://arxiv.org/abs/1404.7188v2) - [pdf](http://arxiv.org/pdf/1404.7188v2)

> Polynomial chaos expansions are used to reduce the computational cost in the Bayesian solutions of inverse problems by creating a surrogate posterior that can be evaluated inexpensively. We show, by analysis and example, that when the data contain significant information beyond what is assumed in the prior, the surrogate posterior can be very different from the posterior, and the resulting estimates become inaccurate. One can improve the accuracy by adaptively increasing the order of the polynomial chaos, but the cost may increase too fast for this to be cost effective compared to Monte Carlo sampling without a surrogate posterior.

</details>

<details>

<summary>2014-10-25 02:21:20 - Using a Birth-Death Process to Account for Reporting Errors in Longitudinal Self-reported Counts of Behavior</summary>

- *Jihey Lee, Robert E. Weiss, Marc A. Suchard*

- `1410.6870v1` - [abs](http://arxiv.org/abs/1410.6870v1) - [pdf](http://arxiv.org/pdf/1410.6870v1)

> We analyze longitudinal self-reported counts of sexual partners from youth living with HIV. In self-reported survey data, subjects recall counts of events or behaviors such as the number of sexual partners or the number of drug uses in the past three months. Subjects with small counts may report the exact number, whereas subjects with large counts may have difficulty recalling the exact number. Thus, self-reported counts are noisy, and mis-reporting induces errors in the count variable. As a naive method for analyzing self-reported counts, the Poisson random effects model treats the observed counts as true counts and reporting errors in the outcome variable are ignored. Inferences are therefore based on incorrect information and may lead to conclusions unsupported by the data. We describe a Bayesian model for analyzing longitudinal self-reported count data that formally accounts for reporting error. We model reported counts conditional on underlying true counts using a linear birth-death process and use a Poisson random effects model to model the underlying true counts. A regression version of our model can identify characteristics of subjects with greater or lesser reporting error. We demonstrate several approaches to prior specification.

</details>

<details>

<summary>2014-10-25 15:02:52 - Modelling and Forecasting the Realized Range Conditional Quantiles</summary>

- *Giovanni Bonaccolto, Massimiliano Caporin*

- `1410.6926v1` - [abs](http://arxiv.org/abs/1410.6926v1) - [pdf](http://arxiv.org/pdf/1410.6926v1)

> Several studies have focused on the Realized Range Volatility, an estimator of the quadratic variation of financial prices, taking into account the impact of microstructure noise and jumps. However, none has considered direct modeling and forecasting of the Realized Range conditional quantiles. This study carries out a quantile regression analysis to fill this gap. The proposed model takes into account as quantile predictors both the lagged values of the estimated volatility and some key macroeconomic and financial variables, which provide important information about the overall market trend and risk. In this way, and without distributional assumptions on the realized range innovations, it is possible to assess the entire conditional distribution of the estimated volatility. This issue is a critical one for financial decision-makers in terms of pricing, asset allocation, and risk management. The quantile regression approach allows how the links among the involved variables change across the quantiles levels to be analyzed. In addition, a rolling analysis is performed in order to determine how the relationships that characterize the proposed model evolve over time. The analysis is applied to sixteen stocks issued by companies that operate in differing economic sectors of the U.S. market, and the forecast accuracy is validated by means of suitable tests. The results show evidence of the selected variables' relevant impacts and, particularly during periods of market stress, highlights heterogeneous effects across quantiles.

</details>

<details>

<summary>2014-10-25 18:45:26 - Data Assimilation of Satellite Fire Detection in Coupled Atmosphere-Fire Simulation by WRF-SFIRE</summary>

- *Jan Mandel, Adam K. Kochanski, Martin Vejmelka, Jonathan D. Beezley*

- `1410.6948v1` - [abs](http://arxiv.org/abs/1410.6948v1) - [pdf](http://arxiv.org/pdf/1410.6948v1)

> Currently available satellite active fire detection products from the VIIRS and MODIS instruments on polar-orbiting satellites produce detection squares in arbitrary locations. There is no global fire/no fire map, no detection under cloud cover, false negatives are common, and the detection squares are much coarser than the resolution of a fire behavior model. Consequently, current active fire satellite detection products should be used to improve fire modeling in a statistical sense only, rather than as a direct input. We describe a new data assimilation method for active fire detection, based on a modification of the fire arrival time to simultaneously minimize the difference from the forecast fire arrival time and maximize the likelihood of the fire detection data. This method is inspired by contour detection methods used in computer vision, and it can be cast as a Bayesian inverse problem technique, or a generalized Tikhonov regularization. After the new fire arrival time on the whole simulation domain is found, the model can be re-run from a time in the past using the new fire arrival time to generate the heat fluxes and to spin up the atmospheric model until the satellite overpass time, when the coupled simulation continues from the modified state.

</details>

<details>

<summary>2014-10-25 20:39:36 - From Statistical Evidence to Evidence of Causality</summary>

- *Philip Dawid, Monica Musio, Stephen E. Fienberg*

- `1311.7513v2` - [abs](http://arxiv.org/abs/1311.7513v2) - [pdf](http://arxiv.org/pdf/1311.7513v2)

> While statisticians and quantitative social scientists typically study the "effects of causes" (EoC), Lawyers and the Courts are more concerned with understanding the "causes of effects" (CoE). EoC can be addressed using experimental design and statistical analysis, but it is less clear how to incorporate statistical or epidemiological evidence into CoE reasoning, as might be required for a case at Law. Some form of counterfactual reasoning, such as the "potential outcomes" approach championed by Rubin, appears unavoidable, but this typically yields "answers" that are sensitive to arbitrary and untestable assumptions. We must therefore recognise that a CoE question simply might not have a well-determined answer. It is nevertheless possible to use statistical data to set bounds within which any answer must lie. With less than perfect data these bounds will themselves be uncertain, leading to a compounding of different kinds of uncertainty. Still further care is required in the presence of possible confounding factors. In addition, even identifying the relevant "counterfactual contrast" may be a matter of Policy as much as of Science. Defining the question is as non-trivial a task as finding a route towards an answer. This paper develops some technical elaborations of these philosophical points, and illustrates them with an analysis of a case study in child protection.   Keywords: benfluorex, causes of effects, counterfactual, child protection, effects of causes, Fre'chet bound, potential outcome, probability of causation

</details>

<details>

<summary>2014-10-26 10:19:56 - Sparse Estimation using Bayesian Hierarchical Prior Modeling for Real and Complex Linear Models</summary>

- *Niels Lovmand Pedersen, Carles Navarro ManchÃ³n, Mihai-Alin Badiu, Dmitriy Shutin, Bernard Henri Fleury*

- `1108.4324v3` - [abs](http://arxiv.org/abs/1108.4324v3) - [pdf](http://arxiv.org/pdf/1108.4324v3)

> In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have been used to model sparsity-inducing priors that realize a class of concave penalty functions for the regression task in real-valued signal models. Motivated by the relative scarcity of formal tools for SBL in complex-valued models, this paper proposes a GSM model - the Bessel K model - that induces concave penalty functions for the estimation of complex sparse signals. The properties of the Bessel K model are analyzed when it is applied to Type I and Type II estimation. This analysis reveals that, by tuning the parameters of the mixing pdf different penalty functions are invoked depending on the estimation type used, the value of the noise variance, and whether real or complex signals are estimated. Using the Bessel K model, we derive a sparse estimator based on a modification of the expectation-maximization algorithm formulated for Type II estimation. The estimator includes as a special instance the algorithms proposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical results show the superiority of the proposed estimator over these state-of-the-art estimators in terms of convergence speed, sparseness, reconstruction error, and robustness in low and medium signal-to-noise ratio regimes.

</details>

<details>

<summary>2014-10-26 16:24:41 - Smoothing, Clustering, and Benchmarking for Small Area Estimation</summary>

- *Rebecca C. Steorts*

- `1410.7056v1` - [abs](http://arxiv.org/abs/1410.7056v1) - [pdf](http://arxiv.org/pdf/1410.7056v1)

> We develop constrained Bayesian estimation methods for small area problems: those requiring smoothness with respect to similarity across areas, such as geographic proximity or clustering by covariates; and benchmarking constraints, requiring (weighted) means of estimates to agree across levels of aggregation. We develop methods for constrained estimation decision-theoretically and discuss their geometric interpretation. Our constrained estimators are the solutions to tractable optimization problems and have closed-form solutions. Mean squared errors of the constrained estimators are calculated via bootstrapping. Our techniques are free of distributional assumptions and apply whether the estimator is linear or non-linear, univariate or multivariate. We illustrate our methods using data from the U.S. Census's Small Area Income and Poverty Estimates program.

</details>

<details>

<summary>2014-10-26 22:06:38 - On Some Distributed Disorder Detection</summary>

- *Krzysztof Szajowski*

- `1410.7091v1` - [abs](http://arxiv.org/abs/1410.7091v1) - [pdf](http://arxiv.org/pdf/1410.7091v1)

> Multivariate data sources with components of different information value seem to appear frequently in practice. Models in which the components change their homogeneity at different times are of significant importance. The fact whether any changes are influential for the whole process is determined not only by the moments of the change, but also depends on which coordinates. This is particularly important in issues such as reliability analysis of complex systems and the location of an intruder in surveillance systems. In this paper we developed a mathematical model for such sources of signals with discrete time having the Markov property given the times of change. The research also comprises a multivariate detection of the transition probabilities changes at certain sensitivity level in the multidimensional process. Additionally, the observation of the random vector is depicted. Each chosen coordinate forms the Markov process with different transition probabilities before and after some unknown moment. The aim of statisticians is to estimate the moments based on the observation of the process. The Bayesian approach is used with the risk function depending on measure of chance of a false alarm and some cost of overestimation. The moment of the system's disorder is determined by the detection of transition probabilities changes at some coordinates. The overall modeling of the critical coordinates is based on the simple game.

</details>

<details>

<summary>2014-10-27 08:14:14 - Randomization does not help much, comparability does</summary>

- *Uwe Saint-Mont*

- `1311.4390v2` - [abs](http://arxiv.org/abs/1311.4390v2) - [pdf](http://arxiv.org/pdf/1311.4390v2)

> Following Fisher, it is widely believed that randomization "relieves the experimenter from the anxiety of considering innumerable causes by which the data may be disturbed." In particular, it is said to control for known and unknown nuisance factors that may considerably challenge the validity of a result. Looking for quantitative advice, we study a number of straightforward, mathematically simple models. However, they all demonstrate that the optimism with respect to randomization is wishful thinking rather than based on fact. In small to medium-sized samples, random allocation of units to treatments typically yields a considerable imbalance between the groups, i.e., confounding due to randomization is the rule rather than the exception.   In the second part of this contribution, we extend the reasoning to a number of traditional arguments for and against randomization. This discussion is rather non-technical, and at times even "foundational" (Frequentist vs. Bayesian). However, its result turns out to be quite similar. While randomization's contribution remains questionable, comparability contributes much to a compelling conclusion. Summing up, classical experimentation based on sound background theory and the systematic construction of exchangeable groups seems to be advisable.

</details>

<details>

<summary>2014-10-28 16:29:29 - Scalable multiscale density estimation</summary>

- *Ye Wang, Antonio Canale, David Dunson*

- `1410.7692v1` - [abs](http://arxiv.org/abs/1410.7692v1) - [pdf](http://arxiv.org/pdf/1410.7692v1)

> Although Bayesian density estimation using discrete mixtures has good performance in modest dimensions, there is a lack of statistical and computational scalability to high-dimensional multivariate cases. To combat the curse of dimensionality, it is necessary to assume the data are concentrated near a lower-dimensional subspace. However, Bayesian methods for learning this subspace along with the density of the data scale poorly computationally. To solve this problem, we propose an empirical Bayes approach, which estimates a multiscale dictionary using geometric multiresolution analysis in a first stage. We use this dictionary within a multiscale mixture model, which allows uncertainty in component allocation, mixture weights and scaling factors over a binary tree. A computational algorithm is proposed, which scales efficiently to massive dimensional problems. We provide some theoretical support for this geometric density estimation (GEODE) method, and illustrate the performance through simulated and real data examples.

</details>

<details>

<summary>2014-10-28 17:58:03 - Bayesian Spatial Change of Support for Count-Valued Survey Data</summary>

- *Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan*

- `1405.7227v2` - [abs](http://arxiv.org/abs/1405.7227v2) - [pdf](http://arxiv.org/pdf/1405.7227v2)

> We introduce Bayesian spatial change of support methodology for count-valued survey data with known survey variances. Our proposed methodology is motivated by the American Community Survey (ACS), an ongoing survey administered by the U.S. Census Bureau that provides timely information on several key demographic variables. Specifically, the ACS produces 1-year, 3-year, and 5-year "period-estimates," and corresponding margins of errors, for published demographic and socio-economic variables recorded over predefined geographies within the United States. Despite the availability of these predefined geographies it is often of interest to data users to specify customized user-defined spatial supports. In particular, it is useful to estimate demographic variables defined on "new" spatial supports in "real-time." This problem is known as spatial change of support (COS), which is typically performed under the assumption that the data follows a Gaussian distribution. However, count-valued survey data is naturally non-Gaussian and, hence, we consider modeling these data using a Poisson distribution. Additionally, survey-data are often accompanied by estimates of error, which we incorporate into our analysis. We interpret Poisson count-valued data in small areas as an aggregation of events from a spatial point process. This approach provides us with the flexibility necessary to allow ACS users to consider a variety of spatial supports in "real-time." We demonstrate the effectiveness of our approach through a simulated example as well as through an analysis using public-use ACS data.

</details>

<details>

<summary>2014-10-28 20:43:05 - Dynamic Model Averaging in Large Model Spaces Using Dynamic Occam's Window</summary>

- *Luca Onorante, Adrian E. Raftery*

- `1410.7799v1` - [abs](http://arxiv.org/abs/1410.7799v1) - [pdf](http://arxiv.org/pdf/1410.7799v1)

> Bayesian model averaging has become a widely used approach to accounting for uncertainty about the structural form of the model generating the data. When data arrive sequentially and the generating model can change over time, Dynamic Model Averaging (DMA) extends model averaging to deal with this situation. Often in macroeconomics, however, many candidate explanatory variables are available and the number of possible models becomes too large for DMA to be applied in its original form. We propose a new method for this situation which allows us to perform DMA without considering the whole model space, but using a subset of models and dynamically optimizing the choice of models at each point in time. This yields a dynamic form of Occam's window. We evaluate the method in the context of the problem of nowcasting GDP in the Euro area. We find that its forecasting performance compares well that of other methods.   Keywords: Bayesian model averaging; Model uncertainty; Nowcasting; Occam's window.

</details>

<details>

<summary>2014-10-29 05:10:03 - Faster graphical model identification of tandem mass spectra using peptide word lattices</summary>

- *Shengjie Wang, John T. Halloran, Jeff A. Bilmes, William S. Noble*

- `1410.7875v1` - [abs](http://arxiv.org/abs/1410.7875v1) - [pdf](http://arxiv.org/pdf/1410.7875v1)

> Liquid chromatography coupled with tandem mass spectrometry, also known as shotgun proteomics, is a widely-used high-throughput technology for identifying proteins in complex biological samples. Analysis of the tens of thousands of fragmentation spectra produced by a typical shotgun proteomics experiment begins by assigning to each observed spectrum the peptide hypothesized to be responsible for generating the spectrum, typically done by searching each spectrum against a database of peptides. We have recently described a machine learning method---Dynamic Bayesian Network for Rapid Identification of Peptides (DRIP)---that not only achieves state-of-the-art spectrum identification performance on a variety of datasets but also provides a trainable model capable of returning valuable auxiliary information regarding specific peptide-spectrum matches. In this work, we present two significant improvements to DRIP. First, we describe how to use word lattices, which are widely used in natural language processing, to significantly speed up DRIP's computations. To our knowledge, all existing shotgun proteomics search engines compute independent scores between a given observed spectrum and each possible candidate peptide from the database. The key idea of the word lattice is to represent the set of candidate peptides in a single data structure, thereby allowing sharing of redundant computations among the different candidates. We demonstrate that using lattices in conjunction with DRIP leads to speedups on the order of tens across yeast and worm data sets. Second, we introduce a variant of DRIP that uses a discriminative training framework, performing maximum mutual entropy estimation rather than maximum likelihood estimation. This modification improves DRIP's statistical power, enabling us to increase the number of identified spectrum at a 1% false discovery rate on yeast and worm data sets.

</details>

<details>

<summary>2014-10-30 07:23:33 - Functional regression approximate Bayesian computation for Gaussian process density estimation</summary>

- *G. S. Rodrigues, David J. Nott, S. A. Sisson*

- `1410.8276v1` - [abs](http://arxiv.org/abs/1410.8276v1) - [pdf](http://arxiv.org/pdf/1410.8276v1)

> We propose a novel Bayesian nonparametric method for hierarchical modelling on a set of related density functions, where grouped data in the form of samples from each density function are available. Borrowing strength across the groups is a major challenge in this context. To address this problem, we introduce a hierarchically structured prior, defined over a set of univariate density functions, using convenient transformations of Gaussian processes. Inference is performed through approximate Bayesian computation (ABC), via a novel functional regression adjustment. The performance of the proposed method is illustrated via a simulation study and an analysis of rural high school exam performance in Brazil.

</details>

<details>

<summary>2014-10-30 22:07:15 - Causal Inference through a Witness Protection Program</summary>

- *Ricardo Silva, Robin Evans*

- `1406.0531v2` - [abs](http://arxiv.org/abs/1406.0531v2) - [pdf](http://arxiv.org/pdf/1406.0531v2)

> One of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest "weak" paths in a unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of "path cancellations" that imply conditional independencies but do not rule out the existence of confounding causal paths. The outcome is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice along with other default tools in observational studies.

</details>

<details>

<summary>2014-10-31 22:43:31 - Semi-Supervised Learning with Deep Generative Models</summary>

- *Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling*

- `1406.5298v2` - [abs](http://arxiv.org/abs/1406.5298v2) - [pdf](http://arxiv.org/pdf/1406.5298v2)

> The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.

</details>


## 2014-11

<details>

<summary>2014-11-01 14:55:46 - Extracting the Italian output gap: a Bayesian approach</summary>

- *Mauro Bernardi, Antonio Di Ruggiero*

- `1411.4898v1` - [abs](http://arxiv.org/abs/1411.4898v1) - [pdf](http://arxiv.org/pdf/1411.4898v1)

> During the last decades particular effort has been directed towards understanding and predicting the relevant state of the business cycle with the objective of decomposing permanent shocks from those having only a transitory impact on real output. This trend--cycle decomposition has a relevant impact on several economic and fiscal variables and constitutes by itself an important indicator for policy purposes. This paper deals with trend--cycle decomposition for the Italian economy having some interesting peculiarities which makes it attractive to analyse from both a statistic and an historical perspective. We propose an univariate model for the quarterly real GDP, subsequently extended to include the price dynamics through a Phillips curve. This study considers a series of the Italian quarterly real GDP recently released by OECD which includes both the 1960s and the recent global financial crisis of 2007--2008. Parameters estimate as well as the signal extraction are performed within the Bayesian paradigm which effectively handles complex models where the parameters enter the log--likelihood function in a strongly nonlinear way. A new Adaptive Independent Metropolis--within--Gibbs sampler is then developed to efficiently simulate the parameters of the unobserved cycle. Our results suggest that inflation influences the Output Gap estimate, making the extracted Italian OG an important indicator of inflation pressures on the real side of the economy, as stated by the Phillips theory. Moreover, our estimate of the sequence of peaks and troughs of the Output Gap is in line with the OECD official dating of the Italian business cycle.

</details>

<details>

<summary>2014-11-02 01:00:14 - Scalable Rejection Sampling for Bayesian Hierarchical Models</summary>

- *Michael Braun, Paul Damien*

- `1401.8236v3` - [abs](http://arxiv.org/abs/1401.8236v3) - [pdf](http://arxiv.org/pdf/1401.8236v3)

> Bayesian hierarchical modeling is a popular approach to capturing unobserved heterogeneity across individual units. However, standard estimation methods such as Markov chain Monte Carlo (MCMC) can be impracticable for modeling outcomes from a large number of units. We develop a new method to sample from posterior distributions of Bayesian models, without using MCMC. Samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. The algorithm is scalable under the weak assumption that individual units are conditionally independent, making it applicable for large datasets. It can also be used to compute marginal likelihoods.

</details>

<details>

<summary>2014-11-02 05:38:50 - Compressed Sensing for Energy-Efficient Wireless Telemonitoring of Noninvasive Fetal ECG via Block Sparse Bayesian Learning</summary>

- *Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao*

- `1205.1287v7` - [abs](http://arxiv.org/abs/1205.1287v7) - [pdf](http://arxiv.org/pdf/1205.1287v7)

> Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. The design of a telemonitoring system via a wireless body-area network with low energy consumption for ambulatory use is highly desirable. As an emerging technique, compressed sensing (CS) shows great promise in compressing/reconstructing data with low energy consumption. However, due to some specific characteristics of raw FECG recordings such as non-sparsity and strong noise contamination, current CS algorithms generally fail in this application.   This work proposes to use the block sparse Bayesian learning (BSBL) framework to compress/reconstruct non-sparse raw FECG recordings. Experimental results show that the framework can reconstruct the raw recordings with high quality. Especially, the reconstruction does not destroy the interdependence relation among the multichannel recordings. This ensures that the independent component analysis decomposition of the reconstructed recordings has high fidelity. Furthermore, the framework allows the use of a sparse binary sensing matrix with much fewer nonzero entries to compress recordings. Particularly, each column of the matrix can contain only two nonzero entries. This shows the framework, compared to other algorithms such as current CS algorithms and wavelet algorithms, can greatly reduce code execution in CPU in the data compression stage.

</details>

<details>

<summary>2014-11-02 05:40:56 - Compressed Sensing of EEG for Wireless Telemonitoring with Low Energy Consumption and Inexpensive Hardware</summary>

- *Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao*

- `1206.3493v3` - [abs](http://arxiv.org/abs/1206.3493v3) - [pdf](http://arxiv.org/pdf/1206.3493v3)

> Telemonitoring of electroencephalogram (EEG) through wireless body-area networks is an evolving direction in personalized medicine. Among various constraints in designing such a system, three important constraints are energy consumption, data compression, and device cost. Conventional data compression methodologies, although effective in data compression, consumes significant energy and cannot reduce device cost. Compressed sensing (CS), as an emerging data compression methodology, is promising in catering to these constraints. However, EEG is non-sparse in the time domain and also non-sparse in transformed domains (such as the wavelet domain). Therefore, it is extremely difficult for current CS algorithms to recover EEG with the quality that satisfies the requirements of clinical diagnosis and engineering applications. Recently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method to the CS problem. This study introduces the technique to the telemonitoring of EEG. Experimental results show that its recovery quality is better than state-of-the-art CS algorithms, and sufficient for practical use. These results suggest that BSBL is very promising for telemonitoring of EEG and other non-sparse physiological signals.

</details>

<details>

<summary>2014-11-02 05:55:59 - Extension of SBL Algorithms for the Recovery of Block Sparse Signals with Intra-Block Correlation</summary>

- *Zhilin Zhang, Bhaskar D. Rao*

- `1201.0862v5` - [abs](http://arxiv.org/abs/1201.0862v5) - [pdf](http://arxiv.org/pdf/1201.0862v5)

> We examine the recovery of block sparse signals and extend the framework in two important directions; one by exploiting signals' intra-block correlation and the other by generalizing signals' block structure. We propose two families of algorithms based on the framework of block sparse Bayesian learning (BSBL). One family, directly derived from the BSBL framework, requires knowledge of the block structure. Another family, derived from an expanded BSBL framework, is based on a weaker assumption on the block structure, and can be used when the block structure is completely unknown. Using these algorithms we show that exploiting intra-block correlation is very helpful in improving recovery performance. These algorithms also shed light on how to modify existing algorithms or design new ones to exploit such correlation and improve performance.

</details>

<details>

<summary>2014-11-02 13:51:59 - Deterministic Bayesian Information Fusion and the Analysis of its Performance</summary>

- *Gaurav Thakur*

- `1311.3755v4` - [abs](http://arxiv.org/abs/1311.3755v4) - [pdf](http://arxiv.org/pdf/1311.3755v4)

> This paper develops a mathematical and computational framework for analyzing the expected performance of Bayesian data fusion, or joint statistical inference, within a sensor network. We use variational techniques to obtain the posterior expectation as the optimal fusion rule under a deterministic constraint and a quadratic cost, and study the smoothness and other properties of its classification performance. For a certain class of fusion problems, we prove that this fusion rule is also optimal in a much wider sense and satisfies strong asymptotic convergence results. We show how these results apply to a variety of examples with Gaussian, exponential and other statistics, and discuss computational methods for determining the fusion system's performance in more general, large-scale problems. These results are motivated by studying the performance of fusing multi-modal radar and acoustic sensors for detecting explosive substances, but have broad applicability to other Bayesian decision problems.

</details>

<details>

<summary>2014-11-03 05:27:58 - Adaptive Image Denoising by Targeted Databases</summary>

- *Enming Luo, Stanley H. Chan, Truong Q. Nguyen*

- `1407.5055v3` - [abs](http://arxiv.org/abs/1407.5055v3) - [pdf](http://arxiv.org/pdf/1407.5055v3)

> We propose a data-dependent denoising procedure to restore noisy images. Different from existing denoising algorithms which search for patches from either the noisy image or a generic database, the new algorithm finds patches from a database that contains only relevant patches. We formulate the denoising problem as an optimal filter design problem and make two contributions. First, we determine the basis function of the denoising filter by solving a group sparsity minimization problem. The optimization formulation generalizes existing denoising algorithms and offers systematic analysis of the performance. Improvement methods are proposed to enhance the patch search process. Second, we determine the spectral coefficients of the denoising filter by considering a localized Bayesian prior. The localized prior leverages the similarity of the targeted database, alleviates the intensive Bayesian computation, and links the new method to the classical linear minimum mean squared error estimation. We demonstrate applications of the proposed method in a variety of scenarios, including text images, multiview images and face images. Experimental results show the superiority of the new algorithm over existing methods.

</details>

<details>

<summary>2014-11-03 08:17:59 - Variational Gaussian Process State-Space Models</summary>

- *Roger Frigola, Yutian Chen, Carl E. Rasmussen*

- `1406.4905v2` - [abs](http://arxiv.org/abs/1406.4905v2) - [pdf](http://arxiv.org/pdf/1406.4905v2)

> State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.

</details>

<details>

<summary>2014-11-03 11:48:07 - Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature</summary>

- *Tom Gunter, Michael A. Osborne, Roman Garnett, Philipp Hennig, Stephen J. Roberts*

- `1411.0439v1` - [abs](http://arxiv.org/abs/1411.0439v1) - [pdf](http://arxiv.org/pdf/1411.0439v1)

> We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.

</details>

<details>

<summary>2014-11-03 15:59:29 - A note on Bayesian logistic regression for spatial exponential family Gibbs point processes</summary>

- *Tuomas Rajala*

- `1411.0539v1` - [abs](http://arxiv.org/abs/1411.0539v1) - [pdf](http://arxiv.org/pdf/1411.0539v1)

> Recently, a very attractive logistic regression inference method for exponential family Gibbs spatial point processes was introduced. We combined it with the technique of quadratic tangential variational approximation and derived a new Bayesian technique for analysing spatial point patterns. The technique is described in detail, and demonstrated on numerical examples.

</details>

<details>

<summary>2014-11-03 18:15:29 - Bayesian feature selection with strongly-regularizing priors maps to the Ising Model</summary>

- *Charles K. Fisher, Pankaj Mehta*

- `1411.0591v1` - [abs](http://arxiv.org/abs/1411.0591v1) - [pdf](http://arxiv.org/pdf/1411.0591v1)

> Identifying small subsets of features that are relevant for prediction and/or classification tasks is a central problem in machine learning and statistics. The feature selection task is especially important, and computationally difficult, for modern datasets where the number of features can be comparable to, or even exceed, the number of samples. Here, we show that feature selection with Bayesian inference takes a universal form and reduces to calculating the magnetizations of an Ising model, under some mild conditions. Our results exploit the observation that the evidence takes a universal form for strongly-regularizing priors --- priors that have a large effect on the posterior probability even in the infinite data limit. We derive explicit expressions for feature selection for generalized linear models, a large class of statistical techniques that include linear and logistic regression. We illustrate the power of our approach by analyzing feature selection in a logistic regression-based classifier trained to distinguish between the letters B and D in the notMNIST dataset.

</details>

<details>

<summary>2014-11-03 21:47:36 - Adaptive Sensing Resource Allocation Over Multiple Hypothesis Tests</summary>

- *Dennis Wei*

- `1410.0950v2` - [abs](http://arxiv.org/abs/1410.0950v2) - [pdf](http://arxiv.org/pdf/1410.0950v2)

> This paper considers multiple binary hypothesis tests with adaptive allocation of sensing resources from a shared budget over a small number of stages. A Bayesian formulation is provided for the multistage allocation problem of minimizing the sum of Bayes risks, which is then recast as a dynamic program. In the single-stage case, the problem is a non-convex optimization, for which an algorithm composed of a series of parallel one-dimensional minimizations is presented. This algorithm ensures a global minimum under a sufficient condition. In the multistage case, the approximate dynamic programming method of open-loop feedback control is employed. In numerical simulations, the proposed allocation policies outperform alternative adaptive procedures when the numbers of true null and alternative hypotheses are not too imbalanced. In the case of few alternative hypotheses, the proposed policies are competitive using only a few stages of adaptation. In all cases substantial gains over non-adaptive sensing are observed.

</details>

<details>

<summary>2014-11-04 05:55:27 - Bayesian two-step estimation in differential equation models</summary>

- *Prithwish Bhaumik, Subhashis Ghosal*

- `1411.0793v1` - [abs](http://arxiv.org/abs/1411.0793v1) - [pdf](http://arxiv.org/pdf/1411.0793v1)

> Ordinary differential equations (ODEs) are used to model dynamic systems appearing in engineering, physics, biomedical sciences and many other fields. These equations contain unknown parameters, say $\bm\theta$ of physical significance which have to be estimated from the noisy data. Often there is no closed form analytic solution of the equations and hence we cannot use the usual non-linear least squares technique to estimate the unknown parameters. There is a two-step approach to solve this problem, where the first step involves fitting the data nonparametrically. In the second step the parameter is estimated by minimizing the distance between the nonparametrically estimated derivative and the derivative suggested by the system of ODEs. The statistical aspects of this approach have been studied under the frequentist framework. We consider this two-step estimation under the Bayesian framework. The response variable is allowed to be multidimensional and the true mean function of it is not assumed to be in the model. We induce a prior on the regression function using a random series based on the B-spline basis functions. We establish the Bernstein-von Mises theorem for the posterior distribution of the parameter of interest. Interestingly, even though the posterior distribution of the regression function based on splines converges at a rate slower than $n^{-1/2}$, the parameter vector $\bm\theta$ is nevertheless estimated at $n^{-1/2}$ rate.

</details>

<details>

<summary>2014-11-04 07:46:28 - The multivariate Dirichlet-multinomial distribution and its application in forensic genetics to adjust for sub-population effects using the Î¸-correction</summary>

- *Torben Tvedebrink, Poul Svante Eriksen, Niels Morling*

- `1406.6508v3` - [abs](http://arxiv.org/abs/1406.6508v3) - [pdf](http://arxiv.org/pdf/1406.6508v3)

> In this paper, we discuss the construction of a multivariate generalisation of the Dirichlet-multinomial distribution. An example from forensic genetics in the statistical analysis of DNA mixtures motivates the study of this multivariate extension.   In forensic genetics, adjustment of the match probabilities due to remote ancestry in the population is often done using the so-called {\theta}-correction. This correction increases the probability of observing multiple copies of rare alleles and thereby reduces the weight of the evidence for rare genotypes.   By numerical examples, we show how the {\theta}-correction incorporated by the use of the multivariate Dirichlet-multinomial distribution affects the weight of evidence. Furthermore, we demonstrate how the {\theta}-correction can be incorporated in a Markov structure needed to make efficient computations in a Bayesian network.

</details>

<details>

<summary>2014-11-04 15:30:25 - Simple approximate MAP Inference for Dirichlet processes</summary>

- *Yordan P. Raykov, Alexis Boukouvalas, Max A. Little*

- `1411.0939v1` - [abs](http://arxiv.org/abs/1411.0939v1) - [pdf](http://arxiv.org/pdf/1411.0939v1)

> The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesian nonparametric statistical model. However, full probabilistic inference in this model is analytically intractable, so that computationally intensive techniques such as Gibb's sampling are required. As a result, DPM-based methods, which have considerable potential, are restricted to applications in which computational resources and time for inference is plentiful. For example, they would not be practical for digital signal processing on embedded hardware, where computational resources are at a serious premium. Here, we develop simplified yet statistically rigorous approximate maximum a-posteriori (MAP) inference algorithms for DPMs. This algorithm is as simple as K-means clustering, performs in experiments as well as Gibb's sampling, while requiring only a fraction of the computational effort. Unlike related small variance asymptotics, our algorithm is non-degenerate and so inherits the "rich get richer" property of the Dirichlet process. It also retains a non-degenerate closed-form likelihood which enables standard tools such as cross-validation to be used. This is a well-posed approximation to the MAP solution of the probabilistic DPM model.

</details>

<details>

<summary>2014-11-05 15:10:19 - Monitoring Count Time Series in R: Aberration Detection in Public Health Surveillance</summary>

- *Salmon MaÃ«lle, Schumacher Dirk, HÃ¶hle Michael*

- `1411.1292v1` - [abs](http://arxiv.org/abs/1411.1292v1) - [pdf](http://arxiv.org/pdf/1411.1292v1)

> Public health surveillance aims at lessening disease burden, e.g., in case of infectious diseases by timely recognizing emerging outbreaks. Seen from a statistical perspective, this implies the use of appropriate methods for monitoring time series of aggregated case reports. This paper presents the tools for such automatic aberration detection offered by the R package surveillance. We introduce the functionality for the visualization, modelling and monitoring of surveillance time series. With respect to modelling we focus on univariate time series modelling based on generalized linear models (GLMs), multivariate GLMs, generalized additive models and generalized additive models for location, shape and scale. This ranges from illustrating implementational improvements and extensions of the well-known Farrington algorithm, e.g, by spline-modelling or by treating it in a Bayesian context. Furthermore, we look at categorical time series and address overdispersion using beta-binomial or Dirichlet-Multinomial modelling. With respect to monitoring we consider detectors based on either a Shewhart-like single timepoint comparison between the observed count and the predictive distribution or by likelihood-ratio based cumulative sum methods. Finally, we illustrate how surveillance can support aberration detection in practice by integrating it into the monitoring workflow of a public health institution. Altogether, the present article shows how well surveillance can support automatic aberration detection in a public health surveillance context.

</details>

<details>

<summary>2014-11-05 16:08:50 - Bayesian inference of cosmic density fields from non-linear, scale-dependent, and stochastic biased tracers</summary>

- *Metin Ata, Francisco-Shu Kitaura, Volker MÃ¼ller*

- `1408.2566v5` - [abs](http://arxiv.org/abs/1408.2566v5) - [pdf](http://arxiv.org/pdf/1408.2566v5)

> We present a Bayesian reconstruction algorithm to generate unbiased samples of the underlying dark matter field from halo catalogues. Our new contribution consists of implementing a non-Poisson likelihood including a deterministic non-linear and scale-dependent bias. In particular we present the Hamiltonian equations of motions for the negative binomial (NB) probability distribution function. This permits us to efficiently sample the posterior distribution function of density fields given a sample of galaxies using the Hamiltonian Monte Carlo technique implemented in the Argo code. We have tested our algorithm with the Bolshoi $N$-body simulation at redshift $z = 0$, inferring the underlying dark matter density field from sub-samples of the halo catalogue with biases smaller and larger than one. Our method shows that we can draw closely unbiased samples (compatible within 1-$\sigma$) from the posterior distribution up to scales of about $k$~1 h/Mpc in terms of power-spectra and cell-to-cell correlations. We find that a Poisson likelihood yields reconstructions with power spectra deviating more than 10% at $k$=0.2 h/Mpc. Our reconstruction algorithm is especially suited for emission line galaxy data for which a complex non-linear stochastic biasing treatment beyond Poissonity becomes indispensable.

</details>

<details>

<summary>2014-11-05 22:45:45 - Sequential Joint Detection and Estimation: Optimum Tests and Applications</summary>

- *Yasin Yilmaz, Shang Li, Xiaodong Wang*

- `1411.1440v1` - [abs](http://arxiv.org/abs/1411.1440v1) - [pdf](http://arxiv.org/pdf/1411.1440v1)

> We treat the statistical inference problems in which one needs to detect and estimate simultaneously using as small number of samples as possible. Conventional methods treat the detection and estimation subproblems separately, ignoring the intrinsic coupling between them. However, a joint detection and estimation problem should be solved to maximize the overall performance. We address the sample size concern through a sequential and Bayesian setup. Specifically, we seek the optimum triplet of stopping time, detector, and estimator(s) that minimizes the number of samples subject to a constraint on the combined detection and estimation cost. A general framework for optimum sequential joint detection and estimation is developed. The resulting optimum detector and estimator(s) are strongly coupled with each other, proving that the separate treatment is strictly sub-optimum. The theoretical results derived for a quite general model are then applied to several problems with linear quadratic Gaussian (LQG) models, including dynamic spectrum access in cognitive radio, and state estimation in smart grid with topological uncertainty. Numerical results corroborate the superior overall detection and estimation performance of the proposed schemes over the conventional methods that handle the subproblems separately.

</details>

<details>

<summary>2014-11-05 23:45:52 - Modelling extremes using approximate Bayesian Computation</summary>

- *Robert Erhardt, Scott A. Sisson*

- `1411.1451v1` - [abs](http://arxiv.org/abs/1411.1451v1) - [pdf](http://arxiv.org/pdf/1411.1451v1)

> By the nature of their construction, many statistical models for extremes result in likelihood functions that are computationally prohibitive to evaluate. This is consequently problematic for the purposes of likelihood-based inference. With a focus on the Bayesian framework, this chapter examines the use of approximate Bayesian computation (ABC) techniques for the fitting and analysis of statistical models for extremes. After introducing the ideas behind ABC algorithms and methods, we demonstrate their application to extremal models in stereology and spatial extremes.

</details>

<details>

<summary>2014-11-06 17:56:21 - Stochastic Variational Inference for Hidden Markov Models</summary>

- *Nicholas J. Foti, Jason Xu, Dillon Laird, Emily B. Fox*

- `1411.1670v1` - [abs](http://arxiv.org/abs/1411.1670v1) - [pdf](http://arxiv.org/pdf/1411.1670v1)

> Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.

</details>

<details>

<summary>2014-11-06 18:43:22 - Posterior convergence rates for estimating large precision matrices using graphical models</summary>

- *Sayantan Banerjee, Subhashis Ghosal*

- `1302.2677v3` - [abs](http://arxiv.org/abs/1302.2677v3) - [pdf](http://arxiv.org/pdf/1302.2677v3)

> We consider Bayesian estimation of a $p\times p$ precision matrix, when $p$ can be much larger than the available sample size $n$. It is well known that consistent estimation in such ultra-high dimensional situations requires regularization such as banding, tapering or thresholding. We consider a banding structure in the model and induce a prior distribution on a banded precision matrix through a Gaussian graphical model, where an edge is present only when two vertices are within a given distance. For a proper choice of the order of graph, we obtain the convergence rate of the posterior distribution and Bayes estimators based on the graphical model in the $L_{\infty}$-operator norm uniformly over a class of precision matrices, even if the true precision matrix may not have a banded structure. Along the way to the proof, we also compute the convergence rate of the maximum likelihood estimator (MLE) under the same set of condition, which is of independent interest. The graphical model based MLE and Bayes estimators are automatically positive definite, which is a desirable property not possessed by some other estimators in the literature. We also conduct a simulation study to compare finite sample performance of the Bayes estimators and the MLE based on the graphical model with that obtained by using a Cholesky decomposition of the precision matrix. Finally, we discuss a practical method of choosing the order of the graphical model using the marginal likelihood function.

</details>

<details>

<summary>2014-11-07 17:50:48 - Differential gene co-expression networks via Bayesian biclustering models</summary>

- *Chuan Gao, Shiwen Zhao, Ian C. McDowell, Christopher D. Brown, Barbara E. Engelhardt*

- `1411.1997v1` - [abs](http://arxiv.org/abs/1411.1997v1) - [pdf](http://arxiv.org/pdf/1411.1997v1)

> Identifying latent structure in large data matrices is essential for exploring biological processes. Here, we consider recovering gene co-expression networks from gene expression data, where each network encodes relationships between genes that are locally co-regulated by shared biological mechanisms. To do this, we develop a Bayesian statistical model for biclustering to infer subsets of co-regulated genes whose covariation may be observed in only a subset of the samples. Our biclustering method, BicMix, has desirable properties, including allowing overcomplete representations of the data, computational tractability, and jointly modeling unknown confounders and biological signals. Compared with related biclustering methods, BicMix recovers latent structure with higher precision across diverse simulation scenarios. Further, we develop a method to recover gene co-expression networks from the estimated sparse biclustering matrices. We apply BicMix to breast cancer gene expression data and recover a gene co-expression network that is differential across ER+ and ER- samples.

</details>

<details>

<summary>2014-11-07 23:53:19 - BADER: Bayesian analysis of differential expression in RNA sequencing data</summary>

- *Matthias Katzfuss, Andreas Neudecker, Simon Anders, Julien Gagneur*

- `1410.4827v2` - [abs](http://arxiv.org/abs/1410.4827v2) - [pdf](http://arxiv.org/pdf/1410.4827v2)

> Identifying differentially expressed genes from RNA sequencing data remains a challenging task because of the considerable uncertainties in parameter estimation and the small sample sizes in typical applications. Here we introduce Bayesian Analysis of Differential Expression in RNA-sequencing data (BADER). Due to our choice of data and prior distributions, full posterior inference for BADER can be carried out efficiently. The method appropriately takes uncertainty in gene variance into account, leading to higher power than existing methods in detecting differentially expressed genes. Moreover, we show that the posterior samples can be naturally integrated into downstream gene set enrichment analyses, with excellent performance in detecting enriched sets. An open-source R package (BADER) that provides a user-friendly interface to a C++ back-end is available on Bioconductor.

</details>

<details>

<summary>2014-11-09 01:43:14 - A Censored Bayesian Hierarchical Model For Precipitation</summary>

- *Yang Liu, Philip Kokic, K. Shuvo Bakar*

- `1411.2182v1` - [abs](http://arxiv.org/abs/1411.2182v1) - [pdf](http://arxiv.org/pdf/1411.2182v1)

> Modelling of precipitation, including extremes, is important for hydrological and agricultural applications. Traditionally, because of large sample properties for data over a large threshold value, generalised Pareto (GP) distributions are often used for modelling extreme rainfall. It can be shown that under certain conditions the generalised hyperbolic (GH) distributions can approximate the power law decay of the GP distribution in the tails. Given their flexible form, this raises the possibility that distributions from the GH family serve as a model for the entire rainfall distribution thus avoiding the need to select a threshold. In this paper, we use a flexible censored hierarchical model that leverages the GH distribution to accommodate data subject to heavy tails and an excessive number of zeros. The fitted model allows estimation of probabilities and return periods of the rainfall extremes, and it produces narrower credible intervals in the tails than the traditional GP method. The model not only fits the tails of the rainfall distribution, but fits the whole distribution very well. It also efficiently represents short-term dependencies in the data so it is suitable for evaluating duration over and below thresholds as well as duration of zero rainfall.

</details>

<details>

<summary>2014-11-10 06:42:03 - Cluster detection and risk estimation for spatio-temporal health data</summary>

- *Duncan Lee, Andrew Lawson*

- `1408.1191v2` - [abs](http://arxiv.org/abs/1408.1191v2) - [pdf](http://arxiv.org/pdf/1408.1191v2)

> In epidemiological disease mapping one aims to estimate the spatio-temporal pattern in disease risk and identify high-risk clusters, allowing health interventions to be appropriately targeted. Bayesian spatio-temporal models are used to estimate smoothed risk surfaces, but this is contrary to the aim of identifying groups of areal units that exhibit elevated risks compared with their neighbours. Therefore, in this paper we propose a new Bayesian hierarchical modelling approach for simultaneously estimating disease risk and identifying high-risk clusters in space and time. Inference for this model is based on Markov chain Monte Carlo simulation, using the freely available R package CARBayesST that has been developed in conjunction with this paper. Our methodology is motivated by two case studies, the first of which assesses if there is a relationship between Public health Districts and colon cancer clusters in Georgia, while the second looks at the impact of the smoking ban in public places in England on cardiovascular disease clusters.

</details>

<details>

<summary>2014-11-10 13:01:10 - Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior</summary>

- *Zahra Sabetsarvestani, Hamidreza Amindavar*

- `1411.2405v1` - [abs](http://arxiv.org/abs/1411.2405v1) - [pdf](http://arxiv.org/pdf/1411.2405v1)

> In this paper, the use of the Generalized Beta Mixture (GBM) and Horseshoe distributions as priors in the Bayesian Compressive Sensing framework is proposed. The distributions are considered in a two-layer hierarchical model, making the corresponding inference problem amenable to Expectation Maximization (EM). We present an explicit, algebraic EM-update rule for the models, yielding two fast and experimentally validated algorithms for signal recovery. Experimental results show that our algorithms outperform state-of-the-art methods on a wide range of sparsity levels and amplitudes in terms of reconstruction accuracy, convergence rate and sparsity. The largest improvement can be observed for sparse signals with high amplitudes.

</details>

<details>

<summary>2014-11-11 09:05:49 - Quantiles as minimizers</summary>

- *Michel Valadier*

- `1411.2732v1` - [abs](http://arxiv.org/abs/1411.2732v1) - [pdf](http://arxiv.org/pdf/1411.2732v1)

> A real random variable admits median(s) and quantiles. These values minimize convex functions on $\mathbb R$. We show by "Convex Analysis" arguments that the function to be minimized is very natural. The relationship with some notions about functions of bounded variation developed by J.J.~Moreau is emphasized.

</details>

<details>

<summary>2014-11-11 18:42:05 - Marginal Pseudo-Likelihood Learning of Markov Network structures</summary>

- *Johan Pensar, Henrik Nyman, Juha Niiranen, Jukka Corander*

- `1401.4988v2` - [abs](http://arxiv.org/abs/1401.4988v2) - [pdf](http://arxiv.org/pdf/1401.4988v2)

> Undirected graphical models known as Markov networks are popular for a wide variety of applications ranging from statistical physics to computational biology. Traditionally, learning of the network structure has been done under the assumption of chordality which ensures that efficient scoring methods can be used. In general, non-chordal graphs have intractable normalizing constants which renders the calculation of Bayesian and other scores difficult beyond very small-scale systems. Recently, there has been a surge of interest towards the use of regularized pseudo-likelihood methods for structural learning of large-scale Markov network models, as such an approach avoids the assumption of chordality. The currently available methods typically necessitate the use of a tuning parameter to adapt the level of regularization for a particular dataset, which can be optimized for example by cross-validation. Here we introduce a Bayesian version of pseudo-likelihood scoring of Markov networks, which enables an automatic regularization through marginalization over the nuisance parameters in the model. We prove consistency of the resulting MPL estimator for the network structure via comparison with the pseudo information criterion. Identification of the MPL-optimal network on a prescanned graph space is considered with both greedy hill climbing and exact pseudo-Boolean optimization algorithms. We find that for reasonable sample sizes the hill climbing approach most often identifies networks that are at a negligible distance from the restricted global optimum. Using synthetic and existing benchmark networks, the marginal pseudo-likelihood method is shown to generally perform favorably against recent popular inference methods for Markov networks.

</details>

<details>

<summary>2014-11-11 20:56:09 - Variability as a Predictor: A Bayesian Variability Model for Small Samples and Few Repeated Measures</summary>

- *Joshua F. Wiley, Bei Bei, John Trinder, Rachel Manber*

- `1411.2961v1` - [abs](http://arxiv.org/abs/1411.2961v1) - [pdf](http://arxiv.org/pdf/1411.2961v1)

> Whilst most psychological research focuses on differences in means, a growing body of literature demonstrates the value of considering differences in intra-individual variability. Compared to the number of methods available for analyzing mean differences, there is a paucity of methods available for analyzing intra-individual variability, particularly when variability is treated as a predictor. In the present article, we first reviewed methods of analyzing intra-individual variability as an outcome, including the individual standard deviation (ISD) and some recent methods. We then introduced a novel Bayesian method for analyzing intra-individual variability as a predictor. To make this method easily accessible to the research community, we developed an open source R package, VARIAN. To compare the accuracy of parameter estimates using the proposed Bayesian analysis against the ISD as a predictor in a regression, we carried out a simulation study. We then demonstrated, using empirical data, how the estimated intra-individual variability derived from the proposed Bayesian analysis can be used to answer the following two questions: (1) is intra-individual variability in daily time-in-bed associated with subjective sleep quality? (2) does subjective sleep quality mediate the association between time-in-bed variability and depressive symptoms? We concluded with a discussion of methodological and practical considerations that can help guide researchers in choosing methods for evaluating intra-individual variability.

</details>

<details>

<summary>2014-11-12 18:46:14 - On the Super-Additivity and Estimation Biases of Quantile Contributions</summary>

- *Nassim N Taleb, Raphael Douady*

- `1405.1791v3` - [abs](http://arxiv.org/abs/1405.1791v3) - [pdf](http://arxiv.org/pdf/1405.1791v3)

> Sample measures of top centile contributions to the total (concentration) are downward biased, unstable estimators, extremely sensitive to sample size and concave in accounting for large deviations. It makes them particularly unfit in domains with power law tails, especially for low values of the exponent. These estimators can vary over time and increase with the population size, as shown in this article, thus providing the illusion of structural changes in concentration. They are also inconsistent under aggregation and mixing distributions, as the weighted average of concentration measures for A and B will tend to be lower than that from A U B. In addition, it can be shown that under such fat tails, increases in the total sum need to be accompanied by increased sample size of the concentration measurement. We examine the estimation superadditivity and bias under homogeneous and mixed distributions.

</details>

<details>

<summary>2014-11-13 01:01:01 - Multi-view Anomaly Detection via Probabilistic Latent Variable Models</summary>

- *Tomoharu Iwata, Makoto Yamada*

- `1411.3413v1` - [abs](http://arxiv.org/abs/1411.3413v1) - [pdf](http://arxiv.org/pdf/1411.3413v1)

> We propose a nonparametric Bayesian probabilistic latent variable model for multi-view anomaly detection, which is the task of finding instances that have inconsistent views. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies and imputing missing values in multi-view data with anomalies.

</details>

<details>

<summary>2014-11-13 09:55:51 - Nonlinear predictive latent process models for integrating spatio-temporal exposure data from multiple sources</summary>

- *Nikolay Bliznyuk, Christopher J. Paciorek, Joel Schwartz, Brent Coull*

- `1411.3479v1` - [abs](http://arxiv.org/abs/1411.3479v1) - [pdf](http://arxiv.org/pdf/1411.3479v1)

> Spatio-temporal prediction of levels of an environmental exposure is an important problem in environmental epidemiology. Our work is motivated by multiple studies on the spatio-temporal distribution of mobile source, or traffic related, particles in the greater Boston area. When multiple sources of exposure information are available, a joint model that pools information across sources maximizes data coverage over both space and time, thereby reducing the prediction error. We consider a Bayesian hierarchical framework in which a joint model consists of a set of submodels, one for each data source, and a model for the latent process that serves to relate the submodels to one another. If a submodel depends on the latent process nonlinearly, inference using standard MCMC techniques can be computationally prohibitive. The implications are particularly severe when the data for each submodel are aggregated at different temporal scales. To make such problems tractable, we linearize the nonlinear components with respect to the latent process and induce sparsity in the covariance matrix of the latent process using compactly supported covariance functions. We propose an efficient MCMC scheme that takes advantage of these approximations. We use our model to address a temporal change of support problem whereby interest focuses on pooling daily and multiday black carbon readings in order to maximize the spatial coverage of the study region.

</details>

<details>

<summary>2014-11-13 10:11:57 - Joint modeling of multiple time series via the beta process with application to motion capture segmentation</summary>

- *Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, Michael I. Jordan*

- `1308.4747v3` - [abs](http://arxiv.org/abs/1308.4747v3) - [pdf](http://arxiv.org/pdf/1308.4747v3)

> We propose a Bayesian nonparametric approach to the problem of jointly modeling multiple related time series. Our model discovers a latent set of dynamical behaviors shared among the sequences, and segments each time series into regions defined by a subset of these behaviors. Using a beta process prior, the size of the behavior set and the sharing pattern are both inferred from data. We develop Markov chain Monte Carlo (MCMC) methods based on the Indian buffet process representation of the predictive distribution of the beta process. Our MCMC inference algorithm efficiently adds and removes behaviors via novel split-merge moves as well as data-driven birth and death proposals, avoiding the need to consider a truncated model. We demonstrate promising results on unsupervised segmentation of human motion capture data.

</details>

<details>

<summary>2014-11-13 11:02:54 - Importance sampling schemes for evidence approximation in mixture models</summary>

- *Jeong Eun Lee, Christian P. Robert*

- `1311.6000v2` - [abs](http://arxiv.org/abs/1311.6000v2) - [pdf](http://arxiv.org/pdf/1311.6000v2)

> The marginal likelihood is a central tool for drawing Bayesian inference about the number of components in mixture models. It is often approximated since the exact form is unavailable. A bias in the approximation may be due to an incomplete exploration by a simulated Markov chain (e.g., a Gibbs sequence) of the collection of posterior modes, a phenomenon also known as lack of label switching, as all possible label permutations must be simulated by a chain in order to converge and hence overcome the bias. In an importance sampling approach, imposing label switching to the importance function results in an exponential increase of the computational cost with the number of components. In this paper, two importance sampling schemes are proposed through choices for the importance function; a MLE proposal and a Rao-Blackwellised importance function. The second scheme is called dual importance sampling. We demonstrate that this dual importance sampling is a valid estimator of the evidence and moreover show that the statistical efficiency of estimates increases. To reduce the induced high demand in computation, the original importance function is approximated but a suitable approximation can produce an estimate with the same precision and with reduced computational workload.

</details>

<details>

<summary>2014-11-13 17:59:16 - Dynamic Bayesian Nonlinear Calibration</summary>

- *Derick L. Rivers, Edward L. Boone*

- `1411.3637v1` - [abs](http://arxiv.org/abs/1411.3637v1) - [pdf](http://arxiv.org/pdf/1411.3637v1)

> Statistical calibration where the curve is nonlinear is important in many areas, such as analytical chemistry and radiometry. Especially in radiometry, instrument characteristics change over time, thus calibration is a process that must be conducted as long as the instrument is in use. We propose a dynamic Bayesian method to perform calibration in the presence of a curvilinear relationship between the reference measurements and the response variable. The dynamic calibration approach adequately derives time dependent calibration distributions in the presence of drifting regression parameters. The method is applied to microwave radiometer data and simulated spectroscopy data based on work by Lundberg and de Mar\'{e} (1980).

</details>

<details>

<summary>2014-11-14 19:40:26 - Detecting change points in the large-scale structure of evolving networks</summary>

- *Leto Peel, Aaron Clauset*

- `1403.0989v2` - [abs](http://arxiv.org/abs/1403.0989v2) - [pdf](http://arxiv.org/pdf/1403.0989v2)

> Interactions among people or objects are often dynamic in nature and can be represented as a sequence of networks, each providing a snapshot of the interactions over a brief period of time. An important task in analyzing such evolving networks is change-point detection, in which we both identify the times at which the large-scale pattern of interactions changes fundamentally and quantify how large and what kind of change occurred. Here, we formalize for the first time the network change-point detection problem within an online probabilistic learning framework and introduce a method that can reliably solve it. This method combines a generalized hierarchical random graph model with a Bayesian hypothesis test to quantitatively determine if, when, and precisely how a change point has occurred. We analyze the detectability of our method using synthetic data with known change points of different types and magnitudes, and show that this method is more accurate than several previously used alternatives. Applied to two high-resolution evolving social networks, this method identifies a sequence of change points that align with known external "shocks" to these networks.

</details>

<details>

<summary>2014-11-14 23:01:38 - A framework for studying synaptic plasticity with neural spike train data</summary>

- *Scott W. Linderman, Christopher H. Stock, Ryan P. Adams*

- `1411.4077v1` - [abs](http://arxiv.org/abs/1411.4077v1) - [pdf](http://arxiv.org/pdf/1411.4077v1)

> Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.

</details>

<details>

<summary>2014-11-15 01:53:29 - Spatiotemporal Sparse Bayesian Learning with Applications to Compressed Sensing of Multichannel Physiological Signals</summary>

- *Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao*

- `1404.5122v2` - [abs](http://arxiv.org/abs/1404.5122v2) - [pdf](http://arxiv.org/pdf/1404.5122v2)

> Energy consumption is an important issue in continuous wireless telemonitoring of physiological signals. Compressed sensing (CS) is a promising framework to address it, due to its energy-efficient data compression procedure. However, most CS algorithms have difficulty in data recovery due to non-sparsity characteristic of many physiological signals. Block sparse Bayesian learning (BSBL) is an effective approach to recover such signals with satisfactory recovery quality. However, it is time-consuming in recovering multichannel signals, since its computational load almost linearly increases with the number of channels.   This work proposes a spatiotemporal sparse Bayesian learning algorithm to recover multichannel signals simultaneously. It not only exploits temporal correlation within each channel signal, but also exploits inter-channel correlation among different channel signals. Furthermore, its computational load is not significantly affected by the number of channels. The proposed algorithm was applied to brain computer interface (BCI) and EEG-based driver's drowsiness estimation. Results showed that the algorithm had both better recovery performance and much higher speed than BSBL. Particularly, the proposed algorithm ensured that the BCI classification and the drowsiness estimation had little degradation even when data were compressed by 80%, making it very suitable for continuous wireless telemonitoring of multichannel signals.

</details>

<details>

<summary>2014-11-17 21:00:10 - A unifying framework for relaxations of the causal assumptions in Bell's theorem</summary>

- *Rafael Chaves, Richard Kueng, Jonatan Bohr Brask, David Gross*

- `1411.4648v1` - [abs](http://arxiv.org/abs/1411.4648v1) - [pdf](http://arxiv.org/pdf/1411.4648v1)

> Bell's Theorem shows that quantum mechanical correlations can violate the constraints that the causal structure of certain experiments impose on any classical explanation. It is thus natural to ask to which degree the causal assumptions -- e.g. locality or measurement independence -- have to be relaxed in order to allow for a classical description of such experiments. Here, we develop a conceptual and computational framework for treating this problem. We employ the language of Bayesian networks to systematically construct alternative causal structures and bound the degree of relaxation using quantitative measures that originate from the mathematical theory of causality. The main technical insight is that the resulting problems can often be expressed as computationally tractable linear programs. We demonstrate the versatility of the framework by applying it to a variety of scenarios, ranging from relaxations of the measurement independence, locality and bilocality assumptions, to a novel causal interpretation of CHSH inequality violations.

</details>

<details>

<summary>2014-11-18 01:54:21 - Predictive Inference for Spatio-temporal Precipitation Data and Its Extremes</summary>

- *Yang Liu, Philip Kokic*

- `1411.4715v1` - [abs](http://arxiv.org/abs/1411.4715v1) - [pdf](http://arxiv.org/pdf/1411.4715v1)

> Modelling of precipitation and its extremes is important for urban and agriculture planning purposes. We present a method for producing spatial predictions and measures of uncertainty for spatio-temporal data that is heavy-tailed and subject to substaintial skewness which often arise in measurements of many environmental processes, and we apply the method to precipitation data in south-west Western Australia. A generalised hyperbolic Bayesian hierarchical model is constructed for the intensity, frequency and duration of daily precipitation, including the extremes. Unlike models based on extreme value theory, which only model maxima of finite-sized blocks or exceedances above a large threshold, the proposed model uses all the data available efficiently, and hence not only fits the extremes but also models the entire rainfall distribution. It captures spatial and temporal clustering, as well as spatially and temporally varying volatility and skewness. The model assumes that the regional precipitation is driven by a latent process characterised by geographical and climatological covariates. Effects not fully described by the covariates are captured by spatial and temporal structure in the hierarchies. Inference is provided by MCMC using a Metropolis-Hastings algorithm and spatial interpolation method, which provide a natural approach for estimating uncertainty. Similarly both spatial and temporal predictions with uncertainty can be produced with the model.

</details>

<details>

<summary>2014-11-18 03:12:37 - Smoothed Gradients for Stochastic Variational Inference</summary>

- *Stephan Mandt, David Blei*

- `1406.3650v2` - [abs](http://arxiv.org/abs/1406.3650v2) - [pdf](http://arxiv.org/pdf/1406.3650v2)

> Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.

</details>

<details>

<summary>2014-11-18 12:28:16 - Quantile of a Mixture</summary>

- *Carole Bernard, Steven Vanduffel*

- `1411.4824v1` - [abs](http://arxiv.org/abs/1411.4824v1) - [pdf](http://arxiv.org/pdf/1411.4824v1)

> In this note, we give an explicit expression for the quantile of a mixture of two random variables. We carefully examine all possible cases of discrete and continuous variables with possibly unbounded support. The result is useful for finding bounds on the Value-at-Risk of risky portfolios when only partial information is available (Bernard and Vanduffel (2014)).

</details>

<details>

<summary>2014-11-18 13:23:11 - The NLMS algorithm with time-variant optimum stepsize derived from a Bayesian network perspective</summary>

- *Christian Huemmer, Roland Maas, Walter Kellermann*

- `1411.4834v1` - [abs](http://arxiv.org/abs/1411.4834v1) - [pdf](http://arxiv.org/pdf/1411.4834v1)

> In this article, we derive a new stepsize adaptation for the normalized least mean square algorithm (NLMS) by describing the task of linear acoustic echo cancellation from a Bayesian network perspective. Similar to the well-known Kalman filter equations, we model the acoustic wave propagation from the loudspeaker to the microphone by a latent state vector and define a linear observation equation (to model the relation between the state vector and the observation) as well as a linear process equation (to model the temporal progress of the state vector). Based on additional assumptions on the statistics of the random variables in observation and process equation, we apply the expectation-maximization (EM) algorithm to derive an NLMS-like filter adaptation. By exploiting the conditional independence rules for Bayesian networks, we reveal that the resulting EM-NLMS algorithm has a stepsize update equivalent to the optimal-stepsize calculation proposed by Yamamoto and Kitayama in 1982, which has been adopted in many textbooks. As main difference, the instantaneous stepsize value is estimated in the M step of the EM algorithm (instead of being approximated by artificially extending the acoustic echo path). The EM-NLMS algorithm is experimentally verified for synthesized scenarios with both, white noise and male speech as input signal.

</details>

<details>

<summary>2014-11-18 14:05:23 - Modelling and analysis of time in-homogeneous recurrent event processes in a heterogeneous population: A case study of HRTs</summary>

- *Madhuchhanda Bhattacharjee, Elja Arjas*

- `1411.4846v1` - [abs](http://arxiv.org/abs/1411.4846v1) - [pdf](http://arxiv.org/pdf/1411.4846v1)

> In this work we present a method for the statistical analysis of continually monitored data arising in a recurrent diseases problem. The model enables individual level inference in the presence of time transience and population heterogeneity. This is achieved by applying Bayesian hierarchical modelling, where marked point processes are used as descriptions of the individual data, with latent variables providing a means of modelling long range dependence and transience over time. In addition to providing a sound probabilistic formulation of a rather complex data set, the proposed method is also successful in prediction of future outcomes. Computational difficulties arising from the analytic intractability of this Bayesian model were solved by implementing the method into the BUGS software and using standard computational facilities.   We illustrate this approach by an analysis of a data set on hormone replacement therapies (HRTs). The data contain, in the form of diaries on bleeding patterns maintained by individual patients, detailed information on how they responded to different HRTs. The proposed model is able to capture the essential features of these treatments as well as provide realistic individual level predictions on the future bleeding patterns.

</details>

<details>

<summary>2014-11-19 08:54:26 - Least quantile regression via modern optimization</summary>

- *Dimitris Bertsimas, Rahul Mazumder*

- `1310.8625v2` - [abs](http://arxiv.org/abs/1310.8625v2) - [pdf](http://arxiv.org/pdf/1310.8625v2)

> We address the Least Quantile of Squares (LQS) (and in particular the Least Median of Squares) regression problem using modern optimization methods. We propose a Mixed Integer Optimization (MIO) formulation of the LQS problem which allows us to find a provably global optimal solution for the LQS problem. Our MIO framework has the appealing characteristic that if we terminate the algorithm early, we obtain a solution with a guarantee on its sub-optimality. We also propose continuous optimization methods based on first-order subdifferential methods, sequential linear optimization and hybrid combinations of them to obtain near optimal solutions to the LQS problem. The MIO algorithm is found to benefit significantly from high quality solutions delivered by our continuous optimization based methods. We further show that the MIO approach leads to (a) an optimal solution for any dataset, where the data-points $(y_i,\mathbf{x}_i)$'s are not necessarily in general position, (b) a simple proof of the breakdown point of the LQS objective value that holds for any dataset and (c) an extension to situations where there are polyhedral constraints on the regression coefficient vector. We report computational results with both synthetic and real-world datasets showing that the MIO algorithm with warm starts from the continuous optimization methods solve small ($n=100$) and medium ($n=500$) size problems to provable optimality in under two hours, and outperform all publicly available methods for large-scale ($n={}$10,000) LQS problems.

</details>

<details>

<summary>2014-11-19 15:59:55 - Structural Change in Sparsity</summary>

- *Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin*

- `1411.3062v2` - [abs](http://arxiv.org/abs/1411.3062v2) - [pdf](http://arxiv.org/pdf/1411.3062v2)

> In the high-dimensional sparse modeling literature, it has been crucially assumed that the sparsity structure of the model is homogeneous over the entire population. That is, the identities of important regressors are invariant across the population and across the individuals in the collected sample. In practice, however, the sparsity structure may not always be invariant in the population, due to heterogeneity across different sub-populations. We consider a general, possibly non-smooth M-estimation framework, allowing a possible structural change regarding the identities of important regressors in the population. Our penalized M-estimator not only selects covariates but also discriminates between a model with homogeneous sparsity and a model with a structural change in sparsity. As a result, it is not necessary to know or pretest whether the structural change is present, or where it occurs. We derive asymptotic bounds on the estimation loss of the penalized M-estimators, and achieve the oracle properties. We also show that when there is a structural change, the estimator of the threshold parameter is super-consistent. If the signal is relatively strong, the rates of convergence can be further improved and asymptotic distributional properties of the estimators including the threshold estimator can be established using an adaptive penalization. The proposed methods are then applied to quantile regression and logistic regression models and are illustrated via Monte Carlo experiments.

</details>

<details>

<summary>2014-11-19 22:40:39 - SIMD Parallel MCMC Sampling with Applications for Big-Data Bayesian Analytics</summary>

- *Alireza S. Mahani, Mansour T. A. Sharabiani*

- `1310.1537v2` - [abs](http://arxiv.org/abs/1310.1537v2) - [pdf](http://arxiv.org/pdf/1310.1537v2)

> Computational intensity and sequential nature of estimation techniques for Bayesian methods in statistics and machine learning, combined with their increasing applications for big data analytics, necessitate both the identification of potential opportunities to parallelize techniques such as MCMC sampling, and the development of general strategies for mapping such parallel algorithms to modern CPUs in order to elicit the performance up the compute-based and/or memory-based hardware limits. Two opportunities for Single-Instruction Multiple-Data (SIMD) parallelization of MCMC sampling for probabilistic graphical models are presented. In exchangeable models with many observations such as Bayesian Generalized Linear Models, child-node contributions to the conditional posterior of each node can be calculated concurrently. In undirected graphs with discrete nodes, concurrent sampling of conditionally-independent nodes can be transformed into a SIMD form. High-performance libraries with multi-threading and vectorization capabilities can be readily applied to such SIMD opportunities to gain decent speedup, while a series of high-level source-code and runtime modifications provide further performance boost by reducing parallelization overhead and increasing data locality for NUMA architectures. For big-data Bayesian GLM graphs, the end-result is a routine for evaluating the conditional posterior and its gradient vector that is 5 times faster than a naive implementation using (built-in) multi-threaded Intel MKL BLAS, and reaches within the striking distance of the memory-bandwidth-induced hardware limit. The proposed optimization strategies improve the scaling of performance with number of cores and width of vector units (applicable to many-core SIMD processors such as Intel Xeon Phi and GPUs), resulting in cost-effectiveness, energy efficiency, and higher speed on multi-core x86 processors.

</details>

<details>

<summary>2014-11-21 07:37:34 - Bayesian modeling of bacterial growth for multiple populations</summary>

- *A. Paula Palacios, J. Miguel MarÃ­n, Emiliano J. Quinto, Michael P. Wiper*

- `1411.5780v1` - [abs](http://arxiv.org/abs/1411.5780v1) - [pdf](http://arxiv.org/pdf/1411.5780v1)

> Bacterial growth models are commonly used for the prediction of microbial safety and the shelf life of perishable foods. Growth is affected by several environmental factors such as temperature, acidity level and salt concentration. In this study, we develop two models to describe bacterial growth for multiple populations under both equal and different environmental conditions. First, a semi-parametric model based on the Gompertz equation is proposed. Assuming that the parameters of the Gompertz equation may vary in relation to the running conditions under which the experiment is performed, we use feedforward neural networks to model the influence of these environmental factors on the growth parameters. Second, we propose a more general model which does not assume any underlying parametric form for the growth function. Thus, we consider a neural network as a primary growth model which includes the influencing environmental factors as inputs to the network. One of the main disadvantages of neural networks models is that they are often very difficult to tune, which complicates fitting procedures. Here, we show that a simple Bayesian approach to fitting these models can be implemented via the software package WinBugs. Our approach is illustrated using real experimental Listeria monocytogenes growth data.

</details>

<details>

<summary>2014-11-21 14:14:07 - Bayesian sparse graphical models for classification with application to protein expression data</summary>

- *Veerabhadran Baladandayuthapani, Rajesh Talluri, Yuan Ji, Kevin R. Coombes, Yiling Lu, Bryan T. Hennessy, Michael A. Davies, Bani K. Mallick*

- `1403.7672v2` - [abs](http://arxiv.org/abs/1403.7672v2) - [pdf](http://arxiv.org/pdf/1403.7672v2)

> Reverse-phase protein array (RPPA) analysis is a powerful, relatively new platform that allows for high-throughput, quantitative analysis of protein networks. One of the challenges that currently limit the potential of this technology is the lack of methods that allow for accurate data modeling and identification of related networks and samples. Such models may improve the accuracy of biological sample classification based on patterns of protein network activation and provide insight into the distinct biological relationships underlying different types of cancer. Motivated by RPPA data, we propose a Bayesian sparse graphical modeling approach that uses selection priors on the conditional relationships in the presence of class information. The novelty of our Bayesian model lies in the ability to draw information from the network data as well as from the associated categorical outcome in a unified hierarchical model for classification. In addition, our method allows for intuitive integration of a priori network information directly in the model and allows for posterior inference on the network topologies both within and between classes. Applying our methodology to an RPPA data set generated from panels of human breast cancer and ovarian cancer cell lines, we demonstrate that the model is able to distinguish the different cancer cell types more accurately than several existing models and to identify differential regulation of components of a critical signaling network (the PI3K-AKT pathway) between these two types of cancer. This approach represents a powerful new tool that can be used to improve our understanding of protein networks in cancer.

</details>

<details>

<summary>2014-11-22 17:43:03 - BayesCAT: Bayesian Co-estimation of Alignment and Tree</summary>

- *Heejung Shim, Bret Larget*

- `1411.6150v1` - [abs](http://arxiv.org/abs/1411.6150v1) - [pdf](http://arxiv.org/pdf/1411.6150v1)

> Traditionally, phylogeny and sequence alignment are estimated separately: first estimate a multiple sequence alignment and then infer a phylogeny based on the sequence alignment estimated in the previous step. However, uncertainty in the alignment estimation is ignored, resulting, possibly, in overstated certainty in phylogeny estimates. We develop a joint model for co-estimating phylogeny and sequence alignment which improves estimates from the traditional approach by accounting for uncertainty in the alignment in phylogenetic inferences. Our insertion and deletion (indel) model allows arbitrary-length overlapping indel events and a general distribution for indel fragment size. We employ a Bayesian approach using MCMC to estimate the joint posterior distribution of a phylogenetic tree and a multiple sequence alignment. Our approach has a tree and a complete history of indel events mapped onto the tree as the state space of the Markov Chain while alternative previous approaches have a tree and an alignment. A large state space containing a complete history of indel events makes our MCMC approach more challenging, but it enables us to infer more information about the indel process. The performances of this joint method and traditional sequential methods are compared using simulated data as well as real data. Software named BayesCAT (Bayesian Co-estimation of Alignment and Tree) is available at https://github.com/heejungshim/BayesCAT.

</details>

<details>

<summary>2014-11-23 22:11:34 - Diversifying Sparsity Using Variational Determinantal Point Processes</summary>

- *Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis Kellis, Polina Golland, Luke Bornn*

- `1411.6307v1` - [abs](http://arxiv.org/abs/1411.6307v1) - [pdf](http://arxiv.org/pdf/1411.6307v1)

> We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy.

</details>

<details>

<summary>2014-11-24 11:29:48 - A Bayesian approach for predicting the popularity of tweets</summary>

- *Tauhid Zaman, Emily B. Fox, Eric T. Bradlow*

- `1304.6777v3` - [abs](http://arxiv.org/abs/1304.6777v3) - [pdf](http://arxiv.org/pdf/1304.6777v3)

> We predict the popularity of short messages called tweets created in the micro-blogging site known as Twitter. We measure the popularity of a tweet by the time-series path of its retweets, which is when people forward the tweet to others. We develop a probabilistic model for the evolution of the retweets using a Bayesian approach, and form predictions using only observations on the retweet times and the local network or "graph" structure of the retweeters. We obtain good step ahead forecasts and predictions of the final total number of retweets even when only a small fraction (i.e., less than one tenth) of the retweet path is observed. This translates to good predictions within a few minutes of a tweet being posted, and has potential implications for understanding the spread of broader ideas, memes, or trends in social networks.

</details>

<details>

<summary>2014-11-24 17:30:57 - Noise Benefits in Expectation-Maximization Algorithms</summary>

- *Osonde Adekorede Osoba*

- `1411.6622v1` - [abs](http://arxiv.org/abs/1411.6622v1) - [pdf](http://arxiv.org/pdf/1411.6622v1)

> This dissertation shows that careful injection of noise into sample data can substantially speed up Expectation-Maximization algorithms. Expectation-Maximization algorithms are a class of iterative algorithms for extracting maximum likelihood estimates from corrupted or incomplete data. The convergence speed-up is an example of a noise benefit or "stochastic resonance" in statistical signal processing. The dissertation presents derivations of sufficient conditions for such noise-benefits and demonstrates the speed-up in some ubiquitous signal-processing algorithms. These algorithms include parameter estimation for mixture models, the $k$-means clustering algorithm, the Baum-Welch algorithm for training hidden Markov models, and backpropagation for training feedforward artificial neural networks. This dissertation also analyses the effects of data and model corruption on the more general Bayesian inference estimation framework. The main finding is a theorem guaranteeing that uniform approximators for Bayesian model functions produce uniform approximators for the posterior pdf via Bayes theorem. This result also applies to hierarchical and multidimensional Bayesian models.

</details>

<details>

<summary>2014-11-24 19:18:39 - Characterising variation of nonparametric random probability measures using the Kullback-Leibler divergence</summary>

- *James Watson, Luis Nieto-Barajas, Chris Holmes*

- `1411.6578v1` - [abs](http://arxiv.org/abs/1411.6578v1) - [pdf](http://arxiv.org/pdf/1411.6578v1)

> This work studies the variation in Kullback-Leibler divergence between random draws from some popular nonparametric processes and their baseline measure. In particular we focus on the Dirichlet process, the P\'olya tree and the frequentist and Bayesian bootstrap. The results shed light on the support of these nonparametric processes. Of particular note are results for finite P\'olya trees that are used to model continuous random probability measures. Our results provide guidance for specifying the parameterisation of the P\'olya tree process that allows for greater understanding while highlighting limitations of the standard canonical choice of parameter settings.

</details>

<details>

<summary>2014-11-24 19:21:22 - bartMachine: Machine Learning with Bayesian Additive Regression Trees</summary>

- *Adam Kapelner, Justin Bleich*

- `1312.2171v3` - [abs](http://arxiv.org/abs/1312.2171v3) - [pdf](http://arxiv.org/pdf/1312.2171v3)

> We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data.

</details>

<details>

<summary>2014-11-24 21:27:37 - A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network Structure</summary>

- *Amir Arsalan Soltani*

- `1411.6651v1` - [abs](http://arxiv.org/abs/1411.6651v1) - [pdf](http://arxiv.org/pdf/1411.6651v1)

> In this report paper we first present a report of the Advanced Machine Learning Course Project on the provided data set and then present a novel heuristic algorithm for exact Bayesian network (BN) structure discovery that uses decomposable scoring functions. Our algorithm follows a different approach to solve the problem of BN structure discovery than the previously used methods such as Dynamic Programming (DP) and Branch and Bound to reduce the search space and find the global optima space for the problem. The algorithm we propose has some degree of flexibility that can make it more or less greedy. The more the algorithm is set to be greedy, the more the speed of the algorithm will be, and the less optimal the final structure. Our algorithm runs in a much less time than the previously known methods and guarantees to have an optimality of close to 99%. Therefore, it sacrifices less than one percent of score of an optimal structure in order to gain a much lower running time and make the algorithm feasible for large data sets (we may note that we never used any toolbox except for result validation)

</details>

<details>

<summary>2014-11-24 22:41:08 - Implications of uniformly distributed, empirically informed priors for phylogeographical model selection: A reply to Hickerson et al</summary>

- *Jamie R. Oaks, Charles W. Linkem, Jeet Sukumaran*

- `1402.6397v2` - [abs](http://arxiv.org/abs/1402.6397v2) - [pdf](http://arxiv.org/pdf/1402.6397v2)

> Establishing that a set of population-splitting events occurred at the same time can be a potentially persuasive argument that a common process affected the populations. Oaks et al. (2013) assessed the ability of an approximate-Bayesian method (msBayes) to estimate such a pattern of simultaneous divergence across taxa, to which Hickerson et al. (2014) responded. Both papers agree the method is sensitive to prior assumptions and often erroneously supports shared divergences; the papers differ about the explanation and solution. Oaks et al. (2013) suggested the method's behavior is caused by the strong weight of uniform priors on divergence times leading to smaller marginal likelihoods of models with more divergence-time parameters (Hypothesis 1); they proposed alternative priors to avoid strongly weighted posteriors. Hickerson et al. (2014) suggested numerical approximation error causes msBayes analyses to be biased toward models of clustered divergences (Hypothesis 2); they proposed using narrow, empirical uniform priors. Here, we demonstrate that the approach of Hickerson et al. (2014) does not mitigate the method's tendency to erroneously support models of clustered divergences, and often excludes the true parameter values. Our results also show that the tendency of msBayes analyses to support models of shared divergences is primarily due to Hypothesis 1. This series of papers demonstrate that if our prior assumptions place too much weight in unlikely regions of parameter space such that the exact posterior supports the wrong model of evolutionary history, no amount of computation can rescue our inference. Fortunately, more flexible distributions that accommodate prior uncertainty about parameters without placing excessive weight in vast regions of parameter space with low likelihood increase the method's robustness and power to detect temporal variation in divergences.

</details>

<details>

<summary>2014-11-25 18:04:24 - Bayesian nonparametric estimation of Tsallis diversity indices under Gnedin-Pitman priors</summary>

- *Annalisa Cerquetti*

- `1404.3441v2` - [abs](http://arxiv.org/abs/1404.3441v2) - [pdf](http://arxiv.org/pdf/1404.3441v2)

> Tsallis entropy is a generalized diversity index first derived in Patil and Taillie (1982) and then rediscovered in community ecology by Keylock (2005). Bayesian nonparametric estimation of Shannon entropy and Simpson's diversity under uniform and symmetric Dirichlet priors has been already advocated as an alternative to maximum likelihood estimation based on frequency counts, which is negatively biased in the undersampled regime. Here we present a fully general Bayesian nonparametric estimation of the whole class of Tsallis diversity indices under Gnedin-Pitman priors, a large family of random discrete distributions recently deeply investigated in posterior predictive species richness and discovery probability estimation. We provide both prior and posterior analysis. The results, illustrated through examples and an application to a real dataset, show the procedure is easily implementable, flexible and overcomes limitations of previous frequentist and Bayesian solutions.

</details>

<details>

<summary>2014-11-25 20:28:28 - Additive Gaussian Process Regression</summary>

- *Shaan Qamar, Surya T. Tokdar*

- `1411.7009v1` - [abs](http://arxiv.org/abs/1411.7009v1) - [pdf](http://arxiv.org/pdf/1411.7009v1)

> Additive-interactive regression has recently been shown to offer attractive minimax error rates over traditional nonparametric multivariate regression in a wide variety of settings, including cases where the predictor count is much larger than the sample size and many of the predictors have important effects on the response, potentially through complex interactions. We present a Bayesian implementation of additive-interactive regression using an additive Gaussian process (AGP) prior and develop an efficient Markov chain sampler that extends stochastic search variable selection in this setting. Careful prior and hyper-parameter specification are developed in light of performance and computational considerations, and key innovations address difficulties in exploring a joint posterior distribution over multiple subsets of high dimensional predictor inclusion vectors. The method offers state-of-the-art support and interaction recovery while improving dramatically over competitors in terms of prediction accuracy on a diverse set of simulated and real data. Results from real data studies provide strong evidence that the additive-interactive framework is an attractive modeling platform for high-dimensional nonparametric regression.

</details>

<details>

<summary>2014-11-27 07:26:12 - Type Ia Supernova Colors and Ejecta Velocities: Hierarchical Bayesian Regression with Non-Gaussian Distributions</summary>

- *Kaisey S. Mandel, Ryan J. Foley, Robert P. Kirshner*

- `1402.7079v3` - [abs](http://arxiv.org/abs/1402.7079v3) - [pdf](http://arxiv.org/pdf/1402.7079v3)

> We investigate the statistical dependence of the peak intrinsic colors of Type Ia supernovae (SN Ia) on their expansion velocities at maximum light, measured from the Si II 6355 spectral feature. We construct a new hierarchical Bayesian regression model, accounting for the random effects of intrinsic scatter, measurement error, and reddening by host galaxy dust, and implement a Gibbs sampler and deviance information criteria to estimate the correlation. The method is applied to the apparent colors from BVRI light curves and Si II velocity data for 79 nearby SNe Ia. The apparent color distributions of high (HV) and normal velocity (NV) supernovae exhibit significant discrepancies for B-V and B-R, but not other colors. Hence, they are likely due to intrinsic color differences originating in the B-band, rather than dust reddening. The mean intrinsic B-V and B-R color differences between HV and NV groups are 0.06 +/- 0.02 and 0.09 +/- 0.02 mag, respectively. A linear model finds significant slopes of -0.021 +/- 0.006 and -0.030 +/- 0.009 mag/(1000 km/s) for intrinsic B-V and B-R colors versus velocity, respectively. Since the ejecta velocity distribution is skewed towards high velocities, these effects imply non-Gaussian intrinsic color distributions with skewness up to +0.3. Accounting for the intrinsic color-velocity correlation results in corrections to A_V extinction estimates as large as -0.12 mag for HV SNe Ia and +0.06 mag for NV events. Velocity measurements from SN Ia spectra have potential to diminish systematic errors from the confounding of intrinsic colors and dust reddening affecting supernova distances.

</details>

<details>

<summary>2014-11-27 13:40:22 - Estimating Operational Risk Capital with Greater Accuracy, Precision, and Robustness</summary>

- *J. D. Opdyke*

- `1406.0389v6` - [abs](http://arxiv.org/abs/1406.0389v6) - [pdf](http://arxiv.org/pdf/1406.0389v6)

> The largest US banks are required by regulatory mandate to estimate the operational risk capital they must hold using an Advanced Measurement Approach (AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution Approach (LDA) which defines the aggregate loss distribution as the convolution of a frequency and a severity distribution representing the number and magnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th percentile) estimate of this annual loss distribution. In practice, the severity distribution drives the capital estimate, which is essentially a very high quantile of the estimated severity distribution. Unfortunately, because the relevant severities are heavy-tailed AND the quantiles being estimated are so high, VaR always appears to be a convex function of the severity parameters, causing all widely-used estimators to generate biased capital estimates (apparently) due to Jensen's Inequality. The observed capital inflation is sometimes enormous, even at the unit-of-measure (UoM) level (even billions USD). Herein I present an estimator of capital that essentially eliminates this upward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with the regulatory intent of the LDA framework than implementations that fail to mitigate this bias. RCE also notably increases the precision of the capital estimate and consistently increases its robustness to violations of the i.i.d. data presumption (which are endemic to operational risk loss event data). So with greater capital accuracy, precision, and robustness, RCE lowers capital requirements at both the UoM and enterprise levels, increases capital stability from quarter to quarter, ceteris paribus, and does both while more accurately and precisely reflecting regulatory intent. RCE is straightforward to implement using any major statistical software package.

</details>

<details>

<summary>2014-11-27 21:17:47 - A Nonparametric Bayesian Approach to Uncovering Rat Hippocampal Population Codes During Spatial Navigation</summary>

- *Scott W. Linderman, Matthew J. Johnson, Matthew A. Wilson, Zhe Chen*

- `1411.7706v1` - [abs](http://arxiv.org/abs/1411.7706v1) - [pdf](http://arxiv.org/pdf/1411.7706v1)

> Rodent hippocampal population codes represent important spatial information about the environment during navigation. Several computational methods have been developed to uncover the neural representation of spatial topology embedded in rodent hippocampal ensemble spike activity. Here we extend our previous work and propose a nonparametric Bayesian approach to infer rat hippocampal population codes during spatial navigation. To tackle the model selection problem, we leverage a nonparametric Bayesian model. Specifically, to analyze rat hippocampal ensemble spiking activity, we apply a hierarchical Dirichlet process-hidden Markov model (HDP-HMM) using two Bayesian inference methods, one based on Markov chain Monte Carlo (MCMC) and the other based on variational Bayes (VB). We demonstrate the effectiveness of our Bayesian approaches on recordings from a freely-behaving rat navigating in an open field environment. We find that MCMC-based inference with Hamiltonian Monte Carlo (HMC) hyperparameter sampling is flexible and efficient, and outperforms VB and MCMC approaches with hyperparameters set by empirical Bayes.

</details>

<details>

<summary>2014-11-28 09:00:14 - Combining regional estimation and historical floods: a multivariate semi-parametric peaks-over-threshold model with censored data</summary>

- *Anne Sabourin, Benjamin Renard*

- `1411.7782v1` - [abs](http://arxiv.org/abs/1411.7782v1) - [pdf](http://arxiv.org/pdf/1411.7782v1)

> The estimation of extreme flood quantiles is challenging due to the relative scarcity of extreme data compared to typical target return periods. Several approaches have been developed over the years to face this challenge, including regional estimation and the use of historical flood data. This paper investigates the combination of both approaches using a multivariate peaks-over-threshold model, that allows estimating altogether the intersite dependence structure and the marginal distributions at each site. The joint distribution of extremes at several sites is constructed using a semi-parametric Dirichlet Mixture model. The existence of partially missing and censored observations (historical data) is accounted for within a data augmentation scheme. This model is applied to a case study involving four catchments in Southern France, for which historical data are available since 1604. The comparison of marginal estimates from four versions of the model (with or without regionalizing the shape parameter; using or ignoring historical floods) highlights significant differences in terms of return level estimates. Moreover, the availability of historical data on several nearby catchments allows investigating the asymptotic dependence properties of extreme floods. Catchments display a a significant amount of asymptotic dependence, calling for adapted multivariate statistical models.

</details>

<details>

<summary>2014-11-28 13:38:40 - Efficient inference of overlapping communities in complex networks</summary>

- *Bjarne Ãrum Fruergaard, Tue Herlau*

- `1411.7864v1` - [abs](http://arxiv.org/abs/1411.7864v1) - [pdf](http://arxiv.org/pdf/1411.7864v1)

> We discuss two views on extending existing methods for complex network modeling which we dub the communities first and the networks first view, respectively. Inspired by the networks first view that we attribute to White, Boorman, and Breiger (1976)[1], we formulate the multiple-networks stochastic blockmodel (MNSBM), which seeks to separate the observed network into subnetworks of different types and where the problem of inferring structure in each subnetwork becomes easier. We show how this model is specified in a generative Bayesian framework where parameters can be inferred efficiently using Gibbs sampling. The result is an effective multiple-membership model without the drawbacks of introducing complex definitions of "groups" and how they interact. We demonstrate results on the recovery of planted structure in synthetic networks and show very encouraging results on link prediction performances using multiple-networks models on a number of real-world network data sets.

</details>

<details>

<summary>2014-11-29 21:45:18 - An Efficient Bayesian Inference Framework for Coalescent-Based Nonparametric Phylodynamics</summary>

- *Shiwei Lan, Julia A. Palacios, Michael Karcher, Vladimir N. Minin, Babak Shahbaba*

- `1412.0158v1` - [abs](http://arxiv.org/abs/1412.0158v1) - [pdf](http://arxiv.org/pdf/1412.0158v1)

> Phylodynamics focuses on the problem of reconstructing past population size dynamics from current genetic samples taken from the population of interest. This technique has been extensively used in many areas of biology, but is particularly useful for studying the spread of quickly evolving infectious diseases agents, e.g.,\ influenza virus. Phylodynamics inference uses a coalescent model that defines a probability density for the genealogy of randomly sampled individuals from the population. When we assume that such a genealogy is known, the coalescent model, equipped with a Gaussian process prior on population size trajectory, allows for nonparametric Bayesian estimation of population size dynamics. While this approach is quite powerful, large data sets collected during infectious disease surveillance challenge the state-of-the-art of Bayesian phylodynamics and demand computationally more efficient inference framework. To satisfy this demand, we provide a computationally efficient Bayesian inference framework based on Hamiltonian Monte Carlo for coalescent process models. Moreover, we show that by splitting the Hamiltonian function we can further improve the efficiency of this approach. Using several simulated and real datasets, we show that our method provides accurate estimates of population size dynamics and is substantially faster than alternative methods based on elliptical slice sampler and Metropolis-adjusted Langevin algorithm.

</details>


## 2014-12

<details>

<summary>2014-12-01 00:35:24 - Bernstein-von Mises Theorems for Functionals of Covariance Matrix</summary>

- *Chao Gao, Harrison H. Zhou*

- `1412.0313v1` - [abs](http://arxiv.org/abs/1412.0313v1) - [pdf](http://arxiv.org/pdf/1412.0313v1)

> We provide a general theoretical framework to derive Bernstein-von Mises theorems for matrix functionals. The conditions on functionals and priors are explicit and easy to check. Results are obtained for various functionals including entries of covariance matrix, entries of precision matrix, quadratic forms, log-determinant, eigenvalues in the Bayesian Gaussian covariance/precision matrix estimation setting, as well as for Bayesian linear and quadratic discriminant analysis.

</details>

<details>

<summary>2014-12-02 05:23:23 - Beta Process Non-negative Matrix Factorization with Stochastic Structured Mean-Field Variational Inference</summary>

- *Dawen Liang, Matthew D. Hoffman*

- `1411.1804v2` - [abs](http://arxiv.org/abs/1411.1804v2) - [pdf](http://arxiv.org/pdf/1411.1804v2)

> Beta process is the standard nonparametric Bayesian prior for latent factor model. In this paper, we derive a structured mean-field variational inference algorithm for a beta process non-negative matrix factorization (NMF) model with Poisson likelihood. Unlike the linear Gaussian model, which is well-studied in the nonparametric Bayesian literature, NMF model with beta process prior does not enjoy the conjugacy. We leverage the recently developed stochastic structured mean-field variational inference to relax the conjugacy constraint and restore the dependencies among the latent variables in the approximating variational distribution. Preliminary results on both synthetic and real examples demonstrate that the proposed inference algorithm can reasonably recover the hidden structure of the data.

</details>

<details>

<summary>2014-12-02 10:10:08 - Asymptotic theory of generalized information criterion for geostatistical regression model selection</summary>

- *Chih-Hao Chang, Hsin-Cheng Huang, Ching-Kang Ing*

- `1412.0836v1` - [abs](http://arxiv.org/abs/1412.0836v1) - [pdf](http://arxiv.org/pdf/1412.0836v1)

> Information criteria, such as Akaike's information criterion and Bayesian information criterion are often applied in model selection. However, their asymptotic behaviors for selecting geostatistical regression models have not been well studied, particularly under the fixed domain asymptotic framework with more and more data observed in a bounded fixed region. In this article, we study the generalized information criterion (GIC) for selecting geostatistical regression models under a more general mixed domain asymptotic framework. Via uniform convergence developments of some statistics, we establish the selection consistency and the asymptotic loss efficiency of GIC under some regularity conditions, regardless of whether the covariance model is correctly or wrongly specified. We further provide specific examples with different types of explanatory variables that satisfy the conditions. For example, in some situations, GIC is selection consistent, even when some spatial covariance parameters cannot be estimated consistently. On the other hand, GIC fails to select the true polynomial order consistently under the fixed domain asymptotic framework. Moreover, the growth rate of the domain and the degree of smoothness of candidate regressors in space are shown to play key roles for model selection.

</details>

<details>

<summary>2014-12-02 10:11:49 - Semi-parametric modeling of excesses above high multivariate thresholds with censored data</summary>

- *Anne Sabourin*

- `1412.0838v1` - [abs](http://arxiv.org/abs/1412.0838v1) - [pdf](http://arxiv.org/pdf/1412.0838v1)

> How to include censored data in a statistical analysis is a recur-rent issue in statistics. In multivariate extremes, the dependence structure of large observations can be characterized in terms of a non parametric angular measure, while marginal excesses above asymptotically large thresholds have a parametric distribution. In this work, a flexible semi-parametric Dirichlet mix-ture model for angular measures is adapted to the context of censored data and missing components. One major issue is to take into account censoring intervals overlapping the extremal threshold, without knowing whether the correspond-ing hidden data is actually extreme. Further, the censored likelihood needed for Bayesian inference has no analytic expression. The first issue is tackled using a Poisson process model for extremes, whereas a data augmentation scheme avoids multivariate integration of the Poisson process intensity over both the censored intervals and the failure region above threshold. The implemented MCMC algorithm allows simultaneous estimation of marginal and dependence parameters, so that all sources of uncertainty other than model bias are cap-tured by posterior credible intervals. The method is illustrated on simulated and real data.

</details>

<details>

<summary>2014-12-02 15:56:23 - Application of some new heavy-tailed survival distributions</summary>

- *Rose Baker*

- `1412.0952v1` - [abs](http://arxiv.org/abs/1412.0952v1) - [pdf](http://arxiv.org/pdf/1412.0952v1)

> Some new survival distributions are introduced based on a generalised exponential function. This class of distributions includes heavy-tailed generalisations of exponential, Weibull and gamma distributions. Properties of the distributions are described, and R code is available for computation of pdf, quantiles, inverse quantiles, random numbers, etc. A use of these distributions for robust inference is suggested, and this is exemplified with a Monte-Carlo study.

</details>

<details>

<summary>2014-12-02 23:56:49 - MAD Bayes for Tumor Heterogeneity Feature Allocation with Non-Normal Sampling</summary>

- *Yanxun Xu, Peter Mueller, Yuan Yuan, Kamalakar Gulukota, Yuan Ji*

- `1402.5090v4` - [abs](http://arxiv.org/abs/1402.5090v4) - [pdf](http://arxiv.org/pdf/1402.5090v4)

> We propose small-variance asymptotic approximations for the inference of tumor heterogeneity (TH) using next-generation sequencing data. Understanding TH is an important and open research problem in biology. The lack of appropriate statistical inference is a critical gap in existing methods that the proposed approach aims to fill. We build on a hierarchical model with an exponential family likelihood and a feature allocation prior. The proposed approach generalizes similar small-variance approximations proposed by Kulis and Jordan (2012) and Broderick et.al (2012) for inference with Dirichlet process mixture and Indian buffet prior models under normal sampling. We show that the new algorithm can successfully recover latent structures of different subclones and is also magnitude faster than available Markov chain Monte Carlo samplers, the latter often practically infeasible for high-dimensional genomics data. The proposed approach is scalable, simple to implement and benefits from the flexibility of Bayesian nonparametric models. More importantly, it provides a useful tool for the biological community for estimating cell subtypes in tumor samples.

</details>

<details>

<summary>2014-12-03 08:38:24 - Variable selection for BART: An application to gene regulation</summary>

- *Justin Bleich, Adam Kapelner, Edward I. George, Shane T. Jensen*

- `1310.4887v3` - [abs](http://arxiv.org/abs/1310.4887v3) - [pdf](http://arxiv.org/pdf/1310.4887v3)

> We consider the task of discovering gene regulatory networks, which are defined as sets of genes and the corresponding transcription factors which regulate their expression levels. This can be viewed as a variable selection problem, potentially with high dimensionality. Variable selection is especially challenging in high-dimensional settings, where it is difficult to detect subtle individual effects and interactions between predictors. Bayesian Additive Regression Trees [BART, Ann. Appl. Stat. 4 (2010) 266-298] provides a novel nonparametric alternative to parametric regression approaches, such as the lasso or stepwise regression, especially when the number of relevant predictors is sparse relative to the total number of available predictors and the fundamental relationships are nonlinear. We develop a principled permutation-based inferential approach for determining when the effect of a selected predictor is likely to be real. Going further, we adapt the BART procedure to incorporate informed prior information about variable importance. We present simulations demonstrating that our method compares favorably to existing parametric and nonparametric procedures in a variety of data settings. To demonstrate the potential of our approach in a biological context, we apply it to the task of inferring the gene regulatory network in yeast (Saccharomyces cerevisiae). We find that our BART-based procedure is best able to recover the subset of covariates with the largest signal compared to other variable selection methods. The methods developed in this work are readily available in the R package bartMachine.

</details>

<details>

<summary>2014-12-03 12:01:02 - Joint estimation of multiple related biological networks</summary>

- *Chris J. Oates, Jim Korkola, Joe W. Gray, Sach Mukherjee*

- `1302.1969v4` - [abs](http://arxiv.org/abs/1302.1969v4) - [pdf](http://arxiv.org/pdf/1302.1969v4)

> Graphical models are widely used to make inferences concerning interplay in multivariate systems. In many applications, data are collected from multiple related but nonidentical units whose underlying networks may differ but are likely to share features. Here we present a hierarchical Bayesian formulation for joint estimation of multiple networks in this nonidentically distributed setting. The approach is general: given a suitable class of graphical models, it uses an exchangeability assumption on networks to provide a corresponding joint formulation. Motivated by emerging experimental designs in molecular biology, we focus on time-course data with interventions, using dynamic Bayesian networks as the graphical models. We introduce a computationally efficient, deterministic algorithm for exact joint inference in this setting. We provide an upper bound on the gains that joint estimation offers relative to separate estimation for each network and empirical results that support and extend the theory, including an extensive simulation study and an application to proteomic data from human cancer cell lines. Finally, we describe approximations that are still more computationally efficient than the exact algorithm and that also demonstrate good empirical performance.

</details>

<details>

<summary>2014-12-04 09:39:42 - Lazy ABC</summary>

- *Dennis Prangle*

- `1405.7867v3` - [abs](http://arxiv.org/abs/1405.7867v3) - [pdf](http://arxiv.org/pdf/1405.7867v3)

> Approximate Bayesian computation (ABC) performs statistical inference for otherwise intractable probability models by accepting parameter proposals when corresponding simulated datasets are sufficiently close to the observations. Producing the large quantity of simulations needed requires considerable computing time. However, it is often clear before a simulation ends that it is unpromising: it is likely to produce a poor match or require excessive time. This paper proposes lazy ABC, an ABC importance sampling algorithm which saves time by sometimes abandoning such simulations. This makes ABC more scalable to applications where simulation is expensive. By using a random stopping rule and appropriate reweighting step, the target distribution is unchanged from that of standard ABC. Theory and practical methods to tune lazy ABC are presented and illustrated on a simple epidemic model example. They are also demonstrated on the computationally demanding spatial extremes application of Erhardt and Smith (2012), producing efficiency gains, in terms of effective sample size per unit CPU time, of roughly 3 times for a 20 location dataset, and 8 times for 35 locations.

</details>

<details>

<summary>2014-12-04 12:53:28 - A Class of Conjugate Priors Defined on the Unit Simplex</summary>

- *Xuenan Feng*

- `1412.1649v1` - [abs](http://arxiv.org/abs/1412.1649v1) - [pdf](http://arxiv.org/pdf/1412.1649v1)

> Dirichlet distribution and Dirichlet process as its infinite dimensional generalization are primarily used conjugate prior of categorical and multinomial distributions in Bayesian statistics. Extensions have been proposed to broaden applications for different purposes. In this article, we explore a class of prior distributions closely related to Dirichlet distribution incorporating additional information on the data generating mechanism. Examples are given to show potential use of the models.

</details>

<details>

<summary>2014-12-04 14:06:04 - A Bayesian hierarchical spatial point process model for multi-type neuroimaging meta-analysis</summary>

- *Jian Kang, Thomas E. Nichols, Tor D. Wager, Timothy D. Johnson*

- `1412.1670v1` - [abs](http://arxiv.org/abs/1412.1670v1) - [pdf](http://arxiv.org/pdf/1412.1670v1)

> Neuroimaging meta-analysis is an important tool for finding consistent effects over studies that each usually have 20 or fewer subjects. Interest in meta-analysis in brain mapping is also driven by a recent focus on so-called "reverse inference": where as traditional "forward inference" identifies the regions of the brain involved in a task, a reverse inference identifies the cognitive processes that a task engages. Such reverse inferences, however, require a set of meta-analysis, one for each possible cognitive domain. However, existing methods for neuroimaging meta-analysis have significant limitations. Commonly used methods for neuroimaging meta-analysis are not model based, do not provide interpretable parameter estimates, and only produce null hypothesis inferences; further, they are generally designed for a single group of studies and cannot produce reverse inferences. In this work we address these limitations by adopting a nonparametric Bayesian approach for meta-analysis data from multiple classes or types of studies. In particular, foci from each type of study are modeled as a cluster process driven by a random intensity function that is modeled as a kernel convolution of a gamma random field. The type-specific gamma random fields are linked and modeled as a realization of a common gamma random field, shared by all types, that induces correlation between study types and mimics the behavior of a univariate mixed effects model. We illustrate our model on simulation studies and a meta-analysis of five emotions from 219 studies and check model fit by a posterior predictive assessment. In addition, we implement reverse inference by using the model to predict study type from a newly presented study. We evaluate this predictive performance via leave-one-out cross-validation that is efficiently implemented using importance sampling techniques.

</details>

<details>

<summary>2014-12-04 21:17:18 - Functional Time Series Models for Ultrafine Particle Distributions</summary>

- *Heidi J. Fischer, Qunfang Zhang, Yifang Zhu, Robert E. Weiss*

- `1412.1843v1` - [abs](http://arxiv.org/abs/1412.1843v1) - [pdf](http://arxiv.org/pdf/1412.1843v1)

> We propose Bayesian random effect functional time series models to model the impact of engine idling on ultrafine particle (UFP) counts inside school buses. UFPs are toxic to humans with health effects strongly linked to particle size. School engines emit particles primarily in the UFP size range and as school buses idle at bus stops, UFPs penetrate into cabins through cracks, doors, and windows. How UFP counts inside buses vary by particle size over time and under different idling conditions is not yet well understood. We model UFP counts at a given time with a cubic B-spline basis as a function of size and allow counts to increase over time at a size dependent rate once the engine turns on. We explore alternate parametric models for the engine-on increase which also vary smoothly over size. The log residual variance over size is modeled using a quadratic B-spline basis to account for heterogeneity and an autoregressive model is used for the residual. Model predictions are communicated graphically. These methods provide information needed for regulating vehicle emissions to minimize UFP exposure in the future.

</details>

<details>

<summary>2014-12-05 09:18:31 - Quantile universal threshold: model selection at the detection edge for high-dimensional linear regression</summary>

- *Jairo Diaz-Rodriguez, Sylvain Sardy*

- `1412.1927v1` - [abs](http://arxiv.org/abs/1412.1927v1) - [pdf](http://arxiv.org/pdf/1412.1927v1)

> To estimate a sparse linear model from data with Gaussian noise, consilience from lasso and compressed sensing literatures is that thresholding estimators like lasso and the Dantzig selector have the ability in some situations to identify with high probability part of the significant covariates asymptotically, and are numerically tractable thanks to convexity.   Yet, the selection of a threshold parameter $\lambda$ remains crucial in practice. To that aim we propose Quantile Universal Thresholding, a selection of $\lambda$ at the detection edge. We show with extensive simulations and real data that an excellent compromise between high true positive rate and low false discovery rate is achieved, leading also to good predictive risk.

</details>

<details>

<summary>2014-12-05 16:24:32 - Alternative statistical methods for cytogenetic radiation biological dosimetry</summary>

- *Krzysztof Wojciech Fornalski*

- `1412.2048v1` - [abs](http://arxiv.org/abs/1412.2048v1) - [pdf](http://arxiv.org/pdf/1412.2048v1)

> The paper presents alternative statistical methods for biological dosimetry, such as the Bayesian and Monte Carlo method. The classical Gaussian and robust Bayesian fit algorithms for the linear, linear-quadratic as well as saturated and critical calibration curves are described. The Bayesian model selection algorithm for those curves is also presented. In addition, five methods of dose estimation for a mixed neutron and gamma irradiation field were described: two classical methods, two Bayesian methods and one Monte Carlo method. Bayesian methods were also enhanced and generalized for situations with many types of mixed radiation. All algorithms were presented in easy-to-use form, which can be applied to any computational programming language. The presented algorithm is universal, although it was originally dedicated to cytogenetic biological dosimetry of victims of a nuclear reactor accident.

</details>

<details>

<summary>2014-12-07 04:44:43 - Iterative Bayesian Reconstruction of Non-IID Block-Sparse Signals</summary>

- *Mehdi Korki, Jingxin Zhang, Cishen Zhang, Hadi Zayyani*

- `1412.2316v1` - [abs](http://arxiv.org/abs/1412.2316v1) - [pdf](http://arxiv.org/pdf/1412.2316v1)

> This paper presents a novel Block Iterative Bayesian Algorithm (Block-IBA) for reconstructing block-sparse signals with unknown block structures. Unlike the existing algorithms for block sparse signal recovery which assume the cluster structure of the nonzero elements of the unknown signal to be independent and identically distributed (i.i.d.), we use a more realistic Bernoulli-Gaussian hidden Markov model (BGHMM) to characterize the non-i.i.d. block-sparse signals commonly encountered in practice. The Block-IBA iteratively estimates the amplitudes and positions of the block-sparse signal using the steepest-ascent based Expectation-Maximization (EM), and optimally selects the nonzero elements of the block-sparse signal by adaptive thresholding. The global convergence of Block-IBA is analyzed and proved, and the effectiveness of Block-IBA is demonstrated by numerical experiments and simulations on synthetic and real-life data.

</details>

<details>

<summary>2014-12-07 20:19:28 - Multiple Quantitative Trait Analysis Using Bayesian Networks</summary>

- *Marco Scutari, Phil Howell, David J. Balding, Ian Mackay*

- `1402.2905v4` - [abs](http://arxiv.org/abs/1402.2905v4) - [pdf](http://arxiv.org/pdf/1402.2905v4)

> Models for genome-wide prediction and association studies usually target a single phenotypic trait. However, in animal and plant genetics it is common to record information on multiple phenotypes for each individual that will be genotyped. Modeling traits individually disregards the fact that they are most likely associated due to pleiotropy and shared biological basis, thus providing only a partial, confounded view of genetic effects and phenotypic interactions. In this paper we use data from a Multiparent Advanced Generation Inter-Cross (MAGIC) winter wheat population to explore Bayesian networks as a convenient and interpretable framework for the simultaneous modeling of multiple quantitative traits. We show that they are equivalent to multivariate genetic best linear unbiased prediction (GBLUP), and that they are competitive with single-trait elastic net and single-trait GBLUP in predictive performance. Finally, we discuss their relationship with other additive-effects models and their advantages in inference and interpretation. MAGIC populations provide an ideal setting for this kind of investigation because the very low population structure and large sample size result in predictive models with good power and limited confounding due to relatedness.

</details>

<details>

<summary>2014-12-08 13:05:04 - Bayesian T-optimal discriminating designs</summary>

- *Holger Dette, Viatcheslav B. Melas, Roman Guchenko*

- `1412.2548v1` - [abs](http://arxiv.org/abs/1412.2548v1) - [pdf](http://arxiv.org/pdf/1412.2548v1)

> The problem of constructing Bayesian optimal discriminating designs for a class of regression models with respect to the T-optimality criterion introduced by Atkinson and Fedorov (1975a) is considered. It is demonstrated that the discretization of the integral with respect to the prior distribution leads to locally T-optimal discrimination designs can only deal with a few comparisons, but the discretization of the Bayesian prior easily yields to discrimination design problems for more than 100 competing models. A new efficient method is developed to deal with problems of this type. It combines some features of the classical exchange type algorithm with the gradient methods. Convergence is proved and it is demonstrated that the new method can find Bayesian optimal discriminating designs in situations where all currently available procedures fail.

</details>

<details>

<summary>2014-12-09 11:43:04 - Bayesian threshold selection for extremal models using measures of surprise</summary>

- *J. Lee, Y. Fan, S. A. Sisson*

- `1311.2994v2` - [abs](http://arxiv.org/abs/1311.2994v2) - [pdf](http://arxiv.org/pdf/1311.2994v2)

> Statistical extreme value theory is concerned with the use of asymptotically motivated models to describe the extreme values of a process. A number of commonly used models are valid for observed data that exceed some high threshold. However, in practice a suitable threshold is unknown and must be determined for each analysis. While there are many threshold selection methods for univariate extremes, there are relatively few that can be applied in the multivariate setting. In addition, there are only a few Bayesian-based methods, which are naturally attractive in the modelling of extremes due to data scarcity. The use of Bayesian measures of surprise to determine suitable thresholds for extreme value models is proposed. Such measures quantify the level of support for the proposed extremal model and threshold, without the need to specify any model alternatives. This approach is easily implemented for both univariate and multivariate extremes.

</details>

<details>

<summary>2014-12-09 12:01:03 - Bayesian Fisher's Discriminant for Functional Data</summary>

- *Yao-Hsiang Yang, Lu-Hung Chen, Chieh-Chih Wang, Chu-Song Chen*

- `1412.2929v1` - [abs](http://arxiv.org/abs/1412.2929v1) - [pdf](http://arxiv.org/pdf/1412.2929v1)

> We propose a Bayesian framework of Gaussian process in order to extend Fisher's discriminant to classify functional data such as spectra and images. The probability structure for our extended Fisher's discriminant is explicitly formulated, and we utilize the smoothness assumptions of functional data as prior probabilities. Existing methods which directly employ the smoothness assumption of functional data can be shown as special cases within this framework given corresponding priors while their estimates of the unknowns are one-step approximations to the proposed MAP estimates. Empirical results on various simulation studies and different real applications show that the proposed method significantly outperforms the other Fisher's discriminant methods for functional data.

</details>

<details>

<summary>2014-12-09 16:35:10 - Efficient Bayesian inference for stochastic volatility models with ensemble MCMC methods</summary>

- *Alexander Y. Shestopaloff, Radford M. Neal*

- `1412.3013v1` - [abs](http://arxiv.org/abs/1412.3013v1) - [pdf](http://arxiv.org/pdf/1412.3013v1)

> In this paper, we introduce efficient ensemble Markov Chain Monte Carlo (MCMC) sampling methods for Bayesian computations in the univariate stochastic volatility model. We compare the performance of our ensemble MCMC methods with an improved version of a recent sampler of Kastner and Fruwirth-Schnatter (2014). We show that ensemble samplers are more efficient than this state of the art sampler by a factor of about 3.1, on a data set simulated from the stochastic volatility model. This performance gain is achieved without the ensemble MCMC sampler relying on the assumption that the latent process is linear and Gaussian, unlike the sampler of Kastner and Fruwirth-Schnatter.

</details>

<details>

<summary>2014-12-09 18:51:07 - POPE: Post Optimization Posterior Evaluation of Likelihood Free Models</summary>

- *Edward Meeds, Michael Chiang, Mary Lee, Olivier Cinquin, John Lowengrub, Max Welling*

- `1412.3051v1` - [abs](http://arxiv.org/abs/1412.3051v1) - [pdf](http://arxiv.org/pdf/1412.3051v1)

> In many domains, scientists build complex simulators of natural phenomena that encode their hypotheses about the underlying processes. These simulators can be deterministic or stochastic, fast or slow, constrained or unconstrained, and so on. Optimizing the simulators with respect to a set of parameter values is common practice, resulting in a single parameter setting that minimizes an objective subject to constraints. We propose a post optimization posterior analysis that computes and visualizes all the models that can generate equally good or better simulation results, subject to constraints. These optimization posteriors are desirable for a number of reasons among which easy interpretability, automatic parameter sensitivity and correlation analysis and posterior predictive analysis. We develop a new sampling framework based on approximate Bayesian computation (ABC) with one-sided kernels. In collaboration with two groups of scientists we applied POPE to two important biological simulators: a fast and stochastic simulator of stem-cell cycling and a slow and deterministic simulator of tumor growth patterns.

</details>

<details>

<summary>2014-12-10 09:04:42 - Max-factor individual risk models with application to credit portfolios</summary>

- *Michel Denuit, Anna Kiriliouk, Johan Segers*

- `1412.3230v1` - [abs](http://arxiv.org/abs/1412.3230v1) - [pdf](http://arxiv.org/pdf/1412.3230v1)

> Individual risk models need to capture possible correlations as failing to do so typically results in an underestimation of extreme quantiles of the aggregate loss. Such dependence modelling is particularly important for managing credit risk, for instance, where joint defaults are a major cause of concern. Often, the dependence between the individual loss occurrence indicators is driven by a small number of unobservable factors. Conditional loss probabilities are then expressed as monotone functions of linear combinations of these hidden factors. However, combining the factors in a linear way allows for some compensation between them. Such diversification effects are not always desirable and this is why the present work proposes a new model replacing linear combinations with maxima. These max-factor models give more insight into which of the factors is dominant.

</details>

<details>

<summary>2014-12-10 12:28:34 - Generalised Entropy MDPs and Minimax Regret</summary>

- *Emmanouil G. Androulakis, Christos Dimitrakakis*

- `1412.3276v1` - [abs](http://arxiv.org/abs/1412.3276v1) - [pdf](http://arxiv.org/pdf/1412.3276v1)

> Bayesian methods suffer from the problem of how to specify prior beliefs. One interesting idea is to consider worst-case priors. This requires solving a stochastic zero-sum game. In this paper, we extend well-known results from bandit theory in order to discover minimax-Bayes policies and discuss when they are practical.

</details>

<details>

<summary>2014-12-10 22:57:24 - Proceedings of the First Astrostatistics School: Bayesian Methods in Cosmology</summary>

- *HÃ©ctor J. HortÃºa*

- `1409.4294v2` - [abs](http://arxiv.org/abs/1409.4294v2) - [pdf](http://arxiv.org/pdf/1409.4294v2)

> These are the proceedings of the First Astrostatistics School: Bayesian Methods in Cosmology, held in Bogot\'a D.C., Colombia, June 9-13, 2014. The first astrostatistics school has been the first event in Colombia where statisticians and cosmologists from some universities in Bogot\'a met to discuss the statistic methods applied to cosmology, especially the use of Bayesian statistics in the study of Cosmic Microwave Background (CMB), Baryonic Acoustic Oscillations (BAO), Large Scale Structure (LSS) and weak lensing.

</details>

<details>

<summary>2014-12-11 19:51:34 - Biips: Software for Bayesian Inference with Interacting Particle Systems</summary>

- *Adrien Todeschini, FranÃ§ois Caron, Marc Fuentes, Pierrick Legrand, Pierre Del Moral*

- `1412.3779v1` - [abs](http://arxiv.org/abs/1412.3779v1) - [pdf](http://arxiv.org/pdf/1412.3779v1)

> Biips is a software platform for automatic Bayesian inference with interacting particle systems. Biips allows users to define their statistical model in the probabilistic programming BUGS language, as well as to add custom functions or samplers within this language. Then it runs sequential Monte Carlo based algorithms (particle filters, particle independent Metropolis-Hastings, particle marginal Metropolis-Hastings) in a black-box manner so that to approximate the posterior distribution of interest as well as the marginal likelihood. The software is developed in C++ with interfaces with the softwares R, Matlab and Octave.

</details>

<details>

<summary>2014-12-14 13:28:34 - Designs for generalized linear models with random block effects via information matrix approximations</summary>

- *Timothy W. Waite, David C. Woods*

- `1412.4355v1` - [abs](http://arxiv.org/abs/1412.4355v1) - [pdf](http://arxiv.org/pdf/1412.4355v1)

> The selection of optimal designs for generalized linear mixed models is complicated by the fact that the Fisher information matrix, on which most optimality criteria depend, is computationally expensive to evaluate. Our focus is on the design of experiments for likelihood estimation of parameters in the conditional model. We provide two novel approximations that substantially reduce the computational cost of evaluating the information matrix by complete enumeration of response outcomes, or Monte Carlo approximations thereof: (i) an asymptotic approximation which is accurate when there is strong dependence between observations in the same block; (ii) an approximation via Kriging interpolators. For logistic random intercept models, we show how interpolation can be especially effective for finding pseudo-Bayesian designs that incorporate uncertainty in the values of the model parameters. The new results are used to provide the first evaluation of the efficiency, for estimating conditional models, of optimal designs from closed-form approximations to the information matrix derived from marginal models. It is found that correcting for the marginal attenuation of parameters in binary-response models yields much improved designs, typically with very high efficiencies. However, in some experiments exhibiting strong dependence, designs for marginal models may still be inefficient for conditional modelling. Our asymptotic results provide some theoretical insights into why such inefficiencies occur.

</details>

<details>

<summary>2014-12-14 17:40:02 - Robust Bayesian compressive sensing for signals in structural health monitoring</summary>

- *Yong Huang, James L. Beck, Stephen Wu, Hui Li*

- `1412.4383v1` - [abs](http://arxiv.org/abs/1412.4383v1) - [pdf](http://arxiv.org/pdf/1412.4383v1)

> In structural health monitoring (SHM) systems, massive amounts of data are often generated that need data compression techniques to reduce the cost of signal transfer and storage. Compressive sensing (CS) is a novel data acquisition method whereby the compression is done in a sensor simultaneously with the sampling. If the original sensed signal is sufficiently sparse in terms of some basis, the decompression can be done essentially perfectly up to some critical compression ratio. In this article, a Bayesian compressive sensing (BCS) method is investigated that uses sparse Bayesian learning to reconstruct signals from a compressive sensor. By explicitly quantifying the uncertainty in the reconstructed signal, the BCS technique exhibits an obvious benefit over existing regularized norm-minimization CS methods that provide a single signal estimate. However, current BCS algorithms suffer from a robustness problem: sometimes the reconstruction errors are very large when the number of measurements is a lot less than the number of signal degrees of freedom that are needed to capture the signal accurately in a directly sampled form. In this paper, we present improvements to the BCS reconstruction method to enhance its robustness so that even higher compression ratios can be used and we examine the tradeoff between efficiently compressing data and accurately decompressing it. Synthetic data and actual acceleration data collected from a bridge SHM system are used as examples. Compared with the state-of-the-art BCS reconstruction algorithms, the improved BCS algorithm demonstrates superior performance. With the same reconstruction error, the proposed BCS algorithm works with relatively large compression ratios and it can achieve perfect lossless compression performance with quite high compression ratios. Furthermore, the error bars for the signal reconstruction are also quantified effectively.

</details>

<details>

<summary>2014-12-14 17:43:53 - Bayesian Hierarchical Model of Total Variation Regularisation for Image Deblurring</summary>

- *Marko JÃ¤rvenpÃ¤Ã¤, Robert PichÃ©*

- `1412.4384v1` - [abs](http://arxiv.org/abs/1412.4384v1) - [pdf](http://arxiv.org/pdf/1412.4384v1)

> A Bayesian hierarchical model for total variation regularisation is presented in this paper. All the parameters of an inverse problem, including the "regularisation parameter", are estimated simultaneously from the data in the model. The model is based on the characterisation of the Laplace density prior as a scale mixture of Gaussians. With different priors on the mixture variable, other total variation like regularisations e.g. a prior that is related to t-distribution, are also obtained. An approximation of the resulting posterior mean is found using a variational Bayes method. In addition, an iterative alternating sequential algorithm for computing the maximum a posteriori estimate is presented. The methods are illustrated with examples of image deblurring. Results show that the proposed model can be used for automatic edge-preserving inversion in the case of image deblurring. Despite promising results, some difficulties with the model were encountered and are subject to future work.

</details>

<details>

<summary>2014-12-15 04:28:25 - Sequential Monte Carlo Methods for Bayesian Elliptic Inverse Problems</summary>

- *Alex Beskos, Ajay Jasra, Ege Muzaffer, Andrew Stuart*

- `1412.4459v1` - [abs](http://arxiv.org/abs/1412.4459v1) - [pdf](http://arxiv.org/pdf/1412.4459v1)

> In this article we consider a Bayesian inverse problem associated to elliptic partial differential equations (PDEs) in two and three dimensions. This class of inverse problems is important in applications such as hydrology, but the complexity of the link function between unknown field and measurements can make it difficult to draw inference from the associated posterior. We prove that for this inverse problem a basic SMC method has a Monte Carlo rate of convergence with constants which are independent of the dimension of the discretization of the problem; indeed convergence of the SMC method is established in a function space setting. We also develop an enhancement of the sequential Monte Carlo (SMC) methods for inverse problems which were introduced in \cite{kantas}; the enhancement is designed to deal with the additional complexity of this elliptic inverse problem. The efficacy of the methodology, and its desirable theoretical properties, are demonstrated on numerical examples in both two and three dimensions.

</details>

<details>

<summary>2014-12-15 07:21:34 - Controlling for unmeasured confounding and spatial misalignment in long-term air pollution and health studies</summary>

- *Duncan Lee, Christophe Sarran*

- `1412.4479v1` - [abs](http://arxiv.org/abs/1412.4479v1) - [pdf](http://arxiv.org/pdf/1412.4479v1)

> The health impact of long-term exposure to air pollution is now routinely estimated using spatial ecological studies, due to the recent widespread availability of spatial referenced pollution and disease data. However, this areal unit study design presents a number of statistical challenges, which if ignored have the potential to bias the estimated pollution-health relationship. One such challenge is how to control for the spatial autocorrelation present in the data after accounting for the known covariates, which is caused by unmeasured confounding. A second challenge is how to adjust the functional form of the model to account for the spatial misalignment between the pollution and disease data, which causes within-area variation in the pollution data. These challenges have largely been ignored in existing long-term spatial air pollution and health studies, so here we propose a novel Bayesian hierarchical model that addresses both challenges, and provide software to allow others to apply our model to their own data. The effectiveness of the proposed model is compared by simulation against a number of state of the art alternatives proposed in the literature, and is then used to estimate the impact of nitrogen dioxide and particulate matter concentrations on respiratory hospital admissions in a new epidemiological study in England in 2010 at the Local Authority level.

</details>

<details>

<summary>2014-12-15 10:10:42 - Bayesian Non-Parametric Inference for Infectious Disease Data</summary>

- *Edward S. Knock, Theodore Kypraios*

- `1411.2624v2` - [abs](http://arxiv.org/abs/1411.2624v2) - [pdf](http://arxiv.org/pdf/1411.2624v2)

> We propose a framework for Bayesian non-parametric estimation of the rate at which new infections occur assuming that the epidemic is partially observed. The developed methodology relies on modelling the rate at which new infections occur as a function which only depends on time. Two different types of prior distributions are proposed namely using step-functions and B-splines. The methodology is illustrated using both simulated and real datasets and we show that certain aspects of the epidemic such as seasonality and super-spreading events are picked up without having to explicitly incorporate them into a parametric model.

</details>

<details>

<summary>2014-12-16 08:18:48 - A comparison of emulation methods for Approximate Bayesian Computation</summary>

- *Franck Jabot, Guillaume Lagarrigues, BenoÃ®t Courbaud, Nicolas Dumoulin*

- `1412.7560v1` - [abs](http://arxiv.org/abs/1412.7560v1) - [pdf](http://arxiv.org/pdf/1412.7560v1)

> Approximate Bayesian Computation (ABC) is a family of statistical inference techniques, which is increasingly used in biology and other scientific fields. Its main benefit is to be applicable to models for which the computation of the model likelihood is intractable. The basic idea of ABC is to empirically approximate the model likelihood by using intensive realizations of model runs. Due to computing time limitations, ABC has thus been mainly applied to models that are relatively quick to simulate. We here aim at briefly introducing the field of statistical emulation of computer code outputs and to demonstrate its potential for ABC applications. Emulation consists in replacing the costly to simulate model by another (quick to simulate) statistical model called emulator or meta-model. This emulator is fitted to a small number of outputs of the original model, and is subsequently used as a surrogate during the inference procedure. In this contribution, we first detail the principles of model emulation, with a special reference to the ABC context in which the description of the stochasticity of model realizations is as important as the description of the trends linking model parameters and outputs. We then compare several emulation strategies in an ABC context, using as case study a stochastic ecological model of community dynamics. We finally describe a novel emulation-based sequential ABC algorithm which is shown to decrease computing time by a factor of two on the studied example, compared to previous sequential ABC algorithms. Routines to perform emulation-based ABC were made available within the R package EasyABC.

</details>

<details>

<summary>2014-12-17 01:16:31 - The supervised hierarchical Dirichlet process</summary>

- *Andrew M. Dai, Amos J. Storkey*

- `1412.5236v1` - [abs](http://arxiv.org/abs/1412.5236v1) - [pdf](http://arxiv.org/pdf/1412.5236v1)

> We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.

</details>

<details>

<summary>2014-12-17 03:07:10 - Upscaling Uncertainty with Dynamic Discrepancy for a Multi-scale Carbon Capture System</summary>

- *K. Sham Bhat, David S. Mebane, Curtis B. Storlie, Priyadarshi Mahapatra*

- `1411.2578v2` - [abs](http://arxiv.org/abs/1411.2578v2) - [pdf](http://arxiv.org/pdf/1411.2578v2)

> Uncertainties from model parameters and model discrepancy from small-scale models impact the accuracy and reliability of predictions of large-scale systems. Inadequate representation of these uncertainties may result in inaccurate and overconfident predictions during scale-up to larger models. Hence multiscale modeling efforts must quantify the effect of the propagation of uncertainties during upscaling. Using a Bayesian approach, we calibrate a small-scale solid sorbent model to Thermogravimetric (TGA) data on a functional profile using chemistry-based priors. Crucial to this effort is the representation of model discrepancy, which uses a Bayesian Smoothing Splines (BSS-ANOVA) framework. We use an intrusive uncertainty quantification (UQ) approach by including the discrepancy function within the chemical rate expressions; resulting in a set of stochastic differential equations. Such an approach allows for easily propagating uncertainty by propagating the joint model parameter and discrepancy posterior into the larger-scale system of rate expressions. The broad UQ framework presented here may have far-reaching impact into virtually all areas of science where multiscale modeling is used.

</details>

<details>

<summary>2014-12-17 12:01:00 - Optimal design for step-stress accelerated test with random discrete stress elevating times based on gamma degradation process</summary>

- *Morteza Amini, Soudabeh Shemehsavar, Zhengqiang Pan*

- `1404.3806v3` - [abs](http://arxiv.org/abs/1404.3806v3) - [pdf](http://arxiv.org/pdf/1404.3806v3)

> Recently, a step-stress accelerated degradation test (SSADT) plan, in which the stress level is elevated when the degradation value of a product crosses a pre-specified value, was proposed. The times of stress level elevating are random and vary from product to product. In this paper we extend this model to a more economic plan. The proposed extended model has two economical advantages compared with the previous one. The first is that the times of stress level elevating in the new model are identical for all products, which enable us to use only one chamber (oven) for testing all test units. The second is that, the new method does not require continuous inspection and to elevate the stress level, it is not necessary for the experimenter to inspect the value of the degradation continually. The new method decrease the cost of measurement and also there is no need to use electronic sensors to detect the first passage time of the degradation to the threshold value in the new method. We assume that the degradation path follows a gamma process. The stress level is elevated as soon as the measurement of the degradation of one of the test units, at one of the specified times, exceeds the threshold value. Under the constraint that the total experimental cost does not exceed a pre-specified budget, the optimal settings including the optimal threshold value, sample size, measurement frequency and termination time are obtained by minimizing the asymptotic variance of an estimated quantile of the lifetime distribution of the product. A case study is presented to illustrate the proposed method.

</details>

<details>

<summary>2014-12-17 13:15:55 - Nonparametric tests for detecting breaks in the jump behaviour of a time-continuous process</summary>

- *Axel BÃ¼cher, Michael Hoffmann, Mathias Vetter, Holger Dette*

- `1412.5376v1` - [abs](http://arxiv.org/abs/1412.5376v1) - [pdf](http://arxiv.org/pdf/1412.5376v1)

> This paper is concerned with tests for changes in the jump behaviour of a time-continuous process. Based on results on weak convergence of a sequential empirical tail integral process, asymptotics of certain tests statistics for breaks in the jump measure of an Ito semimartingale are constructed. Whenever limiting distributions depend in a complicated way on the unknown jump measure, empirical quantiles are obtained using a multiplier bootstrap scheme. An extensive simulation study shows a good performance of our tests in finite samples.

</details>

<details>

<summary>2014-12-17 17:50:02 - Bayesian Degree-Corrected Stochastic Blockmodels for Community Detection</summary>

- *Lijun Peng, Luis Carvalho*

- `1309.4796v2` - [abs](http://arxiv.org/abs/1309.4796v2) - [pdf](http://arxiv.org/pdf/1309.4796v2)

> Community detection in networks has drawn much attention in diverse fields, especially social sciences. Given its significance, there has been a large body of literature with approaches from many fields. Here we present a statistical framework that is representative, extensible, and that yields an estimator with good properties. Our proposed approach considers a stochastic blockmodel based on a logistic regression formulation with node correction terms. We follow a Bayesian approach that explicitly captures the community behavior via prior specification. We further adopt a data augmentation strategy with latent Polya-Gamma variables to obtain posterior samples. We conduct inference based on a principled, canonically mapped centroid estimator that formally addresses label non-identifiability and captures representative community assignments. We demonstrate the proposed model and estimation on real-world as well as simulated benchmark networks and show that the proposed model and estimator are more flexible, representative, and yield smaller error rates when compared to the MAP estimator from classical degree-corrected stochastic blockmodels.

</details>

<details>

<summary>2014-12-17 22:48:26 - Plotting positions close to the exact unbiased solution: application to the Pozzuoli's bradeysism earthquake data</summary>

- *Pasquale Erto, Antonio Lepore*

- `1412.5663v1` - [abs](http://arxiv.org/abs/1412.5663v1) - [pdf](http://arxiv.org/pdf/1412.5663v1)

> Graphical techniques are recommended for critical applications in order to share information with non-statisticians, since they allow for a visual analysis and helpful understanding of the results. However, graphical estimation methods are often underestimated because of their minor efficiency with respect to the analytical ones. Therefore, finding unbiased plotting positions can contribute to rise their reputation and to encourage their strategic use. This paper proposes a new general plotting position formula which can be as close as needed to the exact unbiased plotting positions. The ability of the new solution in estimating quantiles for both symmetrical and skewed location-scale distributions is shown via Monte Carlo simulation. An applicative example shows how the proposed formula enables to perform, with known accuracy, the graphical analysis of critical data, such as the earthquake magnitudes registered during the serious 1983-1984 bradyseismic crisis in Campi Flegrei (Italy). Moreover, the proposed formula gives a unified look at existing plotting positions and a definitive insight into plotting position controversies recently renewed in the literature.

</details>

<details>

<summary>2014-12-18 01:12:14 - A shotgun sampling solution for the common input problem in neural connectivity inference</summary>

- *Daniel Soudry, Suraj Keshri, Patrick Stinson, Min-hwan Oh, Garud Iyengar, Liam Paninski*

- `1309.3724v2` - [abs](http://arxiv.org/abs/1309.3724v2) - [pdf](http://arxiv.org/pdf/1309.3724v2)

> Inferring connectivity in neuronal networks remains a key challenge in statistical neuroscience. The `common input' problem presents the major roadblock: it is difficult to reliably distinguish causal connections between pairs of observed neurons from correlations induced by common input from unobserved neurons. Since available recording techniques allow us to sample from only a small fraction of large networks simultaneously with sufficient temporal resolution, naive connectivity estimators that neglect these common input effects are highly biased. This work proposes a `shotgun' experimental design, in which we observe multiple sub-networks briefly, in a serial manner. Thus, while the full network cannot be observed simultaneously at any given time, we may be able to observe most of it during the entire experiment. Using a generalized linear model for a spiking recurrent neural network, we develop scalable approximate Bayesian methods to perform network inference given this type of data, in which only a small fraction of the network is observed in each time bin. We demonstrate in simulation that, using this method: (1) The shotgun experimental design can eliminate the biases induced by common input effects. (2) Networks with thousands of neurons, in which only a small fraction of the neurons is observed in each time bin, could be quickly and accurately estimated. (3) Performance can be improved if we exploit prior information about the probability of having a connection between two neurons, its dependence on neuronal cell types (e.g., Dale's law), or its dependence on the distance between neurons.

</details>

<details>

<summary>2014-12-18 13:00:59 - A Modified Reference Prior for the Generalized Gamma Distribution</summary>

- *Pedro L. Ramos, Francisco Louzada*

- `1412.5843v1` - [abs](http://arxiv.org/abs/1412.5843v1) - [pdf](http://arxiv.org/pdf/1412.5843v1)

> In this paper we propose an objective Bayesian estimation approach for the parameters of the generalized gamma distribution. Various reference priors are obtained, but showing that they lead to improper posterior distributions. We overcome this problem by proposing a modification in a reference priori distribution, allowing for a proper posterior distribution for the parameters of the generalized gamma distribution. We perform a simulation study in order to study the efficiency of the proposed methodology, which is also fully illustrated on a real data set.

</details>

<details>

<summary>2014-12-19 18:08:02 - A hidden Markov model for decoding and the analysis of replay in spike trains</summary>

- *Marc Box, Matt W. Jones, Nick Whiteley*

- `1412.6469v1` - [abs](http://arxiv.org/abs/1412.6469v1) - [pdf](http://arxiv.org/pdf/1412.6469v1)

> We present a hidden Markov model that describes variation in an animal's position associated with varying levels of activity in action potential spike trains of individual place cell neurons. The model incorporates a coarse-graining of position, which we find to be a more parsimonious description of the system than other models. We use a sequential Monte Carlo algorithm for Bayesian inference of model parameters, including the state space dimension, and we explain how to estimate position from spike train observations (decoding). We obtain greater accuracy over other methods in the conditions of high temporal resolution and small neuronal sample size. We also present a novel, model-based approach to the study of replay: the expression of spike train activity related to behaviour during times of motionlessness or sleep, thought to be integral to the consolidation of long-term memories. We demonstrate how we can detect the time, information content and compression rate of replay events in simulated and real hippocampal data recorded from rats in two different environments, and verify the correlation between the times of detected replay events and of sharp wave/ripples in the local field potential.

</details>

<details>

<summary>2014-12-20 02:29:27 - Poisson Latent Feature Calculus for Generalized Indian Buffet Processes</summary>

- *Lancelot F. James*

- `1411.2936v3` - [abs](http://arxiv.org/abs/1411.2936v3) - [pdf](http://arxiv.org/pdf/1411.2936v3)

> The purpose of this work is to describe a unified, and indeed simple, mechanism for non-parametric Bayesian analysis, construction and generative sampling of a large class of latent feature models which one can describe as generalized notions of Indian Buffet Processes(IBP). This is done via the Poisson Process Calculus as it now relates to latent feature models. The IBP was ingeniously devised by Griffiths and Ghahramani in (2005) and its generative scheme is cast in terms of customers entering sequentially an Indian Buffet restaurant and selecting previously sampled dishes as well as new dishes. In this metaphor dishes corresponds to latent features, attributes, preferences shared by individuals. The IBP, and its generalizations, represent an exciting class of models well suited to handle high dimensional statistical problems now common in this information age. The IBP is based on the usage of conditionally independent Bernoulli random variables, coupled with completely random measures acting as Bayesian priors, that are used to create sparse binary matrices. This Bayesian non-parametric view was a key insight due to Thibaux and Jordan (2007). One way to think of generalizations is to to use more general random variables. Of note in the current literature are models employing Poisson and Negative-Binomial random variables. However, unlike their closely related counterparts, generalized Chinese restaurant processes, the ability to analyze IBP models in a systematic and general manner is not yet available. The limitations are both in terms of knowledge about the effects of different priors and in terms of models based on a wider choice of random variables. This work will not only provide a thorough description of the properties of existing models but also provide a simple template to devise and analyze new models.

</details>

<details>

<summary>2014-12-22 22:01:46 - Accelerating inference for diffusions observed with measurement error and large sample sizes using Approximate Bayesian Computation</summary>

- *Umberto Picchini, Julie Lyng Forman*

- `1310.0973v3` - [abs](http://arxiv.org/abs/1310.0973v3) - [pdf](http://arxiv.org/pdf/1310.0973v3)

> In recent years dynamical modelling has been provided with a range of breakthrough methods to perform exact Bayesian inference. However it is often computationally unfeasible to apply exact statistical methodologies in the context of large datasets and complex models. This paper considers a nonlinear stochastic differential equation model observed with correlated measurement errors and an application to protein folding modelling. An Approximate Bayesian Computation (ABC) MCMC algorithm is suggested to allow inference for model parameters within reasonable time constraints. The ABC algorithm uses simulations of "subsamples" from the assumed data generating model as well as a so-called "early rejection" strategy to speed up computations in the ABC-MCMC sampler. Using a considerate amount of subsamples does not seem to degrade the quality of the inferential results for the considered applications. A simulation study is conducted to compare our strategy with exact Bayesian inference, the latter resulting two orders of magnitude slower than ABC-MCMC for the considered setup. Finally the ABC algorithm is applied to a large size protein data. The suggested methodology is fairly general and not limited to the exemplified model and data.

</details>

<details>

<summary>2014-12-23 16:43:13 - Extreme value statistics for truncated Pareto-type distributions</summary>

- *Jan Beirlant, Isabel Fraga Alves, Ivette Gomes, Mark M. Meerschaert*

- `1410.4097v3` - [abs](http://arxiv.org/abs/1410.4097v3) - [pdf](http://arxiv.org/pdf/1410.4097v3)

> Recently attention has been drawn to practical problems with the use of unbounded Pareto distributions, for instance when there are natural upper bounds that truncate the probability tail. Aban, Meerschaert and Panorska (2006) derived the maximum likelihood estimator for the Pareto tail index of a truncated Pareto distribution with a right truncation point $T$. The Hill (1975) estimator is then obtained by letting $T \to \infty$. The problem of extreme value estimation under right truncation was also introduced in Nuyts (2010) who proposed a similar estimator for the tail index and considered trimming of the number of extreme order statistics. Given that in practice one does not always know whether the distribution is truncated or not, we discuss estimators for the Pareto index and extreme quantiles both under truncated and non-truncated Pareto-type distributions. We also propose a truncated Pareto QQ-plot in order to help deciding between a truncated and a non-truncated case. In this way we extend the classical extreme value methodology adding the truncated Pareto-type model with truncation point $T \to \infty$ as the sample size $n \to \infty$. Finally we present some practical examples, asymptotics and simulation results.

</details>

<details>

<summary>2014-12-23 17:40:10 - Bayesian Inference for Generalized Extreme Value Distributions via Hamiltonian Monte Carlo</summary>

- *Marcelo Hartmann, Ricardo Ehlers*

- `1410.4534v2` - [abs](http://arxiv.org/abs/1410.4534v2) - [pdf](http://arxiv.org/pdf/1410.4534v2)

> In this paper we propose to evaluate and compare Markov chain Monte Carlo (MCMC) methods to estimate the parameters in a generalized extreme value model. We employed the Bayesian approach using traditional Metropolis-Hastings methods, Hamiltonian Monte Carlo (HMC) and Riemann manifold HMC (RMHMC) methods to obtain the approximations to the posterior marginal distributions of interest. Applications to real datasets of maxima illustrate illustrate how HMC can be much more efficient computationally than traditional MCMC and simulation studies are conducted to compare the algorithms in terms of how fast they get close enough to the stationary distribution so as to provide good estimates with a smaller number of iterations.

</details>

<details>

<summary>2014-12-23 18:49:19 - Model Selection in High-Dimensional Misspecified Models</summary>

- *Pallavi Basu, Yang Feng, Jinchi Lv*

- `1412.7468v1` - [abs](http://arxiv.org/abs/1412.7468v1) - [pdf](http://arxiv.org/pdf/1412.7468v1)

> Model selection is indispensable to high-dimensional sparse modeling in selecting the best set of covariates among a sequence of candidate models. Most existing work assumes implicitly that the model is correctly specified or of fixed dimensions. Yet model misspecification and high dimensionality are common in real applications. In this paper, we investigate two classical Kullback-Leibler divergence and Bayesian principles of model selection in the setting of high-dimensional misspecified models. Asymptotic expansions of these principles reveal that the effect of model misspecification is crucial and should be taken into account, leading to the generalized AIC and generalized BIC in high dimensions. With a natural choice of prior probabilities, we suggest the generalized BIC with prior probability which involves a logarithmic factor of the dimensionality in penalizing model complexity. We further establish the consistency of the covariance contrast matrix estimator in a general setting. Our results and new method are supported by numerical studies.

</details>

<details>

<summary>2014-12-25 04:26:32 - Multivariate-from-Univariate MCMC Sampler: R Package MfUSampler</summary>

- *Alireza S. Mahani, Mansour T. A. Sharabiani*

- `1412.7784v1` - [abs](http://arxiv.org/abs/1412.7784v1) - [pdf](http://arxiv.org/pdf/1412.7784v1)

> The R package MfUSampler provides Monte Carlo Markov Chain machinery for generating samples from multivariate probability distributions using univariate sampling algorithms such as Slice Sampler and Adaptive Rejection Sampler. The sampler function performs a full cycle of univariate sampling steps, one coordinate at a time. In each step, the latest sample values obtained for other coordinates are used to form the conditional distributions. The concept is an extension of Gibbs sampling where each step involves, not an independent sample from the conditional distribution, but a Markov transition for which the conditional distribution is invariant. The software relies on proportionality of conditional distributions to the joint distribution to implement a thin wrapper for producing conditionals. Examples illustrate basic usage as well as methods for improving performance. By encapsulating the multivariate-from-univariate logic, MfUSampler provides a reliable library for rapid prototyping of custom Bayesian models while allowing for incremental performance optimizations such as utilization of conjugacy, conditional independence, and porting function evaluations to compiled languages.

</details>

<details>

<summary>2014-12-25 06:22:32 - Relations Between the Conditional Normalized Maximum Likelihood Distributions and the Latent Information Priors</summary>

- *Mutsuki Kojima, Fumiyasu Komaki*

- `1412.7794v1` - [abs](http://arxiv.org/abs/1412.7794v1) - [pdf](http://arxiv.org/pdf/1412.7794v1)

> We reveal the relations between the conditional normalized maximum likelihood (CNML) distributions and Bayesian predictive densities based on the latent information priors (LIPs). In particular, CNML3, which is one type of CNML distributions, is investigated. The Bayes projection of a predictive density, which is an information projection of the predictive density on a set of Bayesian predictive densities, is considered. We prove that the sum of the Bayes projection divergence of CNML3 and the conditional mutual information is asymptotically constant. This result implies that the Bayes projection of CNML3 (BPCNML3) is asymptotically identical to the Bayesian predictive density based on LIP. In addition, under some stronger assumptions, we show that BPCNML3 exactly coincides with the Bayesian predictive density based on LIP.

</details>

<details>

<summary>2014-12-26 22:54:32 - Gradient-based stochastic optimization methods in Bayesian experimental design</summary>

- *Xun Huan, Youssef M. Marzouk*

- `1212.2228v3` - [abs](http://arxiv.org/abs/1212.2228v3) - [pdf](http://arxiv.org/pdf/1212.2228v3)

> Optimal experimental design (OED) seeks experiments expected to yield the most useful data for some purpose. In practical circumstances where experiments are time-consuming or resource-intensive, OED can yield enormous savings. We pursue OED for nonlinear systems from a Bayesian perspective, with the goal of choosing experiments that are optimal for parameter inference. Our objective in this context is the expected information gain in model parameters, which in general can only be estimated using Monte Carlo methods. Maximizing this objective thus becomes a stochastic optimization problem.   This paper develops gradient-based stochastic optimization methods for the design of experiments on a continuous parameter space. Given a Monte Carlo estimator of expected information gain, we use infinitesimal perturbation analysis to derive gradients of this estimator. We are then able to formulate two gradient-based stochastic optimization approaches: (i) Robbins-Monro stochastic approximation, and (ii) sample average approximation combined with a deterministic quasi-Newton method. A polynomial chaos approximation of the forward model accelerates objective and gradient evaluations in both cases. We discuss the implementation of these optimization methods, then conduct an empirical comparison of their performance. To demonstrate design in a nonlinear setting with partial differential equation forward models, we use the problem of sensor placement for source inversion. Numerical results yield useful guidelines on the choice of algorithm and sample sizes, assess the impact of estimator bias, and quantify tradeoffs of computational cost versus solution quality and robustness.

</details>

<details>

<summary>2014-12-28 19:14:48 - Bayesian Melding of the Dead-Reckoned Path and GPS Measurements for an Accurate and High-Resolution Path of Marine Mammals</summary>

- *Yang Liu, Brian C. Battaile, James V. Zidek, Andrew W. Trites*

- `1411.6683v2` - [abs](http://arxiv.org/abs/1411.6683v2) - [pdf](http://arxiv.org/pdf/1411.6683v2)

> With the recent advances in electrical engineering, devices attached to free-ranging marine mammals today can collect oceanographic data in remarkable high spatial-temporal resolution. However, those data cannot be fully utilized without a matching high-resolution and accurate path of the animal, which is currently missing in this field. In this paper, we develop a Bayesian melding approach based on a Brownian Bridge process to combine the fine-resolution but seriously biased Dead-Reckoned path and the precise but sparse GPS measurements, which results in an accurate and high-resolution estimated path together with credible bands as quantified uncertainty statements. We also exploit the properties of underlying processes and some approximations to the likelihood to dramatically reduce the computational burden of handling those big high resolution data sets.

</details>

<details>

<summary>2014-12-29 07:52:32 - Individual adaptation: an adaptive MCMC scheme for variable selection problems</summary>

- *Jim Griffin, Krzysztof Latuszynski, Mark Steel*

- `1412.6760v2` - [abs](http://arxiv.org/abs/1412.6760v2) - [pdf](http://arxiv.org/pdf/1412.6760v2)

> The increasing size of data sets has lead to variable selection in regression becoming increasingly important. Bayesian approaches are attractive since they allow uncertainty about the choice of variables to be formally included in the analysis. The application of fully Bayesian variable selection methods to large data sets is computationally challenging. We describe an adaptive Markov chain Monte Carlo approach called Individual Adaptation which adjusts a general proposal to the data. We show that the algorithm is ergodic and discuss its use within parallel tempering and sequential Monte Carlo approaches. We illustrate the use of the method on two data sets including a gene expression analysis with 22 577 variables.

</details>

<details>

<summary>2014-12-30 06:32:47 - A Bayesian encourages dropout</summary>

- *Shin-ichi Maeda*

- `1412.7003v3` - [abs](http://arxiv.org/abs/1412.7003v3) - [pdf](http://arxiv.org/pdf/1412.7003v3)

> Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout.

</details>

<details>

<summary>2014-12-30 11:24:59 - A note on the Karhunen-LoÃ¨ve expansions for infinite-dimensional Bayesian inverse problems</summary>

- *Jinglai Li*

- `1412.8604v1` - [abs](http://arxiv.org/abs/1412.8604v1) - [pdf](http://arxiv.org/pdf/1412.8604v1)

> In this note, we consider the truncated Karhunen-Lo\`eve expansion for approximating solutions to infinite dimensional inverse problems. We show that, under certain conditions, the bound of the error between a solution and its finite-dimensional approximation can be estimated without the knowledge of the solution.

</details>

<details>

<summary>2014-12-30 21:53:44 - Transdimensional Approximate Bayesian Computation for Inference on Invasive Species Models with Latent Variables of Unknown Dimension</summary>

- *Oksana A. Chkrebtii, Erin K. Cameron, David A. Campbell, Erin M. Bayne*

- `1310.2888v3` - [abs](http://arxiv.org/abs/1310.2888v3) - [pdf](http://arxiv.org/pdf/1310.2888v3)

> Accurate information on patterns of introduction and spread of non-native species is essential for making predictions and management decisions. In many cases, estimating unknown rates of introduction and spread from observed data requires evaluating intractable variable-dimensional integrals. In general, inference on the large class of models containing latent variables of large or variable dimension precludes exact sampling techniques. Approximate Bayesian computation (ABC) methods provide an alternative to exact sampling but rely on inefficient conditional simulation of the latent variables. To accomplish this task efficiently, a new transdimensional Monte Carlo sampler is developed for approximate Bayesian model inference and used to estimate rates of introduction and spread for the non-native earthworm species Dendrobaena octaedra (Savigny) along roads in the boreal forest of northern Alberta. Using low and high estimates of introduction and spread rates, the extent of earthworm invasions in northeastern Alberta was simulated to project the proportion of suitable habitat invaded in the year following data collection.

</details>

<details>

<summary>2014-12-31 00:23:35 - Detailed Derivations of Small-Variance Asymptotics for some Hierarchical Bayesian Nonparametric Models</summary>

- *Jonathan H. Huggins, Ardavan Saeedi, Matthew J. Johnson*

- `1501.00052v1` - [abs](http://arxiv.org/abs/1501.00052v1) - [pdf](http://arxiv.org/pdf/1501.00052v1)

> In this note we provide detailed derivations of two versions of small-variance asymptotics for hierarchical Dirichlet process (HDP) mixture models and the HDP hidden Markov model (HDP-HMM, a.k.a. the infinite HMM). We include derivations for the probabilities of certain CRP and CRF partitions, which are of more general interest.

</details>

<details>

<summary>2014-12-31 06:21:23 - Variable Selection in Bayesian Semiparametric Regression Models</summary>

- *Ofir Harari, David M. Steinberg*

- `1501.00083v1` - [abs](http://arxiv.org/abs/1501.00083v1) - [pdf](http://arxiv.org/pdf/1501.00083v1)

> In this paper we extend existing Bayesian methods for variable selection in Gaussian process regression, to select both the regression terms and the active covariates in the spatial correlation structure. We then use the estimated posterior probabilities to choose between relatively few modes through cross-validation, and consequently improve prediction.

</details>

<details>

<summary>2014-12-31 22:52:17 - Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling</summary>

- *Mingyuan Zhou*

- `1410.7812v2` - [abs](http://arxiv.org/abs/1410.7812v2) - [pdf](http://arxiv.org/pdf/1410.7812v2)

> The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.

</details>

