# 2013

## TOC

- [2013-01](#2013-01)
- [2013-02](#2013-02)
- [2013-03](#2013-03)
- [2013-04](#2013-04)
- [2013-05](#2013-05)
- [2013-06](#2013-06)
- [2013-07](#2013-07)
- [2013-08](#2013-08)
- [2013-09](#2013-09)
- [2013-10](#2013-10)
- [2013-11](#2013-11)
- [2013-12](#2013-12)

## 2013-01

<details>

<summary>2013-01-01 23:54:15 - Markov Chain Order estimation with Conditional Mutual Information</summary>

- *Maria Papapetrou, Dimitris Kugiumtzis*

- `1301.0148v1` - [abs](http://arxiv.org/abs/1301.0148v1) - [pdf](http://arxiv.org/pdf/1301.0148v1)

> We introduce the Conditional Mutual Information (CMI) for the estimation of the Markov chain order. For a Markov chain of $K$ symbols, we define CMI of order $m$, $I_c(m)$, as the mutual information of two variables in the chain being $m$ time steps apart, conditioning on the intermediate variables of the chain. We find approximate analytic significance limits based on the estimation bias of CMI and develop a randomization significance test of $I_c(m)$, where the randomized symbol sequences are formed by random permutation of the components of the original symbol sequence. The significance test is applied for increasing $m$ and the Markov chain order is estimated by the last order for which the null hypothesis is rejected. We present the appropriateness of CMI-testing on Monte Carlo simulations and compare it to the Akaike and Bayesian information criteria, the maximal fluctuation method (Peres-Shields estimator) and a likelihood ratio test for increasing orders using $\phi$-divergence. The order criterion of CMI-testing turns out to be superior for orders larger than one, but its effectiveness for large orders depends on data availability. In view of the results from the simulations, we interpret the estimated orders by the CMI-testing and the other criteria on genes and intergenic regions of DNA chains.

</details>

<details>

<summary>2013-01-02 19:40:24 - An optimal bound on the quantiles of a certain kind of distributions</summary>

- *Iosif Pinelis*

- `1301.0294v1` - [abs](http://arxiv.org/abs/1301.0294v1) - [pdf](http://arxiv.org/pdf/1301.0294v1)

> An optimal bound on the quantiles of a certain kind of distributions is given. Such a bound is used in applications to Berry--Esseen-type bounds for nonlinear statistics.

</details>

<details>

<summary>2013-01-02 23:29:19 - A Latent-Variable Bayesian Nonparametric Regression Model</summary>

- *George Karabatsos, Stephen G. Walker*

- `1212.3712v2` - [abs](http://arxiv.org/abs/1212.3712v2) - [pdf](http://arxiv.org/pdf/1212.3712v2)

> We introduce a random partition model for Bayesian nonparametric regression. The model is based on infinitely-many disjoint regions of the range of a latent covariate-dependent Gaussian process. Given a realization of the process, the cluster of dependent variable responses that share a common region are assumed to arise from the same distribution. Also, the latent Gaussian process prior allows for the random partitions (i.e., clusters of the observations) to exhibit dependencies among one another. The model is illustrated through the analysis of a real data set arising from education, and through the analysis of simulated data that were generated from complex data-generating models.

</details>

<details>

<summary>2013-01-03 04:05:33 - Martingale Expansion in Mixed Normal Limit</summary>

- *Nakahiro Yoshida*

- `1210.3680v3` - [abs](http://arxiv.org/abs/1210.3680v3) - [pdf](http://arxiv.org/pdf/1210.3680v3)

> The quasi-likelihood estimator and the Bayesian type estimator of the volatility parameter are in general asymptotically mixed normal. In case the limit is normal, the asymptotic expansion was derived in Yoshida (1997) as an application of the martingale expansion. The expansion for the asymptotically mixed normal distribution is then indispensable to develop the higher-order approximation and inference for the volatility. The classical approaches in limit theorems, where the limit is a process with independent increments or a simple mixture, do not work. We present asymptotic expansion of a martingale with asymptotically mixed normal distribution. The expansion formula is expressed by the adjoint of a random symbol with coefficients described by the Malliavin calculus, differently from the standard invariance principle. Applications to a quadratic form of a diffusion process ("realized volatility") are discussed.

</details>

<details>

<summary>2013-01-03 14:21:23 - A Simple Approach to Maximum Intractable Likelihood Estimation</summary>

- *F. J. Rubio, Adam M. Johansen*

- `1301.0463v1` - [abs](http://arxiv.org/abs/1301.0463v1) - [pdf](http://arxiv.org/pdf/1301.0463v1)

> Approximate Bayesian Computation (ABC) can be viewed as an analytic approximation of an intractable likelihood coupled with an elementary simulation step. Such a view, combined with a suitable instrumental prior distribution permits maximum-likelihood (or maximum-a-posteriori) inference to be conducted, approximately, using essentially the same techniques. An elementary approach to this problem which simply obtains a nonparametric approximation of the likelihood surface which is then used as a smooth proxy for the likelihood in a subsequent maximisation step is developed here and the convergence of this class of algorithms is characterised theoretically. The use of non-sufficient summary statistics in this context is considered. Applying the proposed method to four problems demonstrates good performance. The proposed approach provides an alternative for approximating the maximum likelihood estimator (MLE) in complex scenarios.

</details>

<details>

<summary>2013-01-03 19:41:18 - A robust Bayesian approach to modelling epistemic uncertainty in common-cause failure models</summary>

- *Matthias C. M. Troffaes, Gero Walter, Dana Kelly*

- `1301.0533v1` - [abs](http://arxiv.org/abs/1301.0533v1) - [pdf](http://arxiv.org/pdf/1301.0533v1)

> In a standard Bayesian approach to the alpha-factor model for common-cause failure, a precise Dirichlet prior distribution models epistemic uncertainty in the alpha-factors. This Dirichlet prior is then updated with observed data to obtain a posterior distribution, which forms the basis for further inferences.   In this paper, we adapt the imprecise Dirichlet model of Walley to represent epistemic uncertainty in the alpha-factors. In this approach, epistemic uncertainty is expressed more cautiously via lower and upper expectations for each alpha-factor, along with a learning parameter which determines how quickly the model learns from observed data. For this application, we focus on elicitation of the learning parameter, and find that values in the range of 1 to 10 seem reasonable. The approach is compared with Kelly and Atwood's minimally informative Dirichlet prior for the alpha-factor model, which incorporated precise mean values for the alpha-factors, but which was otherwise quite diffuse.   Next, we explore the use of a set of Gamma priors to model epistemic uncertainty in the marginal failure rate, expressed via a lower and upper expectation for this rate, again along with a learning parameter. As zero counts are generally less of an issue here, we find that the choice of this learning parameter is less crucial.   Finally, we demonstrate how both epistemic uncertainty models can be combined to arrive at lower and upper expectations for all common-cause failure rates. Thereby, we effectively provide a full sensitivity analysis of common-cause failure rates, properly reflecting epistemic uncertainty of the analyst on all levels of the common-cause failure model.

</details>

<details>

<summary>2013-01-04 07:19:23 - Nonparametric Bayes Classification via Learning of Affine Subspaces</summary>

- *Abhishek Bhattacharya*

- `1301.0681v1` - [abs](http://arxiv.org/abs/1301.0681v1) - [pdf](http://arxiv.org/pdf/1301.0681v1)

> The goal of this presentation is to build an efficient non-parametric Bayes classifier in the presence of large numbers of predictors. When analyzing such data, parametric models are often too inflexible while non-parametric procedures tend to be non-robust because of insufficient data on these high dimensional spaces. When dealing with these types of data, it is often the case that most of the variability tends to lie along a few directions, or more generally along a much smaller dimensional subspace of the feature space. Hence a class of regression models is proposed that flexibly learn about this subspace while simultaneously performing dimension reduction in classification. This methodology, allows the cell probabilities to vary non-parametrically based on a few coordinates expressed as linear combinations of the predictors. Also, as opposed to many black-box methods for dimensionality reduction, the proposed model is appealing in having clearly interpretable and identifiable parameters which provide insight into which predictors are important in determining accurate classification boundaries. Gibbs sampling methods are developed for posterior computations. The estimated cell probabilities are theoretically shown to be consistent, and real data applications are included to support the findings.

</details>

<details>

<summary>2013-01-04 16:46:55 - Bootstrap Testing of the Rank of a Matrix via Least Squared Constrained Estimation</summary>

- *FranÃ§ois Portier, Bernard Delyon*

- `1301.0768v1` - [abs](http://arxiv.org/abs/1301.0768v1) - [pdf](http://arxiv.org/pdf/1301.0768v1)

> In order to test if an unknown matrix has a given rank (null hypothesis), we consider the family of statistics that are minimum squared distances between an estimator and the manifold of fixed-rank matrix. Under the null hypothesis, every statistic of this family converges to a weighted chi-squared distribution. In this paper, we introduce the constrained bootstrap to build bootstrap estimate of the law under the null hypothesis of such statistics. As a result, the constrained bootstrap is employed to estimate the quantile for testing the rank. We provide the consistency of the procedure and the simulations shed light one the accuracy of the constrained bootstrap with respect to the traditional asymptotic comparison. More generally, the results are extended to test if an unknown parameter belongs to a sub-manifold locally smooth. Finally, the constrained bootstrap is easy to compute, it handles a large family of tests and it works under mild assumptions.

</details>

<details>

<summary>2013-01-07 09:45:55 - Probabilistic wind speed forecasting in Hungary</summary>

- *SÃ¡ndor Baran, DÃ³ra Nemoda, AndrÃ¡s HorÃ¡nyi*

- `1202.4442v4` - [abs](http://arxiv.org/abs/1202.4442v4) - [pdf](http://arxiv.org/pdf/1202.4442v4)

> Prediction of various weather quantities is mostly based on deterministic numerical weather forecasting models. Multiple runs of these models with different initial conditions result ensembles of forecasts which are applied for estimating the distribution of future weather quantities. However, the ensembles are usually under-dispersive and uncalibrated, so post-processing is required.   In the present work Bayesian Model Averaging (BMA) is applied for calibrating ensembles of wind speed forecasts produced by the operational Limited Area Model Ensemble Prediction System of the Hungarian Meteorological Service (HMS).   We describe two possible BMA models for wind speed data of the HMS and show that BMA post-processing significantly improves the calibration and precision of forecasts.

</details>

<details>

<summary>2013-01-07 17:28:37 - Bayes and Frequentism: a Particle Physicist's perspective</summary>

- *Louis Lyons*

- `1301.1273v1` - [abs](http://arxiv.org/abs/1301.1273v1) - [pdf](http://arxiv.org/pdf/1301.1273v1)

> In almost every scientific field, an experiment involves collecting data and then analysing it. The analysis stage will often consist in trying to extract some physical parameter and estimating its uncertainty; this is known as Parameter Determination. An example would be the determination of the mass of the top quark, from data collected from high energy proton-proton collisions. A different aim is to choose between two possible hypotheses. For example, are data on the recession speed s of distant galaxies proportional to their distance d, or do they fit better to a model where the expansion of the Universe is accelerating?   There are two fundamental approaches to such statistical analyses - Bayesian and Frequentist. This article discusses the way they differ in their approach to probability, and then goes on to consider how this affects the way they deal with Parameter Determination and Hypothesis Testing. The examples are taken from every-day life and from Particle Physics.

</details>

<details>

<summary>2013-01-07 18:45:49 - Bayesian Gaussian Copula Factor Models for Mixed Data</summary>

- *Jared S. Murray, David B. Dunson, Lawrence Carin, Joseph E. Lucas*

- `1111.0317v2` - [abs](http://arxiv.org/abs/1111.0317v2) - [pdf](http://arxiv.org/pdf/1111.0317v2)

> Gaussian factor models have proven widely useful for parsimoniously characterizing dependence in multivariate data. There is a rich literature on their extension to mixed categorical and continuous variables, using latent Gaussian variables or through generalized latent trait models acommodating measurements in the exponential family. However, when generalizing to non-Gaussian measured variables the latent variables typically influence both the dependence structure and the form of the marginal distributions, complicating interpretation and introducing artifacts. To address this problem we propose a novel class of Bayesian Gaussian copula factor models which decouple the latent factors from the marginal distributions. A semiparametric specification for the marginals based on the extended rank likelihood yields straightforward implementation and substantial computational gains, critical for scaling to high-dimensional applications. We provide new theoretical and empirical justifications for using this likelihood in Bayesian inference. We propose new default priors for the factor loadings and develop efficient parameter-expanded Gibbs sampling for posterior computation. The methods are evaluated through simulations and applied to a dataset in political science. The methods in this paper are implemented in the R package bfa.

</details>

<details>

<summary>2013-01-08 13:45:08 - Finding a consensus on credible features among several paleoclimate reconstructions</summary>

- *Panu ErÃ¤stÃ¶, Lasse HolmstrÃ¶m, Atte Korhola, Jan WeckstrÃ¶m*

- `1301.1527v1` - [abs](http://arxiv.org/abs/1301.1527v1) - [pdf](http://arxiv.org/pdf/1301.1527v1)

> We propose a method to merge several paleoclimate time series into one that exhibits a consensus on the features of the individual times series. The paleoclimate time series can be noisy, nonuniformly sampled and the dates at which the paleoclimate is reconstructed can have errors. Bayesian inference is used to model the various sources of uncertainty and smoothing of the posterior distribution of the consensus is used to capture its credible features in different time scales. The technique is demonstrated by analyzing a collection of six Holocene temperature reconstructions from Finnish Lapland based on various biological proxies. Although the paper focuses on paleoclimate time series, the proposed method can be applied in other contexts where one seeks to infer features that are jointly supported by an ensemble of irregularly sampled noisy time series.

</details>

<details>

<summary>2013-01-08 13:50:46 - A hierarchical max-stable spatial model for extreme precipitation</summary>

- *Brian J. Reich, Benjamin A. Shaby*

- `1301.1530v1` - [abs](http://arxiv.org/abs/1301.1530v1) - [pdf](http://arxiv.org/pdf/1301.1530v1)

> Extreme environmental phenomena such as major precipitation events manifestly exhibit spatial dependence. Max-stable processes are a class of asymptotically-justified models that are capable of representing spatial dependence among extreme values. While these models satisfy modeling requirements, they are limited in their utility because their corresponding joint likelihoods are unknown for more than a trivial number of spatial locations, preventing, in particular, Bayesian analyses. In this paper, we propose a new random effects model to account for spatial dependence. We show that our specification of the random effect distribution leads to a max-stable process that has the popular Gaussian extreme value process (GEVP) as a limiting case. The proposed model is used to analyze the yearly maximum precipitation from a regional climate model.

</details>

<details>

<summary>2013-01-08 19:47:19 - Relabeling and Summarizing Posterior Distributions in Signal Decomposition Problems when the Number of Components is Unknown</summary>

- *Alireza Roodaki, Julien Bect, Gilles Fleury*

- `1301.1650v1` - [abs](http://arxiv.org/abs/1301.1650v1) - [pdf](http://arxiv.org/pdf/1301.1650v1)

> This paper addresses the problems of relabeling and summarizing posterior distributions that typically arise, in a Bayesian framework, when dealing with signal decomposition problems with an unknown number of components. Such posterior distributions are defined over union of subspaces of differing dimensionality and can be sampled from using modern Monte Carlo techniques, for instance the increasingly popular RJ-MCMC method. No generic approach is available, however, to summarize the resulting variable-dimensional samples and extract from them component-specific parameters. We propose a novel approach, named Variable-dimensional Approximate Posterior for Relabeling and Summarizing (VAPoRS), to this problem, which consists in approximating the posterior distribution of interest by a "simple"---but still variable-dimensional---parametric distribution. The distance between the two distributions is measured using the Kullback-Leibler divergence, and a Stochastic EM-type algorithm, driven by the RJ-MCMC sampler, is proposed to estimate the parameters. Two signal decomposition problems are considered, to show the capability of VAPoRS both for relabeling and for summarizing variable dimensional posterior distributions: the classical problem of detecting and estimating sinusoids in white Gaussian noise on the one hand, and a particle counting problem motivated by the Pierre Auger project in astrophysics on the other hand.

</details>

<details>

<summary>2013-01-10 02:02:01 - Heteroscedastic Relevance Vector Machine</summary>

- *Daniel Khashabi, Mojtaba Ziyadi, Feng Liang*

- `1301.2015v1` - [abs](http://arxiv.org/abs/1301.2015v1) - [pdf](http://arxiv.org/pdf/1301.2015v1)

> In this work we propose a heteroscedastic generalization to RVM, a fast Bayesian framework for regression, based on some recent similar works. We use variational approximation and expectation propagation to tackle the problem. The work is still under progress and we are examining the results and comparing with the previous works.

</details>

<details>

<summary>2013-01-10 16:23:01 - Conditions Under Which Conditional Independence and Scoring Methods Lead to Identical Selection of Bayesian Network Models</summary>

- *Robert G. Cowell*

- `1301.2262v1` - [abs](http://arxiv.org/abs/1301.2262v1) - [pdf](http://arxiv.org/pdf/1301.2262v1)

> It is often stated in papers tackling the task of inferring Bayesian network structures from data that there are these two distinct approaches: (i) Apply conditional independence tests when testing for the presence or otherwise of edges; (ii) Search the model space using a scoring metric. Here I argue that for complete data and a given node ordering this division is a myth, by showing that cross entropy methods for checking conditional independence are mathematically identical to methods based upon discriminating between models by their overall goodness-of-fit logarithmic scores.

</details>

<details>

<summary>2013-01-10 16:23:09 - Using Bayesian Networks to Identify the Causal Effect of Speeding in Individual Vehicle/Pedestrian Collisions</summary>

- *Gary A. Davis*

- `1301.2264v1` - [abs](http://arxiv.org/abs/1301.2264v1) - [pdf](http://arxiv.org/pdf/1301.2264v1)

> On roads showing significant violations of posted speed limits, one measure of the safety effect of speeding is the difference between the road's actual accident count and the count that would have occurred if the posted speed limit had been strictly obeyed. An estimate of this accident reduction can be had by computing the probability that speeding was a necessary condition for each of set of accidents. This is an instance of assessing individual probabilities of causation, which is generally not possible absent prior knowledge of causal structure. For traffic accidents such prior knowledge is often available and this paper illustrates how, for a commonly occurring class of vehicle/pedestrian accidents, approaches to uncertainty and causal analyses appearing in the accident reconstruction literature can be unified using Bayesian networks. Measured skidmarks, pedestrian throw distances, and pedestrian injury severity are treated as evidence, and using the Gibbs Sampling routine BUGS, the posterior probability distribution over exogenous variables, such as the vehicle's initial speed, location, and driver reaction time, is computed. This posterior distribution is then used to compute the "probability of necessity" for speeding.

</details>

<details>

<summary>2013-01-10 16:23:18 - Variational MCMC</summary>

- *Nando de Freitas, Pedro Hojen-Sorensen, Michael I. Jordan, Stuart Russell*

- `1301.2266v1` - [abs](http://arxiv.org/abs/1301.2266v1) - [pdf](http://arxiv.org/pdf/1301.2266v1)

> We propose a new class of learning algorithms that combines variational approximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithms that use the variational approximation as proposal distribution can perform poorly because this approximation tends to underestimate the true variance and other features of the data. We solve this problem by introducing more sophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMC kernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH) kernel with a variational approximation as proposaldistribution. The MH kernel allows one to locate regions of high probability efficiently. The Metropolis kernel allows us to explore the vicinity of these regions. This algorithm outperforms variationalapproximations because it yields slightly better estimates of the mean and considerably better estimates of higher moments, such as covariances. It also outperforms standard MCMC algorithms because it locates theregions of high probability quickly, thus speeding up convergence. We demonstrate this algorithm on the problem of Bayesian parameter estimation for logistic (sigmoid) belief networks.

</details>

<details>

<summary>2013-01-10 16:23:30 - Learning the Dimensionality of Hidden Variables</summary>

- *Gal Elidan, Nir Friedman*

- `1301.2269v1` - [abs](http://arxiv.org/abs/1301.2269v1) - [pdf](http://arxiv.org/pdf/1301.2269v1)

> A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. Detecting hidden variables poses two problems: determining the relations to other variables in the model and determining the number of states of the hidden variable. In this paper, we address the latter problem in the context of Bayesian networks. We describe an approach that utilizes a score-based agglomerative state-clustering. As we show, this approach allows us to efficiently evaluate models with a range of cardinalities for the hidden variable. We show how to extend this procedure to deal with multiple interacting hidden variables. We demonstrate the effectiveness of this approach by evaluating it on synthetic and real-life data. We show that our approach learns models with hidden variables that generalize better and have better structure than previous approaches.

</details>

<details>

<summary>2013-01-10 16:23:36 - Multivariate Information Bottleneck</summary>

- *Nir Friedman, Ori Mosenzon, Noam Slonim, Naftali Tishby*

- `1301.2270v1` - [abs](http://arxiv.org/abs/1301.2270v1) - [pdf](http://arxiv.org/pdf/1301.2270v1)

> The Information bottleneck method is an unsupervised non-parametric data organization technique. Given a joint distribution P(A,B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. The information bottleneck has already been applied to document classification, gene expression, neural code, and spectral analysis. In this paper, we introduce a general principled framework for multivariate extensions of the information bottleneck method. This allows us to consider multiple systems of data partitions that are inter-related. Our approach utilizes Bayesian networks for specifying the systems of clusters and what information each captures. We show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations. We also present a general framework for iterative algorithms for constructing solutions, and apply it to several examples.

</details>

<details>

<summary>2013-01-10 16:24:19 - Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures</summary>

- *Geoff A. Jarrad*

- `1301.2280v1` - [abs](http://arxiv.org/abs/1301.2280v1) - [pdf](http://arxiv.org/pdf/1301.2280v1)

> A novel method for estimating Bayesian network (BN) parameters from data is presented which provides improved performance on test data. Previous research has shown the value of representing conditional probability distributions (CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network (BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local distributions,each having a different set of parents.This increases the space of possible structures which can be considered,enabling the CPDs to have finer-grained dependencies.The resulting estimation procedure induces a modelthat is better able to emulate the underlying interactions occurring in the data than conventional conditional Bernoulli network models.The results for artificially generated data indicate that overfitting is best reduced by restricting the complexity of candidate mixture substructures local to each node. Furthermore, mixtures of very simple substructures can perform almost as well as more complex ones.The BMN is also applied to data collected from an online adventure game with an application to keyhole plan recognition. The results show that the BMN-based model brings a dramatic improvement in performance over a conventional BN model.

</details>

<details>

<summary>2013-01-10 16:24:32 - Improved learning of Bayesian networks</summary>

- *Tomas Kocka, Robert Castelo*

- `1301.2283v1` - [abs](http://arxiv.org/abs/1301.2283v1) - [pdf](http://arxiv.org/pdf/1301.2283v1)

> The search space of Bayesian Network structures is usually defined as Acyclic Directed Graphs (DAGs) and the search is done by local transformations of DAGs. But the space of Bayesian Networks is ordered by DAG Markov model inclusion and it is natural to consider that a good search policy should take this into account. First attempt to do this (Chickering 1996) was using equivalence classes of DAGs instead of DAGs itself. This approach produces better results but it is significantly slower. We present a compromise between these two approaches. It uses DAGs to search the space in such a way that the ordering by inclusion is taken into account. This is achieved by repetitive usage of local moves within the equivalence class of DAGs. We show that this new approach produces better results than the original DAGs approach without substantial change in time complexity. We present empirical results, within the framework of heuristic search and Markov Chain Monte Carlo, provided through the Alarm dataset.

</details>

<details>

<summary>2013-01-10 16:24:36 - Classifier Learning with Supervised Marginal Likelihood</summary>

- *Petri Kontkanen, Petri Myllymaki, Henry Tirri*

- `1301.2284v1` - [abs](http://arxiv.org/abs/1301.2284v1) - [pdf](http://arxiv.org/pdf/1301.2284v1)

> It has been argued that in supervised classification tasks, in practice it may be more sensible to perform model selection with respect to some more focused model selection score, like the supervised (conditional) marginal likelihood, than with respect to the standard marginal likelihood criterion. However, for most Bayesian network models, computing the supervised marginal likelihood score takes exponential time with respect to the amount of observed data. In this paper, we consider diagnostic Bayesian network classifiers where the significant model parameters represent conditional distributions for the class variable, given the values of the predictor variables, in which case the supervised marginal likelihood can be computed in linear time with respect to the data. As the number of model parameters grows in this case exponentially with respect to the number of predictors, we focus on simple diagnostic models where the number of relevant predictors is small, and suggest two approaches for applying this type of models in classification. The first approach is based on mixtures of simple diagnostic models, while in the second approach we apply the small predictor sets of the simple diagnostic models for augmenting the Naive Bayes classifier.

</details>

<details>

<summary>2013-01-11 09:26:53 - Truth and memory: Linking instantaneous and retrospective self-reported cigarette consumption</summary>

- *Hao Wang, Saul Shiffman, Sandra D. Griffith, Daniel F. Heitjan*

- `1301.2429v1` - [abs](http://arxiv.org/abs/1301.2429v1) - [pdf](http://arxiv.org/pdf/1301.2429v1)

> Studies of smoking behavior commonly use the time-line follow-back (TLFB) method, or periodic retrospective recall, to gather data on daily cigarette consumption. TLFB is considered adequate for identifying periods of abstinence and lapse but not for measurement of daily cigarette consumption, thanks to substantial recall and digit preference biases. With the development of the hand-held electronic diary (ED), it has become possible to collect cigarette consumption data using ecological momentary assessment (EMA), or the instantaneous recording of each cigarette as it is smoked. EMA data, because they do not rely on retrospective recall, are thought to more accurately measure cigarette consumption. In this article we present an analysis of consumption data collected simultaneously by both methods from 236 active smokers in the pre-quit phase of a smoking cessation study. We define a statistical model that describes the genesis of the TLFB records as a two-stage process of mis-remembering and rounding, including fixed and random effects at each stage. We use Bayesian methods to estimate the model, and we evaluate its adequacy by studying histograms of imputed values of the latent remembered cigarette count. Our analysis suggests that both mis-remembering and heaping contribute substantially to the distortion of self-reported cigarette counts. Higher nicotine dependence, white ethnicity and male sex are associated with greater remembered smoking given the EMA count. The model is potentially useful in other applications where it is desirable to understand the process by which subjects remember and report true observations.

</details>

<details>

<summary>2013-01-11 12:20:52 - Latent demographic profile estimation in hard-to-reach groups</summary>

- *Tyler H. McCormick, Tian Zheng*

- `1301.2473v1` - [abs](http://arxiv.org/abs/1301.2473v1) - [pdf](http://arxiv.org/pdf/1301.2473v1)

> The sampling frame in most social science surveys excludes members of certain groups, known as hard-to-reach groups. These groups, or subpopulations, may be difficult to access (the homeless, e.g.), camouflaged by stigma (individuals with HIV/AIDS), or both (commercial sex workers). Even basic demographic information about these groups is typically unknown, especially in many developing nations. We present statistical models which leverage social network structure to estimate demographic characteristics of these subpopulations using Aggregated relational data (ARD), or questions of the form "How many X's do you know?" Unlike other network-based techniques for reaching these groups, ARD require no special sampling strategy and are easily incorporated into standard surveys. ARD also do not require respondents to reveal their own group membership. We propose a Bayesian hierarchical model for estimating the demographic characteristics of hard-to-reach groups, or latent demographic profiles, using ARD. We propose two estimation techniques. First, we propose a Markov-chain Monte Carlo algorithm for existing data or cases where the full posterior distribution is of interest. For cases when new data can be collected, we propose guidelines and, based on these guidelines, propose a simple estimate motivated by a missing data approach. Using data from McCarty et al. [Human Organization 60 (2001) 28-39], we estimate the age and gender profiles of six hard-to-reach groups, such as individuals who have HIV, women who were raped, and homeless persons. We also evaluate our simple estimates using simulation studies.

</details>

<details>

<summary>2013-01-11 17:24:20 - Information field theory</summary>

- *Torsten EnÃlin*

- `1301.2556v1` - [abs](http://arxiv.org/abs/1301.2556v1) - [pdf](http://arxiv.org/pdf/1301.2556v1)

> Non-linear image reconstruction and signal analysis deal with complex inverse problems. To tackle such problems in a systematic way, I present information field theory (IFT) as a means of Bayesian, data based inference on spatially distributed signal fields. IFT is a statistical field theory, which permits the construction of optimal signal recovery algorithms even for non-linear and non-Gaussian signal inference problems. IFT algorithms exploit spatial correlations of the signal fields and benefit from techniques developed to investigate quantum and statistical field theories, such as Feynman diagrams, re-normalisation calculations, and thermodynamic potentials. The theory can be used in many areas, and applications in cosmology and numerics are presented.

</details>

<details>

<summary>2013-01-12 20:43:14 - Support Vector Regression for Right Censored Data</summary>

- *Yair Goldberg, Michael R. Kosorok*

- `1202.5130v2` - [abs](http://arxiv.org/abs/1202.5130v2) - [pdf](http://arxiv.org/pdf/1202.5130v2)

> We develop a unified approach for classification and regression support vector machines for data subject to right censoring. We provide finite sample bounds on the generalization error of the algorithm, prove risk consistency for a wide class of probability measures, and study the associated learning rates. We apply the general methodology to estimation of the (truncated) mean, median, quantiles, and for classification problems. We present a simulation study that demonstrates the performance of the proposed approach.

</details>

<details>

<summary>2013-01-14 09:40:10 - A sequential algorithm for fast fitting of Dirichlet process mixture models</summary>

- *David Nott, Xiaole Zhang, Chris Yau, Ajay Jasra*

- `1301.2897v1` - [abs](http://arxiv.org/abs/1301.2897v1) - [pdf](http://arxiv.org/pdf/1301.2897v1)

> In this article we propose an improvement on the sequential updating and greedy search (SUGS) algorithm Wang and Dunson for fast fitting of Dirichlet process mixture models. The SUGS algorithm provides a means for very fast approximate Bayesian inference for mixture data which is particularly of use when data sets are so large that many standard Markov chain Monte Carlo (MCMC) algorithms cannot be applied efficiently, or take a prohibitively long time to converge. In particular, these ideas are used to initially interrogate the data, and to refine models such that one can potentially apply exact data analysis later on. SUGS relies upon sequentially allocating data to clusters and proceeding with an update of the posterior on the subsequent allocations and parameters which assumes this allocation is correct. Our modification softens this approach, by providing a probability distribution over allocations, with a similar computational cost; this approach has an interpretation as a variational Bayes procedure and hence we term it variational SUGS (VSUGS). It is shown in simulated examples that VSUGS can out-perform, in terms of density estimation and classification, the original SUGS algorithm in many scenarios. In addition, we present a data analysis for flow cytometry data, and SNP data via a three-class dirichlet process mixture model illustrating the apparent improvement over SUGS.

</details>

<details>

<summary>2013-01-14 12:06:44 - Bayesian inference and the parametric bootstrap</summary>

- *Bradley Efron*

- `1301.2936v1` - [abs](http://arxiv.org/abs/1301.2936v1) - [pdf](http://arxiv.org/pdf/1301.2936v1)

> The parametric bootstrap can be used for the efficient computation of Bayes posterior distributions. Importance sampling formulas take on an easy form relating to the deviance in exponential families and are particularly simple starting from Jeffreys invariant prior. Because of the i.i.d. nature of bootstrap sampling, familiar formulas describe the computational accuracy of the Bayes estimates. Besides computational methods, the theory provides a connection between Bayesian and frequentist analysis. Efficient algorithms for the frequentist accuracy of Bayesian inferences are developed and demonstrated in a model selection example.

</details>

<details>

<summary>2013-01-14 21:10:59 - A remarkably simple and accurate method for computing the Bayes Factor from a Markov chain Monte Carlo Simulation of the Posterior Distribution in high dimension</summary>

- *Martin D. Weinberg, Ilsang Yoon, Neal Katz*

- `1301.3156v1` - [abs](http://arxiv.org/abs/1301.3156v1) - [pdf](http://arxiv.org/pdf/1301.3156v1)

> Weinberg (2012) described a constructive algorithm for computing the marginal likelihood, Z, from a Markov chain simulation of the posterior distribution. Its key point is: the choice of an integration subdomain that eliminates subvolumes with poor sampling owing to low tail-values of posterior probability. Conversely, this same idea may be used to choose the subdomain that optimizes the accuracy of Z. Here, we explore using the simulated distribution to define a small region of high posterior probability, followed by a numerical integration of the sample in the selected region using the volume tessellation algorithm described in Weinberg (2012). Even more promising is the resampling of this small region followed by a naive Monte Carlo integration. The new enhanced algorithm is computationally trivial and leads to a dramatic improvement in accuracy. For example, this application of the new algorithm to a four-component mixture with random locations in 16 dimensions yields accurate evaluation of Z with 5% errors. This enables Bayes-factor model selection for real-world problems that have been infeasible with previous methods.

</details>

<details>

<summary>2013-01-14 21:47:58 - Diagnostic tools of approximate Bayesian computation using the coverage property</summary>

- *D. Prangle, M. G. B. Blum, G. Popovic, S. A. Sisson*

- `1301.3166v1` - [abs](http://arxiv.org/abs/1301.3166v1) - [pdf](http://arxiv.org/pdf/1301.3166v1)

> Approximate Bayesian computation (ABC) is an approach for sampling from an approximate posterior distribution in the presence of a computationally intractable likelihood function. A common implementation is based on simulating model, parameter and dataset triples, (m,\theta,y), from the prior, and then accepting as samples from the approximate posterior, those pairs (m,\theta) for which y, or a summary of y, is "close" to the observed data. Closeness is typically determined though a distance measure and a kernel scale parameter, \epsilon. Appropriate choice of \epsilon is important to producing a good quality approximation. This paper proposes diagnostic tools for the choice of \epsilon based on assessing the coverage property, which asserts that credible intervals have the correct coverage levels. We provide theoretical results on coverage for both model and parameter inference, and adapt these into diagnostics for the ABC context. We re-analyse a study on human demographic history to determine whether the adopted posterior approximation was appropriate. R code implementing the proposed methodology is freely available in the package "abc."

</details>

<details>

<summary>2013-01-16 08:05:04 - Sequential Bayesian Inference in Hidden Markov Stochastic Kinetic Models with Application to Detection and Response to Seasonal Epidemics</summary>

- *Junjing Lin, Michael Ludkovski*

- `1301.3617v1` - [abs](http://arxiv.org/abs/1301.3617v1) - [pdf](http://arxiv.org/pdf/1301.3617v1)

> We study sequential Bayesian inference in stochastic kinetic models with latent factors. Assuming continuous observation of all the reactions, our focus is on joint inference of the unknown reaction rates and the dynamic latent states, modeled as a hidden Markov factor. Using insights from nonlinear filtering of continuous-time jump Markov processes we develop a novel sequential Monte Carlo algorithm for this purpose. Our approach applies the ideas of particle learning to minimize particle degeneracy and exploit the analytical jump Markov structure. A motivating application of our methods is modeling of seasonal infectious disease outbreaks represented through a compartmental epidemic model. We demonstrate inference in such models with several numerical illustrations and also discuss predictive analysis of epidemic countermeasures using sequential Bayes estimates.

</details>

<details>

<summary>2013-01-16 12:34:51 - A dynamic nonstationary spatio-temporal model for short term prediction of precipitation</summary>

- *Fabio Sigrist, Hans R. KÃ¼nsch, Werner A. Stahel*

- `1102.4210v6` - [abs](http://arxiv.org/abs/1102.4210v6) - [pdf](http://arxiv.org/pdf/1102.4210v6)

> Precipitation is a complex physical process that varies in space and time. Predictions and interpolations at unobserved times and/or locations help to solve important problems in many areas. In this paper, we present a hierarchical Bayesian model for spatio-temporal data and apply it to obtain short term predictions of rainfall. The model incorporates physical knowledge about the underlying processes that determine rainfall, such as advection, diffusion and convection. It is based on a temporal autoregressive convolution with spatially colored and temporally white innovations. By linking the advection parameter of the convolution kernel to an external wind vector, the model is temporally nonstationary. Further, it allows for nonseparable and anisotropic covariance structures. With the help of the Voronoi tessellation, we construct a natural parametrization, that is, space as well as time resolution consistent, for data lying on irregular grid points. In the application, the statistical model combines forecasts of three other meteorological variables obtained from a numerical weather prediction model with past precipitation observations. The model is then used to predict three-hourly precipitation over 24 hours. It performs better than a separable, stationary and isotropic version, and it performs comparably to a deterministic numerical weather prediction model for precipitation and has the advantage that it quantifies prediction uncertainty.

</details>

<details>

<summary>2013-01-16 15:48:42 - Reversible Jump MCMC Simulated Annealing for Neural Networks</summary>

- *Christophe Andrieu, Nando de Freitas, Arnaud Doucet*

- `1301.3833v1` - [abs](http://arxiv.org/abs/1301.3833v1) - [pdf](http://arxiv.org/pdf/1301.3833v1)

> We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated annealing algorithm to optimize radial basis function (RBF) networks. This algorithm enables us to maximize the joint posterior distribution of the network parameters and the number of basis functions. It performs a global search in the joint space of the parameters and number of parameters, thereby surmounting the problem of local minima. We also show that by calibrating a Bayesian model, we can obtain the classical AIC, BIC and MDL model selection criteria within a penalized likelihood framework. Finally, we show theoretically and empirically that the algorithm converges to the modes of the full posterior distribution in an efficient way.

</details>

<details>

<summary>2013-01-16 15:48:59 - Dynamic Bayesian Multinets</summary>

- *Jeff A. Bilmes*

- `1301.3837v1` - [abs](http://arxiv.org/abs/1301.3837v1) - [pdf](http://arxiv.org/pdf/1301.3837v1)

> In this work, dynamic Bayesian multinets are introduced where a Markov chain state at time t determines conditional independence patterns between random variables lying within a local time window surrounding t. It is shown how information-theoretic criterion functions can be used to induce sparse, discriminative, and class-conditional network structures that yield an optimal approximation to the class posterior probability, and therefore are useful for the classification task. Using a new structure learning heuristic, the resulting models are tested on a medium-vocabulary isolated-word speech recognition task. It is demonstrated that these discriminatively structured dynamic Bayesian multinets, when trained in a maximum likelihood setting using EM, can outperform both HMMs and other dynamic Bayesian networks with a similar number of parameters.

</details>

<details>

<summary>2013-01-16 15:49:03 - Variational Relevance Vector Machines</summary>

- *Christopher M. Bishop, Michael Tipping*

- `1301.3838v1` - [abs](http://arxiv.org/abs/1301.3838v1) - [pdf](http://arxiv.org/pdf/1301.3838v1)

> The Support Vector Machine (SVM) of Vapnik (1998) has become widely established as one of the leading approaches to pattern recognition and machine learning. It expresses predictions in terms of a linear combination of kernel functions centred on a subset of the training data, known as support vectors.   Despite its widespread success, the SVM suffers from some important limitations, one of the most significant being that it makes point predictions rather than generating predictive distributions. Recently Tipping (1999) has formulated the Relevance Vector Machine (RVM), a probabilistic model whose functional form is equivalent to the SVM. It achieves comparable recognition accuracy to the SVM, yet provides a full predictive distribution, and also requires substantially fewer kernel functions.   The original treatment of the RVM relied on the use of type II maximum likelihood (the `evidence framework') to provide point estimates of the hyperparameters which govern model sparsity. In this paper we show how the RVM can be formulated and solved within a completely Bayesian paradigm through the use of variational inference, thereby giving a posterior distribution over both parameters and hyperparameters. We demonstrate the practicality and performance of the variational RVM using both synthetic and real world examples.

</details>

<details>

<summary>2013-01-16 15:49:23 - Bayesian Classification and Feature Selection from Finite Data Sets</summary>

- *Frans Coetzee, Steve Lawrence, C. Lee Giles*

- `1301.3843v1` - [abs](http://arxiv.org/abs/1301.3843v1) - [pdf](http://arxiv.org/pdf/1301.3843v1)

> Feature selection aims to select the smallest subset of features for a specified level of performance. The optimal achievable classification performance on a feature subset is summarized by its Receiver Operating Curve (ROC). When infinite data is available, the Neyman- Pearson (NP) design procedure provides the most efficient way of obtaining this curve. In practice the design procedure is applied to density estimates from finite data sets. We perform a detailed statistical analysis of the resulting error propagation on finite alphabets. We show that the estimated performance curve (EPC) produced by the design procedure is arbitrarily accurate given sufficient data, independent of the size of the feature set. However, the underlying likelihood ranking procedure is highly sensitive to errors that reduces the probability that the EPC is in fact the ROC. In the worst case, guaranteeing that the EPC is equal to the ROC may require data sizes exponential in the size of the feature set. These results imply that in theory the NP design approach may only be valid for characterizing relatively small feature subsets, even when the performance of any given classifier can be estimated very accurately. We discuss the practical limitations for on-line methods that ensures that the NP procedure operates in a statistically valid region.

</details>

<details>

<summary>2013-01-16 15:49:50 - A Two-round Variant of EM for Gaussian Mixtures</summary>

- *Sanjoy Dasgupta, Leonard Schulman*

- `1301.3850v1` - [abs](http://arxiv.org/abs/1301.3850v1) - [pdf](http://arxiv.org/pdf/1301.3850v1)

> Given a set of possible models (e.g., Bayesian network structures) and a data sample, in the unsupervised model selection problem the task is to choose the most accurate model with respect to the domain joint probability distribution. In contrast to this, in supervised model selection it is a priori known that the chosen model will be used in the future for prediction tasks involving more ``focused' predictive distributions. Although focused predictive distributions can be produced from the joint probability distribution by marginalization, in practice the best model in the unsupervised sense does not necessarily perform well in supervised domains. In particular, the standard marginal likelihood score is a criterion for the unsupervised task, and, although frequently used for supervised model selection also, does not perform well in such tasks. In this paper we study the performance of the marginal likelihood score empirically in supervised Bayesian network selection tasks by using a large number of publicly available classification data sets, and compare the results to those obtained by alternative model selection criteria, including empirical crossvalidation methods, an approximation of a supervised marginal likelihood measure, and a supervised version of Dawids prequential(predictive sequential) principle.The results demonstrate that the marginal likelihood score does NOT perform well FOR supervised model selection, WHILE the best results are obtained BY using Dawids prequential r napproach.

</details>

<details>

<summary>2013-01-16 15:49:54 - Minimum Message Length Clustering Using Gibbs Sampling</summary>

- *Ian Davidson*

- `1301.3851v1` - [abs](http://arxiv.org/abs/1301.3851v1) - [pdf](http://arxiv.org/pdf/1301.3851v1)

> The K-Mean and EM algorithms are popular in clustering and mixture modeling, due to their simplicity and ease of implementation. However, they have several significant limitations. Both coverage to a local optimum of their respective objective functions (ignoring the uncertainty in the model space), require the apriori specification of the number of classes/clsuters, and are inconsistent. In this work we overcome these limitations by using the Minimum Message Length (MML) principle and a variation to the K-Means/EM observation assignment and parameter calculation scheme. We maintain the simplicity of these approaches while constructing a Bayesian mixture modeling tool that samples/searches the model space using a Markov Chain Monte Carlo (MCMC) sampler known as a Gibbs sampler. Gibbs sampling allows us to visit each model according to its posterior probability. Therefore, if the model space is multi-modal we will visit all models and not get stuck in local optima. We call our approach multiple chains at equilibrium (MCE) MML sampling.

</details>

<details>

<summary>2013-01-16 15:49:57 - Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed Continuous And Discrete Variables</summary>

- *Scott Davies, Andrew Moore*

- `1301.3852v1` - [abs](http://arxiv.org/abs/1301.3852v1) - [pdf](http://arxiv.org/pdf/1301.3852v1)

> Recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous space. In particular, mixtures of Gaussians can be fitted to data very quickly using an accelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999). In this paper, we propose a kind of Bayesian networks in which low-dimensional mixtures of Gaussians over different subsets of the domain's variables are combined into a coherent joint probability model over the entire domain. The network is also capable of modeling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables. We present efficient heuristic algorithms for automatically learning these networks from data, and perform comparative experiments illustrated how well these networks model real scientific data and synthetic data. We also briefly discuss some possible improvements to the networks, as well as possible applications.

</details>

<details>

<summary>2013-01-16 15:50:01 - Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks</summary>

- *Arnaud Doucet, Nando de Freitas, Kevin Murphy, Stuart Russell*

- `1301.3853v1` - [abs](http://arxiv.org/abs/1301.3853v1) - [pdf](http://arxiv.org/pdf/1301.3853v1)

> Particle filters (PFs) are powerful sampling-based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probability distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as "condensation", "sequential Monte Carlo" and "survival of the fittest". In this paper, we show how we can exploit the structure of the DBN to increase the efficiency of particle filtering, using a technique known as Rao-Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite dimensional optimal filter. We show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial basis function networks and robot localization and map building. We also discuss other potential application areas and provide references to some finite dimensional optimal filters.

</details>

<details>

<summary>2013-01-16 15:50:14 - Being Bayesian about Network Structure</summary>

- *Nir Friedman, Daphne Koller*

- `1301.3856v1` - [abs](http://arxiv.org/abs/1301.3856v1) - [pdf](http://arxiv.org/pdf/1301.3856v1)

> In many domains, we are interested in analyzing the structure of the underlying distribution, e.g., whether one variable is a direct parent of the other. Bayesian model-selection attempts to find the MAP model and use its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed ordering over network variables. This allows us to compute, for a given ordering, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orderings rather than over network structures. The space of orderings is much smaller and more regular than the space of structures, and has a smoother posterior `landscape'. We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach.

</details>

<details>

<summary>2013-01-16 15:50:18 - Gaussian Process Networks</summary>

- *Nir Friedman, Iftach Nachman*

- `1301.3857v1` - [abs](http://arxiv.org/abs/1301.3857v1) - [pdf](http://arxiv.org/pdf/1301.3857v1)

> In this paper we address the problem of learning the structure of a Bayesian network in domains with continuous variables. This task requires a procedure for comparing different candidate structures. In the Bayesian framework, this is done by evaluating the {em marginal likelihood/} of the data given a candidate structure. This term can be computed in closed-form for standard parametric families (e.g., Gaussians), and can be approximated, at some computational cost, for some semi-parametric families (e.g., mixtures of Gaussians).   We present a new family of continuous variable probabilistic networks that are based on {em Gaussian Process/} priors. These priors are semi-parametric in nature and can learn almost arbitrary noisy functional relations. Using these priors, we can directly compute marginal likelihoods for structure learning. The resulting method can discover a wide range of functional dependencies in multivariate data. We develop the Bayesian score of Gaussian Process Networks and describe how to learn them from data. We present empirical results on artificial data as well as on real-life domains with non-linear dependencies.

</details>

<details>

<summary>2013-01-16 15:50:50 - Feature Selection and Dualities in Maximum Entropy Discrimination</summary>

- *Tony S. Jebara, Tommi S. Jaakkola*

- `1301.3865v1` - [abs](http://arxiv.org/abs/1301.3865v1) - [pdf](http://arxiv.org/pdf/1301.3865v1)

> Incorporating feature selection into a classification or regression method often carries a number of advantages. In this paper we formalize feature selection specifically from a discriminative perspective of improving classification/regression accuracy. The feature selection method is developed as an extension to the recently proposed maximum entropy discrimination (MED) framework. We describe MED as a flexible (Bayesian) regularization approach that subsumes, e.g., support vector classification, regression and exponential family models. For brevity, we restrict ourselves primarily to feature selection in the context of linear classification/regression methods and demonstrate that the proposed approach indeed carries substantial improvements in practice. Moreover, we discuss and develop various extensions of feature selection, including the problem of dealing with example specific but unobserved degrees of freedom -- alignments or invariants.

</details>

<details>

<summary>2013-01-16 15:51:30 - Tractable Bayesian Learning of Tree Belief Networks</summary>

- *Marina Meila, Tommi S. Jaakkola*

- `1301.3875v1` - [abs](http://arxiv.org/abs/1301.3875v1) - [pdf](http://arxiv.org/pdf/1301.3875v1)

> In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time. This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. 1995) constrain the tree parameter priors to be a compactly parameterized product of Dirichlet distributions. Beside allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures.

</details>

<details>

<summary>2013-01-16 15:52:56 - A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks</summary>

- *Jin Tian*

- `1301.3897v1` - [abs](http://arxiv.org/abs/1301.3897v1) - [pdf](http://arxiv.org/pdf/1301.3897v1)

> This paper extends the work in [Suzuki, 1996] and presents an efficient depth-first branch-and-bound algorithm for learning Bayesian network structures, based on the minimum description length (MDL) principle, for a given (consistent) variable ordering. The algorithm exhaustively searches through all network structures and guarantees to find the network with the best MDL score. Preliminary experiments show that the algorithm is efficient, and that the time complexity grows slowly with the sample size. The algorithm is useful for empirically studying both the performance of suboptimal heuristic search algorithms and the adequacy of the MDL principle in learning Bayesian networks.

</details>

<details>

<summary>2013-01-16 15:53:05 - Model-Based Hierarchical Clustering</summary>

- *Shivakumar Vaithyanathan, Byron E Dom*

- `1301.3899v1` - [abs](http://arxiv.org/abs/1301.3899v1) - [pdf](http://arxiv.org/pdf/1301.3899v1)

> We present an approach to model-based hierarchical clustering by formulating an objective function based on a Bayesian analysis. This model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model. Features can have either a unique distribution in every cluster or a common distribution over some (or even all) of the clusters. The cluster subsets over which these features have such a common distribution correspond to the nodes (clusters) of the tree representing the hierarchy. We apply this general model to the problem of document clustering for which we use a multinomial likelihood function and Dirichlet priors. Our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters. The regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters, the depth of the tree and the subset of features to be modeled as having a common distribution at each node. We present experimental results on both synthetic data and a real document collection.

</details>

<details>

<summary>2013-01-16 15:53:20 - Model Criticism of Bayesian Networks with Latent Variables</summary>

- *David M. Williamson, Russell Almond, Robert Mislevy*

- `1301.3902v1` - [abs](http://arxiv.org/abs/1301.3902v1) - [pdf](http://arxiv.org/pdf/1301.3902v1)

> The application of Bayesian networks (BNs) to cognitive assessment and intelligent tutoring systems poses new challenges for model construction. When cognitive task analyses suggest constructing a BN with several latent variables, empirical model criticism of the latent structure becomes both critical and complex. This paper introduces a methodology for criticizing models both globally (a BN in its entirety) and locally (observable nodes), and explores its value in identifying several kinds of misfit: node errors, edge errors, state errors, and prior probability errors in the latent structure. The results suggest the indices have potential for detecting model misfit and assisting in locating problematic components of the model.

</details>

<details>

<summary>2013-01-17 16:08:00 - Non-parametric Bayesian modelling of digital gene expression data</summary>

- *Dimitrios V. Vavoulis, Julian Gough*

- `1301.4144v1` - [abs](http://arxiv.org/abs/1301.4144v1) - [pdf](http://arxiv.org/pdf/1301.4144v1)

> Next-generation sequencing technologies provide a revolutionary tool for generating gene expression data. Starting with a fixed RNA sample, they construct a library of millions of differentially abundant short sequence tags or "reads", which constitute a fundamentally discrete measure of the level of gene expression. A common limitation in experiments using these technologies is the low number or even absence of biological replicates, which complicates the statistical analysis of digital gene expression data. Analysis of this type of data has often been based on modified tests originally devised for analysing microarrays; both these and even de novo methods for the analysis of RNA-seq data are plagued by the common problem of low replication. We propose a novel, non-parametric Bayesian approach for the analysis of digital gene expression data. We begin with a hierarchical model for modelling over-dispersed count data and a blocked Gibbs sampling algorithm for inferring the posterior distribution of model parameters conditional on these counts. The algorithm compensates for the problem of low numbers of biological replicates by clustering together genes with tag counts that are likely sampled from a common distribution and using this augmented sample for estimating the parameters of this distribution. The number of clusters is not decided a priori, but it is inferred along with the remaining model parameters. We demonstrate the ability of this approach to model biological data with high fidelity by applying the algorithm on a public dataset obtained from cancerous and non-cancerous neural tissues.

</details>

<details>

<summary>2013-01-18 09:39:06 - Bayesian model selection for exponential random graph models</summary>

- *Alberto Caimo, Nial Friel*

- `1201.2337v3` - [abs](http://arxiv.org/abs/1201.2337v3) - [pdf](http://arxiv.org/pdf/1201.2337v3)

> Exponential random graph models are a class of widely used exponential family models for social networks. The topological structure of an observed network is modelled by the relative prevalence of a set of local sub-graph configurations termed network statistics. One of the key tasks in the application of these models is which network statistics to include in the model. This can be thought of as statistical model selection problem. This is a very challenging problem---the posterior distribution for each model is often termed "doubly intractable" since computation of the likelihood is rarely available, but also, the evidence of the posterior is, as usual, intractable. The contribution of this paper is the development of a fully Bayesian model selection method based on a reversible jump Markov chain Monte Carlo algorithm extension of Caimo and Friel (2011) which estimates the posterior probability for each competing model.

</details>

<details>

<summary>2013-01-18 16:32:03 - Evolutionary Inference via the Poisson Indel Process</summary>

- *Alexandre Bouchard-CÃ´tÃ©, Michael I. Jordan*

- `1207.6327v2` - [abs](http://arxiv.org/abs/1207.6327v2) - [pdf](http://arxiv.org/pdf/1207.6327v2)

> We address the problem of the joint statistical inference of phylogenetic trees and multiple sequence alignments from unaligned molecular sequences. This problem is generally formulated in terms of string-valued evolutionary processes along the branches of a phylogenetic tree. The classical evolutionary process, the TKF91 model, is a continuous-time Markov chain model comprised of insertion, deletion and substitution events. Unfortunately this model gives rise to an intractable computational problem---the computation of the marginal likelihood under the TKF91 model is exponential in the number of taxa. In this work, we present a new stochastic process, the Poisson Indel Process (PIP), in which the complexity of this computation is reduced to linear. The new model is closely related to the TKF91 model, differing only in its treatment of insertions, but the new model has a global characterization as a Poisson process on the phylogeny. Standard results for Poisson processes allow key computations to be decoupled, which yields the favorable computational profile of inference under the PIP model. We present illustrative experiments in which Bayesian inference under the PIP model is compared to separate inference of phylogenies and alignments.

</details>

<details>

<summary>2013-01-19 18:35:35 - Semi-parametric Robust Event Detection for Massive Time-Domain Databases</summary>

- *Alexander W Blocker, Pavlos Protopapas*

- `1301.3027v2` - [abs](http://arxiv.org/abs/1301.3027v2) - [pdf](http://arxiv.org/pdf/1301.3027v2)

> The detection and analysis of events within massive collections of time-series has become an extremely important task for time-domain astronomy. In particular, many scientific investigations (e.g. the analysis of microlensing and other transients) begin with the detection of isolated events in irregularly-sampled series with both non-linear trends and non-Gaussian noise. We outline a semi-parametric, robust, parallel method for identifying variability and isolated events at multiple scales in the presence of the above complications. This approach harnesses the power of Bayesian modeling while maintaining much of the speed and scalability of more ad-hoc machine learning approaches. We also contrast this work with event detection methods from other fields, highlighting the unique challenges posed by astronomical surveys. Finally, we present results from the application of this method to 87.2 million EROS-2 sources, where we have obtained a greater than 100-fold reduction in candidates for certain types of phenomena while creating high-quality features for subsequent analyses.

</details>

<details>

<summary>2013-01-20 05:11:40 - Intrinsic posterior regret gamma-minimax estimation for the exponential family of distributions</summary>

- *Mohammad Jafari Jozani, Nahid Jafari Tabrizi*

- `1301.4628v1` - [abs](http://arxiv.org/abs/1301.4628v1) - [pdf](http://arxiv.org/pdf/1301.4628v1)

> In practice, it is desired to have estimates that are invariant under reparameterization. The invariance property of the estimators helps to formulate a unified solution to the underlying estimation problem. In robust Bayesian analysis, a frequent criticism is that the optimal estimators are not invariant under smooth reparameterizations. This paper considers the problem of posterior regret gamma-minimax (PRGM) estimation of the natural parameter of the exponential family of distributions under intrinsic loss functions. We show that under the class of Jeffrey's Conjugate Prior (JCP) distributions, PRGM estimators are invariant to smooth one-to-one reparameterizations. We apply our results to several distributions and different classes of JCP, as well as the usual conjugate prior distributions. We observe that, in many cases, invariant PRGM estimators in the class of JCP distributions can be obtained by some modifications of PRGM estimators in the usual class of conjugate priors.   Moreover, when the class of priors are convex or dependant on a hyper-parameter belonging to a connected set, we show that the PRGM estimator under the intrinsic loss function could be Bayes with respect to a prior distribution in the original prior class. Theoretical results are supplemented with several examples and illustrations.

</details>

<details>

<summary>2013-01-21 16:22:40 - Advanced Interacting Sequential Monte Carlo Sampling for Inverse Scattering</summary>

- *FranÃ§ois Giraud, Pierre Minvielle, Pierre Del Moral*

- `1301.4913v1` - [abs](http://arxiv.org/abs/1301.4913v1) - [pdf](http://arxiv.org/pdf/1301.4913v1)

> The following electromagnetism (EM) inverse problem is addressed. It consists in estimating local radioelectric properties of materials recovering an object from global EM scattering measurements, at various incidences and wave frequencies. This large scale ill-posed inverse problem is explored by an intensive exploitation of an efficient 2D Maxwell solver, distributed on high performance computing machines. Applied to a large training data set, a statistical analysis reduces the problem to a simpler probabilistic metamodel, on which Bayesian inference can be performed. Considering the radioelectric properties as a hidden dynamic stochastic process, that evolves in function of the frequency, it is shown how advanced Markov Chain Monte Carlo methods, called Sequential Monte Carlo (SMC) or interacting particles, can take benefit of the structure and provide local EM property estimates.

</details>

<details>

<summary>2013-01-21 18:32:48 - Bayesian Conditional Tensor Factorizations for High-Dimensional Classification</summary>

- *Yun Yang, David B. Dunson*

- `1301.4950v1` - [abs](http://arxiv.org/abs/1301.4950v1) - [pdf](http://arxiv.org/pdf/1301.4950v1)

> In many application areas, data are collected on a categorical response and high-dimensional categorical predictors, with the goals being to build a parsimonious model for classification while doing inferences on the important predictors. In settings such as genomics, there can be complex interactions among the predictors. By using a carefully-structured Tucker factorization, we define a model that can characterize any conditional probability, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the predictors to be included. Under near sparsity assumptions, the posterior distribution for the conditional probability is shown to achieve close to the parametric rate of contraction even in ultra high-dimensional settings. The methods are illustrated using simulation examples and biomedical applications.

</details>

<details>

<summary>2013-01-22 15:44:36 - Fast estimation of posterior probabilities in change-point models through a constrained hidden Markov model</summary>

- *The-Minh Luong, Yves Rozenholc, Gregory Nuel*

- `1203.4394v2` - [abs](http://arxiv.org/abs/1203.4394v2) - [pdf](http://arxiv.org/pdf/1203.4394v2)

> The detection of change-points in heterogeneous sequences is a statistical challenge with applications across a wide variety of fields. In bioinformatics, a vast amount of methodology exists to identify an ideal set of change-points for detecting Copy Number Variation (CNV). While considerable efficient algorithms are currently available for finding the best segmentation of the data in CNV, relatively few approaches consider the important problem of assessing the uncertainty of the change-point location. Asymptotic and stochastic approaches exist but often require additional model assumptions to speed up the computations, while exact methods have quadratic complexity which usually are intractable for large datasets of tens of thousands points or more. In this paper, we suggest an exact method for obtaining the posterior distribution of change-points with linear complexity, based on a constrained hidden Markov model. The methods are implemented in the R package postCP, which uses the results of a given change-point detection algorithm to estimate the probability that each observation is a change-point. We present the results of the package on a publicly available CNV data set (n=120). Due to its frequentist framework, postCP obtains less conservative confidence intervals than previously published Bayesian methods, but with linear complexity instead of quadratic. Simulations showed that postCP provided comparable loss to a Bayesian MCMC method when estimating posterior means, specifically when assessing larger-scale changes, while being more computationally efficient. On another high-resolution CNV data set (n=14,241), the implementation processed information in less than one second on a mid-range laptop computer.

</details>

<details>

<summary>2013-01-23 15:56:44 - Inferring Parameters and Structure of Latent Variable Models by Variational Bayes</summary>

- *Hagai Attias*

- `1301.6676v1` - [abs](http://arxiv.org/abs/1301.6676v1) - [pdf](http://arxiv.org/pdf/1301.6676v1)

> Current methods for learning graphical models with latent variables and a fixed structure estimate optimal values for the model parameters. Whereas this approach usually produces overfitting and suboptimal generalization performance, carrying out the Bayesian program of computing the full posterior distributions over the parameters remains a difficult problem. Moreover, learning the structure of models with latent variables, for which the Bayesian approach is crucial, is yet a harder problem. In this paper I present the Variational Bayes framework, which provides a solution to these problems. This approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner without resorting to sampling methods. Unlike in the Laplace approximation, these posteriors are generally non-Gaussian and no Hessian needs to be computed. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. I demonstrate that this algorithm can be applied to a large class of models in several domains, including unsupervised clustering and blind source separation.

</details>

<details>

<summary>2013-01-23 15:57:14 - Comparing Bayesian Network Classifiers</summary>

- *Jie Cheng, Russell Greiner*

- `1301.6684v1` - [abs](http://arxiv.org/abs/1301.6684v1) - [pdf](http://arxiv.org/pdf/1301.6684v1)

> In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN augmented Naive-Bayes and general BNs, where the latter two are learned using two variants of a conditional-independence (CI) based BN-learning algorithm. Experimental results show the obtained classifiers, learned using the CI based algorithms, are competitive with (or superior to) the best known classifiers, based on both Bayesian networks and other formalisms; and that the computational time for learning and using these classifiers is relatively small. Moreover, these results also suggest a way to learn yet more effective classifiers; we demonstrate empirically that this new algorithm does work as expected. Collectively, these results argue that BN classifiers deserve more attention in machine learning and data mining communities.

</details>

<details>

<summary>2013-01-23 15:58:00 - Data Analysis with Bayesian Networks: A Bootstrap Approach</summary>

- *Nir Friedman, Moises Goldszmidt, Abraham Wyner*

- `1301.6695v1` - [abs](http://arxiv.org/abs/1301.6695v1) - [pdf](http://arxiv.org/pdf/1301.6695v1)

> In recent years there has been significant progress in algorithms and methods for inducing Bayesian networks from data. However, in complex data analysis problems, we need to go beyond being satisfied with inducing networks with high scores. We need to provide confidence measures on features of these networks: Is the existence of an edge between two nodes warranted? Is the Markov blanket of a given node robust? Can we say something about the ordering of the variables? We should be able to address these questions, even when the amount of data is not enough to induce a high scoring network. In this paper we propose Efron's Bootstrap as a computationally efficient approach for answering these questions. In addition, we propose to use these confidence measures to induce better structures from the data, and to detect the presence of latent variables.

</details>

<details>

<summary>2013-01-23 15:58:05 - Learning Bayesian Network Structure from Massive Datasets: The "Sparse Candidate" Algorithm</summary>

- *Nir Friedman, Iftach Nachman, Dana Pe'er*

- `1301.6696v1` - [abs](http://arxiv.org/abs/1301.6696v1) - [pdf](http://arxiv.org/pdf/1301.6696v1)

> Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of instances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates. We then search for a network that satisfies these constraints. The learned network is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures.

</details>

<details>

<summary>2013-01-23 15:59:02 - On Supervised Selection of Bayesian Networks</summary>

- *Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri*

- `1301.6710v1` - [abs](http://arxiv.org/abs/1301.6710v1) - [pdf](http://arxiv.org/pdf/1301.6710v1)

> Given a set of possible models (e.g., Bayesian network structures) and a data sample, in the unsupervised model selection problem the task is to choose the most accurate model with respect to the domain joint probability distribution. In contrast to this, in supervised model selection it is a priori known that the chosen model will be used in the future for prediction tasks involving more ``focused' predictive distributions. Although focused predictive distributions can be produced from the joint probability distribution by marginalization, in practice the best model in the unsupervised sense does not necessarily perform well in supervised domains. In particular, the standard marginal likelihood score is a criterion for the unsupervised task, and, although frequently used for supervised model selection also, does not perform well in such tasks. In this paper we study the performance of the marginal likelihood score empirically in supervised Bayesian network selection tasks by using a large number of publicly available classification data sets, and compare the results to those obtained by alternative model selection criteria, including empirical crossvalidation methods, an approximation of a supervised marginal likelihood measure, and a supervised version of Dawids prequential(predictive sequential) principle.The results demonstrate that the marginal likelihood score does NOT perform well FOR supervised model selection, WHILE the best results are obtained BY using Dawids prequential r napproach.

</details>

<details>

<summary>2013-01-23 15:59:50 - Bayes Nets in Educational Assessment: Where Do the Numbers Come From?</summary>

- *Robert Mislevy, Russell Almond, Duanli Yan, Linda S. Steinberg*

- `1301.6722v1` - [abs](http://arxiv.org/abs/1301.6722v1) - [pdf](http://arxiv.org/pdf/1301.6722v1)

> As observations and student models become complex, educational assessments that exploit advances in technology and cognitive psychology can outstrip familiar testing models and analytic methods. Within the Portal conceptual framework for assessment design, Bayesian inference networks (BINs) record beliefs about students' knowledge and skills, in light of what they say and do. Joining evidence model BIN fragments- which contain observable variables and pointers to student model variables - to the student model allows one to update belief about knowledge and skills as observations arrive. Markov Chain Monte Carlo (MCMC) techniques can estimate the required conditional probabilities from empirical data, supplemented by expert judgment or substantive theory. Details for the special cases of item response theory (IRT) and multivariate latent class modeling are given, with a numerical example of the latter.

</details>

<details>

<summary>2013-01-23 15:59:54 - A Bayesian Network Classifier that Combines a Finite Mixture Model and a Naive Bayes Model</summary>

- *Stefano Monti, Gregory F. Cooper*

- `1301.6723v1` - [abs](http://arxiv.org/abs/1301.6723v1) - [pdf](http://arxiv.org/pdf/1301.6723v1)

> In this paper we present a new Bayesian network model for classification that combines the naive-Bayes (NB) classifier and the finite-mixture (FM) classifier. The resulting classifier aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance, both in terms of accuracy and in terms of calibration of the estimated probabilities. The proposed classifier is obtained by superimposing a finite mixture model on the set of feature variables of a naive Bayes model. We present experimental results that compare the predictive performance on real datasets of the new classifier with the predictive performance of the NB classifier and the FM classifier.

</details>

<details>

<summary>2013-01-23 15:59:58 - A Variational Approximation for Bayesian Networks with Discrete and Continuous Latent Variables</summary>

- *Kevin Murphy*

- `1301.6724v1` - [abs](http://arxiv.org/abs/1301.6724v1) - [pdf](http://arxiv.org/pdf/1301.6724v1)

> We show how to use a variational approximation to the logistic function to perform approximate inference in Bayesian networks containing discrete nodes with continuous parents. Essentially, we convert the logistic function to a Gaussian, which facilitates exact inference, and then iteratively adjust the variational parameters to improve the quality of the approximation. We demonstrate experimentally that this approximation is faster and potentially more accurate than sampling. We also introduce a simple new technique for handling evidence, which allows us to handle arbitrary distributions on observed nodes, as well as achieving a significant speedup in networks with discrete variables of large cardinality.

</details>

<details>

<summary>2013-01-23 16:00:10 - Learning Bayesian Networks with Restricted Causal Interactions</summary>

- *Julian R. Neil, Chris S. Wallace, Kevin B. Korb*

- `1301.6727v1` - [abs](http://arxiv.org/abs/1301.6727v1) - [pdf](http://arxiv.org/pdf/1301.6727v1)

> A major problem for the learning of Bayesian networks (BNs) is the exponential number of parameters needed for conditional probability tables. Recent research reduces this complexity by modeling local structure in the probability tables. We examine the use of log-linear local models. While log-linear models in this context are not new (Whittaker, 1990; Buntine, 1991; Neal, 1992; Heckerman and Meek, 1997), for structure learning they are generally subsumed under a naive Bayes model. We describe an alternative interpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric for structure learning of networks exhibiting causal independence, which we term first-order networks (FONs). We also investigate local model selection on a node-by-node basis.

</details>

<details>

<summary>2013-01-23 16:00:53 - Approximate Learning in Complex Dynamic Bayesian Networks</summary>

- *Raffaella Settimi, Jim Q. Smith, A. S. Gargoum*

- `1301.6738v1` - [abs](http://arxiv.org/abs/1301.6738v1) - [pdf](http://arxiv.org/pdf/1301.6738v1)

> In this paper we extend the work of Smith and Papamichail (1999) and present fast approximate Bayesian algorithms for learning in complex scenarios where at any time frame, the relationships between explanatory state space variables can be described by a Bayesian network that evolve dynamically over time and the observations taken are not necessarily Gaussian. It uses recent developments in approximate Bayesian forecasting methods in combination with more familiar Gaussian propagation algorithms on junction trees. The procedure for learning state parameters from data is given explicitly for common sampling distributions and the methodology is illustrated through a real application. The efficiency of the dynamic approximation is explored by using the Hellinger divergence measure and theoretical bounds for the efficacy of such a procedure are discussed.

</details>

<details>

<summary>2013-01-25 22:03:25 - Mixture Gaussian Process Conditional Heteroscedasticity</summary>

- *Emmanouil A. Platanios, Sotirios P. Chatzis*

- `1211.4410v4` - [abs](http://arxiv.org/abs/1211.4410v4) - [pdf](http://arxiv.org/pdf/1211.4410v4)

> Generalized autoregressive conditional heteroscedasticity (GARCH) models have long been considered as one of the most successful families of approaches for volatility modeling in financial return series. In this paper, we propose an alternative approach based on methodologies widely used in the field of statistical machine learning. Specifically, we propose a novel nonparametric Bayesian mixture of Gaussian process regression models, each component of which models the noise variance process that contaminates the observed data as a separate latent Gaussian process driven by the observed data. This way, we essentially obtain a mixture Gaussian process conditional heteroscedasticity (MGPCH) model for volatility modeling in financial return series. We impose a nonparametric prior with power-law nature over the distribution of the model mixture components, namely the Pitman-Yor process prior, to allow for better capturing modeled data distributions with heavy tails and skewness. Finally, we provide a copula- based approach for obtaining a predictive posterior for the covariances over the asset returns modeled by means of a postulated MGPCH model. We evaluate the efficacy of our approach in a number of benchmark scenarios, and compare its performance to state-of-the-art methodologies.

</details>

<details>

<summary>2013-01-26 19:31:26 - AABC: approximate approximate Bayesian computation when simulating a large number of data sets is computationally infeasible</summary>

- *Erkan O. Buzbas, Noah A. Rosenberg*

- `1301.6282v1` - [abs](http://arxiv.org/abs/1301.6282v1) - [pdf](http://arxiv.org/pdf/1301.6282v1)

> Approximate Bayesian computation (ABC) methods perform inference on model-specific parameters of mechanistically motivated parametric statistical models when evaluating likelihoods is difficult. Central to the success of ABC methods is computationally inexpensive simulation of data sets from the parametric model of interest. However, when simulating data sets from a model is so computationally expensive that the posterior distribution of parameters cannot be adequately sampled by ABC, inference is not straightforward. We present approximate approximate Bayesian computation" (AABC), a class of methods that extends simulation-based inference by ABC to models in which simulating data is expensive. In AABC, we first simulate a limited number of data sets that is computationally feasible to simulate from the parametric model. We use these data sets as fixed background information to inform a non-mechanistic statistical model that approximates the correct parametric model and enables efficient simulation of a large number of data sets by Bayesian resampling methods. We show that under mild assumptions, the posterior distribution obtained by AABC converges to the posterior distribution obtained by ABC, as the number of data sets simulated from the parametric model and the sample size of the observed data set increase simultaneously. We illustrate the performance of AABC on a population-genetic model of natural selection, as well as on a model of the admixture history of hybrid populations.

</details>

<details>

<summary>2013-01-26 19:47:16 - Generalized double Pareto shrinkage</summary>

- *Artin Armagan, David Dunson, Jaeyong Lee*

- `1104.0861v4` - [abs](http://arxiv.org/abs/1104.0861v4) - [pdf](http://arxiv.org/pdf/1104.0861v4)

> We propose a generalized double Pareto prior for Bayesian shrinkage estimation and inferences in linear models. The prior can be obtained via a scale mixture of Laplace or normal distributions, forming a bridge between the Laplace and Normal-Jeffreys' priors. While it has a spike at zero like the Laplace density, it also has a Student's $t$-like tail behavior. Bayesian computation is straightforward via a simple Gibbs sampling algorithm. We investigate the properties of the maximum a posteriori estimator, as sparse estimation plays an important role in many problems, reveal connections with some well-established regularization procedures, and show some asymptotic results. The performance of the prior is tested through simulations and an application.

</details>

<details>

<summary>2013-01-27 05:30:35 - Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines</summary>

- *Louis Yuanlong Shao*

- `1210.8442v3` - [abs](http://arxiv.org/abs/1210.8442v3) - [pdf](http://arxiv.org/pdf/1210.8442v3)

> One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.

</details>

<details>

<summary>2013-01-29 19:09:59 - Feature allocations, probability functions, and paintboxes</summary>

- *Tamara Broderick, Jim Pitman, Michael I. Jordan*

- `1301.6647v2` - [abs](http://arxiv.org/abs/1301.6647v2) - [pdf](http://arxiv.org/pdf/1301.6647v2)

> The problem of inferring a clustering of a data set has been the subject of much research in Bayesian analysis, and there currently exists a solid mathematical foundation for Bayesian approaches to clustering. In particular, the class of probability distributions over partitions of a data set has been characterized in a number of ways, including via exchangeable partition probability functions (EPPFs) and the Kingman paintbox. Here, we develop a generalization of the clustering problem, called feature allocation, where we allow each data point to belong to an arbitrary, non-negative integer number of groups, now called features or topics. We define and study an "exchangeable feature probability function" (EFPF)---analogous to the EPPF in the clustering setting---for certain types of feature models. Moreover, we introduce a "feature paintbox" characterization---analogous to the Kingman paintbox for clustering---of the class of exchangeable feature models. We provide a further characterization of the subclass of feature allocations that have EFPF representations.

</details>

<details>

<summary>2013-01-30 15:03:37 - The Bayesian Structural EM Algorithm</summary>

- *Nir Friedman*

- `1301.7373v1` - [abs](http://arxiv.org/abs/1301.7373v1) - [pdf](http://arxiv.org/pdf/1301.7373v1)

> In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data- that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof.

</details>

<details>

<summary>2013-01-30 15:03:52 - Graphical Models and Exponential Families</summary>

- *Dan Geiger, Christopher Meek*

- `1301.7376v1` - [abs](http://arxiv.org/abs/1301.7376v1) - [pdf](http://arxiv.org/pdf/1301.7376v1)

> We provide a classification of graphical models according to their representation as subfamilies of exponential families. Undirected graphical models with no hidden variables are linear exponential families (LEFs), directed acyclic graphical models and chain graphs with no hidden variables, including Bayesian networks with several families of local distributions, are curved exponential families (CEFs) and graphical models with hidden variables are stratified exponential families (SEFs). An SEF is a finite union of CEFs satisfying a frontier condition. In addition, we illustrate how one can automatically generate independence and non-independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables. The relevance of these results for model selection is examined.

</details>

<details>

<summary>2013-01-30 15:06:43 - On the Geometry of Bayesian Graphical Models with Hidden Variables</summary>

- *Raffaella Settimi, Jim Q. Smith*

- `1301.7411v1` - [abs](http://arxiv.org/abs/1301.7411v1) - [pdf](http://arxiv.org/pdf/1301.7411v1)

> In this paper we investigate the geometry of the likelihood of the unknown parameters in a simple class of Bayesian directed graphs with hidden variables. This enables us, before any numerical algorithms are employed, to obtain certain insights in the nature of the unidentifiability inherent in such models, the way posterior densities will be sensitive to prior densities and the typical geometrical form these posterior densities might take. Many of these insights carry over into more complicated Bayesian networks with systematic missing data.

</details>

<details>

<summary>2013-01-31 10:02:15 - Consistent nonparametric Bayesian inference for discretely observed scalar diffusions</summary>

- *Frank van der Meulen, Harry van Zanten*

- `1301.7567v1` - [abs](http://arxiv.org/abs/1301.7567v1) - [pdf](http://arxiv.org/pdf/1301.7567v1)

> We study Bayes procedures for the problem of nonparametric drift estimation for one-dimensional, ergodic diffusion models from discrete-time, low-frequency data. We give conditions for posterior consistency and verify these conditions for concrete priors, including priors based on wavelet expansions.

</details>

<details>

<summary>2013-01-31 13:17:44 - Backfitting and smooth backfitting for additive quantile models</summary>

- *Young Kyung Lee, Enno Mammen, Byeong U. Park*

- `1011.2592v2` - [abs](http://arxiv.org/abs/1011.2592v2) - [pdf](http://arxiv.org/pdf/1011.2592v2)

> In this paper, we study the ordinary backfitting and smooth backfitting as methods of fitting additive quantile models. We show that these backfitting quantile estimators are asymptotically equivalent to the corresponding backfitting estimators of the additive components in a specially-designed additive mean regression model. This implies that the theoretical properties of the backfitting quantile estimators are not unlike those of backfitting mean regression estimators. We also assess the finite sample properties of the two backfitting quantile estimators.

</details>

<details>

<summary>2013-01-31 14:17:28 - Rank regularization and Bayesian inference for tensor completion and extrapolation</summary>

- *Juan Andres Bazerque, Gonzalo Mateos, Georgios B. Giannakis*

- `1301.7619v1` - [abs](http://arxiv.org/abs/1301.7619v1) - [pdf](http://arxiv.org/pdf/1301.7619v1)

> A novel regularizer of the PARAFAC decomposition factors capturing the tensor's rank is proposed in this paper, as the key enabler for completion of three-way data arrays with missing entries. Set in a Bayesian framework, the tensor completion method incorporates prior information to enhance its smoothing and prediction capabilities. This probabilistic approach can naturally accommodate general models for the data distribution, lending itself to various fitting criteria that yield optimum estimates in the maximum-a-posteriori sense. In particular, two algorithms are devised for Gaussian- and Poisson-distributed data, that minimize the rank-regularized least-squares error and Kullback-Leibler divergence, respectively. The proposed technique is able to recover the "ground-truth'' tensor rank when tested on synthetic data, and to complete brain imaging and yeast gene expression datasets with 50% and 15% of missing entries respectively, resulting in recovery errors at -10dB and -15dB.

</details>


## 2013-02

<details>

<summary>2013-02-01 09:34:20 - A Bayesian nonparametric approach to modeling market share dynamics</summary>

- *Igor PrÃ¼nster, Matteo Ruggiero*

- `1302.0115v1` - [abs](http://arxiv.org/abs/1302.0115v1) - [pdf](http://arxiv.org/pdf/1302.0115v1)

> We propose a flexible stochastic framework for modeling the market share dynamics over time in a multiple markets setting, where firms interact within and between markets. Firms undergo stochastic idiosyncratic shocks, which contract their shares, and compete to consolidate their position by acquiring new ones in both the market where they operate and in new markets. The model parameters can meaningfully account for phenomena such as barriers to entry and exit, fixed and sunk costs, costs of expanding to new sectors with different technologies and competitive advantage among firms. The construction is obtained in a Bayesian framework by means of a collection of nonparametric hierarchical mixtures, which induce the dependence between markets and provide a generalization of the Blackwell-MacQueen P\'{o}lya urn scheme, which in turn is used to generate a partially exchangeable dynamical particle system. A Markov Chain Monte Carlo algorithm is provided for simulating trajectories of the system, by means of which we perform a simulation study for transitions to different economic regimes. Moreover, it is shown that the infinite-dimensional properties of the system, when appropriately transformed and rescaled, are those of a collection of interacting Fleming-Viot diffusions.

</details>

<details>

<summary>2013-02-01 13:24:39 - Weak limits for exploratory plots in the analysis of extremes</summary>

- *Bikramjit Das, Souvik Ghosh*

- `1008.2639v2` - [abs](http://arxiv.org/abs/1008.2639v2) - [pdf](http://arxiv.org/pdf/1008.2639v2)

> Exploratory data analysis is often used to test the goodness-of-fit of sample observations to specific target distributions. A few such graphical tools have been extensively used to detect subexponential or heavy-tailed behavior in observed data. In this paper we discuss asymptotic limit behavior of two such plotting tools: the quantile-quantile plot and the mean excess plot. The weak consistency of these plots to fixed limit sets in an appropriate topology of $\mathbb{R}^2$ has been shown in Das and Resnick (Stoch. Models 24 (2008) 103-132) and Ghosh and Resnick (Stochastic Process. Appl. 120 (2010) 1492-1517). In this paper we find asymptotic distributional limits for these plots when the underlying distributions have regularly varying right-tails. As an application we construct confidence bounds around the plots which enable us to statistically test whether the underlying distribution is heavy-tailed or not.

</details>

<details>

<summary>2013-02-01 13:40:40 - Sharp Oracle Inequalities for Aggregation of Affine Estimators</summary>

- *Arnak Dalalyan, Joseph Salmon*

- `1104.3969v4` - [abs](http://arxiv.org/abs/1104.3969v4) - [pdf](http://arxiv.org/pdf/1104.3969v4)

> We consider the problem of combining a (possibly uncountably infinite) set of affine estimators in non-parametric regression model with heteroscedastic Gaussian noise. Focusing on the exponentially weighted aggregate, we prove a PAC-Bayesian type inequality that leads to sharp oracle inequalities in discrete but also in continuous settings. The framework is general enough to cover the combinations of various procedures such as least square regression, kernel ridge regression, shrinking estimators and many other estimators used in the literature on statistical inverse problems. As a consequence, we show that the proposed aggregate provides an adaptive estimator in the exact minimax sense without neither discretizing the range of tuning parameters nor splitting the set of observations. We also illustrate numerically the good performance achieved by the exponentially weighted aggregate.

</details>

<details>

<summary>2013-02-01 13:45:00 - A quantile regression estimator for censored data</summary>

- *Chenlei Leng, Xingwei Tong*

- `1302.0181v1` - [abs](http://arxiv.org/abs/1302.0181v1) - [pdf](http://arxiv.org/pdf/1302.0181v1)

> We propose a censored quantile regression estimator motivated by unbiased estimating equations. Under the usual conditional independence assumption of the survival time and the censoring time given the covariates, we show that the proposed estimator is consistent and asymptotically normal. We develop an efficient computational algorithm which uses existing quantile regression code. As a result, bootstrap-type inference can be efficiently implemented. We illustrate the finite-sample performance of the proposed method by simulation studies and analysis of a survival data set.

</details>

<details>

<summary>2013-02-01 15:09:39 - PAC-Bayesian Estimation and Prediction in Sparse Additive Models</summary>

- *Benjamin Guedj, Pierre Alquier*

- `1208.1211v3` - [abs](http://arxiv.org/abs/1208.1211v3) - [pdf](http://arxiv.org/pdf/1208.1211v3)

> The present paper is about estimation and prediction in high-dimensional additive models under a sparsity assumption ($p\gg n$ paradigm). A PAC-Bayesian strategy is investigated, delivering oracle inequalities in probability. The implementation is performed through recent outcomes in high-dimensional MCMC algorithms, and the performance of our method is assessed on simulated data.

</details>

<details>

<summary>2013-02-04 13:24:37 - Variational Bayesian Adaptation of Noise Covariances in Non-Linear Kalman Filtering</summary>

- *Simo SÃ¤rkkÃ¤ Jouni Hartikainen*

- `1302.0681v1` - [abs](http://arxiv.org/abs/1302.0681v1) - [pdf](http://arxiv.org/pdf/1302.0681v1)

> This paper is considered with joint estimation of state and time-varying noise covariance matrices in non-linear stochastic state space models. We present a variational Bayes and Gaussian filtering based algorithm for efficient computation of the approximate filtering posterior distributions. The Gaussian filtering based formulation of the non-linear state space model computation allows usage of efficient Gaussian integration methods such as unscented transform, cubature integration and Gauss-Hermite integration along with the classical Taylor series approximations. The performance of the algorithm is illustrated in a simulated application.

</details>

<details>

<summary>2013-02-04 22:35:27 - Probabilistic Quantitative Precipitation Forecasting Using Ensemble Model Output Statistics</summary>

- *Michael Scheuerer*

- `1302.0893v1` - [abs](http://arxiv.org/abs/1302.0893v1) - [pdf](http://arxiv.org/pdf/1302.0893v1)

> Statistical post-processing of dynamical forecast ensembles is an essential component of weather forecasting. In this article, we present a post-processing method that generates full predictive probability distributions for precipitation accumulations based on ensemble model output statistics (EMOS). We model precipitation amounts by a generalized extreme value distribution that is left-censored at zero. This distribution permits modelling precipitation on the original scale without prior transformation of the data. A closed form expression for its continuous rank probability score can be derived and permits computationally efficient model fitting. We discuss an extension of our approach that incorporates further statistics characterizing the spatial variability of precipitation amounts in the vicinity of the location of interest. The proposed EMOS method is applied to daily 18-h forecasts of 6-h accumulated precipitation over Germany in 2011 using the COSMO-DE ensemble prediction system operated by the German Meteorological Service. It yields calibrated and sharp predictive distributions and compares favourably with extended logistic regression and Bayesian model averaging which are state of the art approaches for precipitation post-processing. The incorporation of neighbourhood information further improves predictive performance and turns out to be a useful strategy to account for displacement errors of the dynamical forecasts in a probabilistic forecasting framework.

</details>

<details>

<summary>2013-02-05 09:56:47 - Bayesian inference for the multivariate skew-normal model: a Population Monte Carlo approach</summary>

- *Brunero Liseo, Antonio Parisi*

- `1302.0977v1` - [abs](http://arxiv.org/abs/1302.0977v1) - [pdf](http://arxiv.org/pdf/1302.0977v1)

> Frequentist and likelihood methods of inference based on the multivariate skew-normal model encounter several technical difficulties with this model. In spite of the popularity of this class of densities, there are no broadly satisfactory solutions for estimation and testing problems. A general population Monte Carlo algorithm is proposed which: 1) exploits the latent structure stochastic representation of skew-normal random variables to provide a full Bayesian analysis of the model and 2) accounts for the presence of constraints in the parameter space. The proposed approach can be defined as weakly informative, since the prior distribution approximates the actual reference prior for the shape parameter vector. Results are compared with the existing classical solutions and the practical implementation of the algorithm is illustrated via a simulation study and a real data example. A generalization to the matrix variate regression model with skew-normal error is also presented.

</details>

<details>

<summary>2013-02-06 06:29:07 - Real-time semiparametric regression</summary>

- *Jan Luts, Tamara Broderick, Matt P. Wand*

- `1209.3550v2` - [abs](http://arxiv.org/abs/1209.3550v2) - [pdf](http://arxiv.org/pdf/1209.3550v2)

> We develop algorithms for performing semiparametric regression analysis in real time, with data processed as it is collected and made immediately available via modern telecommunications technologies. Our definition of semiparametric regression is quite broad and includes, as special cases, generalized linear mixed models, generalized additive models, geostatistical models, wavelet nonparametric regression models and their various combinations. Fast updating of regression fits is achieved by couching semiparametric regression into a Bayesian hierarchical model or, equivalently, graphical model framework and employing online mean field variational ideas. An internet site attached to this article, realtime-semiparametric-regression.net, illustrates the methodology for continually arriving stock market, real estate and airline data. Flexible real-time analyses, based on increasingly ubiquitous streaming data sources stand to benefit.

</details>

<details>

<summary>2013-02-06 15:53:33 - Update Rules for Parameter Estimation in Bayesian Networks</summary>

- *Eric Bauer, Daphne Koller, Yoram Singer*

- `1302.1519v1` - [abs](http://arxiv.org/abs/1302.1519v1) - [pdf](http://arxiv.org/pdf/1302.1519v1)

> This paper re-examines the problem of parameter estimation in Bayesian networks with missing values and hidden variables from the perspective of recent work in on-line learning [Kivinen & Warmuth, 1994]. We provide a unified framework for parameter estimation that encompasses both on-line learning, where the model is continuously adapted to new data cases as they arrive, and the more traditional batch learning, where a pre-accumulated set of samples is used in a one-time model selection process. In the batch case, our framework encompasses both the gradient projection algorithm and the EM algorithm for Bayesian networks. The framework also leads to new on-line and batch parameter update schemes, including a parameterized version of EM. We provide both empirical and theoretical results indicating that parameterized EM allows faster convergence to the maximum likelihood parameters than does standard EM.

</details>

<details>

<summary>2013-02-06 15:56:07 - Models and Selection Criteria for Regression and Classification</summary>

- *David Heckerman, Christopher Meek*

- `1302.1545v1` - [abs](http://arxiv.org/abs/1302.1545v1) - [pdf](http://arxiv.org/pdf/1302.1545v1)

> When performing regression or classification, we are interested in the conditional probability distribution for an outcome or class variable Y given a set of explanatoryor input variables X. We consider Bayesian models for this task. In particular, we examine a special class of models, which we call Bayesian regression/classification (BRC) models, that can be factored into independent conditional (y|x) and input (x) models. These models are convenient, because the conditional model (the portion of the full model that we care about) can be analyzed by itself. We examine the practice of transforming arbitrary Bayesian models to BRC models, and argue that this practice is often inappropriate because it ignores prior knowledge that may be important for learning. In addition, we examine Bayesian methods for learning models from data. We discuss two criteria for Bayesian model selection that are appropriate for repression/classification: one described by Spiegelhalter et al. (1993), and another by Buntine (1993). We contrast these two criteria using the prequential framework of Dawid (1984), and give sufficient conditions under which the criteria agree.

</details>

<details>

<summary>2013-02-06 15:59:19 - Score and Information for Recursive Exponential Models with Incomplete Data</summary>

- *Bo Thiesson*

- `1302.1571v1` - [abs](http://arxiv.org/abs/1302.1571v1) - [pdf](http://arxiv.org/pdf/1302.1571v1)

> Recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on Bayesian networks. This paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. Besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. It is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. Based on possibly incomplete data, the score and the observed information are derived for these models. This accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. Throughout the paper the specialization into recursive graphical models is accounted for by a simple example.

</details>

<details>

<summary>2013-02-06 23:54:24 - A Bayesian spatio-temporal model of panel design data: airborne particle number concentration in Brisbane, Australia</summary>

- *Sam Clifford, Sama Low Choy, Mandana Mazaheri, Farhad Salimi, Lidia Morawska, Kerrie Mengsersen*

- `1206.3833v2` - [abs](http://arxiv.org/abs/1206.3833v2) - [pdf](http://arxiv.org/pdf/1206.3833v2)

> This paper outlines a methodology for semi-parametric spatio-temporal modelling of data which is dense in time but sparse in space, obtained from a split panel design, the most feasible approach to covering space and time with limited equipment. The data are hourly averaged particle number concentration (PNC) and were collected, as part of the Ultrafine Particles from Transport Emissions and Child Health (UPTECH) project. Two weeks of continuous measurements were taken at each of a number of government primary schools in the Brisbane Metropolitan Area. The monitoring equipment was taken to each school sequentially. The school data are augmented by data from long term monitoring stations at three locations in Brisbane, Australia.   Fitting the model helps describe the spatial and temporal variability at a subset of the UPTECH schools and the long-term monitoring sites. The temporal variation is modelled hierarchically with penalised random walk terms, one common to all sites and a term accounting for the remaining temporal trend at each site. Parameter estimates and their uncertainty are computed in a computationally efficient approximate Bayesian inference environment, R-INLA.   The temporal part of the model explains daily and weekly cycles in PNC at the schools, which can be used to estimate the exposure of school children to ultrafine particles (UFPs) emitted by vehicles. At each school and long-term monitoring site, peaks in PNC can be attributed to the morning and afternoon rush hour traffic and new particle formation events. The spatial component of the model describes the school to school variation in mean PNC at each school and within each school ground. It is shown how the spatial model can be expanded to identify spatial patterns at the city scale with the inclusion of more spatial locations.

</details>

<details>

<summary>2013-02-08 20:59:13 - Simulation-efficient shortest probability intervals</summary>

- *Ying Liu, Andrew Gelman, Tian Zheng*

- `1302.2142v1` - [abs](http://arxiv.org/abs/1302.2142v1) - [pdf](http://arxiv.org/pdf/1302.2142v1)

> Bayesian highest posterior density (HPD) intervals can be estimated directly from simulations via empirical shortest intervals. Unfortunately, these can be noisy (that is, have a high Monte Carlo error). We derive an optimal weighting strategy using bootstrap and quadratic programming to obtain a more compu- tationally stable HPD, or in general, shortest probability interval (Spin). We prove the consistency of our method. Simulation studies on a range of theoret- ical and real-data examples, some with symmetric and some with asymmetric posterior densities, show that intervals constructed using Spin have better cov- erage (relative to the posterior distribution) and lower Monte Carlo error than empirical shortest intervals. We implement the new method in an R package (SPIn) so it can be routinely used in post-processing of Bayesian simulations.

</details>

<details>

<summary>2013-02-10 22:19:30 - Parsimonious Skew Mixture Models for Model-Based Clustering and Classification</summary>

- *Irene Vrbik, Paul D. McNicholas*

- `1302.2373v1` - [abs](http://arxiv.org/abs/1302.2373v1) - [pdf](http://arxiv.org/pdf/1302.2373v1)

> In recent work, robust mixture modelling approaches using skewed distributions have been explored to accommodate asymmetric data. We introduce parsimony by developing skew-t and skew-normal analogues of the popular GPCM family that employ an eigenvalue decomposition of a positive-semidefinite matrix. The methods developed in this paper are compared to existing models in both an unsupervised and semi-supervised classification framework. Parameter estimation is carried out using the expectation-maximization algorithm and models are selected using the Bayesian information criterion. The efficacy of these extensions is illustrated on simulated and benchmark clustering data sets.

</details>

<details>

<summary>2013-02-11 09:57:14 - Bayesian Classification of Astronomical Objects -- and what is behind it</summary>

- *JÃ¶rg P. Rachen*

- `1302.2429v1` - [abs](http://arxiv.org/abs/1302.2429v1) - [pdf](http://arxiv.org/pdf/1302.2429v1)

> We present a Bayesian method for the identification and classification of objects from sets of astronomical catalogs, given a predefined classification scheme. Identification refers here to the association of entries in different catalogs to a single object, and classification refers to the matching of the associated data set to a model selected from a set of parametrized models of different complexity. By the virtue of Bayes' theorem, we can combine both tasks in an efficient way, which allows a largely automated and still reliable way to generate classified astronomical catalogs. A problem to the Bayesian approach is hereby the handling of exceptions, for which no likelihoods can be specified. We present and discuss a simple and practical solution to this problem, emphasizing the role of the "evidence" term in Bayes' theorem for the identification of exceptions. Comparing the practice and logic of Bayesian classification to Bayesian inference, we finally note some interesting links to concepts of the philosophy of science.

</details>

<details>

<summary>2013-02-11 22:57:00 - Asymptotics of the Empirical Cross-over Function</summary>

- *Karthik Bharath, Vladimir Pozdnyakov, Dipak Dey*

- `1112.3427v4` - [abs](http://arxiv.org/abs/1112.3427v4) - [pdf](http://arxiv.org/pdf/1112.3427v4)

> We consider a combination of heavily trimmed sums and sample quantiles which arises when examining properties of clustering criteria and prove limit theorems. The object of interest, which we call the Empirical Cross-over Function, is an L-statistic whose weights do not comply with the requisite regularity conditions for usage of ex- isting limit results. The law of large numbers, CLT and a functional CLT are proven.

</details>

<details>

<summary>2013-02-13 11:21:42 - Reliability estimators for the components of series and parallel systems: The Weibull model</summary>

- *Felipe L. Bhering, Carlos A. de B. Pereira, Adriano Polpo*

- `1302.3053v1` - [abs](http://arxiv.org/abs/1302.3053v1) - [pdf](http://arxiv.org/pdf/1302.3053v1)

> This paper presents a hierarchical Bayesian approach to the estimation of components' reliability (survival) using a Weibull model for each of them. The proposed method can be used to estimation with general survival censored data, because the estimation of a component's reliability in a series (parallel) system is equivalent to the estimation of its survival function with right- (left-) censored data. Besides the Weibull parametric model for reliability data, independent gamma distributions are considered at the first hierarchical level for the Weibull parameters and independent uniform distributions over the real line as priors for the parameters of the gammas. In order to evaluate the model, an example and a simulation study are discussed.

</details>

<details>

<summary>2013-02-13 14:12:46 - Tail Sensitivity Analysis in Bayesian Networks</summary>

- *Enrique F. Castillo, Cristina Solares, Patricia Gomez*

- `1302.3564v1` - [abs](http://arxiv.org/abs/1302.3564v1) - [pdf](http://arxiv.org/pdf/1302.3564v1)

> The paper presents an efficient method for simulating the tails of a target variable Z=h(X) which depends on a set of basic variables X=(X_1, ..., X_n). To this aim, variables X_i, i=1, ..., n are sequentially simulated in such a manner that Z=h(x_1, ..., x_i-1, X_i, ..., X_n) is guaranteed to be in the tail of Z. When this method is difficult to apply, an alternative method is proposed, which leads to a low rejection proportion of sample values, when compared with the Monte Carlo method. Both methods are shown to be very useful to perform a sensitivity analysis of Bayesian networks, when very large confidence intervals for the marginal/conditional probabilities are required, as in reliability or risk analysis. The methods are shown to behave best when all scores coincide. The required modifications for this to occur are discussed. The methods are illustrated with several examples and one example of application to a real case is used to illustrate the whole process.

</details>

<details>

<summary>2013-02-13 14:12:58 - Learning Equivalence Classes of Bayesian Networks Structures</summary>

- *David Maxwell Chickering*

- `1302.3566v1` - [abs](http://arxiv.org/abs/1302.3566v1) - [pdf](http://arxiv.org/pdf/1302.3566v1)

> Approaches to learning Bayesian networks from data typically combine a scoring function with a heuristic search procedure. Given a Bayesian network structure, many of the scoring functions derived in the literature return a score for the entire equivalence class to which the structure belongs. When using such a scoring function, it is appropriate for the heuristic search algorithm to search over equivalence classes of Bayesian networks as opposed to individual structures. We present the general formulation of a search space for which the states of the search correspond to equivalence classes of structures. Using this space, any one of a number of heuristic search algorithms can easily be applied. We compare greedy search performance in the proposed search space to greedy search performance in a search space for which the states correspond to individual Bayesian network structures.

</details>

<details>

<summary>2013-02-13 14:14:02 - Learning Bayesian Networks with Local Structure</summary>

- *Nir Friedman, Moises Goldszmidt*

- `1302.3577v1` - [abs](http://arxiv.org/abs/1302.3577v1) - [pdf](http://arxiv.org/pdf/1302.3577v1)

> In this paper we examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability tables (CPTs), that quantify these networks. This increases the space of possible models, enabling the representation of CPTs with a variable number of parameters that depends on the learned local structures. The resulting learning procedure is capable of inducing models that better emulate the real complexity of the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures, as well as an empirical evaluation of the proposed method. This evaluation indicates that learning curves characterizing the procedure that exploits the local structure converge faster than these of the standard procedure. Our results also show that networks learned with local structure tend to be more complex (in terms of arcs), yet require less parameters.

</details>

<details>

<summary>2013-02-13 14:14:13 - On the Sample Complexity of Learning Bayesian Networks</summary>

- *Nir Friedman, Zohar Yakhini*

- `1302.3579v1` - [abs](http://arxiv.org/abs/1302.3579v1) - [pdf](http://arxiv.org/pdf/1302.3579v1)

> In recent years there has been an increasing interest in learning Bayesian networks from data. One of the most effective methods for learning such networks is based on the minimum description length (MDL) principle. Previous work has shown that this learning procedure is asymptotically successful: with probability one, it will converge to the target distribution, given a sufficient number of samples. However, the rate of this convergence has been hitherto unknown. In this work we examine the sample complexity of MDL based learning procedures for Bayesian networks. We show that the number of samples needed to learn an epsilon-close approximation (in terms of entropy distance) with confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog (1/delta)). This means that the sample complexity is a low-order polynomial in the error threshold and sub-linear in the confidence bound. We also discuss how the constants in this term depend on the complexity of the target distribution. Finally, we address questions of asymptotic minimality and propose a method for using the sample complexity results to speed up the learning process.

</details>

<details>

<summary>2013-02-13 14:15:20 - Bayesian Learning of Loglinear Models for Neural Connectivity</summary>

- *Kathryn Blackmond Laskey, Laura Martignon*

- `1302.3590v1` - [abs](http://arxiv.org/abs/1302.3590v1) - [pdf](http://arxiv.org/pdf/1302.3590v1)

> This paper presents a Bayesian approach to learning the connectivity structure of a group of neurons from data on configuration frequencies. A major objective of the research is to provide statistical tools for detecting changes in firing patterns with changing stimuli. Our framework is not restricted to the well-understood case of pair interactions, but generalizes the Boltzmann machine model to allow for higher order interactions. The paper applies a Markov Chain Monte Carlo Model Composition (MC3) algorithm to search over connectivity structures and uses Laplace's method to approximate posterior probabilities of structures. Performance of the methods was tested on synthetic data. The models were also applied to data obtained by Vaadia on multi-unit recordings of several neurons in the visual cortex of a rhesus monkey in two different attentional states. Results confirmed the experimenters' conjecture that different attentional states were associated with different interaction structures.

</details>

<details>

<summary>2013-02-13 22:09:38 - Adaptive Bayesian density estimation using Pitman-Yor or normalized inverse-Gaussian process kernel mixtures</summary>

- *Catia Scricciolo*

- `1210.8094v2` - [abs](http://arxiv.org/abs/1210.8094v2) - [pdf](http://arxiv.org/pdf/1210.8094v2)

> We consider Bayesian nonparametric density estimation using a Pitman-Yor or a normalized inverse-Gaussian process kernel mixture as the prior distribution for a density. The procedure is studied from a frequentist perspective. Using the stick-breaking representation of the Pitman-Yor process or the expression of the finite-dimensional distributions for the normalized-inverse Gaussian process, we prove that, when the data are replicates from an infinitely smooth density, the posterior distribution concentrates on any shrinking $L^p$-norm ball, $1\leq p\leq\infty$, around the sampling density at a \emph{nearly parametric} rate, up to a logarithmic factor. The resulting hierarchical Bayesian procedure, with a fixed prior, is thus shown to be adaptive to the infinite degree of smoothness of the sampling density.

</details>

<details>

<summary>2013-02-14 22:52:49 - Emulating a gravity model to infer the spatiotemporal dynamics of an infectious disease</summary>

- *Roman Jandarov, Murali Haran, Ottar BjÃ¸rnstad, Bryan Grenfell*

- `1110.6451v3` - [abs](http://arxiv.org/abs/1110.6451v3) - [pdf](http://arxiv.org/pdf/1110.6451v3)

> Probabilistic models for infectious disease dynamics are useful for understanding the mechanism underlying the spread of infection. When the likelihood function for these models is expensive to evaluate, traditional likelihood-based inference may be computationally intractable. Furthermore, traditional inference may lead to poor parameter estimates and the fitted model may not capture important biological characteristics of the observed data. We propose a novel approach for resolving these issues that is inspired by recent work in emulation and calibration for complex computer models. Our motivating example is the gravity time series susceptible-infected-recovered (TSIR) model. Our approach focuses on the characteristics of the process that are of scientific interest. We find a Gaussian process approximation to the gravity model using key summary statistics obtained from model simulations. We demonstrate via simulated examples that the new approach is computationally expedient, provides accurate parameter inference, and results in a good model fit. We apply our method to analyze measles outbreaks in England and Wales in two periods, the pre-vaccination period from 1944-1965 and the vaccination period from 1966-1994. Based on our results, we are able to obtain important scientific insights about the transmission of measles. In general, our method is applicable to problems where traditional likelihood-based inference is computationally intractable or produces a poor model fit. It is also an alternative to approximate Bayesian computation (ABC) when simulations from the model are expensive.

</details>

<details>

<summary>2013-02-15 23:48:57 - MAD-Bayes: MAP-based Asymptotic Derivations from Bayes</summary>

- *Tamara Broderick, Brian Kulis, Michael I. Jordan*

- `1212.2126v2` - [abs](http://arxiv.org/abs/1212.2126v2) - [pdf](http://arxiv.org/pdf/1212.2126v2)

> The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.

</details>

<details>

<summary>2013-02-16 17:29:33 - Gaussian Process Vine Copulas for Multivariate Dependence</summary>

- *David Lopez-Paz, JosÃ© Miguel HernÃ¡ndez-Lobato, Zoubin Ghahramani*

- `1302.3979v1` - [abs](http://arxiv.org/abs/1302.3979v1) - [pdf](http://arxiv.org/pdf/1302.3979v1)

> Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function. Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables. In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.

</details>

<details>

<summary>2013-02-17 09:46:19 - Model selection in regression under structural constraints</summary>

- *Felix Abramovich, Vadim Grinshtein*

- `1206.3422v2` - [abs](http://arxiv.org/abs/1206.3422v2) - [pdf](http://arxiv.org/pdf/1206.3422v2)

> The paper considers model selection in regression under the additional structural constraints on admissible models where the number of potential predictors might be even larger than the available sample size. We develop a Bayesian formalism as a natural tool for generating a wide class of model selection criteria based on penalized least squares estimation with various complexity penalties associated with a prior on a model size. The resulting criteria are adaptive to structural constraints. We establish the upper bound for the quadratic risk of the resulting MAP estimator and the corresponding lower bound for the minimax risk over a set of admissible models of a given size. We then specify the class of priors (and, therefore, the class of complexity penalties) where for the "nearly-orthogonal" design the MAP estimator is asymptotically at least nearly-minimax (up to a log-factor) simultaneously over an entire range of sparse and dense setups. Moreover, when the numbers of admissible models are "small" (e.g., ordered variable selection) or, on the opposite, for the case of complete variable selection, the proposed estimator achieves the exact minimax rates.

</details>

<details>

<summary>2013-02-18 10:50:55 - Reversible jump MCMC for nonparametric drift estimation for diffusion processes</summary>

- *Frank van der Meulen, Moritz Schauer, Harry van Zanten*

- `1206.4910v2` - [abs](http://arxiv.org/abs/1206.4910v2) - [pdf](http://arxiv.org/pdf/1206.4910v2)

> In the context of nonparametric Bayesian estimation a Markov chain Monte Carlo algorithm is devised and implemented to sample from the posterior distribution of the drift function of a continuously or discretely observed one-dimensional diffusion. The drift is modeled by a scaled linear combination of basis functions with a Gaussian prior on the coefficients. The scaling parameter is equipped with a partially conjugate prior. The number of basis function in the drift is equipped with a prior distribution as well. For continuous data, a reversible jump Markov chain algorithm enables the exploration of the posterior over models of varying dimension. Subsequently, it is explained how data-augmentation can be used to extend the algorithm to deal with diffusions observed discretely in time. Some examples illustrate that the method can give satisfactory results. In these examples a comparison is made with another existing method as well.

</details>

<details>

<summary>2013-02-18 11:19:13 - Bayesian optimal adaptive estimation using a sieve prior</summary>

- *Julyan Arbel, Ghislaine Gayraud, Judith Rousseau*

- `1204.2392v2` - [abs](http://arxiv.org/abs/1204.2392v2) - [pdf](http://arxiv.org/pdf/1204.2392v2)

> We derive rates of contraction of posterior distributions on nonparametric models resulting from sieve priors. The aim of the paper is to provide general conditions to get posterior rates when the parameter space has a general structure, and rate adaptation when the parameter space is, e.g., a Sobolev class. The conditions employed, although standard in the literature, are combined in a different way. The results are applied to density, regression, nonlinear autoregression and Gaussian white noise models. In the latter we have also considered a loss function which is different from the usual l2 norm, namely the pointwise loss. In this case it is possible to prove that the adaptive Bayesian approach for the l2 loss is strongly suboptimal and we provide a lower bound on the rate.

</details>

<details>

<summary>2013-02-18 14:25:45 - On the uniform asymptotic validity of subsampling and the bootstrap</summary>

- *Joseph P. Romano, Azeem M. Shaikh*

- `1204.2762v2` - [abs](http://arxiv.org/abs/1204.2762v2) - [pdf](http://arxiv.org/pdf/1204.2762v2)

> This paper provides conditions under which subsampling and the bootstrap can be used to construct estimators of the quantiles of the distribution of a root that behave well uniformly over a large class of distributions $\mathbf{P}$. These results are then applied (i) to construct confidence regions that behave well uniformly over $\mathbf{P}$ in the sense that the coverage probability tends to at least the nominal level uniformly over $\mathbf{P}$ and (ii) to construct tests that behave well uniformly over $\mathbf{P}$ in the sense that the size tends to no greater than the nominal level uniformly over $\mathbf{P}$. Without these stronger notions of convergence, the asymptotic approximations to the coverage probability or size may be poor, even in very large samples. Specific applications include the multivariate mean, testing moment inequalities, multiple testing, the empirical process and U-statistics.

</details>

<details>

<summary>2013-02-18 14:48:53 - Convergence analysis of the Gibbs sampler for Bayesian general linear mixed models with improper priors</summary>

- *Jorge Carlos RomÃ¡n, James P. Hobert*

- `1111.3210v2` - [abs](http://arxiv.org/abs/1111.3210v2) - [pdf](http://arxiv.org/pdf/1111.3210v2)

> Bayesian analysis of data from the general linear mixed model is challenging because any nontrivial prior leads to an intractable posterior density. However, if a conditionally conjugate prior density is adopted, then there is a simple Gibbs sampler that can be employed to explore the posterior density. A popular default among the conditionally conjugate priors is an improper prior that takes a product form with a flat prior on the regression parameter, and so-called power priors on each of the variance components. In this paper, a convergence rate analysis of the corresponding Gibbs sampler is undertaken. The main result is a simple, easily-checked sufficient condition for geometric ergodicity of the Gibbs-Markov chain. This result is close to the best possible result in the sense that the sufficient condition is only slightly stronger than what is required to ensure posterior propriety. The theory developed in this paper is extremely important from a practical standpoint because it guarantees the existence of central limit theorems that allow for the computation of valid asymptotic standard errors for the estimates computed using the Gibbs sampler.

</details>

<details>

<summary>2013-02-19 22:59:44 - Optimal Discriminant Functions Based On Sampled Distribution Distance for Modulation Classification</summary>

- *Paulo Urriza, Eric Rebeiz, Danijela Cabric*

- `1302.4773v1` - [abs](http://arxiv.org/abs/1302.4773v1) - [pdf](http://arxiv.org/pdf/1302.4773v1)

> In this letter, we derive the optimal discriminant functions for modulation classification based on the sampled distribution distance. The proposed method classifies various candidate constellations using a low complexity approach based on the distribution distance at specific testpoints along the cumulative distribution function. This method, based on the Bayesian decision criteria, asymptotically provides the minimum classification error possible given a set of testpoints. Testpoint locations are also optimized to improve classification performance. The method provides significant gains over existing approaches that also use the distribution of the signal features.

</details>

<details>

<summary>2013-02-20 12:33:22 - Bayesian computing with INLA: new features</summary>

- *Thiago G. Martins, Daniel Simpson, Finn Lindgren, HÃ¥vard Rue*

- `1210.0333v2` - [abs](http://arxiv.org/abs/1210.0333v2) - [pdf](http://arxiv.org/pdf/1210.0333v2)

> The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. In this paper we formalize new developments in the R-INLA package and show how these features greatly extend the scope of models that can be analyzed by this interface. We also discuss the current default method in R-INLA to approximate posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration.

</details>

<details>

<summary>2013-02-20 14:10:36 - New Important Developments in Small Area Estimation</summary>

- *Danny Pfeffermann*

- `1302.4907v1` - [abs](http://arxiv.org/abs/1302.4907v1) - [pdf](http://arxiv.org/pdf/1302.4907v1)

> The problem of small area estimation (SAE) is how to produce reliable estimates of characteristics of interest such as means, counts, quantiles, etc., for areas or domains for which only small samples or no samples are available, and how to assess their precision. The purpose of this paper is to review and discuss some of the new important developments in small area estimation methods. Rao [Small Area Estimation (2003)] wrote a very comprehensive book, which covers all the main developments in this topic until that time. A few review papers have been written after 2003, but they are limited in scope. Hence, the focus of this review is on new developments in the last 7-8 years, but to make the review more self-contained, I also mention shortly some of the older developments. The review covers both design-based and model-dependent methods, with the latter methods further classified into frequentist and Bayesian methods. The style of the paper is similar to the style of my previous review on SAE published in 2002, explaining the new problems investigated and describing the proposed solutions, but without dwelling on theoretical details, which can be found in the original articles. I hope that this paper will be useful both to researchers who like to learn more on the research carried out in SAE and to practitioners who might be interested in the application of the new methods.

</details>

<details>

<summary>2013-02-20 15:22:01 - Estimating Continuous Distributions in Bayesian Classifiers</summary>

- *George H. John, Pat Langley*

- `1302.4964v1` - [abs](http://arxiv.org/abs/1302.4964v1) - [pdf](http://arxiv.org/pdf/1302.4964v1)

> When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.

</details>

<details>

<summary>2013-02-22 15:48:31 - Semi-automatic selection of summary statistics for ABC model choice</summary>

- *Dennis Prangle, Paul Fearnhead, Murray P. Cox, Patrick J. Biggs, Nigel P. French*

- `1302.5624v1` - [abs](http://arxiv.org/abs/1302.5624v1) - [pdf](http://arxiv.org/pdf/1302.5624v1)

> A central statistical goal is to choose between alternative explanatory models of data. In many modern applications, such as population genetics, it is not possible to apply standard methods based on evaluating the likelihood functions of the models, as these are numerically intractable. Approximate Bayesian computation (ABC) is a commonly used alternative for such situations. ABC simulates data x for many parameter values under each model, which is compared to the observed data xobs. More weight is placed on models under which S(x) is close to S(xobs), where S maps data to a vector of summary statistics. Previous work has shown the choice of S is crucial to the efficiency and accuracy of ABC. This paper provides a method to select good summary statistics for model choice. It uses a preliminary step, simulating many x values from all models and fitting regressions to this with the model as response. The resulting model weight estimators are used as S in an ABC analysis. Theoretical results are given to justify this as approximating low dimensional sufficient statistics. A substantive application is presented: choosing between competing coalescent models of demographic growth for Campylobacter jejuni in New Zealand using multi-locus sequence typing data.

</details>

<details>

<summary>2013-02-23 00:20:34 - Bayesian inference of inaccuracies in radiation transport physics from inertial confinement fusion experiments</summary>

- *Jim A Gaffney, Dan Clark, Vijay Sonnad, Stephen B Libby*

- `1302.5743v1` - [abs](http://arxiv.org/abs/1302.5743v1) - [pdf](http://arxiv.org/pdf/1302.5743v1)

> First principles microphysics models are essential to the design and analysis of high energy density physics experiments. Using experimental data to investigate the underlying physics is also essential, particularly when simulations and experiments are not consistent with each other. This is a difficult task, due to the large number of physical models that play a role, and due to the complex (and as a result, noisy) nature of the experiments. This results in a large number of parameters that make any inference a daunting task; it is also very important to consistently treat both experimental and prior understanding of the problem. In this paper we present a Bayesian method that includes both these effects, and allows the inference of a set of modifiers which have been constructed to give information about microphysics models from experimental data. We pay particular attention to radiation transport models. The inference takes into account a large set of experimental parameters and an estimate of the prior knowledge through a modified $\chi^{2}$ function, which is minimised using an efficient genetic algorithm. Both factors play an essential role in our analysis. We find that although there is evidence of inaccuracies in off-line calculations of X ray drive intensity and Ge $L$ shell absorption, modifications to radiation transport are unable to reconcile differences between 1D HYDRA simulations and the experiment.

</details>

<details>

<summary>2013-02-23 00:23:51 - Development of a Bayesian method for the analysis of inertial confinement fusion experiments on the NIF</summary>

- *Jim A Gaffney, Dan Clark, Vijay Sonnad, Stephen B Libby*

- `1302.5745v1` - [abs](http://arxiv.org/abs/1302.5745v1) - [pdf](http://arxiv.org/pdf/1302.5745v1)

> The complex nature of inertial confinement fusion (ICF) experiments results in a very large number of experimental parameters that are only known with limited reliability. These parameters, combined with the myriad physical models that govern target evolution, make the reliable extraction of physics from experimental campaigns very difficult. We develop an inference method that allows all important experimental parameters, and previous knowledge, to be taken into account when investigating underlying microphysics models. The result is framed as a modified $\chi^{2}$ analysis which is easy to implement in existing analyses, and quite portable. We present a first application to a recent convergent ablator experiment performed at the NIF, and investigate the effect of variations in all physical dimensions of the target (very difficult to do using other methods). We show that for well characterised targets in which dimensions vary at the 0.5% level there is little effect, but 3% variations change the results of inferences dramatically. Our Bayesian method allows particular inference results to be associated with prior errors in microphysics models; in our example, tuning the carbon opacity to match experimental data (i.e., ignoring prior knowledge) is equivalent to an assumed prior error of 400% in the tabop opacity tables. This large error is unreasonable, underlining the importance of including prior knowledge in the analysis of these experiments.

</details>

<details>

<summary>2013-02-25 13:34:48 - An efficient algorithm for structured sparse quantile regression</summary>

- *Vahid Nassiri, Ignace Loris*

- `1302.6088v1` - [abs](http://arxiv.org/abs/1302.6088v1) - [pdf](http://arxiv.org/pdf/1302.6088v1)

> Quantile regression is studied in combination with a penalty which promotes structured (or group) sparsity. A mixed $\ell_{1,\infty}$-norm on the parameter vector is used to impose structured sparsity on the traditional quantile regression problem. An algorithm is derived to calculate the piece-wise linear solution path of the corresponding minimization problem. A Matlab implementation of the proposed algorithm is provided and some applications of the methods are also studied.

</details>

<details>

<summary>2013-02-25 18:23:43 - Adaptive Hamiltonian and Riemann Manifold Monte Carlo Samplers</summary>

- *ziyu wang, Shakir Mohamed, Nando de Freitas*

- `1302.6182v1` - [abs](http://arxiv.org/abs/1302.6182v1) - [pdf](http://arxiv.org/pdf/1302.6182v1)

> In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and that the use of our adaptive algorithms makes it easy to obtain more efficient samplers, in some cases precluding the need for more complex solutions. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.

</details>

<details>

<summary>2013-02-25 20:41:34 - Non-parametric Bayesian drift estimation for stochastic differential equations</summary>

- *Shota Gugushvili, Peter Spreij*

- `1206.4981v2` - [abs](http://arxiv.org/abs/1206.4981v2) - [pdf](http://arxiv.org/pdf/1206.4981v2)

> We consider non-parametric Bayesian estimation of the drift coefficient of a one-dimensional stochastic differential equation from discrete-time observations on the solution of this equation. Under suitable regularity conditions that are weaker than those previosly suggested in the literature, we establish posterior consistency in this context. Furthermore, we show that posterior consistency extends to the multidimensional setting as well, which, to the best of our knowledge, is a new result in this setting.

</details>

<details>

<summary>2013-02-26 05:48:05 - Bayes Factor Consistency for One-way Random Effects Model</summary>

- *Min Wang, Xiaoqian Sun*

- `1302.6320v1` - [abs](http://arxiv.org/abs/1302.6320v1) - [pdf](http://arxiv.org/pdf/1302.6320v1)

> In this paper, we consider Bayesian hypothesis testing for the balanced one-way random effects model. A special choice of the prior formulation for the ratio of variance components is shown to yield an explicit closed-form Bayes factor without integral representation. Furthermore, we study the consistency issue of the resulting Bayes factor under three asymptotic scenarios: either the number of units goes to infinity, the number of observations per unit goes to infinity, or both go to infinity. Finally, the behavior of the proposed approach is illustrated by simulation studies.

</details>

<details>

<summary>2013-02-26 08:18:20 - Calibration and improved prediction of computer models by universal Kriging</summary>

- *FranÃ§ois Bachoc, Guillaume Bois, Josselin Garnier, Jean-Marc Martinez*

- `1301.4114v2` - [abs](http://arxiv.org/abs/1301.4114v2) - [pdf](http://arxiv.org/pdf/1301.4114v2)

> This paper addresses the use of experimental data for calibrating a computer model and improving its predictions of the underlying physical system. A global statistical approach is proposed in which the bias between the computer model and the physical system is modeled as a realization of a Gaussian process. The application of classical statistical inference to this statistical model yields a rigorous method for calibrating the computer model and for adding to its predictions a statistical correction based on experimental data. This statistical correction can substantially improve the calibrated computer model for predicting the physical system on new experimental conditions. Furthermore, a quantification of the uncertainty of this prediction is provided. Physical expertise on the calibration parameters can also be taken into account in a Bayesian framework. Finally, the method is applied to the thermal-hydraulic code FLICA 4, in a single phase friction model framework. It allows to improve the predictions of the thermal-hydraulic code FLICA 4 significantly.

</details>

<details>

<summary>2013-02-27 01:21:37 - Convex vs nonconvex approaches for sparse estimation: GLasso, Multiple Kernel Learning and Hyperparameter GLasso</summary>

- *Aleksandr Y. Aravkin, James V. Burke, Alessandro Chiuso, Gianluigi Pillonetto*

- `1302.6434v2` - [abs](http://arxiv.org/abs/1302.6434v2) - [pdf](http://arxiv.org/pdf/1302.6434v2)

> The popular Lasso approach for sparse estimation can be derived via marginalization of a joint density associated with a particular stochastic model. A different marginalization of the same probabilistic model leads to a different non-convex estimator where hyperparameters are optimized. Extending these arguments to problems where groups of variables have to be estimated, we study a computational scheme for sparse estimation that differs from the Group Lasso. Although the underlying optimization problem defining this estimator is non-convex, an initialization strategy based on a univariate Bayesian forward selection scheme is presented. This also allows us to define an effective non-convex estimator where only one scalar variable is involved in the optimization process. Theoretical arguments, independent of the correctness of the priors entering the sparse model, are included to clarify the advantages of this non-convex technique in comparison with other convex estimators. Numerical experiments are also used to compare the performance of these approaches.

</details>

<details>

<summary>2013-02-27 12:39:03 - Estimation in functional linear quantile regression</summary>

- *Kengo Kato*

- `1202.4850v2` - [abs](http://arxiv.org/abs/1202.4850v2) - [pdf](http://arxiv.org/pdf/1202.4850v2)

> This paper studies estimation in functional linear quantile regression in which the dependent variable is scalar while the covariate is a function, and the conditional quantile for each fixed quantile index is modeled as a linear functional of the covariate. Here we suppose that covariates are discretely observed and sampling points may differ across subjects, where the number of measurements per subject increases as the sample size. Also, we allow the quantile index to vary over a given subset of the open unit interval, so the slope function is a function of two variables: (typically) time and quantile index. Likewise, the conditional quantile function is a function of the quantile index and the covariate. We consider an estimator for the slope function based on the principal component basis. An estimator for the conditional quantile function is obtained by a plug-in method. Since the so-constructed plug-in estimator not necessarily satisfies the monotonicity constraint with respect to the quantile index, we also consider a class of monotonized estimators for the conditional quantile function. We establish rates of convergence for these estimators under suitable norms, showing that these rates are optimal in a minimax sense under some smoothness assumptions on the covariance kernel of the covariate and the slope function. Empirical choice of the cutoff level is studied by using simulations.

</details>

<details>

<summary>2013-02-27 14:18:05 - Induction of Selective Bayesian Classifiers</summary>

- *Pat Langley, Stephanie Sage*

- `1302.6828v1` - [abs](http://arxiv.org/abs/1302.6828v1) - [pdf](http://arxiv.org/pdf/1302.6828v1)

> In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that c arries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research.

</details>

<details>

<summary>2013-02-27 14:19:04 - Three Approaches to Probability Model Selection</summary>

- *William B. Poland, Ross D. Shachter*

- `1302.6838v1` - [abs](http://arxiv.org/abs/1302.6838v1) - [pdf](http://arxiv.org/pdf/1302.6838v1)

> This paper compares three approaches to the problem of selecting among probability models to fit data (1) use of statistical criteria such as Akaike's information criterion and Schwarz's "Bayesian information criterion," (2) maximization of the posterior probability of the model, and (3) maximization of an effectiveness ratio? trading off accuracy and computational cost. The unifying characteristic of the approaches is that all can be viewed as maximizing a penalized likelihood function. The second approach with suitable prior distributions has been shown to reduce to the first. This paper shows that the third approach reduces to the second for a particular form of the effectiveness ratio, and illustrates all three approaches with the problem of selecting the number of components in a mixture of Gaussian distributions. Unlike the first two approaches, the third can be used even when the candidate models are chosen for computational efficiency, without regard to physical interpretation, so that the likelihood and the prior distribution over models cannot be interpreted literally. As the most general and computationally oriented of the approaches, it is especially useful for artificial intelligence applications.

</details>

<details>

<summary>2013-02-28 17:49:51 - Competing Process Hazard Function Models for Player Ratings in Ice Hockey</summary>

- *A. C. Thomas, Samuel L. Ventura, Shane Jensen, Stephen Ma*

- `1208.0799v2` - [abs](http://arxiv.org/abs/1208.0799v2) - [pdf](http://arxiv.org/pdf/1208.0799v2)

> Evaluating the overall ability of players in the National Hockey League (NHL) is a difficult task. Existing methods such as the famous "plus/minus" statistic have many shortcomings. Standard linear regression methods work well when player substitutions are relatively uncommon and scoring events are relatively common, such as in basketball, but as neither of these conditions exists for hockey, we use an approach that embraces the unique characteristics of the sport. We model the scoring rate for each team as its own semi-Markov process, with hazard functions for each process that depend on the players on the ice. This method yields offensive and defensive player ability ratings which take into account quality of teammates and opponents, the game situation, and other desired factors, that themselves have a meaningful interpretation in terms of game outcomes. Additionally, since the number of parameters in this model can be quite large, we make use of two different shrinkage methods depending on the question of interest: full Bayesian hierarchical models that partially pool parameters according to player position, and penalized maximum likelihood estimation to select a smaller number of parameters that stand out as being substantially different from average. We apply the model to all five-on-five (full-strength) situations for games in five NHL seasons.

</details>

<details>

<summary>2013-02-28 18:40:14 - Bayesian Consensus Clustering</summary>

- *Eric F. Lock, David B. Dunson*

- `1302.7280v1` - [abs](http://arxiv.org/abs/1302.7280v1) - [pdf](http://arxiv.org/pdf/1302.7280v1)

> The task of clustering a set of objects based on multiple sources of data arises in several modern applications. We propose an integrative statistical model that permits a separate clustering of the objects for each data source. These separate clusterings adhere loosely to an overall consensus clustering, and hence they are not independent. We describe a computationally scalable Bayesian framework for simultaneous estimation of both the consensus clustering and the source-specific clusterings. We demonstrate that this flexible approach is more robust than joint clustering of all data sources, and is more powerful than clustering each data source separately. This work is motivated by the integrated analysis of heterogeneous biomedical data, and we present an application to subtype identification of breast cancer tumor samples using publicly available data from The Cancer Genome Atlas. Software is available at http://people.duke.edu/~el113/software.html.

</details>


## 2013-03

<details>

<summary>2013-03-01 13:43:57 - Bayesian recovery of the initial condition for the heat equation</summary>

- *B. T. Knapik, A. W. van der Vaart, J. H. van Zanten*

- `1111.5876v3` - [abs](http://arxiv.org/abs/1111.5876v3) - [pdf](http://arxiv.org/pdf/1111.5876v3)

> We study a Bayesian approach to recovering the initial condition for the heat equation from noisy observations of the solution at a later time. We consider a class of prior distributions indexed by a parameter quantifying "smoothness" and show that the corresponding posterior distributions contract around the true parameter at a rate that depends on the smoothness of the true initial condition and the smoothness and scale of the prior. Correct combinations of these characteristics lead to the optimal minimax rate. One type of priors leads to a rate-adaptive Bayesian procedure. The frequentist coverage of credible sets is shown to depend on the combination of the prior and true parameter as well, with smoother priors leading to zero coverage and rougher priors to (extremely) conservative results. In the latter case credible sets are much larger than frequentist confidence sets, in that the ratio of diameters diverges to infinity. The results are numerically illustrated by a simulated data example.

</details>

<details>

<summary>2013-03-01 18:04:30 - Relative fixed-width stopping rules for Markov chain Monte Carlo simulations</summary>

- *James M. Flegal, Lei Gong*

- `1303.0238v1` - [abs](http://arxiv.org/abs/1303.0238v1) - [pdf](http://arxiv.org/pdf/1303.0238v1)

> Markov chain Monte Carlo (MCMC) simulations are commonly employed for estimating features of a target distribution, particularly for Bayesian inference. A fundamental challenge is determining when these simulations should stop. We consider a sequential stopping rule that terminates the simulation when the width of a confidence interval is sufficiently small relative to the size of the target parameter. Specifically, we propose relative magnitude and relative standard deviation stopping rules in the context of MCMC. In each setting, we develop sufficient conditions for asymptotic validity, that is conditions to ensure the simulation will terminate with probability one and the resulting confidence intervals will have the proper coverage probability. Our results are applicable in a wide variety of MCMC estimation settings, such as expectation, quantile, or simultaneous multivariate estimation. Finally, we investigate the finite sample properties through a variety of examples and provide some recommendations to practitioners.

</details>

<details>

<summary>2013-03-03 00:16:44 - On Bayesian Nonparametric Continuous Time Series Models</summary>

- *George Karabatsos, Stephen G. Walker*

- `1303.0439v1` - [abs](http://arxiv.org/abs/1303.0439v1) - [pdf](http://arxiv.org/pdf/1303.0439v1)

> This paper is a note on the use of Bayesian nonparametric mixture models for continuous time series. We identify a key requirement for such models, and then establish that there is a single type of model which meets this requirement. As it turns out, the model is well known in multiple change-point problems.

</details>

<details>

<summary>2013-03-03 01:55:10 - Bayesian learning of joint distributions of objects</summary>

- *Anjishnu Banerjee, Jared Murray, David B. Dunson*

- `1303.0449v1` - [abs](http://arxiv.org/abs/1303.0449v1) - [pdf](http://arxiv.org/pdf/1303.0449v1)

> There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure. The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types. The ITF prior is formulated as a tensor product of stick-breaking processes. Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties. Focusing on ITF mixtures of product kernels, we develop a new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications.

</details>

<details>

<summary>2013-03-04 11:11:48 - Penalized Likelihood and Bayesian Function Selection in Regression Models</summary>

- *Fabian Scheipl, Thomas Kneib, Ludwig Fahrmeir*

- `1303.0670v1` - [abs](http://arxiv.org/abs/1303.0670v1) - [pdf](http://arxiv.org/pdf/1303.0670v1)

> Challenging research in various fields has driven a wide range of methodological advances in variable selection for regression models with high-dimensional predictors. In comparison, selection of nonlinear functions in models with additive predictors has been considered only more recently. Several competing suggestions have been developed at about the same time and often do not refer to each other. This article provides a state-of-the-art review on function selection, focusing on penalized likelihood and Bayesian concepts, relating various approaches to each other in a unified framework. In an empirical comparison, also including boosting, we evaluate several methods through applications to simulated and real data, thereby providing some guidance on their performance in practice.

</details>

<details>

<summary>2013-03-04 13:19:23 - Nonparametric Bayesian methods for one-dimensional diffusion models</summary>

- *Harry van Zanten*

- `1209.6433v2` - [abs](http://arxiv.org/abs/1209.6433v2) - [pdf](http://arxiv.org/pdf/1209.6433v2)

> In this paper we review recently developed methods for nonparametric Bayesian inference for one-dimensional diffusion models. We discuss different possible prior distributions, computational issues, and asymptotic results.

</details>

<details>

<summary>2013-03-05 22:58:24 - Impulsive Noise Mitigation in Powerline Communications Using Sparse Bayesian Learning</summary>

- *Jing Lin, Marcel Nassar, Brian L. Evans*

- `1303.1217v1` - [abs](http://arxiv.org/abs/1303.1217v1) - [pdf](http://arxiv.org/pdf/1303.1217v1)

> Additive asynchronous and cyclostationary impulsive noise limits communication performance in OFDM powerline communication (PLC) systems. Conventional OFDM receivers assume additive white Gaussian noise and hence experience degradation in communication performance in impulsive noise. Alternate designs assume a parametric statistical model of impulsive noise and use the model parameters in mitigating impulsive noise. These receivers require overhead in training and parameter estimation, and degrade due to model and parameter mismatch, especially in highly dynamic environments. In this paper, we model impulsive noise as a sparse vector in the time domain without any other assumptions, and apply sparse Bayesian learning methods for estimation and mitigation without training. We propose three iterative algorithms with different complexity vs. performance trade-offs: (1) we utilize the noise projection onto null and pilot tones to estimate and subtract the noise impulses; (2) we add the information in the data tones to perform joint noise estimation and OFDM detection; (3) we embed our algorithm into a decision feedback structure to further enhance the performance of coded systems. When compared to conventional OFDM PLC receivers, the proposed receivers achieve SNR gains of up to 9 dB in coded and 10 dB in uncoded systems in the presence of impulsive noise.

</details>

<details>

<summary>2013-03-06 08:43:10 - Stochastic model selection for Mixtures of Matrix-Normals</summary>

- *Cinzia Viroli*

- `1010.2310v3` - [abs](http://arxiv.org/abs/1010.2310v3) - [pdf](http://arxiv.org/pdf/1010.2310v3)

> Finite mixtures of matrix normal distributions are a powerful tool for classifying three-way data in unsupervised problems. The distribution of each component is assumed to be a matrix variate normal density. The mixture model can be estimated through the EM algorithm under the assumption that the number of components is known and fixed. In this work we introduce, develop and explore a Bayesian analysis of the model in order to provide a tool for simultaneous model estimation and model selection. The effectiveness of the proposed method is illustrated on a simulation study and on a real example.

</details>

<details>

<summary>2013-03-06 10:00:25 - The cost of using exact confidence intervals for a binomial proportion</summary>

- *MÃ¥ns Thulin*

- `1303.1288v1` - [abs](http://arxiv.org/abs/1303.1288v1) - [pdf](http://arxiv.org/pdf/1303.1288v1)

> When computing a confidence interval for a binomial proportion p one must choose between using an exact interval, which has a coverage probability of at least 1-{\alpha} for all values of p, and a shorter approximate interval, which may have lower coverage for some p but that on average has coverage equal to 1-\alpha. We investigate the cost of using the exact one and two-sided Clopper--Pearson confidence intervals rather than shorter approximate intervals, first in terms of increased expected length and then in terms of the increase in sample size required to obtain a desired expected length. Using asymptotic expansions, we also give a closed-form formula for determining the sample size for the exact Clopper--Pearson methods. For two-sided intervals, our investigation reveals an interesting connection between the frequentist Clopper--Pearson interval and Bayesian intervals based on noninformative priors.

</details>

<details>

<summary>2013-03-06 12:17:12 - A Fast Iterative Bayesian Inference Algorithm for Sparse Channel Estimation</summary>

- *Niels Lovmand Pedersen, Carles Navarro ManchÃ³n Bernard Henri Fleury*

- `1303.1312v1` - [abs](http://arxiv.org/abs/1303.1312v1) - [pdf](http://arxiv.org/pdf/1303.1312v1)

> In this paper, we present a Bayesian channel estimation algorithm for multicarrier receivers based on pilot symbol observations. The inherent sparse nature of wireless multipath channels is exploited by modeling the prior distribution of multipath components' gains with a hierarchical representation of the Bessel K probability density function; a highly efficient, fast iterative Bayesian inference method is then applied to the proposed model. The resulting estimator outperforms other state-of-the-art Bayesian and non-Bayesian estimators, either by yielding lower mean squared estimation error or by attaining the same accuracy with improved convergence rate, as shown in our numerical evaluation.

</details>

<details>

<summary>2013-03-08 21:24:17 - Mark-Recapture with Multiple Non-invasive Marks</summary>

- *Simon J. Bonner, Jason A. Holmberg*

- `1208.4285v3` - [abs](http://arxiv.org/abs/1208.4285v3) - [pdf](http://arxiv.org/pdf/1208.4285v3)

> Non-invasive marks, including pigmentation patterns, acquired scars,and genetic mark- ers, are often used to identify individuals in mark-recapture experiments. If animals in a population can be identified from multiple, non-invasive marks then some individuals may be counted twice in the observed data. Analyzing the observed histories without accounting for these errors will provide incorrect inference about the population dynamics. Previous approaches to this problem include modeling data from only one mark and combining estimators obtained from each mark separately assuming that they are independent. Motivated by the analysis of data from the ECOCEAN online whale shark (Rhincodon typus) catalog, we describe a Bayesian method to analyze data from multiple, non-invasive marks that is based on the latent-multinomial model of Link et al. (2010). Further to this, we describe a simplification of the Markov chain Monte Carlo algorithm of Link et al. (2010) that leads to more efficient computation. We present results from the analysis of the ECOCEAN whale shark data and from simulation studies comparing our method with the previous approaches.

</details>

<details>

<summary>2013-03-08 21:41:23 - Probabilistic temperature forecasting with statistical calibration in Hungary</summary>

- *SÃ¡ndor Baran, AndrÃ¡s HorÃ¡nyi, DÃ³ra Nemoda*

- `1303.2133v1` - [abs](http://arxiv.org/abs/1303.2133v1) - [pdf](http://arxiv.org/pdf/1303.2133v1)

> Weather forecasting is mostly based on the outputs of deterministic numerical weather forecasting models. Multiple runs of these models with different initial conditions result in forecast ensembles which is are used for estimating the distribution of future atmospheric variables. However, these ensembles are usually under-dispersive and uncalibrated, so post-processing is required.   In the present work Bayesian Model Averaging (BMA) is applied for calibrating ensembles of temperature forecasts produced by the operational Limited Area Model Ensemble Prediction System of the Hungarian Meteorological Service (HMS).   We describe two possible BMA models for temperature data of the HMS and show that BMA post-processing significantly improves calibration and probabilistic forecasts although the accuracy of point forecasts is rather unchanged.

</details>

<details>

<summary>2013-03-11 02:28:43 - Robust Bayesian variable selection with sub-harmonic priors</summary>

- *Yuzo Maruyama, William E. Strawderman*

- `1009.1926v4` - [abs](http://arxiv.org/abs/1009.1926v4) - [pdf](http://arxiv.org/pdf/1009.1926v4)

> This paper studies Bayesian variable selection in linear models with general spherically symmetric error distributions. We propose sub-harmonic priors which arise as a class of mixtures of Zellner's g-priors for which the Bayes factors are independent of the underlying error distribution, as long as it is in the spherically symmetric class. Because of this invariance to spherically symmetric error distribution, we refer to our method as a robust Bayesian variable selection method. We demonstrate that our Bayes factors have model selection consistency and are coherent. We also develop Laplace approximations to Bayes factors for a number of recently studied mixtures of g-priors that have recently appeared in the literature (including our own) for Gaussian errors. These approximations, in each case, are given by the Gaussian Bayes factor based on BIC times a simple rational function of the prior's hyper-parameters and the R^2's for the respective models. We also extend model selection consistency for several g-prior based Bayes factor methods for Gaussian errors to the entire class of spherically symmetric error distributions. Additionally we demonstrate that our class of sub-harmonic priors are the only ones within a large class of mixtures of g-priors studied in the literature which are robust in our sense. A simulation study and an analysis of two real data sets indicates good performance of our robust Bayes factors relative to BIC and to other mixture of g-prior based methods.

</details>

<details>

<summary>2013-03-11 13:06:49 - Monte-Carlo utility estimates for Bayesian reinforcement learning</summary>

- *Christos Dimitrakakis*

- `1303.2506v1` - [abs](http://arxiv.org/abs/1303.2506v1) - [pdf](http://arxiv.org/pdf/1303.2506v1)

> This paper introduces a set of algorithms for Monte-Carlo Bayesian reinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the Bayes-optimal value function is employed to construct an optimistic policy. Secondly, gradient-based algorithms for approximate upper and lower bounds are introduced. Finally, we introduce a new class of gradient algorithms for Bayesian Bellman error minimisation. We theoretically show that the gradient methods are sound. Experimentally, we demonstrate the superiority of the upper bound method in terms of reward obtained. However, we also show that the Bayesian Bellman error method is a close second, despite its significant computational simplicity.

</details>

<details>

<summary>2013-03-12 07:39:31 - Combining Dynamic Predictions from Joint Models for Longitudinal and Time-to-Event Data using Bayesian Model Averaging</summary>

- *Dimitris Rizopoulos, Laura A. Hatfield, Bradley P. Carlin, Johanna J. M. Takkenberg*

- `1303.2797v1` - [abs](http://arxiv.org/abs/1303.2797v1) - [pdf](http://arxiv.org/pdf/1303.2797v1)

> The joint modeling of longitudinal and time-to-event data is an active area of statistics research that has received a lot of attention in the recent years. More recently, a new and attractive application of this type of models has been to obtain individualized predictions of survival probabilities and/or of future longitudinal responses. The advantageous feature of these predictions is that they are dynamically updated as extra longitudinal responses are collected for the subjects of interest, providing real time risk assessment using all recorded information. The aim of this paper is two-fold. First, to highlight the importance of modeling the association structure between the longitudinal and event time responses that can greatly influence the derived predictions, and second, to illustrate how we can improve the accuracy of the derived predictions by suitably combining joint models with different association structures. The second goal is achieved using Bayesian model averaging, which, in this setting, has the very intriguing feature that the model weights are not fixed but they are rather subject- and time-dependent, implying that at different follow-up times predictions for the same subject may be based on different models.

</details>

<details>

<summary>2013-03-12 14:47:51 - Bayesian Ultrahigh-Dimensional Screening Via MCMC</summary>

- *Zuofeng Shang, Ping Li*

- `1302.1154v4` - [abs](http://arxiv.org/abs/1302.1154v4) - [pdf](http://arxiv.org/pdf/1302.1154v4)

> We explore the theoretical and numerical property of a fully Bayesian model selection method in sparse ultrahigh-dimensional settings, i.e., $p\gg n$, where $p$ is the number of covariates and $n$ is the sample size. Our method consists of (1) a hierarchical Bayesian model with a novel prior placed over the model space which includes a hyperparameter $t_n$ controlling the model size, and (2) an efficient MCMC algorithm for automatic and stochastic search of the models. Our theory shows that, when specifying $t_n$ correctly, the proposed method yields selection consistency, i.e., the posterior probability of the true model asymptotically approaches one; when $t_n$ is misspecified, the selected model is still asymptotically nested in the true model. The theory also reveals insensitivity of the selection result with respect to the choice of $t_n$. In implementations, a reasonable prior is further assumed on $t_n$ which allows us to draw its samples stochastically. Our approach conducts selection, estimation and even inference in a unified framework. No additional prescreening or dimension reduction step is needed. Two novel $g$-priors are proposed to make our approach more flexible. A simulation study is given to display the numerical advantage of our method.

</details>

<details>

<summary>2013-03-12 15:14:55 - Bayesian methods in the Shape Invariant Model (II): Identifiability and posterior contraction rates on functional spaces</summary>

- *Dominique Bontemps, Sebastien Gadat*

- `1302.2044v2` - [abs](http://arxiv.org/abs/1302.2044v2) - [pdf](http://arxiv.org/pdf/1302.2044v2)

> In this paper, we consider the so-called Shape Invariant Model which stands for the estimation of a function f0 submitted to a random translation of law g0 in a white noise model. We are interested in such a model when the law of the deformations is unknown. We aim to recover the law of the process P(f0,g0) as well as f0 and g0.   We first provide some identifiability result on this model and then adopt a Bayesian point of view. In this view, we find some prior on f and g such that the posterior distribution concentrates around the functions f0 and g0 when n goes to infinity, we then obtain a contraction rate of order a power of log(n)^(-1). We also obtain a lower bound on the model for the estimation of f0 and g0 in a frequentist paradigm which also decreases following a power of log(n)^(-1).

</details>

<details>

<summary>2013-03-12 15:16:12 - Bayesian methods in the Shape Invariant Model (I): Posterior contraction rates on probability measures</summary>

- *Dominique Bontemps, Sebastien Gadat*

- `1302.2043v2` - [abs](http://arxiv.org/abs/1302.2043v2) - [pdf](http://arxiv.org/pdf/1302.2043v2)

> In this paper, we consider the so-called Shape Invariant Model which stands for the estimation of a function f0 submitted to a random translation of law g0 in a white noise model. We are interested in such a model when the law of the deformations is unknown. We aim to recover the law of the process P(f0,g0). In this perspective, we adopt a Bayesian point of view and find prior on f and g such that the posterior distribution concentrates at a polynomial rate around P(f0,g0) when n goes to infinity. We intensively use some Bayesian non parametric tools coupled with mixture models and believe that some of our results obtained on this mixture framework may be also of interest for frequentist point of view.

</details>

<details>

<summary>2013-03-12 21:17:15 - Bayesian posterior consistency in the functional randomly shifted curves model</summary>

- *Dominique Bontemps, SÃ©bastien Gadat*

- `1212.5429v2` - [abs](http://arxiv.org/abs/1212.5429v2) - [pdf](http://arxiv.org/pdf/1212.5429v2)

> In this paper, we consider the so-called Shape Invariant Model which stands for the estimation of a function $f^0$ submitted to a random translation of law $g^0$ in a white noise model. We are interested in such a model when the law of the deformations is unknown. We aim to recover the law of the process $\PP_{f^0,g^0}$ as well as $f^0$ and $g^0$. In this perspective, we adopt a Bayesian point of view and find prior on $f$ and $g$ such that the posterior distribution concentrates around $\PP_{f^0,g^0}$ at a polynomial rate when $n$ goes to $+\infty$. We obtain a logarithmic posterior contraction rate for the shape $f^0$ and the distribution $g^0$. We also derive logarithmic lower bounds for the estimation of $f^0$ and $g^0$ in a frequentist paradigm.

</details>

<details>

<summary>2013-03-12 22:54:46 - Variational Inference in Nonconjugate Models</summary>

- *Chong Wang, David M. Blei*

- `1209.4360v4` - [abs](http://arxiv.org/abs/1209.4360v4) - [pdf](http://arxiv.org/pdf/1209.4360v4)

> Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest---like the correlated topic model and Bayesian logistic regression---are nonconjuate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world datasets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.

</details>

<details>

<summary>2013-03-14 10:03:50 - Conjugate distributions in hierarchical Bayesian ANOVA for computational efficiency and assessments of both practical and statistical significance</summary>

- *Steven Geinitz, Reinhard Furrer*

- `1303.3390v1` - [abs](http://arxiv.org/abs/1303.3390v1) - [pdf](http://arxiv.org/pdf/1303.3390v1)

> Assessing variability according to distinct factors in data is a fundamental technique of statistics. The method commonly regarded to as analysis of variance (ANOVA) is, however, typically confined to the case where all levels of a factor are present in the data (i.e. the population of factor levels has been exhausted). Random and mixed effects models are used for more elaborate cases, but require distinct nomenclature, concepts and theory, as well as distinct inferential procedures. Following a hierarchical Bayesian approach, a comprehensive ANOVA framework is shown, which unifies the above statistical models, emphasizes practical rather than statistical significance, addresses issues of parameter identifiability for random effects, and provides straightforward computational procedures for inferential steps. Although this is done in a rigorous manner the contents herein can be seen as ideological in supporting a shift in the approach taken towards analysis of variance.

</details>

<details>

<summary>2013-03-14 19:06:33 - Parametric estimation of hidden stochastic model by contrast minimization and deconvolution: application to the Stochastic Volatility Model</summary>

- *Salima El Kolei*

- `1202.2559v2` - [abs](http://arxiv.org/abs/1202.2559v2) - [pdf](http://arxiv.org/pdf/1202.2559v2)

> We study a new parametric approach for particular hidden stochastic models such as the Stochastic Volatility model. This method is based on contrast minimization and deconvolution. After proving consistency and asymptotic normality of the estimation leading to asymptotic confidence intervals, we provide a thorough numerical study, which compares most of the classical methods that are used in practice (Quasi Maximum Likelihood estimator, Simulated Expectation Maximization Likelihood estimator and Bayesian estimators). We prove that our estimator clearly outperforms the Maximum Likelihood Estimator in term of computing time, but also most of the other methods. We also show that this contrast method is the most robust with respect to non Gaussianity of the error and also does not need any tuning parameter.

</details>

<details>

<summary>2013-03-15 18:13:34 - Efficient learning in ABC algorithms</summary>

- *Mohammed Sedki, Pierre Pudlo, Jean-Michel Marin, Christian P. Robert, Jean-Marie Cornuet*

- `1210.1388v2` - [abs](http://arxiv.org/abs/1210.1388v2) - [pdf](http://arxiv.org/pdf/1210.1388v2)

> Approximate Bayesian Computation has been successfully used in population genetics to bypass the calculation of the likelihood. These methods provide accurate estimates of the posterior distribution by comparing the observed dataset to a sample of datasets simulated from the model. Although parallelization is easily achieved, computation times for ensuring a suitable approximation quality of the posterior distribution are still high. To alleviate the computational burden, we propose an adaptive, sequential algorithm that runs faster than other ABC algorithms but maintains accuracy of the approximation. This proposal relies on the sequential Monte Carlo sampler of Del Moral et al. (2012) but is calibrated to reduce the number of simulations from the model. The paper concludes with numerical experiments on a toy example and on a population genetic study of Apis mellifera, where our algorithm was shown to be faster than traditional ABC schemes.

</details>

<details>

<summary>2013-03-15 19:07:48 - Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases and its Application to MRFM</summary>

- *Se Un Park, Nicolas Dobigeon, Alfred O. Hero*

- `1303.3866v1` - [abs](http://arxiv.org/abs/1303.3866v1) - [pdf](http://arxiv.org/pdf/1303.3866v1)

> We present a variational Bayesian method of joint image reconstruction and point spread function (PSF) estimation when the PSF of the imaging device is only partially known. To solve this semi-blind deconvolution problem, prior distributions are specified for the PSF and the 3D image. Joint image reconstruction and PSF estimation is then performed within a Bayesian framework, using a variational algorithm to estimate the posterior distribution. The image prior distribution imposes an explicit atomic measure that corresponds to image sparsity. Importantly, the proposed Bayesian deconvolution algorithm does not require hand tuning. Simulation results clearly demonstrate that the semi-blind deconvolution algorithm compares favorably with previous Markov chain Monte Carlo (MCMC) version of myopic sparse reconstruction. It significantly outperforms mismatched non-blind algorithms that rely on the assumption of the perfect knowledge of the PSF. The algorithm is illustrated on real data from magnetic resonance force microscopy (MRFM).

</details>

<details>

<summary>2013-03-15 23:30:13 - Bayesian Sparse Factor Analysis of Genetic Covariance Matrices</summary>

- *Daniel E Runcie, Sayan Mukherjee*

- `1211.3706v2` - [abs](http://arxiv.org/abs/1211.3706v2) - [pdf](http://arxiv.org/pdf/1211.3706v2)

> Quantitative genetic studies that model complex, multivariate phenotypes are important for both evolutionary prediction and artificial selection. For example, changes in gene expression can provide insight into developmental and physiological mechanisms that link genotype and phenotype. However, classical analytical techniques are poorly suited to quantitative genetic studies of gene expression where the number of traits assayed per individual can reach many thousand. Here, we derive a Bayesian genetic sparse factor model for estimating the genetic covariance matrix (G-matrix) of high-dimensional traits, such as gene expression, in a mixed effects model. The key idea of our model is that we need only consider G-matrices that are biologically plausible. An organism's entire phenotype is the result of processes that are modular and have limited complexity. This implies that the G-matrix will be highly structured. In particular, we assume that a limited number of intermediate traits (or factors, e.g., variations in development or physiology) control the variation in the high-dimensional phenotype, and that each of these intermediate traits is sparse -- affecting only a few observed traits. The advantages of this approach are two-fold. First, sparse factors are interpretable and provide biological insight into mechanisms underlying the genetic architecture. Second, enforcing sparsity helps prevent sampling errors from swamping out the true signal in high-dimensional data. We demonstrate the advantages of our model on simulated data and in an analysis of a published Drosophila melanogaster gene expression data set.

</details>

<details>

<summary>2013-03-18 04:11:06 - Modeling a Sensor to Improve its Efficacy</summary>

- *N. K. Malakar, D. Gladkov, K. H. Knuth*

- `1303.4385v1` - [abs](http://arxiv.org/abs/1303.4385v1) - [pdf](http://arxiv.org/pdf/1303.4385v1)

> Robots rely on sensors to provide them with information about their surroundings. However, high-quality sensors can be extremely expensive and cost-prohibitive. Thus many robotic systems must make due with lower-quality sensors. Here we demonstrate via a case study how modeling a sensor can improve its efficacy when employed within a Bayesian inferential framework. As a test bed we employ a robotic arm that is designed to autonomously take its own measurements using an inexpensive LEGO light sensor to estimate the position and radius of a white circle on a black field. The light sensor integrates the light arriving from a spatially distributed region within its field of view weighted by its Spatial Sensitivity Function (SSF). We demonstrate that by incorporating an accurate model of the light sensor SSF into the likelihood function of a Bayesian inference engine, an autonomous system can make improved inferences about its surroundings. The method presented here is data-based, fairly general, and made with plug-and play in mind so that it could be implemented in similar problems.

</details>

<details>

<summary>2013-03-18 21:34:06 - Generalized Thompson Sampling for Sequential Decision-Making and Causal Inference</summary>

- *Pedro A. Ortega, Daniel A. Braun*

- `1303.4431v1` - [abs](http://arxiv.org/abs/1303.4431v1) - [pdf](http://arxiv.org/pdf/1303.4431v1)

> Recently, it has been shown how sampling actions from the predictive distribution over the optimal action-sometimes called Thompson sampling-can be applied to solve sequential adaptive control problems, when the optimal policy is known for each possible environment. The predictive distribution can then be constructed by a Bayesian superposition of the optimal policies weighted by their posterior probability that is updated by Bayesian inference and causal calculus. Here we discuss three important features of this approach. First, we discuss in how far such Thompson sampling can be regarded as a natural consequence of the Bayesian modeling of policy uncertainty. Second, we show how Thompson sampling can be used to study interactions between multiple adaptive agents, thus, opening up an avenue of game-theoretic analysis. Third, we show how Thompson sampling can be applied to infer causal relationships when interacting with an environment in a sequential fashion. In summary, our results suggest that Thompson sampling might not merely be a useful heuristic, but a principled method to address problems of adaptive sequential decision-making and causal inference.

</details>

<details>

<summary>2013-03-19 03:21:02 - Adaptive Bayesian multivariate density estimation with Dirichlet mixtures</summary>

- *Weining Shen, Surya T. Tokdar, Subhashis Ghosal*

- `1109.6406v3` - [abs](http://arxiv.org/abs/1109.6406v3) - [pdf](http://arxiv.org/pdf/1109.6406v3)

> We show that rate-adaptive multivariate density estimation can be performed using Bayesian methods based on Dirichlet mixtures of normal kernels with a prior distribution on the kernel's covariance matrix parameter. We derive sufficient conditions on the prior specification that guarantee convergence to a true density at a rate that is optimal minimax for the smoothness class to which the true density belongs. No prior knowledge of smoothness is assumed. The sufficient conditions are shown to hold for the Dirichlet location mixture of normals prior with a Gaussian base measure and an inverse-Wishart prior on the covariance matrix parameter. Locally H\"older smoothness classes and their anisotropic extensions are considered. Our study involves several technical novelties, including sharp approximation of finitely differentiable multivariate densities by normal mixtures and a new sieve on the space of such densities.

</details>

<details>

<summary>2013-03-20 15:29:46 - Bayesian Networks Aplied to Therapy Monitoring</summary>

- *Carlo Berzuini, David J. Spiegelhalter, Riccardo Bellazzi*

- `1303.5707v1` - [abs](http://arxiv.org/abs/1303.5707v1) - [pdf](http://arxiv.org/pdf/1303.5707v1)

> We propose a general Bayesian network model for application in a wide class of problems of therapy monitoring. We discuss the use of stochastic simulation as a computational approach to inference on the proposed class of models. As an illustration we present an application to the monitoring of cytotoxic chemotherapy in breast cancer.

</details>

<details>

<summary>2013-03-21 01:39:47 - Dependent Dirichlet Process Rating Model (DDP-RM)</summary>

- *Ken Akira Fujimoto, George Karabatsos*

- `1212.5301v3` - [abs](http://arxiv.org/abs/1212.5301v3) - [pdf](http://arxiv.org/pdf/1212.5301v3)

> Typical IRT rating-scale models assume that the rating category threshold parameters are the same over examinees. However, it can be argued that many rating data sets violate this assumption. To address this practical psychometric problem, we introduce a novel, Bayesian nonparametric IRT model for rating scale items. The model is an infinite-mixture of Rasch partial credit models, based on a localized Dependent Dirichlet process (DDP). The model treats the rating thresholds as the random parameters that are subject to the mixture, and has (stick-breaking) mixture weights that are covariate-dependent. Thus, the novel model allows the rating category thresholds to vary flexibly across items and examinees, and allows the distribution of the category thresholds to vary flexibly as a function of covariates. We illustrate the new model through the analysis of a simulated data set, and through the analysis of a real rating data set that is well-known in the psychometric literature. The model is shown to have better predictive-fit performance, compared to other commonly used IRT rating models.

</details>

<details>

<summary>2013-03-21 07:20:20 - On the optimality of the aggregate with exponential weights for low temperatures</summary>

- *Guillaume LecuÃ©, Shahar Mendelson*

- `1303.5180v1` - [abs](http://arxiv.org/abs/1303.5180v1) - [pdf](http://arxiv.org/pdf/1303.5180v1)

> Given a finite class of functions F, the problem of aggregation is to construct a procedure with a risk as close as possible to the risk of the best element in the class. A classical procedure (PAC-Bayesian statistical learning theory (2004) Paris 6, Statistical Learning Theory and Stochastic Optimization (2001) Springer, Ann. Statist. 28 (2000) 75-87) is the aggregate with exponential weights (AEW), defined by \[\tilde{f}^{\mathrm{AEW}}=\sum_{f\in F}\hat{\theta}(f)f,\qquad where \hat{\theta}(f)=\frac{\exp(-({n}/{T})R_n(f))}{\sum_{g\in F}\exp(-({n}/{T})R_n(g))},\] where $T>0$ is called the temperature parameter and $R_n(\cdot)$ is an empirical risk. In this article, we study the optimality of the AEW in the regression model with random design and in the low-temperature regime. We prove three properties of AEW. First, we show that AEW is a suboptimal aggregation procedure in expectation with respect to the quadratic risk when $T\leq c_1$, where $c_1$ is an absolute positive constant (the low-temperature regime), and that it is suboptimal in probability even for high temperatures. Second, we show that as the cardinality of the dictionary grows, the behavior of AEW might deteriorate, namely, that in the low-temperature regime it might concentrate with high probability around elements in the dictionary with risk greater than the risk of the best function in the dictionary by at least an order of $1/\sqrt{n}$. Third, we prove that if a geometric condition on the dictionary (the so-called "Bernstein condition) is assumed, then AEW is indeed optimal both in high probability and in expectation in the low-temperature regime. Moreover, under that assumption, the complexity term is essentially the logarithm of the cardinality of the set of "almost minimizers" rather than the logarithm of the cardinality of the entire dictionary. This result holds for small values of the temperature parameter, thus complementing an analogous result for high temperatures.

</details>

<details>

<summary>2013-03-22 09:54:46 - Aggregation by exponential weighting, sharp PAC-Bayesian bounds and sparsity</summary>

- *Arnak Dalalyan, Alexandre Tsybakov*

- `0803.2839v2` - [abs](http://arxiv.org/abs/0803.2839v2) - [pdf](http://arxiv.org/pdf/0803.2839v2)

> We study the problem of aggregation under the squared loss in the model of regression with deterministic design. We obtain sharp PAC-Bayesian risk bounds for aggregates defined via exponential weights, under general assumptions on the distribution of errors and on the functions to aggregate. We then apply these results to derive sparsity oracle inequalities.

</details>

<details>

<summary>2013-03-22 13:34:28 - Network Detection Theory and Performance</summary>

- *Steven T. Smith, Kenneth D. Senne, Scott Philips, Edward K. Kao, Garrett Bernstein*

- `1303.5613v1` - [abs](http://arxiv.org/abs/1303.5613v1) - [pdf](http://arxiv.org/pdf/1303.5613v1)

> Network detection is an important capability in many areas of applied research in which data can be represented as a graph of entities and relationships. Oftentimes the object of interest is a relatively small subgraph in an enormous, potentially uninteresting background. This aspect characterizes network detection as a "big data" problem. Graph partitioning and network discovery have been major research areas over the last ten years, driven by interest in internet search, cyber security, social networks, and criminal or terrorist activities. The specific problem of network discovery is addressed as a special case of graph partitioning in which membership in a small subgraph of interest must be determined. Algebraic graph theory is used as the basis to analyze and compare different network detection methods. A new Bayesian network detection framework is introduced that partitions the graph based on prior information and direct observations. The new approach, called space-time threat propagation, is proved to maximize the probability of detection and is therefore optimum in the Neyman-Pearson sense. This optimality criterion is compared to spectral community detection approaches which divide the global graph into subsets or communities with optimal connectivity properties. We also explore a new generative stochastic model for covert networks and analyze using receiver operating characteristics the detection performance of both classes of optimal detection techniques.

</details>

<details>

<summary>2013-03-22 20:15:31 - Bayesian Compressed Regression</summary>

- *Rajarshi Guhaniyogi, David B. Dunson*

- `1303.0642v2` - [abs](http://arxiv.org/abs/1303.0642v2) - [pdf](http://arxiv.org/pdf/1303.0642v2)

> As an alternative to variable selection or shrinkage in high dimensional regression, we propose to randomly compress the predictors prior to analysis. This dramatically reduces storage and computational bottlenecks, performing well when the predictors can be projected to a low dimensional linear subspace with minimal loss of information about the response. As opposed to existing Bayesian dimensionality reduction approaches, the exact posterior distribution conditional on the compressed data is available analytically, speeding up computation by many orders of magnitude while also bypassing robustness issues due to convergence and mixing problems with MCMC. Model averaging is used to reduce sensitivity to the random projection matrix, while accommodating uncertainty in the subspace dimension. Strong theoretical support is provided for the approach by showing near parametric convergence rates for the predictive density in the large p small n asymptotic paradigm. Practical performance relative to competitors is illustrated in simulations and real data applications.

</details>

<details>

<summary>2013-03-23 01:23:34 - Deep Gaussian Processes</summary>

- *Andreas C. Damianou, Neil D. Lawrence*

- `1211.0358v2` - [abs](http://arxiv.org/abs/1211.0358v2) - [pdf](http://arxiv.org/pdf/1211.0358v2)

> In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.

</details>

<details>

<summary>2013-03-23 13:34:19 - Inferential models: A framework for prior-free posterior probabilistic inference</summary>

- *Ryan Martin, Chuanhai Liu*

- `1206.4091v3` - [abs](http://arxiv.org/abs/1206.4091v3) - [pdf](http://arxiv.org/pdf/1206.4091v3)

> Posterior probabilistic statistical inference without priors is an important but so far elusive goal. Fisher's fiducial inference, Dempster-Shafer theory of belief functions, and Bayesian inference with default priors are attempts to achieve this goal but, to date, none has given a completely satisfactory picture. This paper presents a new framework for probabilistic inference, based on inferential models (IMs), which not only provides data-dependent probabilistic measures of uncertainty about the unknown parameter, but does so with an automatic long-run frequency calibration property. The key to this new approach is the identification of an unobservable auxiliary variable associated with observable data and unknown parameter, and the prediction of this auxiliary variable with a random set before conditioning on data. Here we present a three-step IM construction, and prove a frequency-calibration property of the IM's belief function under mild conditions. A corresponding optimality theory is developed, which helps to resolve the non-uniqueness issue. Several examples are presented to illustrate this new approach.

</details>

<details>

<summary>2013-03-23 16:05:34 - Multilevel latent Gaussian process model for mixed discrete and continuous multivariate response data</summary>

- *Erin M. Schliep, Jennifer A. Hoeting*

- `1205.4163v3` - [abs](http://arxiv.org/abs/1205.4163v3) - [pdf](http://arxiv.org/pdf/1205.4163v3)

> We propose a Bayesian model for mixed ordinal and continuous multivariate data to evaluate a latent spatial Gaussian process. Our proposed model can be used in many contexts where mixed continuous and discrete multivariate responses are observed in an effort to quantify an unobservable continuous measurement. In our example, the latent, or unobservable measurement is wetland condition. While predicted values of the latent wetland condition variable produced by the model at each location do not hold any intrinsic value, the relative magnitudes of the wetland condition values are of interest. In addition, by including point-referenced covariates in the model, we are able to make predictions at new locations for both the latent random variable and the multivariate response. Lastly, the model produces ranks of the multivariate responses in relation to the unobserved latent random field. This is an important result as it allows us to determine which response variables are most closely correlated with the latent variable. Our approach offers an alternative to traditional indices based on best professional judgment that are frequently used in ecology. We apply our model to assess wetland condition in the North Platte and Rio Grande River Basins in Colorado. The model facilitates a comparison of wetland condition at multiple locations and ranks the importance of in-field measurements.

</details>

<details>

<summary>2013-03-25 16:27:57 - Distribution and Symmetric Distribution Regression Model for Histogram-Valued Variables</summary>

- *SÃ³nia Dias, Paula Brito*

- `1303.6199v1` - [abs](http://arxiv.org/abs/1303.6199v1) - [pdf](http://arxiv.org/pdf/1303.6199v1)

> Histogram-valued variables are a particular kind of variables studied in Symbolic Data Analysis where to each entity under analysis corresponds a distribution that may be represented by a histogram or by a quantile function. Linear regression models for this type of data are necessarily more complex than a simple generalization of the classical model: the parameters cannot be negative still the linear relationship between the variables must be allowed to be either direct or inverse. In this work we propose a new linear regression model for histogram-valued variables that solves this problem, named Distribution and Symmetric Distribution Regression Model. To determine the parameters of this model it is necessary to solve a quadratic optimization problem, subject to non-negativity constraints on the unknowns; the error measure between the predicted and observed distributions uses the Mallows distance. As in classical analysis, the model is associated with a goodness-of-fit measure whose values range between 0 and 1. Using the proposed model, applications with real and simulated data are presented.

</details>

<details>

<summary>2013-03-26 15:56:19 - Average and Quantile Effects in Nonseparable Panel Models</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Jinyong Hahn, Whitney Newey*

- `0904.1990v4` - [abs](http://arxiv.org/abs/0904.1990v4) - [pdf](http://arxiv.org/pdf/0904.1990v4)

> Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time homogeneity conditions that are like "time is randomly assigned" or "time is an instrument." Partial identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of $T$, the number of time periods, is shown by deriving shrinkage rates for the identified set as $T$ grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally-convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations.

</details>

<details>

<summary>2013-03-26 23:04:41 - Two General Methods for Population Pharmacokinetic Modeling: Non-Parametric Adaptive Grid and Non-Parametric Bayesian</summary>

- *Tatiana Tatarinova, Michael Neely, Jay Bartroff, Michael van Guilder, Walter Yamada, David Bayard, Roger Jelliffe, Robert Leary, Alyona Chubatiuk, Alan Schumitzky*

- `1303.6700v1` - [abs](http://arxiv.org/abs/1303.6700v1) - [pdf](http://arxiv.org/pdf/1303.6700v1)

> Population pharmacokinetic (PK) modeling methods can be statistically classified as either parametric or nonparametric (NP). Each classification can be divided into maximum likelihood (ML) or Bayesian (B) approaches. In this paper we discuss the nonparametric case using both maximum likelihood and Bayesian approaches. We present two nonparametric methods for estimating the unknown joint population distribution of model parameter values in a pharmacokinetic/pharmacodynamic (PK/PD) dataset. The first method is the NP Adaptive Grid (NPAG). The second is the NP Bayesian (NPB) algorithm with a stick-breaking process to construct a Dirichlet prior. Our objective is to compare the performance of these two methods using a simulated PK/PD dataset. Our results showed excellent performance of NPAG and NPB in a realistically simulated PK study. This simulation allowed us to have benchmarks in the form of the true population parameters to compare with the estimates produced by the two methods, while incorporating challenges like unbalanced sample times and sample numbers as well as the ability to include the covariate of patient weight. We conclude that both NPML and NPB can be used in realistic PK/PD population analysis problems. The advantages of one versus the other are discussed in the paper. NPAG and NPB are implemented in R and freely available for download within the Pmetrics package from www.lapk.org.

</details>

<details>

<summary>2013-03-27 06:53:26 - Sequential testing over multiple stages and performance analysis of data fusion</summary>

- *Gaurav Thakur*

- `1303.6750v1` - [abs](http://arxiv.org/abs/1303.6750v1) - [pdf](http://arxiv.org/pdf/1303.6750v1)

> We describe a methodology for modeling the performance of decision-level data fusion between different sensor configurations, implemented as part of the JIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian network formulation of classical probabilistic data fusion, which allows elementary fusion structures to be stacked and analyzed efficiently. We then present an extension of the Wald sequential test for combining the outputs of the Bayesian network over time. We discuss an algorithm to compute its performance statistics and illustrate the approach on some examples. This variant of the sequential test involves multiple, distinct stages, where the evidence accumulated from each stage is carried over into the next one, and is motivated by a need to keep certain sensors in the network inactive unless triggered by other sensors.

</details>

<details>

<summary>2013-03-28 06:32:37 - Quantile Models with Endogeneity</summary>

- *Victor Chernozhukov, Christian Hansen*

- `1303.7050v1` - [abs](http://arxiv.org/abs/1303.7050v1) - [pdf](http://arxiv.org/pdf/1303.7050v1)

> In this article, we review quantile models with endogeneity. We focus on models that achieve identification through the use of instrumental variables and discuss conditions under which partial and point identification are obtained. We discuss key conditions, which include monotonicity and full-rank-type conditions, in detail. In providing this review, we update the identification results of Chernozhukov and Hansen (2005, Econometrica). We illustrate the modeling assumptions through economically motivated examples. We also briefly review the literature on estimation and inference.   Key Words: identification, treatment effects, structural models, instrumental variables

</details>

<details>

<summary>2013-03-28 22:36:29 - A Semiparametric Bayesian Approach for Extreme Values Using Dirichlet Process Mixture of Gamma and Generalized Pareto Densities</summary>

- *Jairo Fuquene*

- `1212.1949v4` - [abs](http://arxiv.org/abs/1212.1949v4) - [pdf](http://arxiv.org/pdf/1212.1949v4)

> For extreme value estimation we propose to use a model with a Dirichlet process mixture of gamma densities in the center and generalized Pareto densities for the tails. Due to the randomness in the center and a heavy tailed density in the tails density estimation and posterior inference for high quantiles are possible. The approach can be used in a "default" manner on the positive reals because it works when prior information is unavailable. The proposed model can be easy to implement and a sensitivity analysis is provided. We applied the proposed model for simulated and real data sets.

</details>

<details>

<summary>2013-03-29 07:44:19 - Approximate Inference for Observation Driven Time Series Models with Intractable Likelihoods</summary>

- *Ajay Jasra, Nikolas Kantas, Elena Ehrlich*

- `1303.7318v1` - [abs](http://arxiv.org/abs/1303.7318v1) - [pdf](http://arxiv.org/pdf/1303.7318v1)

> In the following article we consider approximate Bayesian parameter inference for observation driven time series models. Such statistical models appear in a wide variety of applications, including econometrics and applied mathematics. This article considers the scenario where the likelihood function cannot be evaluated point-wise; in such cases, one cannot perform exact statistical inference, including parameter estimation, which often requires advanced computational algorithms, such as Markov chain Monte Carlo (MCMC). We introduce a new approximation based upon approximate Bayesian computation (ABC). Under some conditions, we show that as $n\rightarrow\infty$, with $n$ the length of the time series, the ABC posterior has, almost surely, a maximum \emph{a posteriori} (MAP) estimator of the parameters which is different from the true parameter. However, a noisy ABC MAP, which perturbs the original data, asymptotically converges to the true parameter, almost surely. In order to draw statistical inference, for the ABC approximation adopted, standard MCMC algorithms can have acceptance probabilities that fall at an exponential rate in $n$ and slightly more advanced algorithms can mix poorly. We develop a new and improved MCMC kernel, which is based upon an exact approximation of a marginal algorithm, whose cost per-iteration is random but the expected cost, for good performance, is shown to be $\mathcal{O}(n^2)$ per-iteration.

</details>

<details>

<summary>2013-03-31 02:51:44 - The Alive Particle Filter</summary>

- *Ajay Jasra, Anthony Lee, Christopher Yau, Xiaole Zhang*

- `1304.0151v1` - [abs](http://arxiv.org/abs/1304.0151v1) - [pdf](http://arxiv.org/pdf/1304.0151v1)

> In the following article we develop a particle filter for approximating Feynman-Kac models with indicator potentials. Examples of such models include approximate Bayesian computation (ABC) posteriors associated with hidden Markov models (HMMs) or rare-event problems. Such models require the use of advanced particle filter or Markov chain Monte Carlo (MCMC) algorithms e.g. Jasra et al. (2012), to perform estimation. One of the drawbacks of existing particle filters, is that they may 'collapse', in that the algorithm may terminate early, due to the indicator potentials. In this article, using a special case of the locally adaptive particle filter in Lee et al. (2013), which is closely related to Le Gland & Oudjane (2004), we use an algorithm which can deal with this latter problem, whilst introducing a random cost per-time step. This algorithm is investigated from a theoretical perspective and several results are given which help to validate the algorithms and to provide guidelines for their implementation. In addition, we show how this algorithm can be used within MCMC, using particle MCMC (Andrieu et al. 2010). Numerical examples are presented for ABC approximations of HMMs.

</details>


## 2013-04

<details>

<summary>2013-04-02 05:45:58 - Dual To Ratio Cum Product Estimator In Stratified Random Sampling</summary>

- *Rajesh Singh, Mukesh Kumar, Manoj K. Chaudhary, Cem Kadilar*

- `1304.0537v1` - [abs](http://arxiv.org/abs/1304.0537v1) - [pdf](http://arxiv.org/pdf/1304.0537v1)

> Tracy et al.[8] have introduced a family of estimators using Srivenkataramana and Tracy ([6],[7]) transformation in simple random sampling. In this article, we have proposed a dual to ratio-cum-product estimator in stratified random sampling. The expressions of the mean square error of the proposed estimators are derived. Also, the theoretical findings are supported by a numerical example.

</details>

<details>

<summary>2013-04-02 11:49:11 - A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process Mixture of Gamma Densities</summary>

- *Jairo Fuquene*

- `1304.0596v1` - [abs](http://arxiv.org/abs/1304.0596v1) - [pdf](http://arxiv.org/pdf/1304.0596v1)

> In this paper we propose a model with a Dirichlet process mixture of gamma densities in the bulk part below threshold and a generalized Pareto density in the tail for extreme value estimation. The proposed model is simple and flexible allowing us posterior density estimation and posterior inference for high quantiles. The model works well even for small sample sizes and in the absence of prior information. We evaluate the performance of the proposed model through a simulation study. Finally, the proposed model is applied to a real environmental data.

</details>

<details>

<summary>2013-04-03 17:47:43 - A note on a Bayesian nonparametric estimator of the discovery probability</summary>

- *Annalisa Cerquetti*

- `1304.1030v1` - [abs](http://arxiv.org/abs/1304.1030v1) - [pdf](http://arxiv.org/pdf/1304.1030v1)

> Favaro, Lijoi, and Pruenster (2012, Biometrics, 68, 1188--1196) derive a novel Bayesian nonparametric estimator of the probability of detecting at the $(n+m+1)$th observation a species already observed with any given frequency in an enlarged sample of size $n+m$, conditionally on a basic sample of size $n$. Unfortunately the general result under Gibbs priors (Theorem 2), and consequently the explicit result under $(\alpha, \theta)$ Poisson-Dirichlet priors (Proposition 3), appear to be wrong. Here we provide the correct formulas for both the results, obtained by means of a new technique devised in Cerquetti (2013). We verify the correctness of our derivation by an explicit counterproof for the two-parameter Poisson-Dirichlet case.

</details>

<details>

<summary>2013-04-04 21:27:40 - Bayesian Odds-Ratio Filters: A Template-Based Method for Online Detection of P300 Evoked Responses</summary>

- *Asim M. Mubeen, Kevin H. Knuth*

- `1304.1565v1` - [abs](http://arxiv.org/abs/1304.1565v1) - [pdf](http://arxiv.org/pdf/1304.1565v1)

> Template-based signal detection most often relies on computing a correlation, or a dot product, between an incoming data stream and a signal template. While such a correlation results in an ongoing estimate of the magnitude of the signal in the data stream, it does not directly indicate the presence or absence of a signal. Instead, the problem of signal detection is one of model-selection. Here we explore the use of the Bayesian odds-ratio (OR), which is the ratio of posterior probabilities of a signal-plus-noise model over a noise-only model. We demonstrate this method by applying it to simulated electroencephalographic (EEG) signals based on the P300 response, which is widely used in both Brain Computer Interface (BCI) and Brain Machine Interface (BMI) systems. The efficacy of this algorithm is demonstrated by comparing the receiver operating characteristic (ROC) curves of the OR-based (logOR) filter to the usual correlation method where we find a significant improvement in P300 detection. The logOR filter promises to improve the accuracy and speed of the detection of evoked brain responses in BCI/BMI applications as well the detection of template signals in general.

</details>

<details>

<summary>2013-04-08 12:30:46 - Fiducial theory and optimal inference</summary>

- *Gunnar Taraldsen, Bo Henry Lindqvist*

- `1301.1717v2` - [abs](http://arxiv.org/abs/1301.1717v2) - [pdf](http://arxiv.org/pdf/1301.1717v2)

> It is shown that the fiducial distribution in a group model, or more generally a quasigroup model, determines the optimal equivariant frequentist inference procedures. The proof does not rely on existence of invariant measures, and generalizes results corresponding to the choice of the right Haar measure as a Bayesian prior. Classical and more recent examples show that fiducial arguments can be used to give good candidates for exact or approximate confidence distributions. It is here suggested that the fiducial algorithm can be considered as an alternative to the Bayesian algorithm for the construction of good frequentist inference procedures more generally.

</details>

<details>

<summary>2013-04-08 18:34:32 - ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process Mixtures</summary>

- *Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka*

- `1304.2302v1` - [abs](http://arxiv.org/abs/1304.2302v1) - [pdf](http://arxiv.org/pdf/1304.2302v1)

> The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. Although MCMC inference methods for the DP often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. We propose a reparameterization of the Dirichlet process that induces conditional independencies between the atoms that form the random measure. This conditional independence enables many of the Markov chain transition operators for DP inference to be simulated in parallel across multiple cores. Applied to mixture modeling, our approach enables the Dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. Unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. It also naturally lends itself to a distributed software implementation in terms of Map-Reduce, which we test in cluster configurations of over 50 machines and 100 cores. We present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1MM data vectors in 256 dimensions.

</details>

<details>

<summary>2013-04-09 09:23:20 - Unsupervised Post-Nonlinear Unmixing of Hyperspectral Images Using a Hamiltonian Monte Carlo Algorithm</summary>

- *Yoann Altmann, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1304.2499v1` - [abs](http://arxiv.org/abs/1304.2499v1) - [pdf](http://arxiv.org/pdf/1304.2499v1)

> This paper presents a nonlinear mixing model for hyperspectral image unmixing. The proposed model assumes that the pixel reflectances are post-nonlinear functions of unknown pure spectral components contaminated by an additive white Gaussian noise. These nonlinear functions are approximated using polynomials leading to a polynomial post-nonlinear mixing model. A Bayesian algorithm is proposed to estimate the parameters involved in the model yielding an unsupervised nonlinear unmixing algorithm. Due to the large number of parameters to be estimated, an efficient Hamiltonian Monte Carlo algorithm is investigated. The classical leapfrog steps of this algorithm are modified to handle the parameter constraints. The performance of the unmixing strategy, including convergence and parameter tuning, is first evaluated on synthetic data. Simulations conducted with real data finally show the accuracy of the proposed unmixing strategy for the analysis of hyperspectral images.

</details>

<details>

<summary>2013-04-10 07:30:46 - Assessment of stochastic and deterministic models of 6304 quasar lightcurves from SDSS Stripe 82</summary>

- *Rene Andrae, Dae-Won Kim, Coryn A. L. Bailer-Jones*

- `1304.2863v1` - [abs](http://arxiv.org/abs/1304.2863v1) - [pdf](http://arxiv.org/pdf/1304.2863v1)

> The optical light curves of many quasars show variations of tenths of a magnitude or more on time scales of months to years. This variation often cannot be described well by a simple deterministic model. We perform a Bayesian comparison of over 20 deterministic and stochastic models on 6304 QSO light curves in SDSS Stripe 82. We include the damped random walk (or Ornstein-Uhlenbeck [OU] process), a particular type of stochastic model which recent studies have focused on. Further models we consider are single and double sinusoids, multiple OU processes, higher order continuous autoregressive processes, and composite models. We find that only 29 out of 6304 QSO lightcurves are described significantly better by a deterministic model than a stochastic one. The OU process is an adequate description of the vast majority of cases (6023). Indeed, the OU process is the best single model for 3462 light curves, with the composite OU process/sinusoid model being the best in 1706 cases. The latter model is the dominant one for brighter/bluer QSOs. Furthermore, a non-negligible fraction of QSO lightcurves show evidence that not only the mean is stochastic but the variance is stochastic, too. Our results confirm earlier work that QSO light curves can be described with a stochastic model, but place this on a firmer footing, and further show that the OU process is preferred over several other stochastic and deterministic models. Of course, there may well exist yet better (deterministic or stochastic) models which have not been considered here.

</details>

<details>

<summary>2013-04-12 10:09:49 - Tutorial for Bayesian forensic likelihood ratio</summary>

- *Niko BrÃ¼mmer*

- `1304.3589v1` - [abs](http://arxiv.org/abs/1304.3589v1) - [pdf](http://arxiv.org/pdf/1304.3589v1)

> In the Bayesian paradigm for presenting forensic evidence to court, it is recommended that the weight of the evidence be summarized as a likelihood ratio (LR) between two opposing hypotheses of how the evidence could have been produced. Such LRs are necessarily based on probabilistic models, the parameters of which may be uncertain. It has been suggested by some authors that the value of the LR, being a function of the model parameters should therefore also be considered uncertain and that this uncertainty should be communicated to the court. In this tutorial, we consider a simple example of a 'fully Bayesian' solution, where model uncertainty is integrated out to produce a value for the LR which is not uncertain. We show that this solution agrees with common sense. In particular, the LR magnitude is a function of the amount of data that is available to estimate the model parameters.

</details>

<details>

<summary>2013-04-12 16:28:41 - Bayesian analysis of matrix data with rstiefel</summary>

- *Peter D. Hoff*

- `1304.3673v1` - [abs](http://arxiv.org/abs/1304.3673v1) - [pdf](http://arxiv.org/pdf/1304.3673v1)

> We illustrate the use of the R-package "rstiefel" for matrix-variate data analysis in the context of two examples. The first example considers estimation of a reduced-rank mean matrix in the presence of normally distributed noise. The second example considers the modeling of a social network of friendships among teenagers. Bayesian estimation for these models requires the ability to simulate from the matrix-variate von Mises-Fisher distributions and the matrix-variate Bingham distributions on the Stiefel manifold.

</details>

<details>

<summary>2013-04-12 16:41:55 - Comment on "Bayesian Nonparametric Inference - Why and How" by Mueller and Mitra</summary>

- *Peter D. Hoff*

- `1304.3676v1` - [abs](http://arxiv.org/abs/1304.3676v1) - [pdf](http://arxiv.org/pdf/1304.3676v1)

> Due to their great flexibility, nonparametric Bayes methods have proven to be a valuable tool for discovering complicated patterns in data. The term "nonparametric Bayes" suggests that these methods inherit model-free operating characteristics of classical nonparametric methods, as well as coherent uncertainty assessments provided by Bayesian procedures. However, as the authors say in the conclusion to their article, nonparametric Bayesian methods may be more aptly described as "massively parametric." Furthermore, I argue that many of the default nonparametric Bayes procedures are only Bayesian in the weakest sense of the term, and cannot be assumed to provide honest assessments of uncertainty merely because they carry the Bayesian label. However useful such procedures may be, we should be cautious about advertising default nonparametric Bayes procedures as either being "assumption free" or providing descriptions of our uncertainty. If we want our nonparametric Bayes procedures to have a Bayesian interpretation, we should modify default NP Bayes methods to accommodate real prior information, or at the very least, carefully evaluate the effects of hyperparameters on posterior quantities of interest.

</details>

<details>

<summary>2013-04-15 01:50:22 - Therapeutic hypothermia: quantification of the transition of core body temperature using the flexible mixture bent-cable model for longitudinal data</summary>

- *Shahedul A Khan, Grace S Chiu, Joel A Dubin*

- `1210.3087v2` - [abs](http://arxiv.org/abs/1210.3087v2) - [pdf](http://arxiv.org/pdf/1210.3087v2)

> By reducing core body temperature, T_c, induced hypothermia is a therapeutic tool to prevent brain damage resulting from physical trauma. However, all physiological systems begin to slow down due to hypothermia that in turn can result in increased risk of mortality. Therefore, quantification of the transition of T_c to early hypothermia is of great clinical interest. Conceptually, T_c may exhibit an either gradual or abrupt transition. Bent-cable regression is an appealing statistical tool to model such data due to the model's flexibility and greatly interpretable regression coefficients. It handles more flexibly models that traditionally have been handled by low-order polynomial models (for gradual transition) or piecewise linear changepoint models (for abrupt change). We consider a rat model for humans to quantify the temporal trend of T_c to primarily address the question: What is the critical time point associated with a breakdown in the compensatory mechanisms following the start of hypothermia therapy? To this end, we develop a Bayesian modelling framework for bent-cable regression of longitudinal data to simultaneously account for gradual and abrupt transitions. Our analysis reveals that: (a) about 39% of rats exhibit a gradual transition in T_c; (b) the critical time point is approximately the same regardless of transition type; (c) both transition types show a significant increase of T_c followed by a significant decrease.

</details>

<details>

<summary>2013-04-15 09:40:34 - Identifying cancer subtypes in glioblastoma by combining genomic, transcriptomic and epigenomic data</summary>

- *Richard S. Savage, Zoubin Ghahramani, Jim E. Griffin, Paul Kirk, David L. Wild*

- `1304.3577v2` - [abs](http://arxiv.org/abs/1304.3577v2) - [pdf](http://arxiv.org/pdf/1304.3577v2)

> We present a nonparametric Bayesian method for disease subtype discovery in multi-dimensional cancer data. Our method can simultaneously analyse a wide range of data types, allowing for both agreement and disagreement between their underlying clustering structure. It includes feature selection and infers the most likely number of disease subtypes, given the data.   We apply the method to 277 glioblastoma samples from The Cancer Genome Atlas, for which there are gene expression, copy number variation, methylation and microRNA data. We identify 8 distinct consensus subtypes and study their prognostic value for death, new tumour events, progression and recurrence. The consensus subtypes are prognostic of tumour recurrence (log-rank p-value of $3.6 \times 10^{-4}$ after correction for multiple hypothesis tests). This is driven principally by the methylation data (log-rank p-value of $2.0 \times 10^{-3}$) but the effect is strengthened by the other 3 data types, demonstrating the value of integrating multiple data types.   Of particular note is a subtype of 47 patients characterised by very low levels of methylation. This subtype has very low rates of tumour recurrence and no new events in 10 years of follow up. We also identify a small gene expression subtype of 6 patients that shows particularly poor survival outcomes. Additionally, we note a consensus subtype that showly a highly distinctive data signature and suggest that it is therefore a biologically distinct subtype of glioblastoma.   The code is available from https://sites.google.com/site/multipledatafusion/

</details>

<details>

<summary>2013-04-15 17:49:03 - MCMC Bayesian Estimation in FIEGARCH Models</summary>

- *Taiane S. Prass, SÃ­lvia R. C. Lopes, Jorge A. Achcar*

- `1304.1733v2` - [abs](http://arxiv.org/abs/1304.1733v2) - [pdf](http://arxiv.org/pdf/1304.1733v2)

> Bayesian inference for fractionally integrated exponential generalized autoregressive conditional heteroskedastic (FIEGARCH) models using Markov Chain Monte Carlo (MCMC) methods is described. A simulation study is presented to access the performance of the procedure, under the presence of long-memory in the volatility. Samples from FIEGARCH processes are obtained upon considering the generalized error distribution (GED) for the innovation process. Different values for the tail-thickness parameter \nu are considered covering both scenarios, innovation processes with lighter (\nu<2) and heavier (\nu>2) tails than the Gaussian distribution (\nu=2). A sensitivity analysis is performed by considering different prior density functions and by integrating (or not) the knowledge on the true parameter values to select the hyperparameter values.

</details>

<details>

<summary>2013-04-15 18:05:11 - On particle filters applied to electricity load forecasting</summary>

- *Tristan Launay, Anne Philippe, Sophie Lamarche*

- `1210.0770v2` - [abs](http://arxiv.org/abs/1210.0770v2) - [pdf](http://arxiv.org/pdf/1210.0770v2)

> We are interested in the online prediction of the electricity load, within the Bayesian framework of dynamic models. We offer a review of sequential Monte Carlo methods, and provide the calculations needed for the derivation of so-called particles filters. We also discuss the practical issues arising from their use, and some of the variants proposed in the literature to deal with them, giving detailed algorithms whenever possible for an easy implementation. We propose an additional step to help make basic particle filters more robust with regard to outlying observations. Finally we use such a particle filter to estimate a state-space model that includes exogenous variables in order to forecast the electricity load for the customers of the French electricity company \'Electricit\'e de France and discuss the various results obtained.

</details>

<details>

<summary>2013-04-16 00:25:41 - Kernel-smoothed conditional quantiles of randomly censored functional stationary ergodic data</summary>

- *Mohamed Chaouch, Salah Khardani*

- `1304.4304v1` - [abs](http://arxiv.org/abs/1304.4304v1) - [pdf](http://arxiv.org/pdf/1304.4304v1)

> This paper, investigates the conditional quantile estimation of a scalar random response and a functional random covariate (i.e. valued in some infinite-dimensional space) whenever {\it functional stationary ergodic data with random censorship} are considered. We introduce a kernel type estimator of the conditional quantile function. We establish the strong consistency with rate of this estimator as well as the asymptotic normality which induces a confidence interval that is usable in practice since it does not depend on any unknown quantity. An application to electricity peak demand interval prediction with censored smart meter data is carried out to show the performance of the proposed estimator.

</details>

<details>

<summary>2013-04-16 05:24:57 - Bayesian Inference for Logistic Regression Models Using Sequential Posterior Simulation</summary>

- *John Geweke, Garland Durham, Huaxin Xu*

- `1304.4333v1` - [abs](http://arxiv.org/abs/1304.4333v1) - [pdf](http://arxiv.org/pdf/1304.4333v1)

> The logistic specification has been used extensively in non-Bayesian statistics to model the dependence of discrete outcomes on the values of specified covariates. Because the likelihood function is globally weakly concave estimation by maximum likelihood is generally straightforward even in commonly arising applications with scores or hundreds of parameters. In contrast Bayesian inference has proven awkward, requiring normal approximations to the likelihood or specialized adaptations of existing Markov chain Monte Carlo and data augmentation methods. This paper approaches Bayesian inference in logistic models using recently developed generic sequential posterior simulaton (SPS) methods that require little more than the ability to evaluate the likelihood function. Compared with existing alternatives SPS is much simpler, and provides numerical standard errors and accurate approximations of marginal likelihoods as by-products. The SPS algorithm for Bayesian inference is amenable to massively parallel implementation, and when implemented using graphical processing units it is more efficient than existing alternatives. The paper demonstrates these points by means of several examples.

</details>

<details>

<summary>2013-04-16 05:57:08 - Adaptive Sequential Posterior Simulators for Massively Parallel Computing Environments</summary>

- *Garland Durham, John Geweke*

- `1304.4334v1` - [abs](http://arxiv.org/abs/1304.4334v1) - [pdf](http://arxiv.org/pdf/1304.4334v1)

> Massively parallel desktop computing capabilities now well within the reach of individual academics modify the environment for posterior simulation in fundamental and potentially quite advantageous ways. But to fully exploit these benefits algorithms that conform to parallel computing environments are needed. Sequential Monte Carlo comes very close to this ideal whereas other approaches like Markov chain Monte Carlo do not. This paper presents a sequential posterior simulator well suited to this computing environment. The simulator makes fewer analytical and programming demands on investigators, and is faster, more reliable and more complete than conventional posterior simulators. The paper extends existing sequential Monte Carlo methods and theory to provide a thorough and practical foundation for sequential posterior simulation that is well suited to massively parallel computing environments. It provides detailed recommendations on implementation, yielding an algorithm that requires only code for simulation from the prior and evaluation of prior and data densities and works well in a variety of applications representative of serious empirical work in economics and finance. The algorithm is robust to pathological posterior distributions, generates accurate marginal likelihood approximations, and provides estimates of numerical standard error and relative numerical efficiency intrinsically. The paper concludes with an application that illustrates the potential of these simulators for applied Bayesian inference.

</details>

<details>

<summary>2013-04-16 13:40:19 - Bayesian analysis of dynamic item response models in educational testing</summary>

- *Xiaojing Wang, James O. Berger, Donald S. Burdick*

- `1304.4441v1` - [abs](http://arxiv.org/abs/1304.4441v1) - [pdf](http://arxiv.org/pdf/1304.4441v1)

> Item response theory (IRT) models have been widely used in educational measurement testing. When there are repeated observations available for individuals through time, a dynamic structure for the latent trait of ability needs to be incorporated into the model, to accommodate changes in ability. Other complications that often arise in such settings include a violation of the common assumption that test results are conditionally independent, given ability and item difficulty, and that test item difficulties may be partially specified, but subject to uncertainty. Focusing on time series dichotomous response data, a new class of state space models, called Dynamic Item Response (DIR) models, is proposed. The models can be applied either retrospectively to the full data or on-line, in cases where real-time prediction is needed. The models are studied through simulated examples and applied to a large collection of reading test data obtained from MetaMetrics, Inc.

</details>

<details>

<summary>2013-04-16 13:49:23 - Clustering for multivariate continuous and discrete longitudinal data</summary>

- *ArnoÅ¡t KomÃ¡rek, Lenka KomÃ¡rkovÃ¡*

- `1304.4448v1` - [abs](http://arxiv.org/abs/1304.4448v1) - [pdf](http://arxiv.org/pdf/1304.4448v1)

> Multiple outcomes, both continuous and discrete, are routinely gathered on subjects in longitudinal studies and during routine clinical follow-up in general. To motivate our work, we consider a longitudinal study on patients with primary biliary cirrhosis (PBC) with a continuous bilirubin level, a discrete platelet count and a dichotomous indication of blood vessel malformations as examples of such longitudinal outcomes. An apparent requirement is to use all the outcome values to classify the subjects into groups (e.g., groups of subjects with a similar prognosis in a clinical setting). In recent years, numerous approaches have been suggested for classification based on longitudinal (or otherwise correlated) outcomes, targeting not only traditional areas like biostatistics, but also rapidly evolving bioinformatics and many others. However, most available approaches consider only continuous outcomes as a basis for classification, or if noncontinuous outcomes are considered, then not in combination with other outcomes of a different nature. Here, we propose a statistical method for clustering (classification) of subjects into a prespecified number of groups with a priori unknown characteristics on the basis of repeated measurements of several longitudinal outcomes of a different nature. This method relies on a multivariate extension of the classical generalized linear mixed model where a mixture distribution is additionally assumed for random effects. We base the inference on a Bayesian specification of the model and simulation-based Markov chain Monte Carlo methodology. To apply the method in practice, we have prepared ready-to-use software for use in R (http://www.R-project.org). We also discuss evaluation of uncertainty in the classification and also discuss usage of a recently proposed methodology for model comparison - the selection of a number of clusters in our case - based on the penalized posterior deviance proposed by Plummer [Biostatistics 9 (2008) 523-539].

</details>

<details>

<summary>2013-04-17 16:41:16 - Consistent Model Selection of Discrete Bayesian Networks from Incomplete Data</summary>

- *Nikolay H. Balov*

- `1105.4507v3` - [abs](http://arxiv.org/abs/1105.4507v3) - [pdf](http://arxiv.org/pdf/1105.4507v3)

> A maximum likelihood based model selection of discrete Bayesian networks is considered. The model selection is performed through scoring function $S$, which, for a given network $G$ and $n$-sample $D_n$, is defined to be the maximum log-likelihood $l$ minus a penalization term $\lambda_n h$ proportional to network complexity $h(G)$, $$ S(G|D_n) = l(G|D_n) - \lambda_n h(G). $$ The data is allowed to have missing values at random that has prompted, to improve the efficiency of estimation, a replacement of the standard log-likelihood with the sum of sample average node log-likelihoods. The latter avoids the exclusion of most partially missing data records and allows the comparison of models fitted to different samples.   Provided that a discrete Bayesian network is identifiable for a given missing data distribution, we show that if the sequence $\lambda_n$ converges to zero at a slower rate than $n^{-{1/2}}$ then the estimation is consistent. Moreover, we establish that BIC model selection ($\lambda_n=0.5\log(n)/n$) applied to the node-average log-likelihood is in general not consistent. This is in contrast to the complete data case where BIC is known to be consistent. The conclusions are confirmed by numerical examples.

</details>

<details>

<summary>2013-04-19 10:10:21 - Consistency of the posterior distribution in generalised linear inverse problems</summary>

- *Natalia Bochkina*

- `1211.3382v2` - [abs](http://arxiv.org/abs/1211.3382v2) - [pdf](http://arxiv.org/pdf/1211.3382v2)

> For ill-posed inverse problems, a regularised solution can be interpreted as a mode of the posterior distribution in a Bayesian framework. This framework enriches the set the solutions, as other posterior estimates can be used as a solution to the inverse problem, such as the posterior mean that can be easier to compute in practice. In this paper we prove consistency of Bayesian solutions of an ill-posed linear inverse problem in the Ky Fan metric for a general class of likelihoods and prior distributions in a finite dimensional setting. This result can be applied to study infinite dimensional problems by letting the dimension of the unknown parameter grow to infinity which can be viewed as discretisation on a grid or spectral approximation of an infinite dimensional problem. Likelihood and the prior distribution are assumed to be in an exponential form that includes distributions from the exponential family, and to be differentiable. The observations can be dependent. No assumption of finite moments of observations, such as expected value or the variance, is necessary thus allowing for possibly non-regular likelihoods, and allowing for non-conjugate and improper priors. If the variance exists, it may be heteroscedastic, namely, it may depend on the unknown function. We observe quite a surprising phenomenon when applying our result to the spectral approximation framework where it is possible to achieve the parametric rate of convergence, i.e the problem becomes self-regularised. We also consider a particular case of the unknown parameter being on the boundary of the parameter set, and show that the rate of convergence in this case is faster than for an interior point parameter.

</details>

<details>

<summary>2013-04-20 15:46:32 - Spatio-Temporal Low Count Processes with Application to Violent Crime Events</summary>

- *Sivan Aldor-Noiman, Lawrence D. Brown, Emily B. Fox, Robert A. Stine*

- `1304.5642v1` - [abs](http://arxiv.org/abs/1304.5642v1) - [pdf](http://arxiv.org/pdf/1304.5642v1)

> There is significant interest in being able to predict where crimes will happen, for example to aid in the efficient tasking of police and other protective measures. We aim to model both the temporal and spatial dependencies often exhibited by violent crimes in order to make such predictions. The temporal variation of crimes typically follows patterns familiar in time series analysis, but the spatial patterns are irregular and do not vary smoothly across the area. Instead we find that spatially disjoint regions exhibit correlated crime patterns. It is this indeterminate inter-region correlation structure along with the low-count, discrete nature of counts of serious crimes that motivates our proposed forecasting tool. In particular, we propose to model the crime counts in each region using an integer-valued first order autoregressive process. We take a Bayesian nonparametric approach to flexibly discover a clustering of these region-specific time series. We then describe how to account for covariates within this framework. Both approaches adjust for seasonality. We demonstrate our approach through an analysis of weekly reported violent crimes in Washington, D.C. between 2001-2008. Our forecasts outperform standard methods while additionally providing useful tools such as prediction intervals.

</details>

<details>

<summary>2013-04-20 21:02:52 - Probabilistic Catalogs for Crowded Stellar Fields</summary>

- *Brendon J. Brewer, Daniel Foreman-Mackey, David W. Hogg*

- `1211.5805v2` - [abs](http://arxiv.org/abs/1211.5805v2) - [pdf](http://arxiv.org/pdf/1211.5805v2)

> We present and implement a probabilistic (Bayesian) method for producing catalogs from images of stellar fields. The method is capable of inferring the number of sources N in the image and can also handle the challenges introduced by noise, overlapping sources, and an unknown point spread function (PSF). The luminosity function of the stars can also be inferred even when the precise luminosity of each star is uncertain, via the use of a hierarchical Bayesian model. The computational feasibility of the method is demonstrated on two simulated images with different numbers of stars. We find that our method successfully recovers the input parameter values along with principled uncertainties even when the field is crowded. We also compare our results with those obtained from the SExtractor software. While the two approaches largely agree about the fluxes of the bright stars, the Bayesian approach provides more accurate inferences about the faint stars and the number of stars, particularly in the crowded case.

</details>

<details>

<summary>2013-04-22 05:34:01 - Counterfactual analyses with graphical models based on local independence</summary>

- *Kjetil RÃ¸ysland*

- `1106.0972v3` - [abs](http://arxiv.org/abs/1106.0972v3) - [pdf](http://arxiv.org/pdf/1106.0972v3)

> We show that one can perform causal inference in a natural way for continuous-time scenarios using tools from stochastic analysis. This provides new alternatives to the positivity condition for inverse probability weighting. The probability distribution that would govern the frequency of observations in the counterfactual scenario can be characterized in terms of a so-called martingale problem. The counterfactual and factual probability distributions may be related through a likelihood ratio given by a stochastic differential equation. We can perform inference for counterfactual scenarios based on the original observations, re-weighted according to this likelihood ratio. This is possible if the solution of the stochastic differential equation is uniformly integrable, a property that can be determined by comparing the corresponding factual and counterfactual short-term predictions. Local independence graphs are directed, possibly cyclic, graphs that represent short-term prediction among sufficiently autonomous stochastic processes. We show through an example that these graphs can be used to identify and provide consistent estimators for counterfactual parameters in continuous time. This is analogous to how Judea Pearl uses graphical information to identify causal effects in finite state Bayesian networks.

</details>

<details>

<summary>2013-04-22 20:23:40 - Stochastic Variational Inference</summary>

- *Matt Hoffman, David M. Blei, Chong Wang, John Paisley*

- `1206.7051v3` - [abs](http://arxiv.org/abs/1206.7051v3) - [pdf](http://arxiv.org/pdf/1206.7051v3)

> We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.

</details>

<details>

<summary>2013-04-23 08:47:44 - Incorporating external information in analyses of clinical trials with binary outcomes</summary>

- *Minge Xie, Regina Y. Liu, C. V. Damaraju, William H. Olson*

- `1304.6208v1` - [abs](http://arxiv.org/abs/1304.6208v1) - [pdf](http://arxiv.org/pdf/1304.6208v1)

> External information, such as prior information or expert opinions, can play an important role in the design, analysis and interpretation of clinical trials. However, little attention has been devoted thus far to incorporating external information in clinical trials with binary outcomes, perhaps due to the perception that binary outcomes can be treated as normally-distributed outcomes by using normal approximations. In this paper we show that these two types of clinical trials could behave differently, and that special care is needed for the analysis of clinical trials with binary outcomes. In particular, we first examine a simple but commonly used univariate Bayesian approach and observe a technical flaw. We then study the full Bayesian approach using different beta priors and a new frequentist approach based on the notion of confidence distribution (CD). These approaches are illustrated and compared using data from clinical studies and simulations. The full Bayesian approach is theoretically sound, but surprisingly, under skewed prior distributions, the estimate derived from the marginal posterior distribution may not fall between those from the marginal prior and the likelihood of clinical trial data. This counterintuitive phenomenon, which we call the "discrepant posterior phenomenon," does not occur in the CD approach. The CD approach is also computationally simpler and can be applied directly to any prior distribution, symmetric or skewed.

</details>

<details>

<summary>2013-04-23 10:04:33 - Efficient computation with a linear mixed model on large-scale data sets with applications to genetic studies</summary>

- *Matti Pirinen, Peter Donnelly, Chris C. A. Spencer*

- `1207.4886v2` - [abs](http://arxiv.org/abs/1207.4886v2) - [pdf](http://arxiv.org/pdf/1207.4886v2)

> Motivated by genome-wide association studies, we consider a standard linear model with one additional random effect in situations where many predictors have been collected on the same subjects and each predictor is analyzed separately. Three novel contributions are (1) a transformation between the linear and log-odds scales which is accurate for the important genetic case of small effect sizes; (2) a likelihood-maximization algorithm that is an order of magnitude faster than the previously published approaches; and (3) efficient methods for computing marginal likelihoods which allow Bayesian model comparison. The methodology has been successfully applied to a large-scale association study of multiple sclerosis including over 20,000 individuals and 500,000 genetic variants.

</details>

<details>

<summary>2013-04-23 11:38:03 - Approximate Bayesian computation (ABC) gives exact results under the assumption of model error</summary>

- *Richard D. Wilkinson*

- `0811.3355v2` - [abs](http://arxiv.org/abs/0811.3355v2) - [pdf](http://arxiv.org/pdf/0811.3355v2)

> Approximate Bayesian computation (ABC) or likelihood-free inference algorithms are used to find approximations to posterior distributions without making explicit use of the likelihood function, depending instead on simulation of sample data sets from the model. In this paper we show that under the assumption of the existence of a uniform additive model error term, ABC algorithms give exact results when sufficient summaries are used. This interpretation allows the approximation made in many previous application papers to be understood, and should guide the choice of metric and tolerance in future work. ABC algorithms can be generalized by replacing the 0-1 cut-off with an acceptance probability that varies with the distance of the simulated data from the observed data. The acceptance density gives the distribution of the error term, enabling the uniform error usually used to be replaced by a general distribution. This generalization can also be applied to approximate Markov chain Monte Carlo algorithms. In light of this work, ABC algorithms can be seen as calibration techniques for implicit stochastic models, inferring parameter values in light of the computer model, data, prior beliefs about the parameter values, and any measurement or model errors.

</details>

<details>

<summary>2013-04-24 10:12:49 - Consistent non-parametric Bayesian estimation for a time-inhomogeneous Brownian motion</summary>

- *Shota Gugushvili, Peter Spreij*

- `1304.6536v1` - [abs](http://arxiv.org/abs/1304.6536v1) - [pdf](http://arxiv.org/pdf/1304.6536v1)

> We establish posterior consistency for non-parametric Bayesian estimation of the dispersion coefficient of a time-inhomogeneous Brownian motion.

</details>

<details>

<summary>2013-04-25 19:52:35 - Functional kernel estimators of large conditional quantiles</summary>

- *Laurent Gardes, StÃ©phane Girard*

- `1107.2261v4` - [abs](http://arxiv.org/abs/1107.2261v4) - [pdf](http://arxiv.org/pdf/1107.2261v4)

> We address the estimation of conditional quantiles when the covariate is functional and when the order of the quantiles converges to one as the sample size increases. In a first time, we investigate to what extent these large conditional quantiles can still be estimated through a functional kernel estimator of the conditional survival function. Sufficient conditions on the rate of convergence of their order to one are provided to obtain asymptotically Gaussian distributed estimators. In a second time, basing on these result, a functional Weissman estimator is derived, permitting to estimate large conditional quantiles of arbitrary large order. These results are illustrated on finite sample situations.

</details>

<details>

<summary>2013-04-30 14:44:20 - Statistical testing of shared genetic control for potentially related traits</summary>

- *Chris Wallace*

- `1301.5510v2` - [abs](http://arxiv.org/abs/1301.5510v2) - [pdf](http://arxiv.org/pdf/1301.5510v2)

> Integration of data from genome-wide single nucleotide polymorphism (SNP) association studies of different traits should allow researchers to disentangle the genetics of potentially related traits within individually associated regions. Formal statistical colocalisation testing of individual regions, which requires selection of a set of SNPs summarizing the association in a region. We show that the SNP selection method greatly affects type 1 error rates, with published studies having used methods expected to result in substantially inflated type 1 error rates. We show that either avoiding variable selection and instead testing the most informative principal components or integrating over variable selection using Bayesian model averaging can lead to correct control of type 1 error rates. Application to data from Graves' disease and Hashimoto's thyroiditis reveals a common genetic signature across seven regions shared between the diseases, and indicates that in five of six regions associated with Graves' disease and not Hashimoto's thyroiditis, this more likely reflects genuine absence of association with the latter rather than lack of power. Our examination, by simulation, of the performance of colocalisation tests and associated software will foster more widespread adoption of formal colocalisation testing. Given the increasing availability of large expression and genetic association data sets from disease-relevant tissue and purified cell populations, coupled with identification of regulatory sequences by projects such as ENCODE, colocalisation analysis has the potential to reveal both shared genetic signatures of related traits and causal disease genes and tissues.

</details>

<details>

<summary>2013-04-30 16:12:56 - Complexity Analysis of Accelerated MCMC Methods for Bayesian Inversion</summary>

- *Viet Ha Hoang, Christoph Schwab, Andrew M. Stuart*

- `1207.2411v2` - [abs](http://arxiv.org/abs/1207.2411v2) - [pdf](http://arxiv.org/pdf/1207.2411v2)

> We study Bayesian inversion for a model elliptic PDE with unknown diffusion coefficient. We provide complexity analyses of several Markov Chain-Monte Carlo (MCMC) methods for the efficient numerical evaluation of expectations under the Bayesian posterior distribution, given data $\delta$. Particular attention is given to bounds on the overall work required to achieve a prescribed error level $\varepsilon$. Specifically, we first bound the computational complexity of "plain" MCMC, based on combining MCMC sampling with linear complexity multilevel solvers for elliptic PDE. Our (new) work versus accuracy bounds show that the complexity of this approach can be quite prohibitive. Two strategies for reducing the computational complexity are then proposed and analyzed: first, a sparse, parametric and deterministic generalized polynomial chaos (gpc) "surrogate" representation of the forward response map of the PDE over the entire parameter space, and, second, a novel Multi-Level Markov Chain Monte Carlo (MLMCMC) strategy which utilizes sampling from a multilevel discretization of the posterior and of the forward PDE.   For both of these strategies we derive asymptotic bounds on work versus accuracy, and hence asymptotic bounds on the computational complexity of the algorithms. In particular we provide sufficient conditions on the regularity of the unknown coefficients of the PDE, and on the approximation methods used, in order for the accelerations of MCMC resulting from these strategies to lead to complexity reductions over "plain" MCMC algorithms for Bayesian inversion of PDEs.}

</details>

<details>

<summary>2013-04-30 20:12:01 - Inferring ground truth from multi-annotator ordinal data: a probabilistic approach</summary>

- *Balaji Lakshminarayanan, Yee Whye Teh*

- `1305.0015v1` - [abs](http://arxiv.org/abs/1305.0015v1) - [pdf](http://arxiv.org/pdf/1305.0015v1)

> A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth from noisy ordinal labels obtained from multiple annotators of varying and unknown expertise levels. Annotation models for ordinal data have been proposed mostly as extensions of their binary/categorical counterparts and have received little attention in the crowdsourcing literature. We propose a new model for crowdsourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational Bayesian inference algorithm for parameter estimation. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on two real world datasets containing ordinal query-URL relevance scores, collected through Amazon's Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to `spammy' annotators (i.e., annotators who assign labels randomly without actually looking at the instance) than popular baselines such as mean, median, and majority vote which do not account for annotator expertise.

</details>


## 2013-05

<details>

<summary>2013-05-02 01:01:13 - MCMC for non-linear state space models using ensembles of latent sequences</summary>

- *Alexander Y. Shestopaloff, Radford M. Neal*

- `1305.0320v1` - [abs](http://arxiv.org/abs/1305.0320v1) - [pdf](http://arxiv.org/pdf/1305.0320v1)

> Non-linear state space models are a widely-used class of models for biological, economic, and physical processes. Fitting these models to observed data is a difficult inference problem that has no straightforward solution. We take a Bayesian approach to the inference of unknown parameters of a non-linear state model; this, in turn, requires the availability of efficient Markov Chain Monte Carlo (MCMC) sampling methods for the latent (hidden) variables and model parameters. Using the ensemble technique of Neal (2010) and the embedded HMM technique of Neal (2003), we introduce a new Markov Chain Monte Carlo method for non-linear state space models. The key idea is to perform parameter updates conditional on an enormously large ensemble of latent sequences, as opposed to a single sequence, as with existing methods. We look at the performance of this ensemble method when doing Bayesian inference in the Ricker model of population dynamics. We show that for this problem, the ensemble method is vastly more efficient than a simple Metropolis method, as well as 1.9 to 12.0 times more efficient than a single-sequence embedded HMM method, when all methods are tuned appropriately. We also introduce a way of speeding up the ensemble method by performing partial backward passes to discard poor proposals at low computational cost, resulting in a final efficiency gain of 3.4 to 20.4 times over the single-sequence method.

</details>

<details>

<summary>2013-05-05 09:44:08 - Efficient Estimation of the number of neighbours in Probabilistic K Nearest Neighbour Classification</summary>

- *Ji Won Yoon, Nial Friel*

- `1305.1002v1` - [abs](http://arxiv.org/abs/1305.1002v1) - [pdf](http://arxiv.org/pdf/1305.1002v1)

> Probabilistic k-nearest neighbour (PKNN) classification has been introduced to improve the performance of original k-nearest neighbour (KNN) classification algorithm by explicitly modelling uncertainty in the classification of each feature vector. However, an issue common to both KNN and PKNN is to select the optimal number of neighbours, $k$. The contribution of this paper is to incorporate the uncertainty in $k$ into the decision making, and in so doing use Bayesian model averaging to provide improved classification. Indeed the problem of assessing the uncertainty in $k$ can be viewed as one of statistical model selection which is one of the most important technical issues in the statistics and machine learning domain. In this paper, a new functional approximation algorithm is proposed to reconstruct the density of the model (order) without relying on time consuming Monte Carlo simulations. In addition, this algorithm avoids cross validation by adopting Bayesian framework. The performance of this algorithm yielded very good performance on several real experimental datasets.

</details>

<details>

<summary>2013-05-06 16:03:48 - Bayesian Modeling and MCMC Computation in Linear Logistic Regression for Presence-only Data</summary>

- *Fabio Divino, Natalia Golini, Giovanna Jona Lasinio, Antti Penttinen*

- `1305.1232v1` - [abs](http://arxiv.org/abs/1305.1232v1) - [pdf](http://arxiv.org/pdf/1305.1232v1)

> Presence-only data are referred to situations in which, given a censoring mechanism, a binary response can be observed only with respect to on outcome, usually called \textit{presence}. In this work we present a Bayesian approach to the problem of presence-only data based on a two levels scheme. A probability law and a case-control design are combined to handle the double source of uncertainty: one due to the censoring and one due to the sampling. We propose a new formalization for the logistic model with presence-only data that allows further insight into inferential issues related to the model. We concentrate on the case of the linear logistic regression and, in order to make inference on the parameters of interest, we present a Markov Chain Monte Carlo algorithm with data augmentation that does not require the a priori knowledge of the population prevalence. A simulation study concerning 24,000 simulated datasets related to different scenarios is presented comparing our proposal to optimal benchmarks.

</details>

<details>

<summary>2013-05-07 12:54:01 - Probabilistic wind speed forecasting using Bayesian model averaging with truncated normal components</summary>

- *SÃ¡ndor Baran*

- `1305.1184v2` - [abs](http://arxiv.org/abs/1305.1184v2) - [pdf](http://arxiv.org/pdf/1305.1184v2)

> Bayesian model averaging (BMA) is a statistical method for post-processing forecast ensembles of atmospheric variables, obtained from multiple runs of numerical weather prediction models, in order to create calibrated predictive probability density functions (PDFs). The BMA predictive PDF of the future weather quantity is the mixture of the individual PDFs corresponding to the ensemble members and the weights and model parameters are estimated using ensemble members and validating observation from a given training period.   In the present paper we introduce a BMA model for calibrating wind speed forecasts, where the components PDFs follow truncated normal distribution with cut-off at zero, and apply it to the ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service. Three parameter estimation methods are proposed and each of the corresponding models outperforms the traditional gamma BMA model both in calibration and in accuracy of predictions. Moreover, since here the maximum likelihood estimation of the parameters does not require numerical optimization, modelling can be performed much faster than in case of gamma mixtures.

</details>

<details>

<summary>2013-05-07 15:04:57 - Optimal design of dilution experiments under volume constraints</summary>

- *Maryam Zolghadr, Sergei Zuyev*

- `1212.3151v2` - [abs](http://arxiv.org/abs/1212.3151v2) - [pdf](http://arxiv.org/pdf/1212.3151v2)

> The paper develops methods to construct a one-stage optimal design of dilution experiments under the total available volume constraint typical for bio-medical applications. We consider various design criteria based on the Fisher information both is Bayesian and non-Bayasian settings and show that the optimal design is typically one-atomic meaning that all the dilutions should be of the same size. The main tool is variational analysis of functions of a measure and the corresponding steepest descent type numerical methods. Our approach is generic in the sense that it allows for inclusion of additional constraints and cost components, like the cost of materials and of the experiment itself.

</details>

<details>

<summary>2013-05-08 03:21:31 - The Extended Parameter Filter</summary>

- *Yusuf Erol, Lei Li, Bharath Ramsundar, Stuart J. Russell*

- `1305.1704v1` - [abs](http://arxiv.org/abs/1305.1704v1) - [pdf](http://arxiv.org/pdf/1305.1704v1)

> The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvik's filter and a Kalman filter in parameter space and establish more general conditions under which Storvik's filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik's method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.

</details>

<details>

<summary>2013-05-08 15:37:49 - Local Identification of Nonparametric and Semiparametric Models</summary>

- *Xiaohong Chen, Victor Chernozhukov, Sokbae Lee, Whitney K. Newey*

- `1105.3007v4` - [abs](http://arxiv.org/abs/1105.3007v4) - [pdf](http://arxiv.org/pdf/1105.3007v4)

> In parametric, nonlinear structural models a classical sufficient condition for local identification, like Fisher (1966) and Rothenberg (1971), is that the vector of moment conditions is differentiable at the true parameter with full rank derivative matrix. We derive an analogous result for the nonparametric, nonlinear structural models, establishing conditions under which an infinite-dimensional analog of the full rank condition is sufficient for local identification. Importantly, we show that additional conditions are often needed in nonlinear, nonparametric models to avoid nonlinearities overwhelming linear effects. We give restrictions on a neighborhood of the true value that are sufficient for local identification. We apply these results to obtain new, primitive identification conditions in several important models, including nonseparable quantile instrumental variable (IV) models, single-index IV models, and semiparametric consumption-based asset pricing models.

</details>

<details>

<summary>2013-05-08 21:02:52 - Kernelized Bayesian Matrix Factorization</summary>

- *Mehmet GÃ¶nen, Suleiman A. Khan, Samuel Kaski*

- `1211.1275v3` - [abs](http://arxiv.org/abs/1211.1275v3) - [pdf](http://arxiv.org/pdf/1211.1275v3)

> We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms.

</details>

<details>

<summary>2013-05-10 01:16:17 - Regularised PCA to denoise and visualise data</summary>

- *Marie Verbanck, Julie Josse, FranÃ§ois Husson*

- `1301.4649v2` - [abs](http://arxiv.org/abs/1301.4649v2) - [pdf](http://arxiv.org/pdf/1301.4649v2)

> Principal component analysis (PCA) is a well-established method commonly used to explore and visualise data. A classical PCA model is the fixed effect model where data are generated as a fixed structure of low rank corrupted by noise. Under this model, PCA does not provide the best recovery of the underlying signal in terms of mean squared error. Following the same principle as in ridge regression, we propose a regularised version of PCA that boils down to threshold the singular values. Each singular value is multiplied by a term which can be seen as the ratio of the signal variance over the total variance of the associated dimension. The regularised term is analytically derived using asymptotic results and can also be justified from a Bayesian treatment of the model. Regularised PCA provides promising results in terms of the recovery of the true signal and the graphical outputs in comparison with classical PCA and with a soft thresholding estimation strategy. The gap between PCA and regularised PCA is all the more important that data are noisy.

</details>

<details>

<summary>2013-05-10 02:50:21 - Logarithmic Quantile Estimation for Rank Statistics</summary>

- *Manfred Denker, Lucia Tabacu*

- `1305.2250v1` - [abs](http://arxiv.org/abs/1305.2250v1) - [pdf](http://arxiv.org/pdf/1305.2250v1)

> We prove an almost sure weak limit theorem for simple linear rank statistics for samples with continuous distributions functions. As a corollary the result extends to samples with ties, and the vector version of an a.s. central limit theorem for vectors of linear rank statistics. Moreover, we derive such a weak convergence result for some quadratic forms. These results are then applied to quantile estimation, and to hypothesis testing for nonparametric statistical designs, here demonstrated by the c-sample problem, where the samples may be dependent. In general, the method is known to be comparable to the bootstrap and other nonparametric methods (\cite{THA, FRI}) and we confirm this finding for the c-sample problem.

</details>

<details>

<summary>2013-05-10 15:09:11 - Revisiting Bayesian Blind Deconvolution</summary>

- *David Wipf, Haichao Zhang*

- `1305.2362v1` - [abs](http://arxiv.org/abs/1305.2362v1) - [pdf](http://arxiv.org/pdf/1305.2362v1)

> Blind deconvolution involves the estimation of a sharp signal or image given only a blurry observation. Because this problem is fundamentally ill-posed, strong priors on both the sharp image and blur kernel are required to regularize the solution space. While this naturally leads to a standard MAP estimation framework, performance is compromised by unknown trade-off parameter settings, optimization heuristics, and convergence issues stemming from non-convexity and/or poor prior selections. To mitigate some of these problems, a number of authors have recently proposed substituting a variational Bayesian (VB) strategy that marginalizes over the high-dimensional image space leading to better estimates of the blur kernel. However, the underlying cost function now involves both integrals with no closed-form solution and complex, function-valued arguments, thus losing the transparency of MAP. Beyond standard Bayesian-inspired intuitions, it thus remains unclear by exactly what mechanism these methods are able to operate, rendering understanding, improvements and extensions more difficult. To elucidate these issues, we demonstrate that the VB methodology can be recast as an unconventional MAP problem with a very particular penalty/prior that couples the image, blur kernel, and noise level in a principled way. This unique penalty has a number of useful characteristics pertaining to relative concavity, local minima avoidance, and scale-invariance that allow us to rigorously explain the success of VB including its existing implementational heuristics and approximations. It also provides strict criteria for choosing the optimal image prior that, perhaps counter-intuitively, need not reflect the statistics of natural scenes. In so doing we challenge the prevailing notion of why VB is successful for blind deconvolution while providing a transparent platform for introducing enhancements.

</details>

<details>

<summary>2013-05-13 03:31:15 - Mean field variational Bayesian inference for support vector machine classification</summary>

- *Jan Luts, John T. Ormerod*

- `1305.2667v1` - [abs](http://arxiv.org/abs/1305.2667v1) - [pdf](http://arxiv.org/pdf/1305.2667v1)

> A mean field variational Bayes approach to support vector machines (SVMs) using the latent variable representation on Polson & Scott (2012) is presented. This representation allows circumvention of many of the shortcomings associated with classical SVMs including automatic penalty parameter selection, the ability to handle dependent samples, missing data and variable selection. We demonstrate on simulated and real datasets that our approach is easily extendable to non-standard situations and outperforms the classical SVM approach whilst remaining computationally efficient.

</details>

<details>

<summary>2013-05-14 09:32:15 - A Gaussian Process Emulator Approach for Rapid Contaminant Characterization with an Integrated Multizone-CFD Model</summary>

- *Piyush M. Tagade, Byeong-Min Jeong, Han-Lim Choi*

- `1305.2565v2` - [abs](http://arxiv.org/abs/1305.2565v2) - [pdf](http://arxiv.org/pdf/1305.2565v2)

> This paper explores a Gaussian process emulator based approach for rapid Bayesian inference of contaminant source location and characteristics in an indoor environment. In the pre-event detection stage, the proposed approach represents transient contaminant fate and transport as a random function with multivariate Gaussian process prior. Hyper-parameters of the Gaussian process prior are inferred using a set of contaminant fate and transport simulation runs obtained at predefined source locations and characteristics. This paper uses an integrated multizone-CFD model to simulate contaminant fate and transport. Mean of the Gaussian process, conditional on the inferred hyper-parameters, is used as an computationally efficient statistical emulator of the multizone-CFD simulator. In the post event-detection stage, the Bayesian framework is used to infer the source location and characteristics using the contaminant concentration data obtained through a sensor network. The Gaussian process emulator of the contaminant fate and transport is used for Markov Chain Monte Carlo sampling to efficiently explore the posterior distribution of source location and characteristics. Efficacy of the proposed method is demonstrated for a hypothetical contaminant release through multiple sources in a single storey seven room building. The method is found to infer location and characteristics of the multiple sources accurately. The posterior distribution obtained using the proposed method is found to agree closely with the posterior distribution obtained by directly coupling the multizone-CFD simulator with the Markov Chain Monte Carlo sampling.

</details>

<details>

<summary>2013-05-14 09:46:00 - Informative Bayesian inference for the skew-normal distribution</summary>

- *Antonio Canale, Bruno Scarpa*

- `1305.3080v1` - [abs](http://arxiv.org/abs/1305.3080v1) - [pdf](http://arxiv.org/pdf/1305.3080v1)

> Motivated by the analysis of the distribution of university grades, which is usually asymmetric, we discuss two informative priors for the shape parameter of the skew-normal distribution, showing that they lead to closed-form full-conditional posterior distributions, particularly useful in MCMC computation. Gibbs sampling algorithms are discussed for the joint vector of parameters, given independent prior distributions for the location and scale parameters. Simulation studies are performed to assess the performance of Gibbs samplers and to compare the choice of informative priors against a non-informative one. The method is used to analyze the grades of the basic statistics examination of the first-year undergraduate students at the School of Economics, University of Padua, Italy.

</details>

<details>

<summary>2013-05-15 17:01:13 - Does "model-free" forecasting really outperform the "true" model? A reply to Perretti et al</summary>

- *Florian Hartig, Carsten F. Dormann*

- `1305.3544v1` - [abs](http://arxiv.org/abs/1305.3544v1) - [pdf](http://arxiv.org/pdf/1305.3544v1)

> Estimating population models from uncertain observations is an important problem in ecology. Perretti et al. observed that standard Bayesian state-space solutions to this problem may provide biased parameter estimates when the underlying dynamics are chaotic. Consequently, forecasts based on these estimates showed poor predictive accuracy compared to simple "model-free" methods, which lead Perretti et al. to conclude that "Model-free forecasting outperforms the correct mechanistic model for simulated and experimental data". However, a simple modification of the statistical methods also suffices to remove the bias and reverse their results.

</details>

<details>

<summary>2013-05-16 02:35:42 - Sparse Adaptive Dirichlet-Multinomial-like Processes</summary>

- *Marcus Hutter*

- `1305.3671v1` - [abs](http://arxiv.org/abs/1305.3671v1) - [pdf](http://arxiv.org/pdf/1305.3671v1)

> Online estimation and modelling of i.i.d. data for short sequences over large or complex "alphabets" is a ubiquitous (sub)problem in machine learning, information theory, data compression, statistical language processing, and document analysis. The Dirichlet-Multinomial distribution (also called Polya urn scheme) and extensions thereof are widely applied for online i.i.d. estimation. Good a-priori choices for the parameters in this regime are difficult to obtain though. I derive an optimal adaptive choice for the main parameter via tight, data-dependent redundancy bounds for a related model. The 1-line recommendation is to set the 'total mass' = 'precision' = 'concentration' parameter to m/2ln[(n+1)/m], where n is the (past) sample size and m the number of different symbols observed (so far). The resulting estimator (i) is simple, (ii) online, (iii) fast, (iv) performs well for all m, small, middle and large, (v) is independent of the base alphabet size, (vi) non-occurring symbols induce no redundancy, (vii) the constant sequence has constant redundancy, (viii) symbols that appear only finitely often have bounded/constant contribution to the redundancy, (ix) is competitive with (slow) Bayesian mixing over all sub-alphabets.

</details>

<details>

<summary>2013-05-18 15:04:27 - Likelihood-free Simulation-based Optimal Design</summary>

- *Markus Hainy, Werner G. MÃ¼ller, Helga Wagner*

- `1305.4273v1` - [abs](http://arxiv.org/abs/1305.4273v1) - [pdf](http://arxiv.org/pdf/1305.4273v1)

> Simulation-based optimal design techniques are a convenient tool for solving a particular class of optimal design problems. The goal is to find the optimal configuration of factor settings with respect to an expected utility criterion. This criterion depends on the specified probability model for the data and on the assumed prior distribution for the model parameters. We develop new simulation-based optimal design methods which incorporate likelihood-free approaches and utilize them in novel applications.   Most simulation-based design strategies solve the intractable expected utility integral at a specific design point by using Monte Carlo simulations from the probability model. Optimizing the criterion over the design points is carried out in a separate step. M\"uller (1999) introduces an MCMC algorithm which simultaneously addresses the simulation as well as the optimization problem. In principle, the optimal design can be found by detecting the utility mode of the sampled design points. Several improvements have been suggested to facilitate this task for multidimensional design problems (see e.g. Amzal et al. 2006).   We aim to extend this simulation-based design methodology to design problems where the likelihood of the probability model is of an unknown analytical form but it is possible to simulate from the probability model. We further assume that prior observations are available. In such a setting it is seems natural to employ approximate Bayesian computation (ABC) techniques in order to be able to simulate from the conditional probability model. We provide a thorough review of adjacent literature and we investigate the benefits and the limitations of our design methodology for a particular paradigmatic example.

</details>

<details>

<summary>2013-05-19 04:56:05 - Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families</summary>

- *Peter Bartlett, Peter Grunwald, Peter Harremoes, Fares Hedayati, Wojciech Kotlowski*

- `1305.4324v1` - [abs](http://arxiv.org/abs/1305.4324v1) - [pdf](http://arxiv.org/pdf/1305.4324v1)

> We study online learning under logarithmic loss with regular parametric models. Hedayati and Bartlett (2012b) showed that a Bayesian prediction strategy with Jeffreys prior and sequential normalized maximum likelihood (SNML) coincide and are optimal if and only if the latter is exchangeable, and if and only if the optimal strategy can be calculated without knowing the time horizon in advance. They put forward the question what families have exchangeable SNML strategies. This paper fully answers this open problem for one-dimensional exponential families. The exchangeability can happen only for three classes of natural exponential family distributions, namely the Gaussian, Gamma, and the Tweedie exponential family of order 3/2. Keywords: SNML Exchangeability, Exponential Family, Online Learning, Logarithmic Loss, Bayesian Strategy, Jeffreys Prior, Fisher Information1

</details>

<details>

<summary>2013-05-20 04:07:12 - Infinite Shift-invariant Grouped Multi-task Learning for Gaussian Processes</summary>

- *Yuyang Wang, Roni Khardon, Pavlos Protopapas*

- `1203.0970v2` - [abs](http://arxiv.org/abs/1203.0970v2) - [pdf](http://arxiv.org/pdf/1203.0970v2)

> Multi-task learning leverages shared information among data sets to improve the learning performance of individual tasks. The paper applies this framework for data where each task is a phase-shifted periodic time series. In particular, we develop a novel Bayesian nonparametric model capturing a mixture of Gaussian processes where each task is a sum of a group-specific function and a component capturing individual variation, in addition to each task being phase shifted. We develop an efficient \textsc{em} algorithm to learn the parameters of the model. As a special case we obtain the Gaussian mixture model and \textsc{em} algorithm for phased-shifted periodic time series. Furthermore, we extend the proposed model by using a Dirichlet Process prior and thereby leading to an infinite mixture model that is capable of doing automatic model selection. A Variational Bayesian approach is developed for inference in this model. Experiments in regression, classification and class discovery demonstrate the performance of the proposed models using both synthetic data and real-world time series data from astrophysics. Our methods are particularly useful when the time series are sparsely and non-synchronously sampled.

</details>

<details>

<summary>2013-05-21 14:36:14 - Semiparametric posterior limits</summary>

- *B. J. K. Kleijn*

- `1305.4836v1` - [abs](http://arxiv.org/abs/1305.4836v1) - [pdf](http://arxiv.org/pdf/1305.4836v1)

> We review the Bayesian theory of semiparametric inference following Bickel and Kleijn (2012) and Kleijn and Knapik (2013). After an overview of efficiency in parametric and semiparametric estimation problems, we consider the Bernstein-von Mises theorem (see, e.g., Le Cam and Yang (1990)) and generalize it to (LAN) regular and (LAE) irregular semiparametric estimation problems. We formulate a version of the semiparametric Bernstein-von Mises theorem that does not depend on least-favourable submodels, thus bypassing the most restrictive condition in the presentation of Bickel and Kleijn (2012). The results are applied to the (regular) estimation of the linear coefficient in partial linear regression (with a Gaussian nuisance prior) and of the kernel bandwidth in a model of normal location mixtures (with a Dirichlet nuisance prior), as well as the (irregular) estimation of the boundary of the support of a monotone family of densities (with a Gaussian nuisance prior).

</details>

<details>

<summary>2013-05-22 19:41:25 - Classification of molecular sequence data using Bayesian phylogenetic mixture models</summary>

- *Elisa Loza-Reyes, Merrilee Hurn, Tony Robinson*

- `1107.5338v5` - [abs](http://arxiv.org/abs/1107.5338v5) - [pdf](http://arxiv.org/pdf/1107.5338v5)

> Rate variation among the sites of a molecular sequence is commonly found in applications of phylogenetic inference. Several approaches exist to account for this feature but they do not usually enable the investigator to pinpoint the sites that evolve under one or another rate of evolution in a straightforward manner. The focus is on Bayesian phylogenetic mixture models, augmented with allocation variables, as tools for site classification and quantification of classification uncertainty. The method does not rely on prior knowledge of site membership to classes or even the number of classes. Furthermore, it does not require correlated sites to be next to one another in the sequence alignment, unlike some phylogenetic hidden Markov or change-point models. In the approach presented, model selection on the number and type of mixture components is conducted ahead of both model estimation and site classification; the steppingstone sampler (SS) is used to select amongst competing mixture models. Example applications of simulated data and mitochondrial DNA of primates illustrate site classification via 'augmented' Bayesian phylogenetic mixtures. In both examples, all mixtures outperform commonly-used models of among-site rate variation and models that do not account for rate heterogeneity. The examples further demonstrate how site classification is readily available from the analysis output. The method is directly relevant to the choice of partitions in Bayesian phylogenetics, and its application may lead to the discovery of structure not otherwise recognised in a molecular sequence alignment. Computational aspects of Bayesian phylogenetic model estimation are discussed, including the use of simple Markov chain Monte Carlo (MCMC) moves that mix efficiently without tempering the chains.

</details>

<details>

<summary>2013-05-23 15:04:27 - A Bayesian localised conditional auto-regressive model for estimating the health effects of air pollution</summary>

- *Duncan Lee, Alastair Rushworth, Sujit K. Sahu*

- `1305.5445v1` - [abs](http://arxiv.org/abs/1305.5445v1) - [pdf](http://arxiv.org/pdf/1305.5445v1)

> Estimation of the long-term health effects of air pollution is a challenging task, especially when modelling small-area disease incidence data in an ecological study design. The challenge comes from the unobserved underlying spatial correlation structure in these data, which is accounted for using random effects modelled by a globally smooth conditional autoregressive model. These smooth random effects confound the effects of air pollution, which are also globally smooth. To avoid this collinearity a Bayesian localised conditional autoregressive model is developed for the random effects. This localised model is flexible spatially, in the sense that it is not only able to model step changes in the random effects surface, but also is able to capture areas of spatial smoothness in the study region. This methodological development allows us to improve the estimation performance of the covariate effects, compared to using traditional conditional auto-regressive models. These results are established using a simulation study, and are then illustrated with our motivating study on air pollution and respiratory ill health in Greater Glasgow, Scotland in 2010. The model shows substantial health effects of particulate matter air pollution and income deprivation, whose effects have been consistently attenuated by the currently available globally smooth models.

</details>

<details>

<summary>2013-05-24 11:45:43 - Bayesian semiparametric analysis for two-phase studies of gene-environment interaction</summary>

- *Jaeil Ahn, Bhramar Mukherjee, Stephen B. Gruber, Malay Ghosh*

- `1305.5695v1` - [abs](http://arxiv.org/abs/1305.5695v1) - [pdf](http://arxiv.org/pdf/1305.5695v1)

> The two-phase sampling design is a cost-efficient way of collecting expensive covariate information on a judiciously selected subsample. It is natural to apply such a strategy for collecting genetic data in a subsample enriched for exposure to environmental factors for gene-environment interaction (G x E) analysis. In this paper, we consider two-phase studies of G x E interaction where phase I data are available on exposure, covariates and disease status. Stratified sampling is done to prioritize individuals for genotyping at phase II conditional on disease and exposure. We consider a Bayesian analysis based on the joint retrospective likelihood of phases I and II data. We address several important statistical issues: (i) we consider a model with multiple genes, environmental factors and their pairwise interactions. We employ a Bayesian variable selection algorithm to reduce the dimensionality of this potentially high-dimensional model; (ii) we use the assumption of gene-gene and gene-environment independence to trade off between bias and efficiency for estimating the interaction parameters through use of hierarchical priors reflecting this assumption; (iii) we posit a flexible model for the joint distribution of the phase I categorical variables using the nonparametric Bayes construction of Dunson and Xing [J. Amer. Statist. Assoc. 104 (2009) 1042-1051].

</details>

<details>

<summary>2013-05-24 16:32:10 - Adapting the Stochastic Block Model to Edge-Weighted Networks</summary>

- *Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset*

- `1305.5782v1` - [abs](http://arxiv.org/abs/1305.5782v1) - [pdf](http://arxiv.org/pdf/1305.5782v1)

> We generalize the stochastic block model to the important case in which edges are annotated with weights drawn from an exponential family distribution. This generalization introduces several technical difficulties for model estimation, which we solve using a Bayesian approach. We introduce a variational algorithm that efficiently approximates the model's posterior distribution for dense graphs. In specific numerical experiments on edge-weighted networks, this weighted stochastic block model outperforms the common approach of first applying a single threshold to all weights and then applying the classic stochastic block model, which can obscure latent block structure in networks. This model will enable the recovery of latent structure in a broader range of network data than was previously possible.

</details>

<details>

<summary>2013-05-24 18:25:32 - Learning Topic Models and Latent Bayesian Networks Under Expansion Constraints</summary>

- *Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade*

- `1209.5350v3` - [abs](http://arxiv.org/abs/1209.5350v3) - [pdf](http://arxiv.org/pdf/1209.5350v3)

> Unsupervised estimation of latent variable models is a fundamental problem central to numerous applications of machine learning and statistics. This work presents a principled approach for estimating broad classes of such models, including probabilistic topic models and latent linear Bayesian networks, using only second-order observed moments. The sufficient conditions for identifiability of these models are primarily based on weak expansion constraints on the topic-word matrix, for topic models, and on the directed acyclic graph, for Bayesian networks. Because no assumptions are made on the distribution among the latent variables, the approach can handle arbitrary correlations among the topics or latent factors. In addition, a tractable learning method via $\ell_1$ optimization is proposed and studied in numerical experiments.

</details>

<details>

<summary>2013-05-24 19:00:28 - Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations</summary>

- *Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet*

- `1305.5826v1` - [abs](http://arxiv.org/abs/1305.5826v1) - [pdf](http://arxiv.org/pdf/1305.5826v1)

> Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP.

</details>

<details>

<summary>2013-05-28 16:04:42 - Fast Approximate Bayesian Computation for discretely observed Markov models using a factorised posterior distribution</summary>

- *Simon R. White, Theodore Kypraios, Simon P. Preston*

- `1301.2975v2` - [abs](http://arxiv.org/abs/1301.2975v2) - [pdf](http://arxiv.org/pdf/1301.2975v2)

> Many modern statistical applications involve inference for complicated stochastic models for which the likelihood function is difficult or even impossible to calculate, and hence conventional likelihood-based inferential echniques cannot be used. In such settings, Bayesian inference can be performed using Approximate Bayesian Computation (ABC). However, in spite of many recent developments to ABC methodology, in many applications the computational cost of ABC necessitates the choice of summary statistics and tolerances that can potentially severely bias the estimate of the posterior.   We propose a new "piecewise" ABC approach suitable for discretely observed Markov models that involves writing the posterior density of the parameters as a product of factors, each a function of only a subset of the data, and then using ABC within each factor. The approach has the advantage of side-stepping the need to choose a summary statistic and it enables a stringent tolerance to be set, making the posterior "less approximate". We investigate two methods for estimating the posterior density based on ABC samples for each of the factors: the first is to use a Gaussian approximation for each factor, and the second is to use a kernel density estimate. Both methods have their merits. The Gaussian approximation is simple, fast, and probably adequate for many applications. On the other hand, using instead a kernel density estimate has the benefit of consistently estimating the true ABC posterior as the number of ABC samples tends to infinity. We illustrate the piecewise ABC approach for three examples; in each case, the approach enables "exact matching" between simulations and data and offers fast and accurate inference.

</details>

<details>

<summary>2013-05-31 02:23:22 - Rearranging Edgeworth-Cornish-Fisher Expansions</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Alfred Galichon*

- `0708.1627v2` - [abs](http://arxiv.org/abs/0708.1627v2) - [pdf](http://arxiv.org/pdf/0708.1627v2)

> This paper applies a regularization procedure called increasing rearrangement to monotonize Edgeworth and Cornish-Fisher expansions and any other related approximations of distribution and quantile functions of sample statistics. Besides satisfying the logical monotonicity, required of distribution and quantile functions, the procedure often delivers strikingly better approximations to the distribution and quantile functions of the sample mean than the original Edgeworth-Cornish-Fisher expansions.

</details>

<details>

<summary>2013-05-31 14:31:52 - Estimating Omissions from Searches</summary>

- *Anthony J Webster, Richard Kemp*

- `1205.1150v3` - [abs](http://arxiv.org/abs/1205.1150v3) - [pdf](http://arxiv.org/pdf/1205.1150v3)

> The mark-recapture method was devised by Petersen in 1896 to estimate the number of fish migrating into the Limfjord, and independently by Lincoln in 1930 to estimate waterfowl abundance. The technique applies to any search for a finite number of items by two or more people or agents, allowing the number of searched-for items to be estimated. This ubiquitous problem appears in fields from ecology and epidemiology, through to mathematics, social sciences, and computing. Here we exactly calculate the moments of the hypergeometric distribution associated with this long-standing problem, confirming that widely used estimates conjectured in 1951 are often too small. Our Bayesian approach highlights how different search strategies will modify the estimates. As an example, we assess the accuracy of a systematic literature review, an application we recommend.

</details>

<details>

<summary>2013-05-31 16:57:33 - A new Bayesian ensemble of trees classifier for identifying multi-class labels in satellite images</summary>

- *Reshu Agarwal, Pritam Ranjan, Hugh Chipman*

- `1304.4077v2` - [abs](http://arxiv.org/abs/1304.4077v2) - [pdf](http://arxiv.org/pdf/1304.4077v2)

> Classification of satellite images is a key component of many remote sensing applications. One of the most important products of a raw satellite image is the classified map which labels the image pixels into meaningful classes. Though several parametric and non-parametric classifiers have been developed thus far, accurate labeling of the pixels still remains a challenge. In this paper, we propose a new reliable multiclass-classifier for identifying class labels of a satellite image in remote sensing applications. The proposed multiclass-classifier is a generalization of a binary classifier based on the flexible ensemble of regression trees model called Bayesian Additive Regression Trees (BART). We used three small areas from the LANDSAT 5 TM image, acquired on August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) over Kings County, Nova Scotia, Canada to classify the land-use. Several prediction accuracy and uncertainty measures have been used to compare the reliability of the proposed classifier with the state-of-the-art classifiers in remote sensing.

</details>


## 2013-06

<details>

<summary>2013-06-01 13:42:46 - One-Class Support Measure Machines for Group Anomaly Detection</summary>

- *Krikamol Muandet, Bernhard SchÃ¶lkopf*

- `1303.0309v2` - [abs](http://arxiv.org/abs/1303.0309v2) - [pdf](http://arxiv.org/pdf/1303.0309v2)

> We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications.

</details>

<details>

<summary>2013-06-02 12:32:11 - Declarative Modeling and Bayesian Inference of Dark Matter Halos</summary>

- *Gabriel Kronberger*

- `1306.0202v1` - [abs](http://arxiv.org/abs/1306.0202v1) - [pdf](http://arxiv.org/pdf/1306.0202v1)

> Probabilistic programming allows specification of probabilistic models in a declarative manner. Recently, several new software systems and languages for probabilistic programming have been developed on the basis of newly developed and improved methods for approximate inference in probabilistic models. In this contribution a probabilistic model for an idealized dark matter localization problem is described. We first derive the probabilistic model for the inference of dark matter locations and masses, and then show how this model can be implemented using BUGS and Infer.NET, two software systems for probabilistic programming. Finally, the different capabilities of both systems are discussed. The presented dark matter model includes mainly non-conjugate factors, thus, it is difficult to implement this model with Infer.NET.

</details>

<details>

<summary>2013-06-02 18:29:10 - Dynamic Covariance Models for Multivariate Financial Time Series</summary>

- *Yue Wu, JosÃ© Miguel HernÃ¡ndez-Lobato, Zoubin Ghahramani*

- `1305.4268v2` - [abs](http://arxiv.org/abs/1305.4268v2) - [pdf](http://arxiv.org/pdf/1305.4268v2)

> The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.

</details>

<details>

<summary>2013-06-03 18:55:03 - New Insights Into Approximate Bayesian Computation</summary>

- *GÃ©rard Biau, FrÃ©dÃ©ric CÃ©rou, Arnaud Guyader*

- `1207.6461v2` - [abs](http://arxiv.org/abs/1207.6461v2) - [pdf](http://arxiv.org/pdf/1207.6461v2)

> Approximate Bayesian Computation (ABC for short) is a family of computational techniques which offer an almost automated solution in situations where evaluation of the posterior likelihood is computationally prohibitive, or whenever suitable likelihoods are not available. In the present paper, we analyze the procedure from the point of view of k-nearest neighbor theory and explore the statistical properties of its outputs. We discuss in particular some asymptotic features of the genuine conditional density estimate associated with ABC, which is an interesting hybrid between a k-nearest neighbor and a kernel method.

</details>

<details>

<summary>2013-06-04 11:28:32 - Fast Gradient-Based Inference with Continuous Latent Variable Models in Auxiliary Form</summary>

- *Diederik P Kingma*

- `1306.0733v1` - [abs](http://arxiv.org/abs/1306.0733v1) - [pdf](http://arxiv.org/pdf/1306.0733v1)

> We propose a technique for increasing the efficiency of gradient-based inference and learning in Bayesian networks with multiple layers of continuous latent vari- ables. We show that, in many cases, it is possible to express such models in an auxiliary form, where continuous latent variables are conditionally deterministic given their parents and a set of independent auxiliary variables. Variables of mod- els in this auxiliary form have much larger Markov blankets, leading to significant speedups in gradient-based inference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-based optimization. The relative efficiency is confirmed in ex- periments.

</details>

<details>

<summary>2013-06-05 10:45:59 - Fast Dual Variational Inference for Non-Conjugate LGMs</summary>

- *Mohammad Emtiyaz Khan, Aleksandr Y. Aravkin, Michael P. Friedlander, Matthias Seeger*

- `1306.1052v1` - [abs](http://arxiv.org/abs/1306.1052v1) - [pdf](http://arxiv.org/pdf/1306.1052v1)

> Latent Gaussian models (LGMs) are widely used in statistics and machine learning. Bayesian inference in non-conjugate LGMs is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods. Algorithms based on variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use. However, the structure of the optimization problems associated with these approximations remains poorly understood, and standard solvers take too long to converge. We derive a novel dual variational inference approach that exploits the convexity property of the VG approximations. We obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods. Using real-world data, we demonstrate these advantages on a variety of LGMs, including Gaussian process classification, and latent Gaussian Markov random fields.

</details>

<details>

<summary>2013-06-05 17:07:58 - Bootstrap Methods for the Empirical Study of Decision-Making and Information Flows in Social Systems</summary>

- *Simon DeDeo, Robert X. D. Hawkins, Sara Klingenstein, Tim Hitchcock*

- `1302.0907v2` - [abs](http://arxiv.org/abs/1302.0907v2) - [pdf](http://arxiv.org/pdf/1302.0907v2)

> We characterize the statistical bootstrap for the estimation of information-theoretic quantities from data, with particular reference to its use in the study of large-scale social phenomena. Our methods allow one to preserve, approximately, the underlying axiomatic relationships of information theory---in particular, consistency under arbitrary coarse-graining---that motivate use of these quantities in the first place, while providing reliability comparable to the state of the art for Bayesian estimators. We show how information-theoretic quantities allow for rigorous empirical study of the decision-making capacities of rational agents and the time-asymmetric flows of information in distributed systems. We provide illustrative examples by reference to ongoing collaborative work on the semantic structure of the British Criminal Court system and the conflict dynamics of the contemporary Afghanistan insurgency.

</details>

<details>

<summary>2013-06-05 19:07:54 - Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians</summary>

- *Ari Pakman, Liam Paninski*

- `1208.4118v3` - [abs](http://arxiv.org/abs/1208.4118v3) - [pdf](http://arxiv.org/pdf/1208.4118v3)

> We present a Hamiltonian Monte Carlo algorithm to sample from multivariate Gaussian distributions in which the target space is constrained by linear and quadratic inequalities or products thereof. The Hamiltonian equations of motion can be integrated exactly and there are no parameters to tune. The algorithm mixes faster and is more efficient than Gibbs sampling. The runtime depends on the number and shape of the constraints but the algorithm is highly parallelizable. In many cases, we can exploit special structure in the covariance matrices of the untruncated Gaussian to further speed up the runtime. A simple extension of the algorithm permits sampling from distributions whose log-density is piecewise quadratic, as in the "Bayesian Lasso" model.

</details>

<details>

<summary>2013-06-06 07:08:51 - Bayesian nonparametric analysis of reversible Markov chains</summary>

- *Sergio Bacallado, Stefano Favaro, Lorenzo Trippa*

- `1306.1318v1` - [abs](http://arxiv.org/abs/1306.1318v1) - [pdf](http://arxiv.org/pdf/1306.1318v1)

> We introduce a three-parameter random walk with reinforcement, called the $(\theta,\alpha,\beta)$ scheme, which generalizes the linearly edge reinforced random walk to uncountable spaces. The parameter $\beta$ smoothly tunes the $(\theta,\alpha,\beta)$ scheme between this edge reinforced random walk and the classical exchangeable two-parameter Hoppe urn scheme, while the parameters $\alpha$ and $\theta$ modulate how many states are typically visited. Resorting to de Finetti's theorem for Markov chains, we use the $(\theta,\alpha,\beta)$ scheme to define a nonparametric prior for Bayesian analysis of reversible Markov chains. The prior is applied in Bayesian nonparametric inference for species sampling problems with data generated from a reversible Markov chain with an unknown transition kernel. As a real example, we analyze data from molecular dynamics simulations of protein folding.

</details>

<details>

<summary>2013-06-07 03:24:56 - Bayesian factorizations of big sparse tensors</summary>

- *Jing Zhou, Anirban Bhattacharya, Amy Herring, David Dunson*

- `1306.1598v1` - [abs](http://arxiv.org/abs/1306.1598v1) - [pdf](http://arxiv.org/pdf/1306.1598v1)

> It has become routine to collect data that are structured as multiway arrays (tensors). There is an enormous literature on low rank and sparse matrix factorizations, but limited consideration of extensions to the tensor case in statistics. The most common low rank tensor factorization relies on parallel factor analysis (PARAFAC), which expresses a rank $k$ tensor as a sum of rank one tensors. When observations are only available for a tiny subset of the cells of a big tensor, the low rank assumption is not sufficient and PARAFAC has poor performance. We induce an additional layer of dimension reduction by allowing the effective rank to vary across dimensions of the table. For concreteness, we focus on a contingency table application. Taking a Bayesian approach, we place priors on terms in the factorization and develop an efficient Gibbs sampler for posterior computation. Theory is provided showing posterior concentration rates in high-dimensional settings, and the methods are shown to have excellent performance in simulations and several real data applications.

</details>

<details>

<summary>2013-06-10 13:40:24 - Combinatorial clustering and the beta negative binomial process</summary>

- *Tamara Broderick, Lester Mackey, John Paisley, Michael I. Jordan*

- `1111.1802v5` - [abs](http://arxiv.org/abs/1111.1802v5) - [pdf](http://arxiv.org/pdf/1111.1802v5)

> We develop a Bayesian nonparametric approach to a general family of latent class problems in which individuals can belong simultaneously to multiple classes and where each class can be exhibited multiple times by an individual. We introduce a combinatorial stochastic process known as the negative binomial process (NBP) as an infinite-dimensional prior appropriate for such problems. We show that the NBP is conjugate to the beta process, and we characterize the posterior distribution under the beta-negative binomial process (BNBP) and hierarchical models based on the BNBP (the HBNBP). We study the asymptotic properties of the BNBP and develop a three-parameter extension of the BNBP that exhibits power-law behavior. We derive MCMC algorithms for posterior inference under the HBNBP, and we present experiments using these algorithms in the domains of image segmentation, object recognition, and document analysis.

</details>

<details>

<summary>2013-06-11 05:34:51 - Bayesian Computational Tools</summary>

- *Christian P. Robert*

- `1304.2048v2` - [abs](http://arxiv.org/abs/1304.2048v2) - [pdf](http://arxiv.org/pdf/1304.2048v2)

> This chapter surveys advances in the field of Bayesian computation over the past twenty years, with missing data. It also contains some novel computational entries on the double-exponential model that may be of interest per se.

</details>

<details>

<summary>2013-06-11 06:08:29 - Model Selection in Linear Mixed Models</summary>

- *Samuel MÃ¼ller, J. L. Scealy, A. H. Welsh*

- `1306.2427v1` - [abs](http://arxiv.org/abs/1306.2427v1) - [pdf](http://arxiv.org/pdf/1306.2427v1)

> Linear mixed effects models are highly flexible in handling a broad range of data types and are therefore widely used in applications. A key part in the analysis of data is model selection, which often aims to choose a parsimonious model with other desirable properties from a possibly very large set of candidate statistical models. Over the last 5-10 years the literature on model selection in linear mixed models has grown extremely rapidly. The problem is much more complicated than in linear regression because selection on the covariance structure is not straightforward due to computational issues and boundary problems arising from positive semidefinite constraints on covariance matrices. To obtain a better understanding of the available methods, their properties and the relationships between them, we review a large body of literature on linear mixed model selection. We arrange, implement, discuss and compare model selection methods based on four major approaches: information criteria such as AIC or BIC, shrinkage methods based on penalized loss functions such as LASSO, the Fence procedure and Bayesian techniques.

</details>

<details>

<summary>2013-06-11 08:09:12 - Variational Inference for Generalized Linear Mixed Models Using Partially Noncentered Parametrizations</summary>

- *Linda S. L. Tan, David J. Nott*

- `1205.3906v3` - [abs](http://arxiv.org/abs/1205.3906v3) - [pdf](http://arxiv.org/pdf/1205.3906v3)

> The effects of different parametrizations on the convergence of Bayesian computational algorithms for hierarchical models are well explored. Techniques such as centering, noncentering and partial noncentering can be used to accelerate convergence in MCMC and EM algorithms but are still not well studied for variational Bayes (VB) methods. As a fast deterministic approach to posterior approximation, VB is attracting increasing interest due to its suitability for large high-dimensional data. Use of different parametrizations for VB has not only computational but also statistical implications, as different parametrizations are associated with different factorized posterior approximations. We examine the use of partially noncentered parametrizations in VB for generalized linear mixed models (GLMMs). Our paper makes four contributions. First, we show how to implement an algorithm called nonconjugate variational message passing for GLMMs. Second, we show that the partially noncentered parametrization can adapt to the quantity of information in the data and determine a parametrization close to optimal. Third, we show that partial noncentering can accelerate convergence and produce more accurate posterior approximations than centering or noncentering. Finally, we demonstrate how the variational lower bound, produced as part of the computation, can be useful for model selection.

</details>

<details>

<summary>2013-06-11 11:18:18 - A Comparative Review of Dimension Reduction Methods in Approximate Bayesian Computation</summary>

- *M. G. B. Blum, M. A. Nunes, D. Prangle, S. A. Sisson*

- `1202.3819v3` - [abs](http://arxiv.org/abs/1202.3819v3) - [pdf](http://arxiv.org/pdf/1202.3819v3)

> Approximate Bayesian computation (ABC) methods make use of comparisons between simulated and observed summary statistics to overcome the problem of computationally intractable likelihood functions. As the practical implementation of ABC requires computations based on vectors of summary statistics, rather than full data sets, a central question is how to derive low-dimensional summary statistics from the observed data with minimal loss of information. In this article we provide a comprehensive review and comparison of the performance of the principal methods of dimension reduction proposed in the ABC literature. The methods are split into three nonmutually exclusive classes consisting of best subset selection methods, projection techniques and regularization. In addition, we introduce two new methods of dimension reduction. The first is a best subset selection method based on Akaike and Bayesian information criteria, and the second uses ridge regression as a regularization procedure. We illustrate the performance of these dimension reduction techniques through the analysis of three challenging models and data sets.

</details>

<details>

<summary>2013-06-12 11:40:37 - Handling Attrition in Longitudinal Studies: The Case for Refreshment Samples</summary>

- *Yiting Deng, D. Sunshine Hillygus, Jerome P. Reiter, Yajuan Si, Siyu Zheng*

- `1306.2791v1` - [abs](http://arxiv.org/abs/1306.2791v1) - [pdf](http://arxiv.org/pdf/1306.2791v1)

> Panel studies typically suffer from attrition, which reduces sample size and can result in biased inferences. It is impossible to know whether or not the attrition causes bias from the observed panel data alone. Refreshment samples - new, randomly sampled respondents given the questionnaire at the same time as a subsequent wave of the panel - offer information that can be used to diagnose and adjust for bias due to attrition. We review and bolster the case for the use of refreshment samples in panel studies. We include examples of both a fully Bayesian approach for analyzing the concatenated panel and refreshment data, and a multiple imputation approach for analyzing only the original panel. For the latter, we document a positive bias in the usual multiple imputation variance estimator. We present models appropriate for three waves and two refreshment samples, including nonterminal attrition. We illustrate the three-wave analysis using the 2007-2008 Associated Press-Yahoo! News Election Poll.

</details>

<details>

<summary>2013-06-12 13:00:47 - What Is Meant by "Missing at Random"?</summary>

- *Shaun Seaman, John Galati, Dan Jackson, John Carlin*

- `1306.2812v1` - [abs](http://arxiv.org/abs/1306.2812v1) - [pdf](http://arxiv.org/pdf/1306.2812v1)

> The concept of missing at random is central in the literature on statistical analysis with missing data. In general, inference using incomplete data should be based not only on observed data values but should also take account of the pattern of missing values. However, it is often said that if data are missing at random, valid inference using likelihood approaches (including Bayesian) can be obtained ignoring the missingness mechanism. Unfortunately, the term "missing at random" has been used inconsistently and not always clearly; there has also been a lack of clarity around the meaning of "valid inference using likelihood". These issues have created potential for confusion about the exact conditions under which the missingness mechanism can be ignored, and perhaps fed confusion around the meaning of "analysis ignoring the missingness mechanism". Here we provide standardised precise definitions of "missing at random" and "missing completely at random", in order to promote unification of the theory. Using these definitions we clarify the conditions that suffice for "valid inference" to be obtained under a variety of inferential paradigms.

</details>

<details>

<summary>2013-06-12 21:40:01 - Censored quantile regression processes under dependence and penalization</summary>

- *Stanislav Volgushev, Jens Wagener, Holger Dette*

- `1208.5467v2` - [abs](http://arxiv.org/abs/1208.5467v2) - [pdf](http://arxiv.org/pdf/1208.5467v2)

> We consider quantile regression processes from censored data under dependent data structures and derive a uniform Bahadur representation for those processes. We also consider cases where the dimension of the parameter in the quantile regression model is large. It is demonstrated that traditional penalized estimators such as the adaptive lasso yield sub-optimal rates if the coefficients of the quantile regression cross zero. New penalization techniques are introduced which are able to deal with specific problems of censored data and yield estimates with an optimal rate. In contrast to most of the literature, the asymptotic analysis does not require the assumption of independent observations, but is based on rather weak assumptions, which are satisfied for many kinds of dependent data.

</details>

<details>

<summary>2013-06-13 01:04:03 - A Greedy Approximation of Bayesian Reinforcement Learning with Probably Optimistic Transition Model</summary>

- *Kenji Kawaguchi, Mauricio Araya*

- `1303.3163v3` - [abs](http://arxiv.org/abs/1303.3163v3) - [pdf](http://arxiv.org/pdf/1303.3163v3)

> Bayesian Reinforcement Learning (RL) is capable of not only incorporating domain knowledge, but also solving the exploration-exploitation dilemma in a natural way. As Bayesian RL is intractable except for special cases, previous work has proposed several approximation methods. However, these methods are usually too sensitive to parameter values, and finding an acceptable parameter setting is practically impossible in many applications. In this paper, we propose a new algorithm that greedily approximates Bayesian RL to achieve robustness in parameter space. We show that for a desired learning behavior, our proposed algorithm has a polynomial sample complexity that is lower than those of existing algorithms. We also demonstrate that the proposed algorithm naturally outperforms other existing algorithms when the prior distributions are not significantly misleading. On the other hand, the proposed algorithm cannot handle greatly misspecified priors as well as the other algorithms can. This is a natural consequence of the fact that the proposed algorithm is greedier than the other algorithms. Accordingly, we discuss a way to select an appropriate algorithm for different tasks based on the algorithms' greediness. We also introduce a new way of simplifying Bayesian planning, based on which future work would be able to derive new algorithms.

</details>

<details>

<summary>2013-06-13 01:20:50 - Non-parametric Power-law Data Clustering</summary>

- *Xuhui Fan, Yiling Zeng, Longbing Cao*

- `1306.3003v1` - [abs](http://arxiv.org/abs/1306.3003v1) - [pdf](http://arxiv.org/pdf/1306.3003v1)

> It has always been a great challenge for clustering algorithms to automatically determine the cluster numbers according to the distribution of datasets. Several approaches have been proposed to address this issue, including the recent promising work which incorporate Bayesian Nonparametrics into the $k$-means clustering procedure. This approach shows simplicity in implementation and solidity in theory, while it also provides a feasible way to inference in large scale datasets. However, several problems remains unsolved in this pioneering work, including the power-law data applicability, mechanism to merge centers to avoid the over-fitting problem, clustering order problem, e.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely \emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor Process, \emph{pyp-means} treats clusters differently by dynamically and adaptively changing the threshold to guarantee the generation of power-law clustering results. Also, one center agglomeration procedure is integrated into the implementation to be able to merge small but close clusters and then adaptively determine the cluster number. With more discussion on the clustering order, the convergence proof, complexity analysis and extension to spectral clustering, our approach is compared with traditional clustering algorithm and variational inference methods. The advantages and properties of pyp-means are validated by experiments on both synthetic datasets and real world datasets.

</details>

<details>

<summary>2013-06-13 16:36:21 - Improving power posterior estimation of statistical evidence</summary>

- *Nial Friel, Merrilee Hurn, Jason Wyse*

- `1209.3198v3` - [abs](http://arxiv.org/abs/1209.3198v3) - [pdf](http://arxiv.org/pdf/1209.3198v3)

> The statistical evidence (or marginal likelihood) is a key quantity in Bayesian statistics, allowing one to assess the probability of the data given the model under investigation. This paper focuses on refining the power posterior approach to improve estimation of the evidence. The power posterior method involves transitioning from the prior to the posterior by powering the likelihood by an inverse temperature. In common with other tempering algorithms, the power posterior involves some degree of tuning. The main contributions of this article are twofold -- we present a result from the numerical analysis literature which can reduce the bias in the estimate of the evidence by addressing the error arising from numerically integrating across the inverse temperatures. We also tackle the selection of the inverse temperature ladder, applying this approach additionally to the Stepping Stone sampler estimation of evidence.

</details>

<details>

<summary>2013-06-14 01:22:16 - Bayesian State-Space Modelling on High-Performance Hardware Using LibBi</summary>

- *Lawrence M. Murray*

- `1306.3277v1` - [abs](http://arxiv.org/abs/1306.3277v1) - [pdf](http://arxiv.org/pdf/1306.3277v1)

> LibBi is a software package for state-space modelling and Bayesian inference on modern computer hardware, including multi-core central processing units (CPUs), many-core graphics processing units (GPUs) and distributed-memory clusters of such devices. The software parses a domain-specific language for model specification, then optimises, generates, compiles and runs code for the given model, inference method and hardware platform. In presenting the software, this work serves as an introduction to state-space models and the specialised methods developed for Bayesian inference with them. The focus is on sequential Monte Carlo (SMC) methods such as the particle filter for state estimation, and the particle Markov chain Monte Carlo (PMCMC) and SMC^2 methods for parameter estimation. All are well-suited to current computer hardware. Two examples are given and developed throughout, one a linear three-element windkessel model of the human arterial system, the other a nonlinear Lorenz '96 model. These are specified in the prescribed modelling language, and LibBi demonstrated by performing inference with them. Empirical results are presented, including a performance comparison of the software with different hardware configurations.

</details>

<details>

<summary>2013-06-16 05:21:19 - Bayesian test of significance for conditional independence: The multinomial model</summary>

- *Pablo de Morais Andrade, Julio Michael Stern, Carlos Alberto de BraganÃ§a Pereira*

- `1306.3627v1` - [abs](http://arxiv.org/abs/1306.3627v1) - [pdf](http://arxiv.org/pdf/1306.3627v1)

> Conditional independence tests (CI tests) have received special attention lately in Machine Learning and Computational Intelligence related literature as an important indicator of the relationship among the variables used by their models. In the field of Probabilistic Graphical Models (PGM)--which includes Bayesian Networks (BN) models--CI tests are especially important for the task of learning the PGM structure from data. In this paper, we propose the Full Bayesian Significance Test (FBST) for tests of conditional independence for discrete datasets. FBST is a powerful Bayesian test for precise hypothesis, as an alternative to frequentist's significance tests (characterized by the calculation of the \emph{p-value}).

</details>

<details>

<summary>2013-06-17 23:31:56 - Bayesian Monotone Regression using Gaussian Process Projection</summary>

- *Lizhen Lin, David B. Dunson*

- `1306.4041v1` - [abs](http://arxiv.org/abs/1306.4041v1) - [pdf](http://arxiv.org/pdf/1306.4041v1)

> Shape constrained regression analysis has applications in dose-response modeling, environmental risk assessment, disease screening and many other areas. Incorporating the shape constraints can improve estimation efficiency and avoid implausible results. We propose two novel methods focusing on Bayesian monotone curve and surface estimation using Gaussian process projections. The first projects samples from an unconstrained prior, while the second projects samples from the Gaussian process posterior. Theory is developed on continuity of the projection, posterior consistency and rates of contraction. The second approach is shown to have an empirical Bayes justification and to lead to simple computation with good performance in finite samples. Our projection approach can be applied in other constrained function estimation problems including in multivariate settings.

</details>

<details>

<summary>2013-06-18 17:38:10 - Mixtures of Skew-t Factor Analyzers</summary>

- *Paula M. Murray, Ryan P. Browne, Paul D. McNicholas*

- `1305.4301v2` - [abs](http://arxiv.org/abs/1305.4301v2) - [pdf](http://arxiv.org/pdf/1305.4301v2)

> In this paper, we introduce a mixture of skew-t factor analyzers as well as a family of mixture models based thereon. The mixture of skew-t distributions model that we use arises as a limiting case of the mixture of generalized hyperbolic distributions. Like their Gaussian and t-distribution analogues, our mixture of skew-t factor analyzers are very well-suited to the model-based clustering of high-dimensional data. Imposing constraints on components of the decomposed covariance parameter results in the development of eight flexible models. The alternating expectation-conditional maximization algorithm is used for model parameter estimation and the Bayesian information criterion is used for model selection. The models are applied to both real and simulated data, giving superior clustering results compared to a well-established family of Gaussian mixture models.

</details>

<details>

<summary>2013-06-19 12:04:02 - Computational Methods for a Class of Network Models</summary>

- *Junshan Wang, Ajay Jasra, Maria De Iorio*

- `1306.4508v1` - [abs](http://arxiv.org/abs/1306.4508v1) - [pdf](http://arxiv.org/pdf/1306.4508v1)

> In the following article we provide an exposition of exact computational methods to perform parameter inference from partially observed network models. In particular, we consider the duplication attachment (DA) model which has a likelihood function that typically cannot be evaluated in any reasonable computational time. We consider a number of importance sampling (IS) and sequential Monte Carlo (SMC) methods for approximating the likelihood of the network model for a fixed parameter value. It is well-known that for IS, the relative variance of the likelihood estimate typically grows at an exponential rate in the time parameter (here this is associated to the size of the network): we prove that, under assumptions, the SMC method will have relative variance which can grow only polynomially. In order to perform parameter estimation, we develop particle Markov chain Monte Carlo (PMCMC) algorithms to perform Bayesian inference. Such algorithms use the afore-mentioned SMC algorithms within the transition dynamics. The approaches are illustrated numerically.

</details>

<details>

<summary>2013-06-20 00:39:14 - A Bayesian changepoint methodology for high dimensional multivariate time series and space-time data: A study of structural change using remotely sensed data</summary>

- *Chris Strickland, Robert Burdett, Robert Denham, Robert Kohn, Kerrie Mengersen*

- `1306.4723v1` - [abs](http://arxiv.org/abs/1306.4723v1) - [pdf](http://arxiv.org/pdf/1306.4723v1)

> A Bayesian approach is developed to analyze change points in multivariate time series and space-time data. The methodology is used to assess the impact of extended inundation on the ecosystem of the Gulf Plains bioregion in northern Australia. The proposed approach can be implemented for dynamic mixture models that have a conditionally Gaussian state space representation. Details are given on how to efficiently implement the algorithm for a general class of multivariate time series and space-time models. This efficient implementation makes it feasible to analyze high dimensional, but of realistic size, space-time data sets because our approach can be appreciably faster, possibly millions of times, than a standard implementation in such cases.

</details>

<details>

<summary>2013-06-20 13:36:55 - Estimation of Parameters in DNA Mixture Analysis</summary>

- *Therese Graversen, Steffen Lauritzen*

- `1108.1884v3` - [abs](http://arxiv.org/abs/1108.1884v3) - [pdf](http://arxiv.org/pdf/1108.1884v3)

> In Cowell et al. (2007), a Bayesian network for analysis of mixed traces of DNA was presented using gamma distributions for modelling peak sizes in the electropherogram. It was demonstrated that the analysis was sensitive to the choice of a variance factor and hence this should be adapted to any new trace analysed. In the present paper we discuss how the variance parameter can be estimated by maximum likelihood to achieve this. The unknown proportions of DNA from each contributor can similarly be estimated by maximum likelihood jointly with the variance parameter. Furthermore we discuss how to incorporate prior knowledge about the parameters in a Bayesian analysis. The proposed estimation methods are illustrated through a few examples of applications for calculating evidential value in casework and for mixture deconvolution.

</details>

<details>

<summary>2013-06-20 16:10:36 - The Bias and Efficiency of Incomplete-Data Estimators in Small Univariate Normal Samples</summary>

- *Paul T. von Hippel*

- `1204.3132v4` - [abs](http://arxiv.org/abs/1204.3132v4) - [pdf](http://arxiv.org/pdf/1204.3132v4)

> Widely used methods for analyzing missing data can be biased in small samples. To understand these biases, we evaluate in detail the situation where a small univariate normal sample, with values missing at random, is analyzed using either observed-data maximum likelihood (ML) or multiple imputation (MI). We evaluate two types of MI: the usual Bayesian approach, which we call posterior draw (PD) imputation, and a little-used alternative, which we call ML imputation, in which values are imputed conditionally on an ML estimate. We find that observed-data ML is more efficient and has lower mean squared error than either type of MI. Between the two types of MI, ML imputation is more efficient than PD imputation, and ML imputation also has less potential for bias in small samples. The bias and efficiency of PD imputation can be improved by a change of prior.

</details>

<details>

<summary>2013-06-20 17:45:09 - Failure of Calibration is Typical</summary>

- *Gordon Belot*

- `1306.4943v1` - [abs](http://arxiv.org/abs/1306.4943v1) - [pdf](http://arxiv.org/pdf/1306.4943v1)

> Schervish (1985b) showed that every forecasting system is noncalibrated for uncountably many data sequences that it might see. This result is strengthened here: from a topological point of view, failure of calibration is typical and calibration rare. Meanwhile, Bayesian forecasters are certain that they are calibrated---this invites worries about the connection between Bayesianism and rationality.

</details>

<details>

<summary>2013-06-21 07:39:35 - Locally adaptive factor processes for multivariate time series</summary>

- *Daniele Durante, Bruno Scarpa, David B. Dunson*

- `1210.2022v2` - [abs](http://arxiv.org/abs/1210.2022v2) - [pdf](http://arxiv.org/pdf/1210.2022v2)

> In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.

</details>

<details>

<summary>2013-06-21 17:49:27 - Nonparametric Bayesian grouping methods for spatial time-series data</summary>

- *Edward B. Baskerville, Trevor Bedford, Robert C. Reiner, Mercedes Pascual*

- `1306.5202v1` - [abs](http://arxiv.org/abs/1306.5202v1) - [pdf](http://arxiv.org/pdf/1306.5202v1)

> We describe an approach for identifying groups of dynamically similar locations in spatial time-series data based on a simple Markov transition model. We give maximum-likelihood, empirical Bayes, and fully Bayesian formulations of the model, and describe exhaustive, greedy, and MCMC-based inference methods. The approach has been employed successfully in several studies to reveal meaningful relationships between environmental patterns and disease dynamics.

</details>

<details>

<summary>2013-06-23 05:09:05 - Low-Rank Separated Representation Surrogates of High-Dimensional Stochastic Functions: Application in Bayesian Inference</summary>

- *AbdoulAhad Validi*

- `1306.5374v1` - [abs](http://arxiv.org/abs/1306.5374v1) - [pdf](http://arxiv.org/pdf/1306.5374v1)

> This study introduces a non-intrusive approach in the context of low-rank separated representation to construct a surrogate of high-dimensional stochastic functions, e.g., PDEs/ODEs, in order to decrease the computational cost of Markov Chain Monte Carlo simulations in Bayesian inference. The surrogate model is constructed via a regularized alternative least-square regression with Tikhonov regularization using a roughening matrix computing the gradient of the solution, in conjunction with a perturbation-based error indicator to detect optimal model complexities. The model approximates a vector of a continuous solution at discrete values of a physical variable. The required number of random realizations to achieve a successful approximation linearly depends on the function dimensionality. The computational cost of the model construction is quadratic in the number of random inputs, which potentially tackles the curse of dimensionality in high-dimensional stochastic functions. Furthermore, this vector valued separated representation-based model, in comparison to the available scalar-valued case, leads to a significant reduction in the cost of approximation by an order of magnitude equal to the vector size. The performance of the method is studied through its application to three numerical examples including a 41-dimensional elliptic PDE and a 21-dimensional cavity flow.

</details>

<details>

<summary>2013-06-24 11:44:30 - vSMC: Parallel Sequential Monte Carlo in C++</summary>

- *Yan Zhou*

- `1306.5583v1` - [abs](http://arxiv.org/abs/1306.5583v1) - [pdf](http://arxiv.org/pdf/1306.5583v1)

> Sequential Monte Carlo is a family of algorithms for sampling from a sequence of distributions. Some of these algorithms, such as particle filters, are widely used in the physics and signal processing researches. More recent developments have established their application in more general inference problems such as Bayesian modeling.   These algorithms have attracted considerable attentions in recent years as they admit natural and scalable parallelizations. However, these algorithms are perceived to be difficult to implement. In addition, parallel programming is often unfamiliar to many researchers though conceptually appealing, especially for sequential Monte Carlo related fields.   A C++ template library is presented for the purpose of implementing general sequential Monte Carlo algorithms on parallel hardware. Two examples are presented: a simple particle filter and a classic Bayesian modeling problem.

</details>

<details>

<summary>2013-06-25 16:49:11 - Adaptive MC^3 and Gibbs algorithms for Bayesian Model Averaging in Linear Regression Models</summary>

- *Demetris Lamnisos, Jim E. Griffin, Mark F. J. Steel*

- `1306.6028v1` - [abs](http://arxiv.org/abs/1306.6028v1) - [pdf](http://arxiv.org/pdf/1306.6028v1)

> The MC$^3$ (Madigan and York, 1995) and Gibbs (George and McCulloch, 1997) samplers are the most widely implemented algorithms for Bayesian Model Averaging (BMA) in linear regression models. These samplers draw a variable at random in each iteration using uniform selection probabilities and then propose to update that variable. This may be computationally inefficient if the number of variables is large and many variables are redundant. In this work, we introduce adaptive versions of these samplers that retain their simplicity in implementation and reduce the selection probabilities of the many redundant variables. The improvements in efficiency for the adaptive samplers are illustrated in real and simulated datasets.

</details>

<details>

<summary>2013-06-26 22:34:16 - On a Reliable Peer-Review Process</summary>

- *Arthur Carvalho, Kate Larson*

- `1204.5963v2` - [abs](http://arxiv.org/abs/1204.5963v2) - [pdf](http://arxiv.org/pdf/1204.5963v2)

> We propose an enhanced peer-review process where the reviewers are encouraged to truthfully disclose their reviews. We start by modelling that process using a Bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. After that, we introduce a scoring function to evaluate the reported reviews. Under mild assumptions, we show that reviewers strictly maximize their expected scores by telling the truth. We also show how those scores can be used in order to reach consensus.

</details>

<details>

<summary>2013-06-27 08:49:59 - Joint likelihood calculation for intervention and observational data from a Gaussian Bayesian network</summary>

- *GrÃ©gory Nuel, Andrea Rau, Florence JaffrÃ©zic*

- `1305.0709v4` - [abs](http://arxiv.org/abs/1305.0709v4) - [pdf](http://arxiv.org/pdf/1305.0709v4)

> Methodological development for the inference of gene regulatory networks from transcriptomic data is an active and important research area. Several approaches have been proposed to infer relationships among genes from observational steady-state expression data alone, mainly based on the use of graphical Gaussian models. However, these methods rely on the estimation of partial correlations and are only able to provide undirected graphs that cannot highlight causal relationships among genes. A major upcoming challenge is to jointly analyze observational transcriptomic data and intervention data obtained by performing knock-out or knock-down experiments in order to uncover causal gene regulatory relationships. To this end, in this technical note we present an explicit formula for the likelihood function for any complex intervention design in the context of Gaussian Bayesian networks, as well as its analytical maximization. This allows a direct calculation of the causal effects for known graph structure. We also show how to obtain the Fisher information in this context, which will be extremely useful for the choice of optimal intervention designs in the future.

</details>

<details>

<summary>2013-06-28 11:18:26 - ABC Reinforcement Learning</summary>

- *Christos Dimitrakakis, Nikolaos Tziortziotis*

- `1303.6977v4` - [abs](http://arxiv.org/abs/1303.6977v4) - [pdf](http://arxiv.org/pdf/1303.6977v4)

> This paper introduces a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The main advantage is that we only require a prior distribution on a class of simulators (generative models). This is useful in domains where an analytical probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique, even in this case. In addition, it can be seen as an extension of rollout algorithms to the case where we do not know what the correct model to draw rollouts from is. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology in principle, even when non-sufficient statistics are used.

</details>

<details>

<summary>2013-06-28 18:58:35 - Objective Bayesian hypothesis testing in binomial regression models with integral prior distributions</summary>

- *Diego Salmeron, Juan Antonio Cano, C. P. Robert*

- `1306.6928v1` - [abs](http://arxiv.org/abs/1306.6928v1) - [pdf](http://arxiv.org/pdf/1306.6928v1)

> In this work we apply the methodology of integral priors to handle Bayesian model selection in binomial regression models with a general link function. These models are very often used to investigate associations and risks in epidemiological studies where one goal is to exhibit whether or not an exposure is a risk factor for developing a certain disease; the purpose of the current paper is to test the effect of specific exposure factors. We formulate the problem as a Bayesian model selection case and solve it using objective Bayes factors. To construct the reference prior distributions on the regression coefficients of the binomial regression models, we rely on the methodology of integral priors that is nearly automatic as it only requires the specification of estimation reference priors and it does not depend on tuning parameters or on hyperparameters within these priors.

</details>

<details>

<summary>2013-06-29 02:36:45 - Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</summary>

- *Vikash K. Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Joshua B. Tenenbaum*

- `1307.0060v1` - [abs](http://arxiv.org/abs/1307.0060v1) - [pdf](http://arxiv.org/pdf/1307.0060v1)

> The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model. Representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately Bayesian inferences about ambiguous real-world images.

</details>

<details>

<summary>2013-06-29 16:36:30 - Concentration and Confidence for Discrete Bayesian Sequence Predictors</summary>

- *Tor Lattimore, Marcus Hutter, Peter Sunehag*

- `1307.0127v1` - [abs](http://arxiv.org/abs/1307.0127v1) - [pdf](http://arxiv.org/pdf/1307.0127v1)

> Bayesian sequence prediction is a simple technique for predicting future symbols sampled from an unknown measure on infinite sequences over a countable alphabet. While strong bounds on the expected cumulative error are known, there are only limited results on the distribution of this error. We prove tight high-probability bounds on the cumulative error, which is measured in terms of the Kullback-Leibler (KL) divergence. We also consider the problem of constructing upper confidence bounds on the KL and Hellinger errors similar to those constructed from Hoeffding-like bounds in the i.i.d. case. The new results are applied to show that Bayesian sequence prediction can be used in the Knows What It Knows (KWIK) framework with bounds that match the state-of-the-art.

</details>


## 2013-07

<details>

<summary>2013-07-01 10:03:58 - Algorithms of the LDA model [REPORT]</summary>

- *Jaka Å peh, Andrej MuhiÄ, Jan Rupnik*

- `1307.0317v1` - [abs](http://arxiv.org/abs/1307.0317v1) - [pdf](http://arxiv.org/pdf/1307.0317v1)

> We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them are variational inference algorithms: Variational Bayesian inference and Online Variational Bayesian inference and one is Markov Chain Monte Carlo (MCMC) algorithm -- Collapsed Gibbs sampling. We compare their time complexity and performance. We find that online variational Bayesian inference is the fastest algorithm and still returns reasonably good results.

</details>

<details>

<summary>2013-07-01 14:14:55 - Gaussian Process Conditional Copulas with Applications to Financial Time Series</summary>

- *JosÃ© Miguel HernÃ¡ndez-Lobato, James Robert Lloyd, Daniel HernÃ¡ndez-Lobato*

- `1307.0373v1` - [abs](http://arxiv.org/abs/1307.0373v1) - [pdf](http://arxiv.org/pdf/1307.0373v1)

> The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be inaccurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.

</details>

<details>

<summary>2013-07-02 03:44:43 - Using Data to Tune Nearshore Dynamics Models: A Bayesian Approach with Parametric Likelihood</summary>

- *Nusret Balci, Juan M. Restrepo, Shankar C. Venkataramani*

- `1307.0584v1` - [abs](http://arxiv.org/abs/1307.0584v1) - [pdf](http://arxiv.org/pdf/1307.0584v1)

> We propose a modification of a maximum likelihood procedure for tuning parameter values in models, based upon the comparison of their output to field data. Our methodology, which uses polynomial approximations of the sample space to increase the computational efficiency, differs from similar Bayesian estimation frameworks in the use of an alternative likelihood distribution, is shown to better address problems in which covariance information is lacking, than its more conventional counterpart.   Lack of covariance information is a frequent challenge in large-scale geophysical estimation. This is the case in the geophysical problem considered here. We use a nearshore model for long shore currents and observational data of the same to show the contrast between both maximum likelihood methodologies.   Beyond a methodological comparison, this study gives estimates of parameter values for the bottom drag and surface forcing that make the particular model most consistent with data; furthermore, we also derive sensitivity estimates that provide useful insights regarding the estimation procedure as well as of the model itself.

</details>

<details>

<summary>2013-07-02 13:33:03 - Quantile regression in high-dimension with breaking</summary>

- *Gabriela Ciuperca*

- `1302.4244v2` - [abs](http://arxiv.org/abs/1302.4244v2) - [pdf](http://arxiv.org/pdf/1302.4244v2)

> The paper considers a linear regression model in high-dimension for which the predictive variables can change the influence on the response variable at unknown times (called change-points). Moreover, the particular case of the heavy-tailed errors is considered. In this case, least square method with LASSO or adaptive LASSO penalty can not be used since the theoretical assumptions do not occur or the estimators are not robust. Then, the quantile model with SCAD penalty or median regression with LASSO-type penalty allows, in the same time, to estimate the parameters on every segment and eliminate the irrelevant variables. We show that, for the two penalized estimation methods, the oracle properties is not affected by the change-point estimation. Convergence rates of the estimators for the change-points and for the regression parameters, by the two methods are found. Monte-Carlo simulations illustrate the performance of the methods.

</details>

<details>

<summary>2013-07-03 12:54:25 - An Efficient Model Selection for Gaussian Mixture Model in a Bayesian Framework</summary>

- *Ji Won Yoon*

- `1307.0995v1` - [abs](http://arxiv.org/abs/1307.0995v1) - [pdf](http://arxiv.org/pdf/1307.0995v1)

> In order to cluster or partition data, we often use Expectation-and-Maximization (EM) or Variational approximation with a Gaussian Mixture Model (GMM), which is a parametric probability density function represented as a weighted sum of $\hat{K}$ Gaussian component densities. However, model selection to find underlying $\hat{K}$ is one of the key concerns in GMM clustering, since we can obtain the desired clusters only when $\hat{K}$ is known. In this paper, we propose a new model selection algorithm to explore $\hat{K}$ in a Bayesian framework. The proposed algorithm builds the density of the model order which any information criterions such as AIC and BIC basically fail to reconstruct. In addition, this algorithm reconstructs the density quickly as compared to the time-consuming Monte Carlo simulation.

</details>

<details>

<summary>2013-07-04 15:39:51 - A quasi-Gaussian approximation for the probability distribution of correlation functions</summary>

- *Philipp Wilking, Peter Schneider*

- `1304.4781v2` - [abs](http://arxiv.org/abs/1304.4781v2) - [pdf](http://arxiv.org/pdf/1304.4781v2)

> Context. Whenever correlation functions are used for inference about cosmological parameters in the context of a Bayesian analysis, the likelihood function of correlation functions needs to be known. Usually, it is approximated as a multivariate Gaussian, though this is not necessarily a good approximation.   Aims. We show how to calculate a better approximation for the probability distribution of correlation functions, which we call "quasi-Gaussian".   Methods. Using the exact univariate PDF as well as constraints on correlation functions previously derived, we transform the correlation functions to an unconstrained variable for which the Gaussian approximation is well justified. From this Gaussian in the transformed space, we obtain the quasi-Gaussian PDF. The two approximations for the probability distributions are compared to the "true" distribution as obtained from simulations. Additionally, we test how the new approximation performs when used as likelihood in a toy-model Bayesian analysis.   Results. The quasi-Gaussian PDF agrees very well with the PDF obtained from simulations; in particular, it provides a significantly better description than a straightforward copula approach. In a simple toy-model likelihood analysis, it yields noticeably different results than the Gaussian likelihood, indicating its possible impact on cosmological parameter estimation.

</details>

<details>

<summary>2013-07-08 19:50:59 - A Bayesian approach for global sensitivity analysis of (multi-fidelity) computer codes</summary>

- *Loic Le Gratiet, Claire Cannamela, Bertrand Iooss*

- `1307.2223v1` - [abs](http://arxiv.org/abs/1307.2223v1) - [pdf](http://arxiv.org/pdf/1307.2223v1)

> Complex computer codes are widely used in science and engineering to model physical phenomena. Furthermore, it is common that they have a large number of input parameters. Global sensitivity analysis aims to identify those which have the most important impact on the output. Sobol indices are a popular tool to perform such analysis. However, their estimations require an important number of simulations and often cannot be processed under reasonable time constraint. To handle this problem, a Gaussian process regression model is built to surrogate the computer code and the Sobol indices are estimated through it. The aim of this paper is to provide a methodology to estimate the Sobol indices through a surrogate model taking into account both the estimation errors and the surrogate model errors. In particular, it allows us to derive non-asymptotic confidence intervals for the Sobol index estimations. Furthermore, we extend the suggested strategy to the case of multi-fidelity computer codes which can be run at different levels of accuracy. For such simulators, we use an extension of Gaussian process regression models for multivariate outputs.

</details>

<details>

<summary>2013-07-09 00:58:10 - Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning</summary>

- *Diane Oyen, Terran Lane*

- `1307.2312v1` - [abs](http://arxiv.org/abs/1307.2312v1) - [pdf](http://arxiv.org/pdf/1307.2312v1)

> Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult, therefore transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data.

</details>

<details>

<summary>2013-07-09 13:32:16 - Power-Conditional-Expected Priors: Using g-priors with Random Imaginary Data for Variable Selection</summary>

- *Dimitris Fouskakis, Ioannis Ntzoufras*

- `1307.2449v1` - [abs](http://arxiv.org/abs/1307.2449v1) - [pdf](http://arxiv.org/pdf/1307.2449v1)

> The Zellner's g-prior and its recent hierarchical extensions are the most popular default prior choices in the Bayesian variable selection context. These prior set-ups can be expressed power-priors with fixed set of imaginary data. In this paper, we borrow ideas from the power-expected-posterior (PEP) priors in order to introduce, under the g-prior approach, an extra hierarchical level that accounts for the imaginary data uncertainty. For normal regression variable selection problems, the resulting power-conditional-expected-posterior (PCEP) prior is a conjugate normal-inverse gamma prior which provides a consistent variable selection procedure and gives support to more parsimonious models than the ones supported using the g-prior and the hyper-g prior for finite samples. Detailed illustrations and comparisons of the variable selection procedures using the proposed method, the g-prior and the hyper-g prior are provided using both simulated and real data examples.

</details>

<details>

<summary>2013-07-10 04:31:06 - Bayesian Quantile Regression for Partially Linear Additive Models</summary>

- *Yuao Hu, Kaifeng Zhao, Heng Lian*

- `1307.2668v1` - [abs](http://arxiv.org/abs/1307.2668v1) - [pdf](http://arxiv.org/pdf/1307.2668v1)

> In this article, we develop a semiparametric Bayesian estimation and model selection approach for partially linear additive models in conditional quantile regression. The asymmetric Laplace distribution provides a mechanism for Bayesian inferences of quantile regression models based on the check loss. The advantage of this new method is that nonlinear, linear and zero function components can be separated automatically and simultaneously during model fitting without the need of pre-specification or parameter tuning. This is achieved by spike-and-slab priors using two sets of indicator variables. For posterior inferences, we design an effective partially collapsed Gibbs sampler. Simulation studies are used to illustrate our algorithm. The proposed approach is further illustrated by applications to two real data sets.

</details>

<details>

<summary>2013-07-11 23:54:48 - A Note on Probabilistic Models over Strings: the Linear Algebra Approach</summary>

- *Alexandre Bouchard-CÃ´tÃ©*

- `1301.5054v2` - [abs](http://arxiv.org/abs/1301.5054v2) - [pdf](http://arxiv.org/pdf/1301.5054v2)

> Probabilistic models over strings have played a key role in developing methods allowing indels to be treated as phylogenetically informative events. There is an extensive literature on using automata and transducers on phylogenies to do inference on these probabilistic models, in which an important theoretical question in the field is the complexity of computing the normalization of a class of string-valued graphical models. This question has been investigated using tools from combinatorics, dynamic programming, and graph theory, and has practical applications in Bayesian phylogenetics. In this work, we revisit this theoretical question from a different point of view, based on linear algebra. The main contribution is a new proof of a known result on the complexity of inference on TKF91, a well-known probabilistic model over strings. Our proof uses a different approach based on classical linear algebra results, and is in some cases easier to extend to other models. The proving method also has consequences on the implementation and complexity of inference algorithms.

</details>

<details>

<summary>2013-07-12 15:30:38 - On-line Bayesian parameter estimation in general non-linear state-space models: A tutorial and new results</summary>

- *Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni, J. Fraser Forbes*

- `1307.3490v1` - [abs](http://arxiv.org/abs/1307.3490v1) - [pdf](http://arxiv.org/pdf/1307.3490v1)

> On-line estimation plays an important role in process control and monitoring. Obtaining a theoretical solution to the simultaneous state-parameter estimation problem for non-linear stochastic systems involves solving complex multi-dimensional integrals that are not amenable to analytical solution. While basic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms for simultaneous estimation exist, it is well recognized that there is a need for making these on-line algorithms non-degenerate, fast and applicable to processes with missing measurements. To overcome the deficiencies in traditional algorithms, this work proposes a Bayesian approach to on-line state and parameter estimation. Its extension to handle missing data in real-time is also provided. The simultaneous estimation is performed by filtering an extended vector of states and parameters using an adaptive sequential-importance-resampling (SIR) filter with a kernel density estimation method. The approach uses an on-line optimization algorithm based on Kullback-Leibler (KL) divergence to allow adaptation of the SIR filter for combined state-parameter estimation. An optimal tuning rule to control the width of the kernel and the variance of the artificial noise added to the parameters is also proposed. The approach is illustrated through numerical examples.

</details>

<details>

<summary>2013-07-14 22:06:12 - Probabilistic inverse reinforcement learning in unknown environments</summary>

- *Aristide C. Y. Tossou, Christos Dimitrakakis*

- `1307.3785v1` - [abs](http://arxiv.org/abs/1307.3785v1) - [pdf](http://arxiv.org/pdf/1307.3785v1)

> We consider the problem of learning by demonstration from agents acting in unknown stochastic Markov environments or games. Our aim is to estimate agent preferences in order to construct improved policies for the same task that the agents are trying to solve. To do so, we extend previous probabilistic approaches for inverse reinforcement learning in known MDPs to the case of unknown dynamics or opponents. We do this by deriving two simplified probabilistic models of the demonstrator's policy and utility. For tractability, we use maximum a posteriori estimation rather than full Bayesian inference. Under a flat prior, this results in a convex optimisation problem. We find that the resulting algorithms are highly competitive against a variety of other methods for inverse reinforcement learning that do have knowledge of the dynamics.

</details>

<details>

<summary>2013-07-15 07:57:56 - Bayesian Structured Prediction Using Gaussian Processes</summary>

- *Sebastien Bratieres, Novi Quadrianto, Zoubin Ghahramani*

- `1307.3846v1` - [abs](http://arxiv.org/abs/1307.3846v1) - [pdf](http://arxiv.org/pdf/1307.3846v1)

> We introduce a conceptually novel structured prediction model, GPstruct, which is kernelized, non-parametric and Bayesian, by design. We motivate the model with respect to existing approaches, among others, conditional random fields (CRFs), maximum margin Markov networks (M3N), and structured support vector machines (SVMstruct), which embody only a subset of its properties. We present an inference procedure based on Markov Chain Monte Carlo. The framework can be instantiated for a wide range of structured objects such as linear chains, trees, grids, and other general graphs. As a proof of concept, the model is benchmarked on several natural language processing tasks and a video gesture segmentation task involving a linear chain structure. We show prediction accuracies for GPstruct which are comparable to or exceeding those of CRFs and SVMstruct.

</details>

<details>

<summary>2013-07-16 18:03:20 - A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization</summary>

- *Ali Jalali, Javad Azimi, Xiaoli Fern, Ruofei Zhang*

- `1204.0047v2` - [abs](http://arxiv.org/abs/1204.0047v2) - [pdf](http://arxiv.org/pdf/1204.0047v2)

> The problem of optimizing unknown costly-to-evaluate functions has been studied for a long time in the context of Bayesian Optimization. Algorithms in this field aim to find the optimizer of the function by asking only a few function evaluations at locations carefully selected based on a posterior model. In this paper, we assume the unknown function is Lipschitz continuous. Leveraging the Lipschitz property, we propose an algorithm with a distinct exploration phase followed by an exploitation phase. The exploration phase aims to select samples that shrink the search space as much as possible. The exploitation phase then focuses on the reduced search space and selects samples closest to the optimizer. Considering the Expected Improvement (EI) as a baseline, we empirically show that the proposed algorithm significantly outperforms EI.

</details>

<details>

<summary>2013-07-16 20:39:51 - Posterior Consistency for Bayesian Inverse Problems through Stability and Regression Results</summary>

- *Sebastian J. Vollmer*

- `1302.4101v3` - [abs](http://arxiv.org/abs/1302.4101v3) - [pdf](http://arxiv.org/pdf/1302.4101v3)

> In the Bayesian approach, the a priori knowledge about the input of a mathematical model is described via a probability measure. The joint distribution of the unknown input and the data is then conditioned, using Bayes' formula, giving rise to the posterior distribution on the unknown input. In this setting we prove posterior consistency for nonlinear inverse problems: a sequence of data is considered, with diminishing fluctuations around a single truth and it is then of interest to show that the resulting sequence of posterior measures arising from this sequence of data concentrates around the truth used to generate the data. Posterior consistency justifies the use of the Bayesian approach very much in the same way as error bounds and convergence results for regularisation techniques do. As a guiding example, we consider the inverse problem of reconstructing the diffusion coefficient from noisy observations of the solution to an elliptic PDE in divergence form. This problem is approached by splitting the forward operator into the underlying continuum model and a simpler observation operator based on the output of the model.   In general, these splittings allow us to conclude posterior consistency provided a deterministic stability result for the underlying inverse problem and a posterior consistency result for the Bayesian regression problem with the push-forward prior.   Moreover, we prove posterior consistency for the Bayesian regression problem based on the regularity, the tail behaviour and the small ball probabilities of the prior.

</details>

<details>

<summary>2013-07-17 12:47:29 - Bayesian estimate of the degree of a polynomial given a noisy data sample</summary>

- *Giovanni Mana, Paolo Alberto Giuliano Albo, Simona Lago*

- `1307.4602v1` - [abs](http://arxiv.org/abs/1307.4602v1) - [pdf](http://arxiv.org/pdf/1307.4602v1)

> A widely used method to create a continuous representation of a discrete data-set is regression analysis. When the regression model is not based on a mathematical description of the physics underlying the data, heuristic techniques play a crucial role and the model choice can have a significant impact on the result. In this paper, the problem of identifying the most appropriate model is formulated and solved in terms of Bayesian selection. Besides, probability calculus is the best way to choose among different alternatives. The results obtained are applied to the case of both univariate and bivariate polynomials used as trial solutions of systems of thermodynamic partial differential equations.

</details>

<details>

<summary>2013-07-17 15:11:46 - The connection between Bayesian estimation of a Gaussian random field and RKHS</summary>

- *Aleksandr Y. Aravkin, Bradley M. Bell, James V. Burke, Gianluigi Pillonetto*

- `1301.5288v3` - [abs](http://arxiv.org/abs/1301.5288v3) - [pdf](http://arxiv.org/pdf/1301.5288v3)

> Reconstruction of a function from noisy data is often formulated as a regularized optimization problem over an infinite-dimensional reproducing kernel Hilbert space (RKHS). The solution describes the observed data and has a small RKHS norm. When the data fit is measured using a quadratic loss, this estimator has a known statistical interpretation. Given the noisy measurements, the RKHS estimate represents the posterior mean (minimum variance estimate) of a Gaussian random field with covariance proportional to the kernel associated with the RKHS. In this paper, we provide a statistical interpretation when more general losses are used, such as absolute value, Vapnik or Huber. Specifically, for any finite set of sampling locations (including where the data were collected), the MAP estimate for the signal samples is given by the RKHS estimate evaluated at these locations.

</details>

<details>

<summary>2013-07-18 14:19:12 - Computational aspects of DNA mixture analysis</summary>

- *Therese Graversen, Steffen Lauritzen*

- `1307.4956v1` - [abs](http://arxiv.org/abs/1307.4956v1) - [pdf](http://arxiv.org/pdf/1307.4956v1)

> Statistical analysis of DNA mixtures is known to pose computational challenges due to the enormous state space of possible DNA profiles. We propose a Bayesian network representation for genotypes, allowing computations to be performed locally involving only a few alleles at each step. In addition, we describe a general method for computing the expectation of a product of discrete random variables using auxiliary variables and probability propagation in a Bayesian network, which in combination with the genotype network allows efficient computation of the likelihood function and various other quantities relevant to the inference. Lastly, we introduce a set of diagnostic tools for assessing the adequacy of the model for describing a particular dataset.

</details>

<details>

<summary>2013-07-19 14:53:06 - Bayesian models for cost-effectiveness analysis in the presence of structural zero costs</summary>

- *Gianluca Baio*

- `1307.5243v1` - [abs](http://arxiv.org/abs/1307.5243v1) - [pdf](http://arxiv.org/pdf/1307.5243v1)

> Bayesian modelling for cost-effectiveness data has received much attention in both the health economics and the statistical literature in recent years. Cost-effectiveness data are characterised by a relatively complex structure of relationships linking the suitable measure of clinical benefit (\eg QALYs) and the associated costs. Simplifying assumptions, such as (bivariate) normality of the underlying distributions are usually not granted, particularly for the cost variable, which is characterised by markedly skewed distributions. In addition, individual-level datasets are often characterised by the presence of structural zeros in the cost variable.   Hurdle models can be used to account for the presence of excess zeros in a distribution and have been applied in the context of cost data. We extend their application to cost-effectiveness data, defining a full Bayesian model which consists of a selection model for the subjects with null costs, a marginal model for the costs and a conditional model for the measure of effectiveness (conditionally on the observed costs). The model is presented using a working example to describe its main features.

</details>

<details>

<summary>2013-07-19 20:33:18 - Sparse Factor Analysis for Learning and Content Analytics</summary>

- *Andrew S. Lan, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk*

- `1303.5685v2` - [abs](http://arxiv.org/abs/1303.5685v2) - [pdf](http://arxiv.org/pdf/1303.5685v2)

> We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest.

</details>

<details>

<summary>2013-07-22 13:35:28 - Residual component analysis of hyperspectral images -- Application to joint nonlinear unmixing and nonlinearity detection</summary>

- *Yoann Altmann, Nicolas Dobigeon, Steve McLaughlin, Jean-Yves Tourneret*

- `1307.5698v1` - [abs](http://arxiv.org/abs/1307.5698v1) - [pdf](http://arxiv.org/pdf/1307.5698v1)

> This paper presents a nonlinear mixing model for joint hyperspectral image unmixing and nonlinearity detection. The proposed model assumes that the pixel reflectances are linear combinations of known pure spectral components corrupted by an additional nonlinear term, affecting the endmembers and contaminated by an additive Gaussian noise. A Markov random field is considered for nonlinearity detection based on the spatial structure of the nonlinear terms. The observed image is segmented into regions where nonlinear terms, if present, share similar statistical properties. A Bayesian algorithm is proposed to estimate the parameters involved in the model yielding a joint nonlinear unmixing and nonlinearity detection algorithm. The performance of the proposed strategy is first evaluated on synthetic data. Simulations conducted with real data show the accuracy of the proposed unmixing and nonlinearity detection strategy for the analysis of hyperspectral images.

</details>

<details>

<summary>2013-07-22 16:19:08 - Bayesian inference for logistic models using Polya-Gamma latent variables</summary>

- *Nicholas G. Polson, James G. Scott, Jesse Windle*

- `1205.0310v3` - [abs](http://arxiv.org/abs/1205.0310v3) - [pdf](http://arxiv.org/pdf/1205.0310v3)

> We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Polya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effects models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that: (1) circumvent the need for analytic approximations, numerical integration, or Metropolis-Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Polya-Gamma distribution, are implemented in the R package BayesLogit.   In the technical supplement appended to the end of the paper, we provide further details regarding the generation of Polya-Gamma random variables; the empirical benchmarks reported in the main manuscript; and the extension of the basic data-augmentation framework to contingency tables and multinomial outcomes.

</details>

<details>

<summary>2013-07-23 02:42:10 - Understanding predictive information criteria for Bayesian models</summary>

- *Andrew Gelman, Jessica Hwang, Aki Vehtari*

- `1307.5928v1` - [abs](http://arxiv.org/abs/1307.5928v1) - [pdf](http://arxiv.org/pdf/1307.5928v1)

> We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.

</details>

<details>

<summary>2013-07-23 15:39:43 - Sequential Monte Carlo Methods for High-Dimensional Inverse Problems: A case study for the Navier-Stokes equations</summary>

- *Nikolas Kantas, Alexandros Beskos, Ajay Jasra*

- `1307.6127v1` - [abs](http://arxiv.org/abs/1307.6127v1) - [pdf](http://arxiv.org/pdf/1307.6127v1)

> We consider the inverse problem of estimating the initial condition of a partial differential equation, which is only observed through noisy measurements at discrete time intervals. In particular, we focus on the case where Eulerian measurements are obtained from the time and space evolving vector field, whose evolution obeys the two-dimensional Navier-Stokes equations defined on a torus. This context is particularly relevant to the area of numerical weather forecasting and data assimilation. We will adopt a Bayesian formulation resulting from a particular regularization that ensures the problem is well posed. In the context of Monte Carlo based inference, it is a challenging task to obtain samples from the resulting high dimensional posterior on the initial condition. In real data assimilation applications it is common for computational methods to invoke the use of heuristics and Gaussian approximations. The resulting inferences are biased and not well-justified in the presence of non-linear dynamics and observations. On the other hand, Monte Carlo methods can be used to assimilate data in a principled manner, but are often perceived as inefficient in this context due to the high-dimensionality of the problem. In this work we will propose a generic Sequential Monte Carlo (SMC) sampling approach for high dimensional inverse problems that overcomes these difficulties. The method builds upon Markov chain Monte Carlo (MCMC) techniques, which are currently considered as benchmarks for evaluating data assimilation algorithms used in practice. In our numerical examples, the proposed SMC approach achieves the same accuracy as MCMC but in a much more efficient manner.

</details>

<details>

<summary>2013-07-23 21:53:53 - Error analysis in Bayesian identification of non-linear state-space models</summary>

- *Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni, J. Fraser Forbes*

- `1307.6254v1` - [abs](http://arxiv.org/abs/1307.6254v1) - [pdf](http://arxiv.org/pdf/1307.6254v1)

> In the last two decades, several methods based on sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have been proposed for Bayesian identification of stochastic non-linear state-space models (SSMs). It is well known that the performance of these simulation based identification methods depends on the numerical approximations used in their design. We propose the use of posterior Cram\'er-Rao lower bound (PCRLB) as a mean square error (MSE) bound. Using PCRLB, a systematic procedure is developed to analyse the estimates delivered by Bayesian identification methods in terms of bias, MSE, and efficiency. The efficacy and utility of the proposed approach is illustrated through a numerical example.

</details>

<details>

<summary>2013-07-23 22:09:04 - Input design for Bayesian identification of non-linear state-space models</summary>

- *Aditya Tulsyan, Swanand R. Khare, Biao Huang, R. Bhushan Gopaluni, J. Fraser Forbes*

- `1307.6258v1` - [abs](http://arxiv.org/abs/1307.6258v1) - [pdf](http://arxiv.org/pdf/1307.6258v1)

> We propose an algorithm for designing optimal inputs for on-line Bayesian identification of stochastic non-linear state-space models. The proposed method relies on minimization of the posterior Cram\'er Rao lower bound derived for the model parameters, with respect to the input sequence. To render the optimization problem computationally tractable, the inputs are parametrized as a multi-dimensional Markov chain in the input space. The proposed approach is illustrated through a simulation example.

</details>

<details>

<summary>2013-07-24 09:02:00 - Divergence rates of Markov order estimators and their application to statistical estimation of stationary ergodic processes</summary>

- *Zsolt Talata*

- `1307.6338v1` - [abs](http://arxiv.org/abs/1307.6338v1) - [pdf](http://arxiv.org/pdf/1307.6338v1)

> Stationary ergodic processes with finite alphabets are estimated by finite memory processes from a sample, an n-length realization of the process, where the memory depth of the estimator process is also estimated from the sample using penalized maximum likelihood (PML). Under some assumptions on the continuity rate and the assumption of non-nullness, a rate of convergence in $\bar{d}$-distance is obtained, with explicit constants. The result requires an analysis of the divergence of PML Markov order estimators for not necessarily finite memory processes. This divergence problem is investigated in more generality for three information criteria: the Bayesian information criterion with generalized penalty term yielding the PML, and the normalized maximum likelihood and the Krichevsky-Trofimov code lengths. Lower and upper bounds on the estimated order are obtained. The notion of consistent Markov order estimation is generalized for infinite memory processes using the concept of oracle order estimates, and generalized consistency of the PML Markov order estimator is presented.

</details>

<details>

<summary>2013-07-24 13:26:08 - Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier</summary>

- *Niko Brummer*

- `1307.6143v2` - [abs](http://arxiv.org/abs/1307.6143v2) - [pdf](http://arxiv.org/pdf/1307.6143v2)

> This report works out the details of a closed-form, fully Bayesian, multiclass, openset, generative pattern classifier using multivariate Gaussian likelihoods, with conjugate priors. The generative model has a common within-class covariance, which is proportional to the between-class covariance in the conjugate prior. The scalar proportionality constant is the only plugin parameter. All other model parameters are intergated out in closed form. An expression is given for the model evidence, which can be used to make plugin estimates for the proportionality constant. Pattern recognition is done via the predictive likeihoods of classes for which training data is available, as well as a predicitve likelihood for any as yet unseen class.

</details>

<details>

<summary>2013-07-24 18:08:49 - Optimal Grouping for Group Minimax Hypothesis Testing</summary>

- *Kush R. Varshney, Lav R. Varshney*

- `1307.6512v1` - [abs](http://arxiv.org/abs/1307.6512v1) - [pdf](http://arxiv.org/pdf/1307.6512v1)

> Bayesian hypothesis testing and minimax hypothesis testing represent extreme instances of detection in which the prior probabilities of the hypotheses are either completely and precisely known, or are completely unknown. Group minimax, also known as Gamma-minimax, is a robust intermediary between Bayesian and minimax hypothesis testing that allows for coarse or partial advance knowledge of the hypothesis priors by using information on sets in which the prior lies. Existing work on group minimax, however, does not consider the question of how to define the sets or groups of priors; it is assumed that the groups are given. In this work, we propose a novel intermediate detection scheme formulated through the quantization of the space of prior probabilities that optimally determines groups and also representative priors within the groups. We show that when viewed from a quantization perspective, group minimax amounts to determining centroids with a minimax Bayes risk error divergence distortion criterion: the appropriate Bregman divergence for this task. Moreover, the optimal partitioning of the space of prior probabilities is a Bregman Voronoi diagram. Together, the optimal grouping and representation points are an epsilon-net with respect to Bayes risk error divergence, and permit a rate-distortion type asymptotic analysis of detection performance with the number of groups. Examples of detecting signals corrupted by additive white Gaussian noise and of distinguishing exponentially-distributed signals are presented.

</details>

<details>

<summary>2013-07-29 18:50:40 - Probability-Matching Predictors for Extreme Extremes</summary>

- *Allan McRobie*

- `1307.7682v1` - [abs](http://arxiv.org/abs/1307.7682v1) - [pdf](http://arxiv.org/pdf/1307.7682v1)

> A location- and scale-invariant predictor is constructed which exhibits good probability matching for extreme predictions outside the span of data drawn from a variety of (stationary) general distributions. It is constructed via the three-parameter {\mu, \sigma, \xi} Generalized Pareto Distribution (GPD). The predictor is designed to provide matching probability exactly for the GPD in both the extreme heavy-tailed limit and the extreme bounded-tail limit, whilst giving a good approximation to probability matching at all intermediate values of the tail parameter \xi. The predictor is valid even for small sample sizes N, even as small as N = 3.   The main purpose of this paper is to present the somewhat lengthy derivations which draw heavily on the theory of hypergeometric functions, particularly the Lauricella functions. Whilst the construction is inspired by the Bayesian approach to the prediction problem, it considers the case of vague prior information about both parameters and model, and all derivations are undertaken using sampling theory.

</details>

<details>

<summary>2013-07-29 20:57:12 - Group Iterative Spectrum Thresholding for Super-Resolution Sparse Spectral Selection</summary>

- *Yiyuan She, Huanghuang Li, Jiangping Wang, Dapeng Wu*

- `1207.6684v2` - [abs](http://arxiv.org/abs/1207.6684v2) - [pdf](http://arxiv.org/pdf/1207.6684v2)

> Recently, sparsity-based algorithms are proposed for super-resolution spectrum estimation. However, to achieve adequately high resolution in real-world signal analysis, the dictionary atoms have to be close to each other in frequency, thereby resulting in a coherent design. The popular convex compressed sensing methods break down in presence of high coherence and large noise. We propose a new regularization approach to handle model collinearity and obtain parsimonious frequency selection simultaneously. It takes advantage of the pairing structure of sine and cosine atoms in the frequency dictionary. A probabilistic spectrum screening is also developed for fast computation in high dimensions. A data-resampling version of high-dimensional Bayesian Information Criterion is used to determine the regularization parameters. Experiments show the efficacy and efficiency of the proposed algorithms in challenging situations with small sample size, high frequency resolution, and low signal-to-noise ratio.

</details>

<details>

<summary>2013-07-30 16:59:06 - Joint estimation of causal effects from observational and intervention gene expression data</summary>

- *Andrea Rau, Florence JaffrÃ©zic, GrÃ©gory Nuel*

- `1307.8046v1` - [abs](http://arxiv.org/abs/1307.8046v1) - [pdf](http://arxiv.org/pdf/1307.8046v1)

> Background: Inference of gene regulatory networks from transcriptomic data has been a wide research area in recent years. Proposed methods are mainly based on the use of graphical Gaussian models for observational wild-type data and provide undirected graphs that are not able to accurately highlight the causal relationships among genes. In the present work, we seek to improve estimation of causal effects among genes by jointly modeling observational transcriptomic data with intervention data obtained by performing knock-outs or knock-downs on a subset of genes. By examining the impact of such expression perturbations on other genes, a more accurate reflection of regulatory relationships may be obtained than through the use of wild-type data alone. Results: Using the framework of Gaussian Bayesian networks, we propose a Markov chain Monte Carlo algorithm with a Mallows model and an analytical likelihood maximization to sample from the posterior distribution of causal node orderings, and in turn, to estimate causal effects. The main advantage of the proposed algorithm over previously proposed methods is that it has the flexibility to accommodate any kind of intervention design, including partial or multiple knock-out experiments. Methods were compared on simulated data as well as data from the DREAM 2007 challenge. Conclusions: The simulation study confirmed the impossibility of estimating causal orderings of genes with observation data only. The proposed algorithm was found, in most cases, to perform better than the previously proposed methods in terms of accuracy for the estimation of causal effects. In addition, multiple knock-outs proved to bring valuable additional information compared to single knock-outs. The choice of optimal intervention design therefore appears to be a crucial aspect for causal inference and an interesting challenge for future research.

</details>

<details>

<summary>2013-07-31 19:57:02 - Scoring and Searching over Bayesian Networks with Causal and Associative Priors</summary>

- *Giorgos Borboudakis, Ioannis Tsamardinos*

- `1209.6561v2` - [abs](http://arxiv.org/abs/1209.6561v2) - [pdf](http://arxiv.org/pdf/1209.6561v2)

> A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. In this paper, a method is presented for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between pairs of variables. This type of knowledge naturally arises from prior experimental and observational data, among others. In addition, a novel search-operator is proposed to take advantage of such prior knowledge. Experiments show that, using path beliefs improves the learning of the skeleton, as well as the edge directions in the network.

</details>

<details>

<summary>2013-07-31 22:16:39 - A composite likelihood approach to computer model calibration using high-dimensional spatial data</summary>

- *Won Chang, Murali Haran, Roman Olson, Klaus Keller*

- `1308.0049v1` - [abs](http://arxiv.org/abs/1308.0049v1) - [pdf](http://arxiv.org/pdf/1308.0049v1)

> Computer models are used to model complex processes in various disciplines. Often, a key source of uncertainty in the behavior of complex computer models is uncertainty due to unknown model input parameters. Statistical computer model calibration is the process of inferring model parameter values, along with associated uncertainties, from observations of the physical process and from model outputs at various parameter settings. Observations and model outputs are often in the form of high-dimensional spatial fields, especially in the environmental sciences. Sound statistical inference may be computationally challenging in such situations. Here we introduce a composite likelihood-based approach to perform computer model calibration with high-dimensional spatial data. While composite likelihood has been studied extensively in the context of spatial statistics, computer model calibration using composite likelihood poses several new challenges. We propose a computationally efficient approach for Bayesian computer model calibration using composite likelihood. We also develop a methodology based on asymptotic theory for adjusting the composite likelihood posterior distribution so that it accurately represents posterior uncertainties. We study the application of our new approach in the context of calibration for a climate model.

</details>


## 2013-08

<details>

<summary>2013-08-01 17:44:53 - An efficient model-free estimation of multiclass conditional probability</summary>

- *Tu Xu, Junhui Wang*

- `1209.4951v3` - [abs](http://arxiv.org/abs/1209.4951v3) - [pdf](http://arxiv.org/pdf/1209.4951v3)

> Conventional multiclass conditional probability estimation methods, such as Fisher's discriminate analysis and logistic regression, often require restrictive distributional model assumption. In this paper, a model-free estimation method is proposed to estimate multiclass conditional probability through a series of conditional quantile regression functions. Specifically, the conditional class probability is formulated as difference of corresponding cumulative distribution functions, where the cumulative distribution functions can be converted from the estimated conditional quantile regression functions. The proposed estimation method is also efficient as its computation cost does not increase exponentially with the number of classes. The theoretical and numerical studies demonstrate that the proposed estimation method is highly competitive against the existing competitors, especially when the number of classes is relatively large.

</details>

<details>

<summary>2013-08-02 09:41:22 - Comment on "Bayes' Theorem in the 21st Century" by Bradley Efron</summary>

- *Valentin Amrhein, Tobias Roth, Fraenzi Korner-Nievergelt*

- `1308.0447v1` - [abs](http://arxiv.org/abs/1308.0447v1) - [pdf](http://arxiv.org/pdf/1308.0447v1)

> In a Perspectives article in Science, Bradley Efron concludes that Bayesian calculations cannot be uncritically accepted when using uninformative priors. We argue that this conclusion is problematic because Efron's example does not use data, hence it is not Bayesian statistics; his priors make little sense and are not uninformative; and using the available data point and an uninformative prior actually leads to a reasonable posterior distribution.

</details>

<details>

<summary>2013-08-02 11:13:29 - Posterior Contraction Rates for the Bayesian Approach to Linear Ill-Posed Inverse Problems</summary>

- *Sergios Agapiou, Stig Larsson, Andrew M. Stuart*

- `1203.5753v5` - [abs](http://arxiv.org/abs/1203.5753v5) - [pdf](http://arxiv.org/pdf/1203.5753v5)

> We consider a Bayesian nonparametric approach to a family of linear inverse problems in a separable Hilbert space setting with Gaussian noise. We assume Gaussian priors, which are conjugate to the model, and present a method of identifying the posterior using its precision operator. Working with the unbounded precision operator enables us to use partial differential equations (PDE) methodology to obtain rates of contraction of the posterior distribution to a Dirac measure centered on the true solution. Our methods assume a relatively weak relation between the prior covariance, noise covariance and forward operator, allowing for a wide range of applications.

</details>

<details>

<summary>2013-08-03 03:34:03 - Metropolis-Hastings Sampling Using Multivariate Gaussian Tangents</summary>

- *Alireza S. Mahani, Mansour T. A. Sharabiani*

- `1308.0657v1` - [abs](http://arxiv.org/abs/1308.0657v1) - [pdf](http://arxiv.org/pdf/1308.0657v1)

> We present MH-MGT, a multivariate technique for sampling from twice-differentiable, log-concave probability density functions. MH-MGT is Metropolis-Hastings sampling using asymmetric, multivariate Gaussian proposal functions constructed from Taylor-series expansion of the log-density function. The mean of the Gaussian proposal function represents the full Newton step, and thus MH-MGT is the stochastic counterpart to Newton optimization. Convergence analysis shows that MH-MGT is well suited for sampling from computationally-expensive log-densities with contributions from many independent observations. We apply the technique to Gibbs sampling analysis of a Hierarchical Bayesian marketing effectiveness model built for a large US foodservice distributor. Compared to univariate slice sampling, MH-MGT shows 6x improvement in sampling efficiency, measured in terms of `function evaluation equivalents per independent sample'. To facilitate wide applicability of MH-MGT to statistical models, we prove that log-concavity of a twice-differentiable distribution is invariant with respect to 'linear-projection' transformations including, but not restricted to, generalized linear models.

</details>

<details>

<summary>2013-08-03 12:39:00 - A semi-empirical Bayesian chart to monitor Weibull percentiles</summary>

- *Pasquale Erto, Giuliana Pallotta, Christina M. Mastrangelo*

- `1308.0691v1` - [abs](http://arxiv.org/abs/1308.0691v1) - [pdf](http://arxiv.org/pdf/1308.0691v1)

> This paper develops a Bayesian control chart for the percentiles of the Weibull distribution, when both its in-control and out-of-control parameters are unknown. The Bayesian approach enhances parameter estimates for small sample sizes that occur when monitoring rare events as in high-reliability applications or genetic mutations. The chart monitors the parameters of the Weibull distribution directly, instead of transforming the data as most Weibull-based charts do in order to comply with their normality assumption. The chart uses the whole accumulated knowledge resulting from the likelihood of the current sample combined with the information given by both the initial prior knowledge and all the past samples. The chart is adapting since its control limits change (e.g. narrow) during the Phase I. An example is presented and good Average Run Length properties are demonstrated. In addition, the paper gives insights into the nature of monitoring Weibull processes by highlighting the relationship between distribution and process parameters.

</details>

<details>

<summary>2013-08-05 02:50:15 - Multichannel Electrophysiological Spike Sorting via Joint Dictionary Learning & Mixture Modeling</summary>

- *David E. Carlson, Joshua T. Vogelstein, Qisong Wu, Wenzhao Lian, Mingyuan Zhou, Colin R. Stoetzner, Daryl Kipke, Douglas Weber, David B. Dunson, Lawrence Carin*

- `1304.0542v2` - [abs](http://arxiv.org/abs/1304.0542v2) - [pdf](http://arxiv.org/pdf/1304.0542v2)

> We propose a construction for joint feature learning and clustering of multichannel extracellular electrophysiological data across multiple recording periods for action potential detection and discrimination ("spike sorting"). Our construction improves over the previous state-of-the art principally in four ways. First, via sharing information across channels, we can better distinguish between single-unit spikes and artifacts. Second, our proposed "focused mixture model" (FMM) elegantly deals with units appearing, disappearing, or reappearing over multiple recording days, an important consideration for any chronic experiment. Third, by jointly learning features and clusters, we improve performance over previous attempts that proceeded via a two-stage ("frequentist") learning process. Fourth, by directly modeling spike rate, we improve detection of sparsely spiking neurons. Moreover, our Bayesian construction seamlessly handles missing data. We present state-of-the-art performance without requiring manually tuning of many hyper-parameters on both a public dataset with partial ground truth and a new experimental dataset.

</details>

<details>

<summary>2013-08-05 08:07:22 - Fitting Bivariate Mixed-Type Data via the Generalized Linear Exponential Cluster-Weighted Model</summary>

- *Salvatore Ingrassia, Antonio Punzo*

- `1304.0150v2` - [abs](http://arxiv.org/abs/1304.0150v2) - [pdf](http://arxiv.org/pdf/1304.0150v2)

> The cluster-weighted model (CWM) is a mixture model with random covariates which allows for flexible clustering and density estimation of a random vector composed by a response variable and by a set of covariates. In this class of models, the generalized linear exponential CWM is here introduced especially for modeling bivariate data of mixed-type. Its natural counterpart, in the family of latent class models, is also defined. Maximum likelihood parameter estimates are derived using the EM algorithm and model selection is carried out using the Bayesian information criterion (BIC). Artificial and real data are finally considered to exemplify and appreciate the proposed model.

</details>

<details>

<summary>2013-08-05 09:43:40 - Bayesian Motion Estimation for Dust Aerosols</summary>

- *Fabian E. Bachl, Alex Lenkoski, Thordis L. Thorarinsdottir, Christoph S. Garbe*

- `1308.0469v2` - [abs](http://arxiv.org/abs/1308.0469v2) - [pdf](http://arxiv.org/pdf/1308.0469v2)

> Dust storms in the earth's major desert regions significantly influence microphysical weather processes, the CO$_2$-cycle and the global climate in general. Recent increases in the spatio-temporal resolution of remote sensing instruments have created new opportunities to understand these phenomena. However, the scale of the data collected and the inherent stochasticity of the underlying process pose significant challenges, requiring a careful combination of image processing and statistical techniques. In particular, using satellite imagery data, we develop a statistical model of atmospheric transport that relies on a latent Gaussian Markov random field (GMRF) for inference. In doing so, we make a link between the optical flow method of Horn and Schunck and the formulation of the transport process as a latent field in a generalized linear model, which enables the use of the integrated nested Laplace approximation for inference. This framework is specified such that it satisfies the so-called integrated continuity equation, thereby intrinsically expressing the divergence of the field as a multiplicative factor covering air compressibility and satellite column projection. The importance of this step -- as well as treating the problem in a fully statistical manner -- is emphasized by a simulation study where inference based on this latent GMRF clearly reduces errors of the estimated flow field. We conclude with a study of the dynamics of dust storms formed over Saharan Africa and show that our methodology is able to accurately and coherently track the storm movement, a critical problem in this field.

</details>

<details>

<summary>2013-08-05 20:40:25 - Deriving Proper Uniform Priors for Regression Coefficients, Part II</summary>

- *H. R. N. van Erp, R. O. Linger, P. H. A. J. M. van Gelder*

- `1308.1114v1` - [abs](http://arxiv.org/abs/1308.1114v1) - [pdf](http://arxiv.org/pdf/1308.1114v1)

> It is a relatively well-known fact that in problems of Bayesian model selection improper priors should, in general, be avoided. In this paper we derive a proper and parsimonious uniform prior for regression coefficients. We then use this prior to derive the corresponding evidence values of the regression models under consideration. By way of these evidence values one may proceed to compute the posterior probabilities of the competing regression models.

</details>

<details>

<summary>2013-08-06 15:39:11 - A computational framework for infinite-dimensional Bayesian inverse problems. Part I: The linearized case, with application to global seismic inversion</summary>

- *Tan Bui-Thanh, Omar Ghattas, James Martin, Georg Stadler*

- `1308.1313v1` - [abs](http://arxiv.org/abs/1308.1313v1) - [pdf](http://arxiv.org/pdf/1308.1313v1)

> We present a computational framework for estimating the uncertainty in the numerical solution of linearized infinite-dimensional statistical inverse problems. We adopt the Bayesian inference formulation: given observational data and their uncertainty, the governing forward problem and its uncertainty, and a prior probability distribution describing uncertainty in the parameter field, find the posterior probability distribution over the parameter field. The prior must be chosen appropriately in order to guarantee well-posedness of the infinite-dimensional inverse problem and facilitate computation of the posterior. Furthermore, straightforward discretizations may not lead to convergent approximations of the infinite-dimensional problem. And finally, solution of the discretized inverse problem via explicit construction of the covariance matrix is prohibitive due to the need to solve the forward problem as many times as there are parameters. Our computational framework builds on the infinite-dimensional formulation proposed by Stuart (A. M. Stuart, Inverse problems: A Bayesian perspective, Acta Numerica, 19 (2010), pp. 451-559), and incorporates a number of components aimed at ensuring a convergent discretization of the underlying infinite-dimensional inverse problem. The framework additionally incorporates algorithms for manipulating the prior, constructing a low rank approximation of the data-informed component of the posterior covariance operator, and exploring the posterior that together ensure scalability of the entire framework to very high parameter dimensions. We demonstrate this computational framework on the Bayesian solution of an inverse problem in 3D global seismic wave propagation with hundreds of thousands of parameters.

</details>

<details>

<summary>2013-08-13 09:08:10 - Modification of Bayesian Updating where Continuous Parameters have Differing Relationships with New and Existing Data</summary>

- *Nicholas Lewis*

- `1308.2791v1` - [abs](http://arxiv.org/abs/1308.2791v1) - [pdf](http://arxiv.org/pdf/1308.2791v1)

> Bayesian analyses are often performed using so-called noninformative priors, with a view to achieving objective inference about unknown parameters on which available data depends. Noninformative priors depend on the relationship of the data to the parameters over the sample space. Combining Bayesian updating - multiplying an existing posterior density for parameters being estimated by a likelihood function derived from independent new data that depend on those parameters and renormalizing - with use of noninformative priors gives rise to inconsistency where existing and new data depend on continuous parameters in different ways. In such cases, noninformative priors for inference from only the existing and from only the new data would differ, so Bayesian updating would give different final posterior densities depending on which set of data was used to derive an initial posterior and which was used to update that posterior. I propose a revised Bayesian updating method, which resolves this inconsistency by updating the prior as well as the likelihood function, and involves only a single application of Bayes' theorem. The revised method is also applicable where actual prior information as to parameter values exists and inference that objectively reflects the existing information as well as new data is sought. I demonstrate by numerical testing the probability-matching superiority of the proposed revised updating method, in two cases.

</details>

<details>

<summary>2013-08-13 12:55:26 - Quantile and quantile-function estimations under density ratio model</summary>

- *Jiahua Chen, Yukun Liu*

- `1308.2845v1` - [abs](http://arxiv.org/abs/1308.2845v1) - [pdf](http://arxiv.org/pdf/1308.2845v1)

> Population quantiles and their functions are important parameters in many applications. For example, the lower quantiles often serve as crucial quality indices for forestry products. Given several independent samples from populations satisfying the density ratio model, we investigate the properties of empirical likelihood (EL) based inferences. The induced EL quantile estimators are shown to admit a Bahadur representation that leads to asymptotically valid confidence intervals for functions of quantiles. We rigorously prove that EL quantiles based on all the samples are more efficient than empirical quantiles based on individual samples. A simulation study shows that the EL quantiles and their functions have superior performance when the density ratio model assumption is satisfied and when it is mildly violated. An example is used to demonstrate the new method and the potential cost savings.

</details>

<details>

<summary>2013-08-14 02:30:40 - On Generalized Bayesian Data Fusion with Complex Models in Large Scale Networks</summary>

- *Nisar Ahmed, Tsung-Lin Yang, Mark Campbell*

- `1308.3015v1` - [abs](http://arxiv.org/abs/1308.3015v1) - [pdf](http://arxiv.org/pdf/1308.3015v1)

> Recent advances in communications, mobile computing, and artificial intelligence have greatly expanded the application space of intelligent distributed sensor networks. This in turn motivates the development of generalized Bayesian decentralized data fusion (DDF) algorithms for robust and efficient information sharing among autonomous agents using probabilistic belief models. However, DDF is significantly challenging to implement for general real-world applications requiring the use of dynamic/ad hoc network topologies and complex belief models, such as Gaussian mixtures or hybrid Bayesian networks. To tackle these issues, we first discuss some new key mathematical insights about exact DDF and conservative approximations to DDF. These insights are then used to develop novel generalized DDF algorithms for complex beliefs based on mixture pdfs and conditional factors. Numerical examples motivated by multi-robot target search demonstrate that our methods lead to significantly better fusion results, and thus have great potential to enhance distributed intelligent reasoning in sensor networks.

</details>

<details>

<summary>2013-08-16 10:21:06 - Bayesian analysis of measurement error models using INLA</summary>

- *Stefanie Muff, Andrea Riebler, Havard Rue, Philippe Saner, Leonhard Held*

- `1302.3065v2` - [abs](http://arxiv.org/abs/1302.3065v2) - [pdf](http://arxiv.org/pdf/1302.3065v2)

> To account for measurement error (ME) in explanatory variables, Bayesian approaches provide a flexible framework, as expert knowledge about unobserved covariates can be incorporated in the prior distributions. However, given the analytic intractability of the posterior distribution, model inference so far has to be performed via time-consuming and complex Markov chain Monte Carlo implementations. In this paper we extend the Integrated nested Laplace approximations (INLA) approach to formulate Gaussian ME models in generalized linear mixed models. We present three applications, and show how parameter estimates are obtained for common ME models, such as the classical and Berkson error model including heteroscedastic variances. To illustrate the practical feasibility, R-code is provided.

</details>

<details>

<summary>2013-08-17 23:14:46 - Proposals which speed-up function-space MCMC</summary>

- *Kody J. H. Law*

- `1212.4767v3` - [abs](http://arxiv.org/abs/1212.4767v3) - [pdf](http://arxiv.org/pdf/1212.4767v3)

> Inverse problems lend themselves naturally to a Bayesian formulation, in which the quantity of interest is a posterior distribution of state and/or parameters given some uncertain observations. For the common case in which the forward operator is smoothing, then the inverse problem is ill-posed. Well-posedness is imposed via regularisation in the form of a prior, which is often Gaussian. Under quite general conditions, it can be shown that the posterior is absolutely continuous with respect to the prior and it may be well-defined on function space in terms of its density with respect to the prior. In this case, by constructing a proposal for which the prior is invariant, one can define Metropolis-Hastings schemes for MCMC which are well-defined on function space, and hence do not degenerate as the dimension of the underlying quantity of interest increases to infinity, e.g. under mesh refinement when approximating PDE in finite dimensions. However, in practice, despite the attractive theoretical properties of the currently available schemes, they may still suffer from long correlation times, particularly if the data is very informative about some of the unknown parameters. In fact, in this case it may be the directions of the posterior which coincide with the (already known) prior which decorrelate the slowest. The information incorporated into the posterior through the data is often contained within some finite-dimensional subspace, in an appropriate basis, perhaps even one defined by eigenfunctions of the prior. We aim to exploit this fact and improve the mixing time of function-space MCMC by careful rescaling of the proposal. To this end, we introduce two new basic methods of increasing complexity, involving (i) characteristic function truncation of high frequencies and (ii) hessian information to interpolate between low and high frequencies.

</details>

<details>

<summary>2013-08-19 03:20:59 - Bayes Regularized Graphical Model Estimation in High Dimensions</summary>

- *Suprateek Kundu, Veera Baladandayuthapani, Bani K. Mallick*

- `1308.3915v1` - [abs](http://arxiv.org/abs/1308.3915v1) - [pdf](http://arxiv.org/pdf/1308.3915v1)

> There has been an intense development of Bayes graphical model estimation approaches over the past decade - however, most of the existing methods are restricted to moderate dimensions. We propose a novel approach suitable for high dimensional settings, by decoupling model fitting and covariance selection. First, a full model based on a complete graph is fit under novel class of continuous shrinkage priors on the precision matrix elements, which induces shrinkage under an equivalence with Cholesky-based regularization while enabling conjugate updates of entire precision matrices. Subsequently, we propose a post-fitting graphical model estimation step which proceeds using penalized joint credible regions to perform neighborhood selection sequentially for each node. The posterior computation proceeds using straightforward fully Gibbs sampling, and the approach is scalable to high dimensions. The proposed approach is shown to be asymptotically consistent in estimating the graph structure for fixed $p$ when the truth is a Gaussian graphical model. Simulations show that our approach compares favorably with Bayesian competitors both in terms of graphical model estimation and computational efficiency. We apply our methods to high dimensional gene expression and microRNA datasets in cancer genomics.

</details>

<details>

<summary>2013-08-19 16:39:40 - A new three-parameter lifetime distribution and associated inference</summary>

- *Min Wang*

- `1308.4128v1` - [abs](http://arxiv.org/abs/1308.4128v1) - [pdf](http://arxiv.org/pdf/1308.4128v1)

> In this paper, a new three-parameter lifetime distribution is introduced and many of its standard properties are discussed. These include shape of the probability density function, hazard rate function and its shape, quantile function, limiting distributions of order statistics, and the moments. The unknown parameters are estimated by the maximum likelihood estimation procedure. We develop an EM algorithm to find the maximum likelihood estimates of the parameters, because they are not available in closed form. The Fisher information matrix is also obtained and it can be used for constructing the asymptotic confidence intervals. Finally, a real-data application is given to demonstrate the performance of the new distribution.

</details>

<details>

<summary>2013-08-19 19:59:22 - The characteristic imset polytope of Bayesian networks with ordered nodes</summary>

- *Jing Xi, Ruriko Yoshida*

- `1206.0406v3` - [abs](http://arxiv.org/abs/1206.0406v3) - [pdf](http://arxiv.org/pdf/1206.0406v3)

> In 2010, M. Studen\'y, R. Hemmecke, and S. Linder explored a new algebraic description of graphical models, called characteristic imsets. Compare with standard imsets, characteristic imsets have several advantages: they are still unique vector representative of conditional independence structures, they are 0-1 vectors, and they are more intuitive in terms of graphs than standard imsets. After defining a characteristic imset polytope (cim-polytope) as the convex hull of all characteristic imsets with a given set of nodes, they also showed that a model selection in graphical models, which maximizes a quality criterion, can be converted into a linear programming problem over the cim-polytope. However, in general, for a fixed set of nodes, the cim-polytope can have exponentially many vertices over an exponentially high dimension. Therefore, in this paper, we focus on the family of directed acyclic graphs (DAGs) whose nodes have a fixed order. This family includes diagnosis models which can be described by Bipartite graphs with a set of $m$ nodes and a set of $n$ nodes for any $m, n \in \Z_+$. In this paper, we first consider cim-polytopes for all diagnosis models and show that these polytopes are direct products of simplices. Then we give a combinatorial description of all edges and all facets of these polytopes. Finally, we generalize these results to the cim-polytopes for all Bayesian networks with a fixed underlying ordering of nodes with or without fixed (or forbidden) edges.

</details>

<details>

<summary>2013-08-20 00:17:15 - Empirical Quantile CLTs For Some Self-Similar Processes</summary>

- *James Kuelbs, Joel Zinn*

- `1308.4194v1` - [abs](http://arxiv.org/abs/1308.4194v1) - [pdf](http://arxiv.org/pdf/1308.4194v1)

> In a paper of Jason Swanson, a CLT for the sample median of independent Brownian motions with value 0 at 0 was proved. Here we extend this result in two ways. We prove such a result for a collection of self-similar processes which include the fractional Brownian motions, all stationary, independent increment symmetric stable processes tied down at 0 as well as iterated and integrated Brownian motions. Second, our results hold uniformly over all quantiles in a compact sub-interval of (0,1). We also examine sample function properties connected with these CLTs.

</details>

<details>

<summary>2013-08-21 01:23:52 - Twisting the Alive Particle Filter</summary>

- *Adam Persing, Ajay Jasra*

- `1308.4462v1` - [abs](http://arxiv.org/abs/1308.4462v1) - [pdf](http://arxiv.org/pdf/1308.4462v1)

> This work focuses on sampling from hidden Markov models (Cappe et al, 2005) whose observations have intractable density functions. We develop a new sequential Monte Carlo (Doucet et al, 2000 and Gordon et al, 1993) algorithm and a new particle marginal Metropolis-Hastings (Andrieu et al, 2010) algorithm for these purposes. We build from Jasra, et al (2013) and Whiteley, et al (2013) to construct the sequential Monte Carlo (SMC) algorithm (which we call the alive twisted particle filter). Like the alive particle filter of Jasra, et al (2013), our new SMC algorithm adopts an approximate Bayesian computation (Tavare et al, 1997) estimate of the HMM. Our alive twisted particle filter also uses a twisted proposal as in Whiteley, et al (2013) to obtain a low-variance estimate of the HMM normalising constant. We demonstrate via numerical examples that, in some scenarios, this estimate has a much lower variance than that of the estimate obtained via the alive particle filter. The low variance of this normalising constant estimate encourages the implementation of our SMC algorithm within a particle marginal Metropolis-Hastings (PMMH) scheme, and we call the resulting methodology ``alive twisted PMMH''. We numerically demonstrate on a stochastic volatility model how our alive twisted PMMH can converge faster than the standard alive PMMH of Jasra, et al (2013).

</details>

<details>

<summary>2013-08-22 14:10:49 - Bayesian model selection for the latent position cluster model for Social Networks</summary>

- *Nial Friel, Caitriona Ryan, Jason Wyse*

- `1308.4871v1` - [abs](http://arxiv.org/abs/1308.4871v1) - [pdf](http://arxiv.org/pdf/1308.4871v1)

> The latent position cluster model is a popular model for the statistical analysis of network data. This approach assumes that there is an underlying latent space in which the actors follow a finite mixture distribution. Moreover, actors which are close in this latent space tend to be tied by an edge. This is an appealing approach since it allows the model to cluster actors which consequently provides the practitioner with useful qualitative information. However, exploring the uncertainty in the number of underlying latent components in the mixture distribution is a very complex task. The current state-of-the-art is to use an approximate form of BIC for this purpose, where an approximation of the log-likelihood is used instead of the true log-likelihood which is unavailable. The main contribution of this paper is to show that through the use of conjugate prior distributions it is possible to analytically integrate out almost all of the model parameters, leaving a posterior distribution which depends on the allocation vector of the mixture model. A consequence of this is that it is possible to carry out posterior inference over the number of components in the latent mixture distribution without using trans-dimensional MCMC algorithms such as reversible jump MCMC. Moreover, our algorithm allows for more reasonable computation times for larger networks than the standard methods using the latentnet package (Krivitsky and Handcock 2008; Krivitsky and Handcock 2013).

</details>

<details>

<summary>2013-08-22 15:55:26 - Relevant statistics for Bayesian model choice</summary>

- *J. -M. Marin, N. Pillai, C. P. Robert, J. Rousseau*

- `1110.4700v4` - [abs](http://arxiv.org/abs/1110.4700v4) - [pdf](http://arxiv.org/pdf/1110.4700v4)

> The choice of the summary statistics used in Bayesian inference and in particular in ABC algorithms has bearings on the validation of the resulting inference. Those statistics are nonetheless customarily used in ABC algorithms without consistency checks. We derive necessary and sufficient conditions on summary statistics for the corresponding Bayes factor to be convergent, namely to asymptotically select the true model. Those conditions, which amount to the expectations of the summary statistics to asymptotically differ under both models, are quite natural and can be exploited in ABC settings to infer whether or not a choice of summary statistics is appropriate, via a Monte Carlo validation.

</details>

<details>

<summary>2013-08-22 23:10:00 - Top-down particle filtering for Bayesian decision trees</summary>

- *Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh*

- `1303.0561v2` - [abs](http://arxiv.org/abs/1303.0561v2) - [pdf](http://arxiv.org/pdf/1303.0561v2)

> Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations---which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data---have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff.

</details>

<details>

<summary>2013-08-25 17:51:55 - A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals from Compressive Samples</summary>

- *Zhao Song, Aleksandar Dogandzic*

- `1209.1064v4` - [abs](http://arxiv.org/abs/1209.1064v4) - [pdf](http://arxiv.org/pdf/1209.1064v4)

> We propose a Bayesian expectation-maximization (EM) algorithm for reconstructing Markov-tree sparse signals via belief propagation. The measurements follow an underdetermined linear model where the regression-coefficient vector is the sum of an unknown approximately sparse signal and a zero-mean white Gaussian noise with an unknown variance. The signal is composed of large- and small-magnitude components identified by binary state variables whose probabilistic dependence structure is described by a Markov tree. Gaussian priors are assigned to the signal coefficients given their state variables and the Jeffreys' noninformative prior is assigned to the noise variance. Our signal reconstruction scheme is based on an EM iteration that aims at maximizing the posterior distribution of the signal and its state variables given the noise variance. We construct the missing data for the EM iteration so that the complete-data posterior distribution corresponds to a hidden Markov tree (HMT) probabilistic graphical model that contains no loops and implement its maximization (M) step via a max-product algorithm. This EM algorithm estimates the vector of state variables as well as solves iteratively a linear system of equations to obtain the corresponding signal estimate. We select the noise variance so that the corresponding estimated signal and state variables obtained upon convergence of the EM iteration have the largest marginal posterior distribution. We compare the proposed and existing state-of-the-art reconstruction methods via signal and image reconstruction experiments.

</details>

<details>

<summary>2013-08-26 13:20:25 - A Comparison of Algorithms for Learning Hidden Variables in Normal Graphs</summary>

- *Francesco A. N. Palmieri*

- `1308.5576v1` - [abs](http://arxiv.org/abs/1308.5576v1) - [pdf](http://arxiv.org/pdf/1308.5576v1)

> A Bayesian factor graph reduced to normal form consists in the interconnection of diverter units (or equal constraint units) and Single-Input/Single-Output (SISO) blocks. In this framework localized adaptation rules are explicitly derived from a constrained maximum likelihood (ML) formulation and from a minimum KL-divergence criterion using KKT conditions. The learning algorithms are compared with two other updating equations based on a Viterbi-like and on a variational approximation respectively. The performance of the various algorithm is verified on synthetic data sets for various architectures. The objective of this paper is to provide the programmer with explicit algorithms for rapid deployment of Bayesian graphs in the applications.

</details>

<details>

<summary>2013-08-26 22:40:37 - Robust Spectral Analysis</summary>

- *Andreas Hagemann*

- `1111.1965v2` - [abs](http://arxiv.org/abs/1111.1965v2) - [pdf](http://arxiv.org/pdf/1111.1965v2)

> In this paper I introduce quantile spectral densities that summarize the cyclical behavior of time series across their whole distribution by analyzing periodicities in quantile crossings. This approach can capture systematic changes in the impact of cycles on the distribution of a time series and allows robust spectral estimation and inference in situations where the dependence structure is not accurately captured by the auto-covariance function. I study the statistical properties of quantile spectral estimators in a large class of nonlinear time series models and discuss inference both at fixed and across all frequencies. Monte Carlo experiments illustrate the advantages of quantile spectral analysis over classical methods when standard assumptions are violated.

</details>

<details>

<summary>2013-08-26 22:53:02 - A Modified Gibbs Sampler on General State Spaces</summary>

- *Alicia A. Johnson, James M. Flegal*

- `1308.5717v1` - [abs](http://arxiv.org/abs/1308.5717v1) - [pdf](http://arxiv.org/pdf/1308.5717v1)

> We present a modified Gibbs sampler for general state spaces. We establish that this modification can lead to substantial gains in statistical efficiency while maintaining the overall quality of convergence. We illustrate our results in two examples including a toy Normal-Normal model and a Bayesian version of the random effects model.

</details>

<details>

<summary>2013-08-27 01:55:17 - A Methodology for Robust Multiproxy Paleoclimate Reconstructions and Modeling of Temperature Conditional Quantiles</summary>

- *Lucas Janson, Bala Rajaratnam*

- `1308.5736v1` - [abs](http://arxiv.org/abs/1308.5736v1) - [pdf](http://arxiv.org/pdf/1308.5736v1)

> Great strides have been made in the field of reconstructing past temperatures based on models relating temperature to temperature-sensitive paleoclimate proxies. One of the goals of such reconstructions is to assess if current climate is anomalous in a millennial context. These regression based approaches model the conditional mean of the temperature distribution as a function of paleoclimate proxies (or vice versa). Some of the recent focus in the area has considered methods which help reduce the uncertainty inherent in such statistical paleoclimate reconstructions, with the ultimate goal of improving the confidence that can be attached to such endeavors. A second important scientific focus in the subject area is the area of forward models for proxies, the goal of which is to understand the way paleoclimate proxies are driven by temperature and other environmental variables. In this paper we introduce novel statistical methodology for (1) quantile regression with autoregressive residual structure, (2) estimation of corresponding model parameters, (3) development of a rigorous framework for specifying uncertainty estimates of quantities of interest, yielding (4) statistical byproducts that address the two scientific foci discussed above. Our statistical methodology demonstrably produces a more robust reconstruction than is possible by using conditional-mean-fitting methods. Our reconstruction shares some of the common features of past reconstructions, but also gains useful insights. More importantly, we are able to demonstrate a significantly smaller uncertainty than that from previous regression methods. In addition, the quantile regression component allows us to model, in a more complete and flexible way than least squares, the conditional distribution of temperature given proxies. This relationship can be used to inform forward models relating how proxies are driven by temperature.

</details>

<details>

<summary>2013-08-28 15:14:47 - Bayesian Conditional Gaussian Network Classifiers with Applications to Mass Spectra Classification</summary>

- *Victor Bellon, Jesus Cerquides, Ivo Grosse*

- `1308.6181v1` - [abs](http://arxiv.org/abs/1308.6181v1) - [pdf](http://arxiv.org/pdf/1308.6181v1)

> Classifiers based on probabilistic graphical models are very effective. In continuous domains, maximum likelihood is usually used to assess the predictions of those classifiers. When data is scarce, this can easily lead to overfitting. In any probabilistic setting, Bayesian averaging (BA) provides theoretically optimal predictions and is known to be robust to overfitting. In this work we introduce Bayesian Conditional Gaussian Network Classifiers, which efficiently perform exact Bayesian averaging over the parameters. We evaluate the proposed classifiers against the maximum likelihood alternatives proposed so far over standard UCI datasets, concluding that performing BA improves the quality of the assessed probabilities (conditional log likelihood) whilst maintaining the error rate.   Overfitting is more likely to occur in domains where the number of data items is small and the number of variables is large. These two conditions are met in the realm of bioinformatics, where the early diagnosis of cancer from mass spectra is a relevant task. We provide an application of our classification framework to that problem, comparing it with the standard maximum likelihood alternative, where the improvement of quality in the assessed probabilities is confirmed.

</details>

<details>

<summary>2013-08-28 21:12:15 - Evidence of bias in the Eurovision song contest: modelling the votes using Bayesian hierarchical models</summary>

- *Marta Blangiardo, Gianluca Baio*

- `1308.6312v1` - [abs](http://arxiv.org/abs/1308.6312v1) - [pdf](http://arxiv.org/pdf/1308.6312v1)

> The Eurovision Song Contest is an annual musical competition held among active members of the European Broadcasting Union since 1956. The event is televised live across Europe. Each participating country presents a song and receive a vote based on a combination of tele-voting and jury. Over the years, this has led to speculations of tactical voting, discriminating against some participants and thus inducing bias in the final results. In this paper we investigate the presence of positive or negative bias (which may roughly indicate favouritisms or discrimination) in the votes based on geographical proximity, migration and cultural characteristics of the participating countries through a Bayesian hierarchical model. Our analysis found no evidence of negative bias, although mild positive bias does seem to emerge systematically, linking voters to performers.

</details>

<details>

<summary>2013-08-29 03:40:58 - Simultaneous adjustment of bias and coverage probabilities for confidence intervals</summary>

- *P. Menendez, Y. Fan, P. H. Garthwaite, S. A. Sisson*

- `1210.3405v2` - [abs](http://arxiv.org/abs/1210.3405v2) - [pdf](http://arxiv.org/pdf/1210.3405v2)

> A new method is proposed for the correction of confidence intervals when the original interval does not have the correct nominal coverage probabilities in the frequentist sense. The proposed method is general and does not require any distributional assumptions. It can be applied to both frequentist and Bayesian inference where interval estimates are desired. We provide theoretical results for the consistency of the proposed estimator, and give two complex examples, on confidence interval correction for composite likelihood estimators and in approximate Bayesian computation (ABC), to demonstrate the wide applicability of the new method. Comparison is made with the double-bootstrap and other methods of improving confidence interval coverage.

</details>

<details>

<summary>2013-08-29 06:50:07 - Ranking relations using analogies in biological and information networks</summary>

- *Ricardo Silva, Katherine Heller, Zoubin Ghahramani, Edoardo M. Airoldi*

- `0912.5193v3` - [abs](http://arxiv.org/abs/0912.5193v3) - [pdf](http://arxiv.org/pdf/0912.5193v3)

> Analogical reasoning depends fundamentally on the ability to learn and generalize about relations between objects. We develop an approach to relational learning which, given a set of pairs of objects $\mathbf{S}=\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\ldots,A^{(N)}:B ^{(N)}\}$, measures how well other pairs A:B fit in with the set $\mathbf{S}$. Our work addresses the following question: is the relation between objects A and B analogous to those relations found in $\mathbf{S}$? Such questions are particularly relevant in information retrieval, where an investigator might want to search for analogous pairs of objects that match the query set of interest. There are many ways in which objects can be related, making the task of measuring analogies very challenging. Our approach combines a similarity measure on function spaces with Bayesian analysis to produce a ranking. It requires data containing features of the objects of interest and a link matrix specifying which relationships exist; no further attributes of such relationships are necessary. We illustrate the potential of our method on text analysis and information networks. An application on discovering functional interactions between pairs of proteins is discussed in detail, where we show that our approach can work in practice even if a small set of protein pairs is provided.

</details>

<details>

<summary>2013-08-30 20:43:10 - Inconsistency of Pitman-Yor process mixtures for the number of components</summary>

- *Jeffrey W. Miller, Matthew T. Harrison*

- `1309.0024v1` - [abs](http://arxiv.org/abs/1309.0024v1) - [pdf](http://arxiv.org/pdf/1309.0024v1)

> In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent --- that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians).

</details>


## 2013-09

<details>

<summary>2013-09-03 08:23:35 - Coherent prior distributions in univariate finite mixture and Markov-switching models</summary>

- *Åukasz Kwiatkowski*

- `1309.0609v1` - [abs](http://arxiv.org/abs/1309.0609v1) - [pdf](http://arxiv.org/pdf/1309.0609v1)

> Finite mixture and Markov-switching models generalize and, therefore, nest specifications featuring only one component. While specifying priors in the two: the general (mixture) model and its special (single-component) case, it may be desirable to ensure that the prior assumptions introduced into both structures are coherent in the sense that the prior distribution in the nested model amounts to the conditional prior in the mixture model under relevant parametric restriction. The study provides the rudiments of setting coherent priors in Bayesian univariate finite mixture and Markov-switching models. Once some primary results are delivered, we derive specific conditions for coherence in the case of three types of continuous priors commonly engaged in Bayesian modeling: the normal, inverse gamma, and gamma distributions. Further, we study the consequences of introducing additional constraints into the mixture model's prior (such as the ones enforcing identifiability or some sort of regularity, e.g. second-order stationarity) on the coherence conditions. Finally, the methodology is illustrated through a discussion of setting coherent priors for a class of Markov-switching AR(2) models.

</details>

<details>

<summary>2013-09-03 17:38:43 - Infinite-dimensional Bayesian filtering for detection of quasi-periodic phenomena in spatio-temporal data</summary>

- *Arno Solin, Simo SÃ¤rkkÃ¤*

- `1303.2530v2` - [abs](http://arxiv.org/abs/1303.2530v2) - [pdf](http://arxiv.org/pdf/1303.2530v2)

> This paper introduces a spatio-temporal resonator model and an inference method for detection and estimation of nearly periodic temporal phenomena in spatio-temporal data. The model is derived as a spatial extension of a stochastic harmonic resonator model, which can be formulated in terms of a stochastic differential equation (SDE). The spatial structure is included by introducing linear operators, which affect both the oscillations and damping, and by choosing the appropriate spatial covariance structure of the driving time-white noise process. With the choice of the linear operators as partial differential operators, the resonator model becomes a stochastic partial differential equation (SPDE), which is compatible with infinite-dimensional Kalman filtering. The resulting infinite-dimensional Kalman filtering problem allows for a computationally efficient solution as the computational cost scales linearly with measurements in the temporal dimension. This framework is applied to weather prediction and to physiological noise elimination in fMRI brain data.

</details>

<details>

<summary>2013-09-03 20:35:07 - Bayesian Model Selection in Complex Linear Systems, as Illustrated in Genetic Association Studies</summary>

- *Xiaoquan Wen*

- `1309.0837v1` - [abs](http://arxiv.org/abs/1309.0837v1) - [pdf](http://arxiv.org/pdf/1309.0837v1)

> Motivated by examples from genetic association studies, this paper considers the model selection problem in a general complex linear model system and in a Bayesian framework. We discuss formulating model selection problems and incorporating context-dependent {\it a priori} information through different levels of prior specifications. We also derive analytic Bayes factors and their approximations to facilitate model selection and discuss their theoretical and computational properties. We demonstrate our Bayesian approach based on an implemented Markov Chain Monte Carlo (MCMC) algorithm in simulations and a real data application of mapping tissue-specific eQTLs. Our novel results on Bayes factors provide a general framework to perform efficient model comparisons in complex linear model systems.

</details>

<details>

<summary>2013-09-05 01:48:01 - Bayesian Analysis of Multiway Tables in Association Studies: A Model Comparison Approach</summary>

- *Xiaoquan Wen*

- `1208.4621v2` - [abs](http://arxiv.org/abs/1208.4621v2) - [pdf](http://arxiv.org/pdf/1208.4621v2)

> We consider the problem of statistical inference on unknown quantities structured as a multiway table. We show that such multiway tables are naturally formed by arranging regression coefficients in complex systems of linear models for association analysis. In genetics and genomics, the resulting two-way and three-way tables cover many important applications. Within the Bayesian hierarchical model framework, we define the structure of a multiway table through prior specification. Focusing on model comparison and selection, we derive analytic expressions of Bayes factors and their approximations and discuss their theoretical and computational properties. Finally, we demonstrate the strength of our approach using a genomic application of mapping tissue-specific eQTLs (expression quantitative loci).

</details>

<details>

<summary>2013-09-07 21:02:01 - Parallel Bayesian Additive Regression Trees</summary>

- *Matthew T. Pratola, Hugh A. Chipman, James R. Gattiker, David M. Higdon, Robert McCulloch, William N. Rust*

- `1309.1906v1` - [abs](http://arxiv.org/abs/1309.1906v1) - [pdf](http://arxiv.org/pdf/1309.1906v1)

> Bayesian Additive Regression Trees (BART) is a Bayesian approach to flexible non-linear regression which has been shown to be competitive with the best modern predictive methods such as those based on bagging and boosting. BART offers some advantages. For example, the stochastic search Markov Chain Monte Carlo (MCMC) algorithm can provide a more complete search of the model space and variation across MCMC draws can capture the level of uncertainty in the usual Bayesian way. The BART prior is robust in that reasonable results are typically obtained with a default prior specification. However, the publicly available implementation of the BART algorithm in the R package BayesTree is not fast enough to be considered interactive with over a thousand observations, and is unlikely to even run with 50,000 to 100,000 observations. In this paper we show how the BART algorithm may be modified and then computed using single program, multiple data (SPMD) parallel computation implemented using the Message Passing Interface (MPI) library. The approach scales nearly linearly in the number of processor cores, enabling the practitioner to perform statistical inference on massive datasets. Our approach can also handle datasets too massive to fit on any single data repository.

</details>

<details>

<summary>2013-09-09 14:38:26 - Adaptive Posterior Convergence Rates in Bayesian Density Deconvolution with Supersmooth Errors</summary>

- *Abhra Sarkar, Debdeep Pati, Bani K. Mallick, Raymond J. Carroll*

- `1308.5427v2` - [abs](http://arxiv.org/abs/1308.5427v2) - [pdf](http://arxiv.org/pdf/1308.5427v2)

> Bayesian density deconvolution using nonparametric prior distributions is a useful alternative to the frequentist kernel based deconvolution estimators due to its potentially wide range of applicability, straightforward uncertainty quantification and generalizability to more sophisticated models. This article is the first substantive effort to theoretically quantify the behavior of the posterior in this recent line of research. In particular, assuming a known supersmooth error density, a Dirichlet process mixture of Normals on the true density leads to a posterior convergence rate same as the minimax rate $(\log n)^{-\eta/\beta}$ adaptively over the smoothness $\eta$ of an appropriate H\"{o}lder space of densities, where $\beta$ is the degree of smoothness of the error distribution. Our main contribution is achieving adaptive minimax rates with respect to the $L_p$ norm for $2 \leq p \leq \infty$ under mild regularity conditions on the true density. En route, we develop tight concentration bounds for a class of kernel based deconvolution estimators which might be of independent interest.

</details>

<details>

<summary>2013-09-10 00:36:44 - Exponentially Fast Parameter Estimation in Networks Using Distributed Dual Averaging</summary>

- *Shahin Shahrampour, Ali Jadbabaie*

- `1309.2350v1` - [abs](http://arxiv.org/abs/1309.2350v1) - [pdf](http://arxiv.org/pdf/1309.2350v1)

> In this paper we present an optimization-based view of distributed parameter estimation and observational social learning in networks. Agents receive a sequence of random, independent and identically distributed (i.i.d.) signals, each of which individually may not be informative about the underlying true state, but the signals together are globally informative enough to make the true state identifiable. Using an optimization-based characterization of Bayesian learning as proximal stochastic gradient descent (with Kullback-Leibler divergence from a prior as a proximal function), we show how to efficiently use a distributed, online variant of Nesterov's dual averaging method to solve the estimation with purely local information. When the true state is globally identifiable, and the network is connected, we prove that agents eventually learn the true parameter using a randomized gossip scheme. We demonstrate that with high probability the convergence is exponentially fast with a rate dependent on the KL divergence of observations under the true state from observations under the second likeliest state. Furthermore, our work also highlights the possibility of learning under continuous adaptation of network which is a consequence of employing constant, unit stepsize for the algorithm.

</details>

<details>

<summary>2013-09-10 10:02:22 - Bayesian Wavelet Shrinkage of the Haar-Fisz Transformed Wavelet Periodogram</summary>

- *Guy P. Nason, Kara N. Stevens*

- `1309.2435v1` - [abs](http://arxiv.org/abs/1309.2435v1) - [pdf](http://arxiv.org/pdf/1309.2435v1)

> It is increasingly being realised that many real world time series are not stationary and exhibit evolving second-order autocovariance or spectral structure. This article introduces a Bayesian approach for modelling the evolving wavelet spectrum of a locally stationary wavelet time series. Our new method works by combining the advantages of a Haar-Fisz transformed spectrum with a simple, but powerful, Bayesian wavelet shrinkage method. Our new method produces excellent and stable spectral estimates and this is demonstrated via simulated data and on differenced infant ECG data. A major additional benefit of the Bayesian paradigm is that we obtain rigorous and useful credible intervals of the evolving spectral structure. We show how the Bayesian credible intervals provide extra insight into the infant ECG data.

</details>

<details>

<summary>2013-09-10 19:48:21 - Weighted quantile regression for longitudinal data</summary>

- *Lu Xiaoming, Fan Zhaozhi*

- `1309.2627v1` - [abs](http://arxiv.org/abs/1309.2627v1) - [pdf](http://arxiv.org/pdf/1309.2627v1)

> Quantile regression is a powerful statistical methodology that complements the classical linear regression by examining how covariates influence the location, scale, and shape of the entire response distribution and offering a global view of the statistical landscape. In this paper we propose a new quantile regression model for longitudinal data. The proposed approach incorporates the correlation structure between repeated measures to enhance the efficiency of the inference. In order to use the Newton-Raphson iteration method to obtain convergent estimates, the estimating functions are redefined as smoothed functions which are differentiable with respect to regression parameters. Our proposed method for quantile regression provides consistent estimates with asymptotically normal distributions. Simulation studies are carried out to evaluate the performance of the proposed method. As an illustration, the proposed method was applied to a real-life data that contains self-reported labor pain for women in two groups.

</details>

<details>

<summary>2013-09-11 06:34:23 - On the Evidence for Cosmic Variation of the Fine Structure Constant (II): A Semi-Parametric Bayesian Model Selection Analysis of the Quasar Dataset</summary>

- *Ewan Cameron, Tony Pettitt*

- `1309.2737v1` - [abs](http://arxiv.org/abs/1309.2737v1) - [pdf](http://arxiv.org/pdf/1309.2737v1)

> In the second paper of this series we extend our Bayesian reanalysis of the evidence for a cosmic variation of the fine structure constant to the semi-parametric modelling regime. By adopting a mixture of Dirichlet processes prior for the unexplained errors in each instrumental subgroup of the benchmark quasar dataset we go some way towards freeing our model selection procedure from the apparent subjectivity of a fixed distributional form. Despite the infinite-dimensional domain of the error hierarchy so constructed we are able to demonstrate a recursive scheme for marginal likelihood estimation with prior-sensitivity analysis directly analogous to that presented in Paper I, thereby allowing the robustness of our posterior Bayes factors to hyper-parameter choice and model specification to be readily verified. In the course of this work we elucidate various similarities between unexplained error problems in the seemingly disparate fields of astronomy and clinical meta-analysis, and we highlight a number of sophisticated techniques for handling such problems made available by past research in the latter. It is our hope that the novel approach to semi-parametric model selection demonstrated herein may serve as a useful reference for others exploring this potentially difficult class of error model.

</details>

<details>

<summary>2013-09-11 22:16:50 - Transmuted Generalized Inverse Weibull Distribution</summary>

- *Faton Merovci, Ibrahim Elbatal, Alaa Ahmed*

- `1309.3268v1` - [abs](http://arxiv.org/abs/1309.3268v1) - [pdf](http://arxiv.org/pdf/1309.3268v1)

> A generalization of the generalized inverse Weibull distribution so-called transmuted generalized inverse Weibull dis- tribution is proposed and studied. We will use the quadratic rank transmutation map (QRTM) in order to generate a flexible family of probability distributions taking generalized inverse Weibull distribution as the base value distribution by introducing a new parameter that would offer more distributional flexibility. Various structural properties including explicit expressions for the mo- ments, quantiles, and moment generating function of the new dis- tribution are derived.We proposed the method of maximum likelihood for estimating the model parameters and obtain the observed information matrix. A real data set are used to compare the exibility of the transmuted version versus the generalized inverseWeibull distribution.

</details>

<details>

<summary>2013-09-12 09:38:43 - Inferring Heterogeneous Evolutionary Processes Through Time: from sequence substitution to phylogeography</summary>

- *Filip Bielejec, Philippe Lemey, Guy Baele, Andrew Rambaut, Marc A Suchard*

- `1309.3075v1` - [abs](http://arxiv.org/abs/1309.3075v1) - [pdf](http://arxiv.org/pdf/1309.3075v1)

> Molecular phylogenetic and phylogeographic reconstructions generally assume time-homogeneous substitution processes. Motivated by computational convenience, this assumption sacrifices biological realism and offers little opportunity to uncover the temporal dynamics in evolutionary histories. Here, we extend and generalize an evolutionary approach that relaxes the time-homogeneous process assumption by allowing the specification of different infinitesimal substitution rate matrices across different time intervals, called epochs, along the evolutionary history. We focus on an epoch model implementation in a Bayesian inference framework that offers great modeling flexibility in drawing inference about any discrete data type characterized as a continuous-time Markov chain, including phylogeographic traits. To alleviate the computational burden that the additional temporal heterogeneity imposes, we adopt a massively parallel approach that achieves both fine- and coarse-grain parallelization of the computations across branches that accommodate epoch transitions, making extensive use of graphics processing units. Through synthetic examples, we assess model performance in recovering evolutionary parameters from data generated according to different evolutionary scenarios that comprise different numbers of epochs for both nucleotide and codon substitution processes. We illustrate the usefulness of our inference framework in two different applications to empirical data sets: the selection dynamics on within-host HIV populations throughout infection and the seasonality of global influenza circulation. In both cases, our epoch model captures key features of temporal heterogeneity that remained difficult to test using ad hoc procedures.

</details>

<details>

<summary>2013-09-12 15:42:08 - Poly-Omic Prediction of Complex Traits: OmicKriging</summary>

- *Heather E. Wheeler, Keston Aquino-Michaels, Eric R. Gamazon, Vassily V. Trubetskoy, M. Eileen Dolan, R. Stephanie Huang, Nancy J. Cox, Hae Kyung Im*

- `1303.1788v2` - [abs](http://arxiv.org/abs/1303.1788v2) - [pdf](http://arxiv.org/pdf/1303.1788v2)

> High-confidence prediction of complex traits such as disease risk or drug response is an ultimate goal of personalized medicine. Although genome-wide association studies have discovered thousands of well-replicated polymorphisms associated with a broad spectrum of complex traits, the combined predictive power of these associations for any given trait is generally too low to be of clinical relevance. We propose a novel systems approach to complex trait prediction, which leverages and integrates similarity in genetic, transcriptomic or other omics-level data. We translate the omic similarity into phenotypic similarity using a method called Kriging, commonly used in geostatistics and machine learning. Our method called OmicKriging emphasizes the use of a wide variety of systems-level data, such as those increasingly made available by comprehensive surveys of the genome, transcriptome and epigenome, for complex trait prediction. Furthermore, our OmicKriging framework allows easy integration of prior information on the function of subsets of omics-level data from heterogeneous sources without the sometimes heavy computational burden of Bayesian approaches. Using seven disease datasets from the Wellcome Trust Case Control Consortium (WTCCC), we show that OmicKriging allows simple integration of sparse and highly polygenic components yielding comparable performance at a fraction of the computing time of a recently published Bayesian sparse linear mixed model method. Using a cellular growth phenotype, we show that integrating mRNA and microRNA expression data substantially increases performance over either dataset alone. We also integrate genotype and expression data to predict change in LDL cholesterol levels after statin treatment and show that OmicKriging performs better than the polygenic score method. We provide an R package to implement OmicKriging.

</details>

<details>

<summary>2013-09-13 15:45:41 - A stochastic diffusion process for Lochner's generalized Dirichlet distribution</summary>

- *J. Bakosi, J. R. Ristorcelli*

- `1309.3490v1` - [abs](http://arxiv.org/abs/1309.3490v1) - [pdf](http://arxiv.org/pdf/1309.3490v1)

> The method of potential solutions of Fokker-Planck equations is used to develop a transport equation for the joint probability of N stochastic variables with Lochner's generalized Dirichlet distribution (R.H. Lochner, A Generalized Dirichlet Distribution in Bayesian Life Testing, Journal of the Royal Statistical Society, Series B, 37(1):pp. 103-113, 1975) as its asymptotic solution. Individual samples of a discrete ensemble, obtained from the system of stochastic differential equations, equivalent to the Fokker-Planck equation developed here, satisfy a unit-sum constraint at all times and ensure a bounded sample space, similarly to the process developed in (J. Bakosi, J.R. Ristorcelli, A stochastic diffusion process for the Dirichlet distribution, Int. J. Stoch. Anal., Article ID, 842981, 2013) for the Dirichlet distribution. Consequently, the generalized Dirichlet diffusion process may be used to represent realizations of a fluctuating ensemble of N variables subject to a conservation principle. Compared to the Dirichlet distribution and process, the additional parameters of the generalized Dirichlet distribution allow a more general class of physical processes to be modeled with a more general covariance matrix.

</details>

<details>

<summary>2013-09-17 06:50:41 - Variational inference for count response semiparametric regression</summary>

- *Jan Luts, Matt P. Wand*

- `1309.4199v1` - [abs](http://arxiv.org/abs/1309.4199v1) - [pdf](http://arxiv.org/pdf/1309.4199v1)

> Fast variational approximate algorithms are developed for Bayesian semiparametric regression when the response variable is a count, i.e. a non-negative integer. We treat both the Poisson and Negative Binomial families as models for the response variable. Our approach utilizes recently developed methodology known as non-conjugate variational message passing. For concreteness, we focus on generalized additive mixed models, although our variational approximation approach extends to a wide class of semiparametric regression models such as those containing interactions and elaborate random effect structure.

</details>

<details>

<summary>2013-09-17 12:43:24 - Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes</summary>

- *Roger Frigola, Carl Edward Rasmussen*

- `1303.2912v3` - [abs](http://arxiv.org/abs/1303.2912v3) - [pdf](http://arxiv.org/pdf/1303.2912v3)

> We introduce GP-FNARX: a new model for nonlinear system identification based on a nonlinear autoregressive exogenous model (NARX) with filtered regressors (F) where the nonlinear regression problem is tackled using sparse Gaussian processes (GP). We integrate data pre-processing with system identification into a fully automated procedure that goes from raw data to an identified model. Both pre-processing parameters and GP hyper-parameters are tuned by maximizing the marginal likelihood of the probabilistic model. We obtain a Bayesian model of the system's dynamics which is able to report its uncertainty in regions where the data is scarce. The automated approach, the modeling of uncertainty and its relatively low computational cost make of GP-FNARX a good candidate for applications in robotics and adaptive control.

</details>

<details>

<summary>2013-09-17 12:56:49 - Spherical Hamiltonian Monte Carlo for Constrained Target Distributions</summary>

- *Shiwei Lan, Bo Zhou, Babak Shahbaba*

- `1309.4289v1` - [abs](http://arxiv.org/abs/1309.4289v1) - [pdf](http://arxiv.org/pdf/1309.4289v1)

> We propose a new Markov Chain Monte Carlo (MCMC) method for constrained target distributions. Our method first maps the $D$-dimensional constrained domain of parameters to the unit ball ${\bf B}_0^D(1)$. Then, it augments the resulting parameter space to the $D$-dimensional sphere, ${\bf S}^D$. The boundary of ${\bf B}_0^D(1)$ corresponds to the equator of ${\bf S}^D$. This change of domains enables us to implicitly handle the original constraints because while the sampler moves freely on the sphere, it proposes states that are within the constraints imposed on the original parameter space. To improve the computational efficiency of our algorithm, we split the Lagrangian dynamics into several parts such that a part of the dynamics can be handled analytically by finding the geodesic flow on the sphere. We apply our method to several examples including truncated Gaussian, Bayesian Lasso, Bayesian bridge regression, and a copula model for identifying synchrony among multiple neurons. Our results show that the proposed method can provide a natural and efficient framework for handling several types of constraints on target distributions.

</details>

<details>

<summary>2013-09-18 06:44:33 - Bayesian rules and stochastic models for high accuracy prediction of solar radiation</summary>

- *Cyril Voyant, C. Darras, Marc Muselli, Christophe Paoli, Marie Laure Nivet, Philippe Poggi*

- `1309.4999v1` - [abs](http://arxiv.org/abs/1309.4999v1) - [pdf](http://arxiv.org/pdf/1309.4999v1)

> It is essential to find solar predictive methods to massively insert renewable energies on the electrical distribution grid. The goal of this study is to find the best methodology allowing predicting with high accuracy the hourly global radiation. The knowledge of this quantity is essential for the grid manager or the private PV producer in order to anticipate fluctuations related to clouds occurrences and to stabilize the injected PV power. In this paper, we test both methodologies: single and hybrid predictors. In the first class, we include the multi-layer perceptron (MLP), auto-regressive and moving average (ARMA), and persistence models. In the second class, we mix these predictors with Bayesian rules to obtain ad-hoc models selections, and Bayesian averages of outputs related to single models. If MLP and ARMA are equivalent (nRMSE close to 40.5% for the both), this hybridization allows a nRMSE gain upper than 14 percentage points compared to the persistence estimation (nRMSE=37% versus 51%).

</details>

<details>

<summary>2013-09-18 14:04:44 - Robust T-optimal discriminating designs</summary>

- *Holger Dette, Viatcheslav B. Melas, Petr Shpilev*

- `1309.4652v1` - [abs](http://arxiv.org/abs/1309.4652v1) - [pdf](http://arxiv.org/pdf/1309.4652v1)

> This paper considers the problem of constructing optimal discriminating experimental designs for competing regression models on the basis of the T-optimality criterion introduced by Atkinson and Fedorov [Biometrika 62 (1975) 57-70]. T-optimal designs depend on unknown model parameters and it is demonstrated that these designs are sensitive with respect to misspecification. As a solution to this problem we propose a Bayesian and standardized maximin approach to construct robust and efficient discriminating designs on the basis of the T-optimality criterion. It is shown that the corresponding Bayesian and standardized maximin optimality criteria are closely related to linear optimality criteria. For the problem of discriminating between two polynomial regression models which differ in the degree by two the robust T-optimal discriminating designs can be found explicitly. The results are illustrated in several examples.

</details>

<details>

<summary>2013-09-18 14:24:03 - Uniformly most powerful Bayesian tests</summary>

- *Valen E. Johnson*

- `1309.4656v1` - [abs](http://arxiv.org/abs/1309.4656v1) - [pdf](http://arxiv.org/pdf/1309.4656v1)

> Uniformly most powerful tests are statistical hypothesis tests that provide the greatest power against a fixed null hypothesis among all tests of a given size. In this article, the notion of uniformly most powerful tests is extended to the Bayesian setting by defining uniformly most powerful Bayesian tests to be tests that maximize the probability that the Bayes factor, in favor of the alternative hypothesis, exceeds a specified threshold. Like their classical counterpart, uniformly most powerful Bayesian tests are most easily defined in one-parameter exponential family models, although extensions outside of this class are possible. The connection between uniformly most powerful tests and uniformly most powerful Bayesian tests can be used to provide an approximate calibration between p-values and Bayes factors. Finally, issues regarding the strong dependence of resulting Bayes factors and p-values on sample size are discussed.

</details>

<details>

<summary>2013-09-18 14:51:52 - Volatility occupation times</summary>

- *Jia Li, Viktor Todorov, George Tauchen*

- `1309.4667v1` - [abs](http://arxiv.org/abs/1309.4667v1) - [pdf](http://arxiv.org/pdf/1309.4667v1)

> We propose nonparametric estimators of the occupation measure and the occupation density of the diffusion coefficient (stochastic volatility) of a discretely observed It\^{o} semimartingale on a fixed interval when the mesh of the observation grid shrinks to zero asymptotically. In a first step we estimate the volatility locally over blocks of shrinking length, and then in a second step we use these estimates to construct a sample analogue of the volatility occupation time and a kernel-based estimator of its density. We prove the consistency of our estimators and further derive bounds for their rates of convergence. We use these results to estimate nonparametrically the quantiles associated with the volatility occupation measure.

</details>

<details>

<summary>2013-09-18 15:03:34 - Inference on Counterfactual Distributions</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Blaise Melly*

- `0904.0951v6` - [abs](http://arxiv.org/abs/0904.0951v6) - [pdf](http://arxiv.org/pdf/0904.0951v6)

> Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article we develop modeling and inference tools for counterfactual distributions based on regression methods. The counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. For either of these scenarios we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. These results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. These confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. We illustrate the results with an empirical application to wage decompositions using data for the United States.   As a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the \textit{entire} conditional distribution. We show that distribution regression encompasses the Cox duration regression and represents a useful alternative to quantile regression. We establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.

</details>

<details>

<summary>2013-09-19 06:07:09 - A simple bootstrap method for constructing nonparametric confidence bands for functions</summary>

- *Peter Hall, Joel Horowitz*

- `1309.4864v1` - [abs](http://arxiv.org/abs/1309.4864v1) - [pdf](http://arxiv.org/pdf/1309.4864v1)

> Standard approaches to constructing nonparametric confidence bands for functions are frustrated by the impact of bias, which generally is not estimated consistently when using the bootstrap and conventionally smoothed function estimators. To overcome this problem it is common practice to either undersmooth, so as to reduce the impact of bias, or oversmooth, and thereby introduce an explicit or implicit bias estimator. However, these approaches, and others based on nonstandard smoothing methods, complicate the process of inference, for example, by requiring the choice of new, unconventional smoothing parameters and, in the case of undersmoothing, producing relatively wide bands. In this paper we suggest a new approach, which exploits to our advantage one of the difficulties that, in the past, has prevented an attractive solution to the problem - the fact that the standard bootstrap bias estimator suffers from relatively high-frequency stochastic error. The high frequency, together with a technique based on quantiles, can be exploited to dampen down the stochastic error term, leading to relatively narrow, simple-to-construct confidence bands.

</details>

<details>

<summary>2013-09-19 15:24:51 - Bayesian Decision-optimal Interval Designs for Phase I Clinical Trials</summary>

- *Suyu Liu, Ying Yuan*

- `1309.5019v1` - [abs](http://arxiv.org/abs/1309.5019v1) - [pdf](http://arxiv.org/pdf/1309.5019v1)

> Interval designs are a class of phase I trial designs for which the decision of dose assignment is determined by comparing the observed toxicity rate at the current dose with a prespecified (toxicity tolerance) interval. If the observed toxicity rate is located within the interval, we retain the current dose; if the observed toxicity rate is greater than the upper boundary of the interval, we deescalate the dose; and if the observed toxicity rate is smaller than the lower boundary of the interval, we escalate the dose. The most critical issue for the interval design is choosing an appropriate interval so that the design has good operating characteristics. By casting dose finding as a Bayesian decision-making problem, we propose new flexible methods to select the interval boundaries so as to minimize the probability of inappropriate dose assignment for patients. We show, both theoretically and numerically, that the resulting optimal interval designs not only have desirable finite- and large-sample properties, but also are particularly easy to implement in practice. Compared to existing designs, the proposed (local) optimal design has comparable average performance, but a lower risk of yielding a poorly performing clinical trial.

</details>

<details>

<summary>2013-09-21 03:42:04 - Latent Fisher Discriminant Analysis</summary>

- *Gang Chen*

- `1309.5427v1` - [abs](http://arxiv.org/abs/1309.5427v1) - [pdf](http://arxiv.org/pdf/1309.5427v1)

> Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. Previous studies have also extended the binary-class case into multi-classes. However, many applications, such as object detection and keyframe extraction cannot provide consistent instance-label pairs, while LDA requires labels on instance level for training. Thus it cannot be directly applied for semi-supervised classification problem. In this paper, we overcome this limitation and propose a latent variable Fisher discriminant analysis model. We relax the instance-level labeling into bag-level, is a kind of semi-supervised (video-level labels of event type are required for semantic frame extraction) and incorporates a data-driven prior over the latent variables. Hence, our method combines the latent variable inference and dimension reduction in an unified bayesian framework. We test our method on MUSK and Corel data sets and yield competitive results compared to the baseline approach. We also demonstrate its capacity on the challenging TRECVID MED11 dataset for semantic keyframe extraction and conduct a human-factors ranking-based experimental evaluation, which clearly demonstrates our proposed method consistently extracts more semantically meaningful keyframes than challenging baselines.

</details>

<details>

<summary>2013-09-21 15:18:34 - Computational Aspects of Optional PÃ³lya Tree</summary>

- *Hui Jiang, John C. Mu, Kun Yang, Chao Du, Luo Lu, Wing Hung Wong*

- `1309.5489v1` - [abs](http://arxiv.org/abs/1309.5489v1) - [pdf](http://arxiv.org/pdf/1309.5489v1)

> Optional P\'{o}lya Tree (OPT) is a flexible non-parametric Bayesian model for density estimation. Despite its merits, the computation for OPT inference is challenging. In this paper we present time complexity analysis for OPT inference and propose two algorithmic improvements. The first improvement, named Limited-Lookahead Optional P\'{o}lya Tree (LL-OPT), aims at greatly accelerate the computation for OPT inference. The second improvement modifies the output of OPT or LL-OPT and produces a continuous piecewise linear density estimate. We demonstrate the performance of these two improvements using simulations.

</details>

<details>

<summary>2013-09-21 19:39:37 - Adaptive construction of surrogates for the Bayesian solution of inverse problems</summary>

- *Jinglai Li, Youssef M. Marzouk*

- `1309.5524v1` - [abs](http://arxiv.org/abs/1309.5524v1) - [pdf](http://arxiv.org/pdf/1309.5524v1)

> The Bayesian approach to inverse problems typically relies on posterior sampling approaches, such as Markov chain Monte Carlo, for which the generation of each sample requires one or more evaluations of the parameter-to-observable map or forward model. When these evaluations are computationally intensive, approximations of the forward model are essential to accelerating sample-based inference. Yet the construction of globally accurate approximations for nonlinear forward models can be computationally prohibitive and in fact unnecessary, as the posterior distribution typically concentrates on a small fraction of the support of the prior distribution. We present a new approach that uses stochastic optimization to construct polynomial approximations over a sequence of measures adaptively determined from the data, eventually concentrating on the posterior distribution. The approach yields substantial gains in efficiency and accuracy over prior-based surrogates, as demonstrated via application to inverse problems in partial differential equations.

</details>

<details>

<summary>2013-09-23 19:19:58 - High-Dimensional Bayesian Inference in Nonparametric Additive Models</summary>

- *Zuofeng Shang, Ping Li*

- `1307.0056v3` - [abs](http://arxiv.org/abs/1307.0056v3) - [pdf](http://arxiv.org/pdf/1307.0056v3)

> A fully Bayesian approach is proposed for ultrahigh-dimensional nonparametric additive models in which the number of additive components may be larger than the sample size, though ideally the true model is believed to include only a small number of components. Bayesian approaches can conduct stochastic model search and fulfill flexible parameter estimation by stochastic draws. The theory shows that the proposed model selection method has satisfactory properties. For instance, when the hyperparameter associated with the model prior is correctly specified, the true model has posterior probability approaching one as the sample size goes to infinity; when this hyperparameter is incorrectly specified, the selected model is still acceptable since asymptotically it is proved to be nested in the true model. To enhance model flexibility, two new $g$-priors are proposed and their theoretical performance is examined. We also propose an efficient MCMC algorithm to handle the computational issues. Several simulation examples are provided to demonstrate the computational advantages of our method.

</details>

<details>

<summary>2013-09-25 03:25:11 - Kernel Approximate Bayesian Computation for Population Genetic Inferences</summary>

- *Shigeki Nakagome, Kenji Fukumizu, Shuhei Mano*

- `1205.3246v3` - [abs](http://arxiv.org/abs/1205.3246v3) - [pdf](http://arxiv.org/pdf/1205.3246v3)

> Approximate Bayesian computation (ABC) is a likelihood-free approach for Bayesian inferences based on a rejection algorithm method that applies a tolerance of dissimilarity between summary statistics from observed and simulated data. Although several improvements to the algorithm have been proposed, none of these improvements avoid the following two sources of approximation: 1) lack of sufficient statistics: sampling is not from the true posterior density given data but from an approximate posterior density given summary statistics; and 2) non-zero tolerance: sampling from the posterior density given summary statistics is achieved only in the limit of zero tolerance. The first source of approximation can be improved by adding a summary statistic, but an increase in the number of summary statistics could introduce additional variance caused by the low acceptance rate. Consequently, many researchers have attempted to develop techniques to choose informative summary statistics. The present study evaluated the utility of a kernel-based ABC method (Fukumizu et al. 2010, arXiv:1009.5736 and 2011, NIPS 24: 1549-1557) for complex problems that demand many summary statistics. Specifically, kernel ABC was applied to population genetic inference. We demonstrate that, in contrast to conventional ABCs, kernel ABC can incorporate a large number of summary statistics while maintaining high performance of the inference.

</details>

<details>

<summary>2013-09-25 06:59:47 - Weak consistency of Markov chain Monte Carlo methods</summary>

- *Kengo Kamatani*

- `1103.5679v3` - [abs](http://arxiv.org/abs/1103.5679v3) - [pdf](http://arxiv.org/pdf/1103.5679v3)

> Markov chain Monte Calro methods (MCMC) are commonly used in Bayesian statistics. In the last twenty years, many results have been established for the calculation of the exact convergence rate of MCMC methods. We introduce another rate of convergence for MCMC methods by approximation techniques. This rate can be obtained by the convergence of the Markov chain to a diffusion process. We apply it to a simple mixture model and obtain its convergence rate. Numerical simulations are performed to illustrate the effect of the rate.

</details>

<details>

<summary>2013-09-25 12:55:01 - A Tricentenary history of the Law of Large Numbers</summary>

- *Eugene Seneta*

- `1309.6488v1` - [abs](http://arxiv.org/abs/1309.6488v1) - [pdf](http://arxiv.org/pdf/1309.6488v1)

> The Weak Law of Large Numbers is traced chronologically from its inception as Jacob Bernoulli's Theorem in 1713, through De Moivre's Theorem, to ultimate forms due to Uspensky and Khinchin in the 1930s, and beyond. Both aspects of Jacob Bernoulli's Theorem: 1. As limit theorem (sample size $n\to\infty$), and: 2. Determining sufficiently large sample size for specified precision, for known and also unknown p (the inversion problem), are studied, in frequentist and Bayesian settings. The Bienaym\'{e}-Chebyshev Inequality is shown to be a meeting point of the French and Russian directions in the history. Particular emphasis is given to less well-known aspects especially of the Russian direction, with the work of Chebyshev, Markov (the organizer of Bicentennial celebrations), and S.N. Bernstein as focal points.

</details>

<details>

<summary>2013-09-26 03:47:47 - Decision Making for Inconsistent Expert Judgments Using Negative Probabilities</summary>

- *J. Acacio de Barros*

- `1307.4101v2` - [abs](http://arxiv.org/abs/1307.4101v2) - [pdf](http://arxiv.org/pdf/1307.4101v2)

> In this paper we provide a simple random-variable example of inconsistent information, and analyze it using three different approaches: Bayesian, quantum-like, and negative probabilities. We then show that, at least for this particular example, both the Bayesian and the quantum-like approaches have less normative power than the negative probabilities one.

</details>

<details>

<summary>2013-09-26 07:46:46 - Particle Efficient Importance Sampling</summary>

- *Marcel Scharth, Robert Kohn*

- `1309.6745v1` - [abs](http://arxiv.org/abs/1309.6745v1) - [pdf](http://arxiv.org/pdf/1309.6745v1)

> The efficient importance sampling (EIS) method is a general principle for the numerical evaluation of high-dimensional integrals that uses the sequential structure of target integrands to build variance minimising importance samplers. Despite a number of successful applications in high dimensions, it is well known that importance sampling strategies are subject to an exponential growth in variance as the dimension of the integration increases. We solve this problem by recognising that the EIS framework has an offline sequential Monte Carlo interpretation. The particle EIS method is based on non-standard resampling weights that take into account the look-ahead construction of the importance sampler. We apply the method for a range of univariate and bivariate stochastic volatility specifications. We also develop a new application of the EIS approach to state space models with Student's t state innovations. Our results show that the particle EIS method strongly outperforms both the standard EIS method and particle filters for likelihood evaluation in high dimensions. Moreover, the ratio between the variances of the particle EIS and particle filter methods remains stable as the time series dimension increases. We illustrate the efficiency of the method for Bayesian inference using the particle marginal Metropolis-Hastings and importance sampling squared algorithms.

</details>

<details>

<summary>2013-09-26 12:35:41 - SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure</summary>

- *Eliot Brenner, David Sontag*

- `1309.6820v1` - [abs](http://arxiv.org/abs/1309.6820v1) - [pdf](http://arxiv.org/pdf/1309.6820v1)

> We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to scorebased structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, we give empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning.

</details>

<details>

<summary>2013-09-26 12:40:36 - Unsupervised Learning of Noisy-Or Bayesian Networks</summary>

- *Yonatan Halpern, David Sontag*

- `1309.6834v1` - [abs](http://arxiv.org/abs/1309.6834v1) - [pdf](http://arxiv.org/pdf/1309.6834v1)

> This paper considers the problem of learning the parameters in Bayesian networks of discrete variables with known structure and hidden variables. Previous approaches in these settings typically use expectation maximization; when the network has high treewidth, the required expectations might be approximated using Monte Carlo or variational methods. We show how to avoid inference altogether during learning by giving a polynomial-time algorithm based on the method-of-moments, building upon recent work on learning discrete-valued mixture models. In particular, we show how to learn the parameters for a family of bipartite noisy-or Bayesian networks. In our experimental results, we demonstrate an application of our algorithm to learning QMR-DT, a large Bayesian network used for medical diagnosis. We show that it is possible to fully learn the parameters of QMR-DT even when only the findings are observed in the training data (ground truth diseases unknown).

</details>

<details>

<summary>2013-09-26 12:42:25 - Constrained Bayesian Inference for Low Rank Multitask Learning</summary>

- *Oluwasanmi Koyejo, Joydeep Ghosh*

- `1309.6840v1` - [abs](http://arxiv.org/abs/1309.6840v1) - [pdf](http://arxiv.org/pdf/1309.6840v1)

> We present a novel approach for constrained Bayesian inference. Unlike current methods, our approach does not require convexity of the constraint set. We reduce the constrained variational inference to a parametric optimization over the feasible set of densities and propose a general recipe for such problems. We apply the proposed constrained Bayesian inference approach to multitask learning subject to rank constraints on the weight matrix. Further, constrained parameter estimation is applied to recover the sparse conditional independence structure encoded by prior precision matrices. Our approach is motivated by reverse inference for high dimensional functional neuroimaging, a domain where the high dimensionality and small number of examples requires the use of constraints to ensure meaningful and effective models. For this application, we propose a model that jointly learns a weight matrix and the prior inverse covariance structure between different tasks. We present experimental validation showing that the proposed approach outperforms strong baseline models in terms of predictive performance and structure recovery.

</details>

<details>

<summary>2013-09-26 12:50:04 - Determinantal Clustering Processes - A Nonparametric Bayesian Approach to Kernel Based Semi-Supervised Clustering</summary>

- *Amar Shah, Zoubin Ghahramani*

- `1309.6862v1` - [abs](http://arxiv.org/abs/1309.6862v1) - [pdf](http://arxiv.org/pdf/1309.6862v1)

> Semi-supervised clustering is the task of clustering data points into clusters where only a fraction of the points are labelled. The true number of clusters in the data is often unknown and most models require this parameter as an input. Dirichlet process mixture models are appealing as they can infer the number of clusters from the data. However, these models do not deal with high dimensional data well and can encounter difficulties in inference. We present a novel nonparameteric Bayesian kernel based method to cluster data points without the need to prespecify the number of clusters or to model complicated densities from which data points are assumed to be generated from. The key insight is to use determinants of submatrices of a kernel matrix as a measure of how close together a set of points are. We explore some theoretical properties of the model and derive a natural Gibbs based algorithm with MCMC hyperparameter learning. The model is implemented on a variety of synthetic and real world data sets.

</details>

<details>

<summary>2013-09-26 12:51:22 - Speedy Model Selection (SMS) for Copula Models</summary>

- *Yaniv Tenzer, Gal Elidan*

- `1309.6867v1` - [abs](http://arxiv.org/abs/1309.6867v1) - [pdf](http://arxiv.org/pdf/1309.6867v1)

> We tackle the challenge of efficiently learning the structure of expressive multivariate real-valued densities of copula graphical models. We start by theoretically substantiating the conjecture that for many copula families the magnitude of Spearman's rank correlation coefficient is monotone in the expected contribution of an edge in network, namely the negative copula entropy. We then build on this theory and suggest a novel Bayesian approach that makes use of a prior over values of Spearman's rho for learning copula-based models that involve a mix of copula families. We demonstrate the generalization effectiveness of our highly efficient approach on sizable and varied real-life datasets.

</details>

<details>

<summary>2013-09-26 14:11:31 - Hellinger Distance and Bayesian Non-Parametrics: Hierarchical Models for Robust and Efficient Bayesian Inference</summary>

- *Yuefeng Wu, Giles Hooker*

- `1309.6906v1` - [abs](http://arxiv.org/abs/1309.6906v1) - [pdf](http://arxiv.org/pdf/1309.6906v1)

> This paper introduces a hierarchical framework to incorporate Hellinger distance methods into Bayesian analysis. We propose to modify a prior over non-parametric densities with the exponential of twice the Hellinger distance between a candidate and a parametric density. By incorporating a prior over the parameters of the second density, we arrive at a hierarchical model in which a non-parametric model is placed between parameters and the data. The parameters of the family can then be estimated as hyperparameters in the model. In frequentist estimation, minimizing the Hellinger distance between a kernel density estimate and a parametric family has been shown to produce estimators that are both robust to outliers and statistically efficient when the parametric model is correct. In this paper, we demonstrate that the same results are applicable when a non-parametric Bayes density estimate replaces the kernel density estimate. We then demonstrate that robustness and efficiency also hold for the proposed hierarchical model. The finite-sample behavior of the resulting estimates is investigated by simulation and on real world data.

</details>

<details>

<summary>2013-09-27 15:52:04 - Retrospective-prospective symmetry in the likelihood and Bayesian analysis of case-control studies</summary>

- *Simon P. J. Byrne, A. Philip Dawid*

- `1202.2683v4` - [abs](http://arxiv.org/abs/1202.2683v4) - [pdf](http://arxiv.org/pdf/1202.2683v4)

> Prentice & Pyke (1979) established that the maximum likelihood estimate of an odds-ratio in a case-control study is the same as would be found by running a logistic regression: in other words, for this specific target the incorrect prospective model is inferentially equivalent to the correct retrospective model. Similar results have been obtained for other models, and conditions have also been identified under which the corresponding Bayesian property holds, namely that the posterior distribution of the odds-ratio be the same, whether computed using the prospective or the retrospective likelihood. Here we demonstrate how these results follow directly from certain parameter independence properties of the models and priors, and identify prior laws that support such reverse analysis, for both standard and stratified designs.

</details>

<details>

<summary>2013-09-27 17:53:57 - Bayesian Inference in Sparse Gaussian Graphical Models</summary>

- *Peter Orchard, Felix Agakov, Amos Storkey*

- `1309.7311v1` - [abs](http://arxiv.org/abs/1309.7311v1) - [pdf](http://arxiv.org/pdf/1309.7311v1)

> One of the fundamental tasks of science is to find explainable relationships between observed phenomena. One approach to this task that has received attention in recent years is based on probabilistic graphical modelling with sparsity constraints on model structures. In this paper, we describe two new approaches to Bayesian inference of sparse structures of Gaussian graphical models (GGMs). One is based on a simple modification of the cutting-edge block Gibbs sampler for sparse GGMs, which results in significant computational gains in high dimensions. The other method is based on a specific construction of the Hamiltonian Monte Carlo sampler, which results in further significant improvements. We compare our fully Bayesian approaches with the popular regularisation-based graphical LASSO, and demonstrate significant advantages of the Bayesian treatment under the same computing costs. We apply the methods to a broad range of simulated data sets, and a real-life financial data set.

</details>

<details>

<summary>2013-09-28 16:46:31 - Confidence-based Optimization for the Newsvendor Problem</summary>

- *Roberto Rossi, Steven Prestwich, S. Armagan Tarim, Brahim Hnich*

- `1207.5781v4` - [abs](http://arxiv.org/abs/1207.5781v4) - [pdf](http://arxiv.org/pdf/1207.5781v4)

> We introduce a novel strategy to address the issue of demand estimation in single-item single-period stochastic inventory optimisation problems. Our strategy analytically combines confidence interval analysis and inventory optimisation. We assume that the decision maker is given a set of past demand samples and we employ confidence interval analysis in order to identify a range of candidate order quantities that, with prescribed confidence probability, includes the real optimal order quantity for the underlying stochastic demand process with unknown stationary parameter(s). In addition, for each candidate order quantity that is identified, our approach can produce an upper and a lower bound for the associated cost. We apply our novel approach to three demand distribution in the exponential family: binomial, Poisson, and exponential. For two of these distributions we also discuss the extension to the case of unobserved lost sales. Numerical examples are presented in which we show how our approach complements existing frequentist - e.g. based on maximum likelihood estimators - or Bayesian strategies.

</details>

<details>

<summary>2013-09-28 20:41:35 - A Model Explaining Correlation Between Observed Values in Contingency Tables</summary>

- *Abhik Ghosh, Samit Roy, Sujatro Chaklader*

- `1309.7501v1` - [abs](http://arxiv.org/abs/1309.7501v1) - [pdf](http://arxiv.org/pdf/1309.7501v1)

> In this article, a model is proposed using Bayesian techniques to account for the high correlation between many observed set of contingency tables. In many real life data this high correlation is encountered. Simulation studies are also given to check the effectiveness of this model.

</details>

<details>

<summary>2013-09-29 15:56:47 - Fast Marginalized Block Sparse Bayesian Learning Algorithm</summary>

- *Benyuan Liu, Zhilin Zhang, Hongqi Fan, Qiang Fu*

- `1211.4909v7` - [abs](http://arxiv.org/abs/1211.4909v7) - [pdf](http://arxiv.org/pdf/1211.4909v7)

> The performance of sparse signal recovery from noise corrupted, underdetermined measurements can be improved if both sparsity and correlation structure of signals are exploited. One typical correlation structure is the intra-block correlation in block sparse signals. To exploit this structure, a framework, called block sparse Bayesian learning (BSBL), has been proposed recently. Algorithms derived from this framework showed superior performance but they are not very fast, which limits their applications. This work derives an efficient algorithm from this framework, using a marginalized likelihood maximization method. Compared to existing BSBL algorithms, it has close recovery performance but is much faster. Therefore, it is more suitable for large scale datasets and applications requiring real-time implementation.

</details>

<details>

<summary>2013-09-30 20:14:10 - Changing the paradigm of fixed significance levels: Testing Hypothesis by Minimizing Sum of Errors Type I and Type II</summary>

- *Luis Pericchi, Carlos Pereira*

- `1310.0039v1` - [abs](http://arxiv.org/abs/1310.0039v1) - [pdf](http://arxiv.org/pdf/1310.0039v1)

> Our purpose, is to put forward a change in the paradigm of testing by generalizing a very natural idea exposed by Morris DeGroot (1975) aiming to an approach that is attractive to all schools of statistics, in a procedure better suited for the needs of science. DeGroot's seminal idea is to base testing statistical hypothesis on minimizing the weighted sum of type I plus type II error instead of of the prevailing paradigm which is fixing type I error and minimizing type II error. DeGroot's result is that in simple vs simple hypothesis the optimal criterion is to reject, according to the likelihood ratio as the evidence (ordering) statistics using a fixed threshold value, instead of a fixed tail probability. By defining expected type I and type II errors, we generalize DeGroot's approach and find that the optimal region is defined by the ratio of evidences, that is, averaged likelihoods (with respect to a prior measure) and a threshold fixed. This approach yields an optimal theory in complete generality, which the Classical Theory of Testing does not. This can be seen as a Bayes-Non-Bayes compromise: the criteria (weighted sum of type I and type II errors) is Frequentist, but the test criterion is the ratio of marginalized likelihood, which is Bayesian. We give arguments, to push the theory still further, so that the weighting measures (priors)of the likelihoods does not have to be proper and highly informative, but just predictively matched, that is that predictively matched priors, give rise to the same evidence (marginal likelihoods) using minimal (smallest) training samples. The theory that emerges, similar to the theories based on Objective Bayes approaches, is a powerful response to criticisms of the prevailing approach of hypothesis testing, see for example Ioannidis (2005) and Siegfried (2010) among many others.

</details>


## 2013-10

<details>

<summary>2013-10-01 07:33:31 - Cluster and Feature Modeling from Combinatorial Stochastic Processes</summary>

- *Tamara Broderick, Michael I. Jordan, Jim Pitman*

- `1206.5862v2` - [abs](http://arxiv.org/abs/1206.5862v2) - [pdf](http://arxiv.org/pdf/1206.5862v2)

> One of the focal points of the modern literature on Bayesian nonparametrics has been the problem of clustering, or partitioning, where each data point is modeled as being associated with one and only one of some collection of groups called clusters or partition blocks. Underlying these Bayesian nonparametric models are a set of interrelated stochastic processes, most notably the Dirichlet process and the Chinese restaurant process. In this paper we provide a formal development of an analogous problem, called feature modeling, for associating data points with arbitrary nonnegative integer numbers of groups, now called features or topics. We review the existing combinatorial stochastic process representations for the clustering problem and develop analogous representations for the feature modeling problem. These representations include the beta process and the Indian buffet process as well as new representations that provide insight into the connections between these processes. We thereby bring the same level of completeness to the treatment of Bayesian nonparametric feature modeling that has previously been achieved for Bayesian nonparametric clustering.

</details>

<details>

<summary>2013-10-01 10:25:11 - A class of fast exact Bayesian filters in dynamical models with jumps</summary>

- *Yohan Petetin, FranÃ§ois Desbouvries*

- `1310.0226v1` - [abs](http://arxiv.org/abs/1310.0226v1) - [pdf](http://arxiv.org/pdf/1310.0226v1)

> In this paper, we focus on the statistical filtering problem in dynamical models with jumps. When a particular application relies on physical properties which are modeled by linear and Gaussian probability density functions with jumps, an usualmethod consists in approximating the optimal Bayesian estimate (in the sense of the Minimum Mean Square Error (MMSE)) in a linear and Gaussian Jump Markov State Space System (JMSS). Practical solutions include algorithms based on numerical approximations or based on Sequential Monte Carlo (SMC) methods. In this paper, we propose a class of alternative methods which consists in building statistical models which share the same physical properties of interest but in which the computation of the optimal MMSE estimate can be done at a computational cost which is linear in the number of observations.

</details>

<details>

<summary>2013-10-01 16:28:59 - Joint Bayesian estimation of close subspaces from noisy measurements</summary>

- *Olivier Besson, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1310.0376v1` - [abs](http://arxiv.org/abs/1310.0376v1) - [pdf](http://arxiv.org/pdf/1310.0376v1)

> In this letter, we consider two sets of observations defined as subspace signals embedded in noise and we wish to analyze the distance between these two subspaces. The latter entails evaluating the angles between the subspaces, an issue reminiscent of the well-known Procrustes problem. A Bayesian approach is investigated where the subspaces of interest are considered as random with a joint prior distribution (namely a Bingham distribution), which allows the closeness of the two subspaces to be adjusted. Within this framework, the minimum mean-square distance estimator of both subspaces is formulated and implemented via a Gibbs sampler. A simpler scheme based on alternative maximum a posteriori estimation is also presented. The new schemes are shown to provide more accurate estimates of the angles between the subspaces, compared to singular value decomposition based independent estimation of the two subspaces.

</details>

<details>

<summary>2013-10-02 07:03:54 - MCMC for Normalized Random Measure Mixture Models</summary>

- *Stefano Favaro, Yee Whye Teh*

- `1310.0595v1` - [abs](http://arxiv.org/abs/1310.0595v1) - [pdf](http://arxiv.org/pdf/1310.0595v1)

> This paper concerns the use of Markov chain Monte Carlo methods for posterior sampling in Bayesian nonparametric mixture models with normalized random measure priors. Making use of some recent posterior characterizations for the class of normalized random measures, we propose novel Markov chain Monte Carlo methods of both marginal type and conditional type. The proposed marginal samplers are generalizations of Neal's well-regarded Algorithm 8 for Dirichlet process mixture models, whereas the conditional sampler is a variation of those recently introduced in the literature. For both the marginal and conditional methods, we consider as a running example a mixture model with an underlying normalized generalized Gamma process prior, and describe comparative simulation results demonstrating the efficacies of the proposed methods.

</details>

<details>

<summary>2013-10-02 09:21:28 - Conflict Diagnostics in Directed Acyclic Graphs, with Applications in Bayesian Evidence Synthesis</summary>

- *Anne M. Presanis, David Ohlssen, David J. Spiegelhalter, Daniela De Angelis*

- `1310.0628v1` - [abs](http://arxiv.org/abs/1310.0628v1) - [pdf](http://arxiv.org/pdf/1310.0628v1)

> Complex stochastic models represented by directed acyclic graphs (DAGs) are increasingly employed to synthesise multiple, imperfect and disparate sources of evidence, to estimate quantities that are difficult to measure directly. The various data sources are dependent on shared parameters and hence have the potential to conflict with each other, as well as with the model. In a Bayesian framework, the model consists of three components: the prior distribution, the assumed form of the likelihood and structural assumptions. Any of these components may be incompatible with the observed data. The detection and quantification of such conflict and of data sources that are inconsistent with each other is therefore a crucial component of the model criticism process. We first review Bayesian model criticism, with a focus on conflict detection, before describing a general diagnostic for detecting and quantifying conflict between the evidence in different partitions of a DAG. The diagnostic is a p-value based on splitting the information contributing to inference about a "separator" node or group of nodes into two independent groups and testing whether the two groups result in the same inference about the separator node(s). We illustrate the method with three comprehensive examples: an evidence synthesis to estimate HIV prevalence; an evidence synthesis to estimate influenza case-severity; and a hierarchical growth model for rat weights.

</details>

<details>

<summary>2013-10-02 11:07:59 - The Whetstone and the Alum Block: Balanced Objective Bayesian Comparison of Nested Models for Discrete Data</summary>

- *Guido Consonni, Jonathan J. Forster, Luca La Rocca*

- `1310.0661v1` - [abs](http://arxiv.org/abs/1310.0661v1) - [pdf](http://arxiv.org/pdf/1310.0661v1)

> When two nested models are compared, using a Bayes factor, from an objective standpoint, two seemingly conflicting issues emerge at the time of choosing parameter priors under the two models. On the one hand, for moderate sample sizes, the evidence in favor of the smaller model can be inflated by diffuseness of the prior under the larger model. On the other hand, asymptotically, the evidence in favor of the smaller model typically accumulates at a slower rate. With reference to finitely discrete data models, we show that these two issues can be dealt with jointly, by combining intrinsic priors and nonlocal priors in a new unified class of priors. We illustrate our ideas in a running Bernoulli example, then we apply them to test the equality of two proportions, and finally we deal with the more general case of logistic regression models.

</details>

<details>

<summary>2013-10-03 00:48:53 - Prior-free and prior-dependent regret bounds for Thompson Sampling</summary>

- *SÃ©bastien Bubeck, Che-Yu Liu*

- `1304.5758v2` - [abs](http://arxiv.org/abs/1304.5758v2) - [pdf](http://arxiv.org/pdf/1304.5758v2)

> We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by $\frac{1}{20} \sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.

</details>

<details>

<summary>2013-10-03 07:12:57 - When to Bite the Bullet? - A Study of Optimal Strategies for Reducing Global Warming</summary>

- *X. Luo, P. V. Shevchenko*

- `1310.0912v1` - [abs](http://arxiv.org/abs/1310.0912v1) - [pdf](http://arxiv.org/pdf/1310.0912v1)

> This work is based on the framework proposed by Conrad (1997) to determine the optimal timing of an investment or policy to slow global warming. While Conrad formulated the problem as a stopping rule option pricing model, we treat the policy decision by considering the total damage function that enables us to make some interesting extensions to the original formulation. We show that Conrad's framework is equivalent to minmization of the expected value of the damage function under the stochastic optimal stopping rule. We extend Conrad's model by allowing for policy cost to grow with time. In addition to closed form solution, we also perform Monte Carlo simulations to find the distribution for the total damage and show that at higher quantiles the damage may become too large and so is the risk on the global economy. We also show that the decision to take action largely depends on the cost of the action. For example, in the case of model parameters calibrated as in Conrad (1997) with a constant cost, there is a rather long wait before the action is expected to be taken, but if the cost increases with the same rate as the global economy growth, then action has to be taken immediately to minimize the damage.

</details>

<details>

<summary>2013-10-03 22:01:11 - Bayesian sparse graphical models and their mixtures using lasso selection priors</summary>

- *Rajesh Talluri, Veerabhadran Baladandayuthapani, Bani K. Mallick*

- `1310.1127v1` - [abs](http://arxiv.org/abs/1310.1127v1) - [pdf](http://arxiv.org/pdf/1310.1127v1)

> We propose Bayesian methods for Gaussian graphical models that lead to sparse and adaptively shrunk estimators of the precision (inverse covariance) matrix. Our methods are based on lasso-type regularization priors leading to parsimonious parameterization of the precision matrix, which is essential in several applications involving learning relationships among the variables. In this context, we introduce a novel type of selection prior that develops a sparse structure on the precision matrix by making most of the elements exactly zero, in addition to ensuring positive definiteness -- thus conducting model selection and estimation simultaneously. We extend these methods to finite and infinite mixtures of Gaussian graphical models for clustered data using Dirichlet process priors. We discuss appropriate posterior simulation schemes to implement posterior inference in the proposed models, including the evaluation of normalizing constants that are functions of parameters of interest which result from the restrictions on the correlation matrix. We evaluate the operating characteristics of our method via several simulations and in application to real data sets.

</details>

<details>

<summary>2013-10-04 07:29:08 - Labeled Directed Acyclic Graphs: a generalization of context-specific independence in directed graphical models</summary>

- *Johan Pensar, Henrik Nyman, Timo Koski, Jukka Corander*

- `1310.1187v1` - [abs](http://arxiv.org/abs/1310.1187v1) - [pdf](http://arxiv.org/pdf/1310.1187v1)

> We introduce a novel class of labeled directed acyclic graph (LDAG) models for finite sets of discrete variables. LDAGs generalize earlier proposals for allowing local structures in the conditional probability distribution of a node, such that unrestricted label sets determine which edges can be deleted from the underlying directed acyclic graph (DAG) for a given context. Several properties of these models are derived, including a generalization of the concept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is enabled by introducing an LDAG-based factorization of the Dirichlet prior for the model parameters, such that the marginal likelihood can be calculated analytically. In addition, we develop a novel prior distribution for the model structures that can appropriately penalize a model for its labeling complexity. A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill climbing approach is used for illustrating the useful properties of LDAG models for both real and synthetic data sets.

</details>

<details>

<summary>2013-10-04 20:19:56 - Sequential Monte Carlo Bandits</summary>

- *Michael Cherkassky, Luke Bornn*

- `1310.1404v1` - [abs](http://arxiv.org/abs/1310.1404v1) - [pdf](http://arxiv.org/pdf/1310.1404v1)

> In this paper we propose a flexible and efficient framework for handling multi-armed bandits, combining sequential Monte Carlo algorithms with hierarchical Bayesian modeling techniques. The framework naturally encompasses restless bandits, contextual bandits, and other bandit variants under a single inferential model. Despite the model's generality, we propose efficient Monte Carlo algorithms to make inference scalable, based on recent developments in sequential Monte Carlo methods. Through two simulation studies, the framework is shown to outperform other empirical methods, while also naturally scaling to more complex problems for which existing approaches can not cope. Additionally, we successfully apply our framework to online video-based advertising recommendation, and show its increased efficacy as compared to current state of the art bandit algorithms.

</details>

<details>

<summary>2013-10-07 10:57:33 - Laplace approximation for logistic Gaussian process density estimation and regression</summary>

- *Jaakko RiihimÃ¤ki, Aki Vehtari*

- `1211.0174v3` - [abs](http://arxiv.org/abs/1211.0174v3) - [pdf](http://arxiv.org/pdf/1211.0174v3)

> Logistic Gaussian process (LGP) priors provide a flexible alternative for modelling unknown densities. The smoothness properties of the density estimates can be controlled through the prior covariance structure of the LGP, but the challenge is the analytically intractable inference. In this paper, we present approximate Bayesian inference for LGP density estimation in a grid using Laplace's method to integrate over the non-Gaussian posterior distribution of latent function values and to determine the covariance function parameters with type-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace's method with MAP is sufficiently fast for practical interactive visualisation of 1D and 2D densities. Our experiments with simulated and real 1D data sets show that the estimation accuracy is close to a Markov chain Monte Carlo approximation and state-of-the-art hierarchical infinite Gaussian mixture models. We also construct a reduced-rank approximation to speed up the computations for dense 2D grids, and demonstrate density regression with the proposed Laplace approach.

</details>

<details>

<summary>2013-10-07 12:26:47 - Making Consensus Tractable</summary>

- *Elchanan Mossel, Omer Tamuz*

- `1007.0959v3` - [abs](http://arxiv.org/abs/1007.0959v3) - [pdf](http://arxiv.org/pdf/1007.0959v3)

> We study a model of consensus decision making, in which a finite group of Bayesian agents has to choose between one of two courses of action. Each member of the group has a private and independent signal at his or her disposal, giving some indication as to which action is optimal. To come to a common decision, the participants perform repeated rounds of voting. In each round, each agent casts a vote in favor of one of the two courses of action, reflecting his or her current belief, and observes the votes of the rest.   We provide an efficient algorithm for the calculation the agents have to perform, and show that consensus is always reached and that the probability of reaching a wrong decision decays exponentially with the number of agents.

</details>

<details>

<summary>2013-10-07 20:43:16 - Bayesian Optimization With Censored Response Data</summary>

- *Frank Hutter, Holger Hoos, Kevin Leyton-Brown*

- `1310.1947v1` - [abs](http://arxiv.org/abs/1310.1947v1) - [pdf](http://arxiv.org/pdf/1310.1947v1)

> Bayesian optimization (BO) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available. Here, we address the problem of BO under partially right-censored response data, where in some evaluations we only obtain a lower bound on the function value. The ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value. One important application giving rise to such censored data is the runtime-minimizing variant of the algorithm configuration problem: finding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution. We demonstrate that terminating slow algorithm runs prematurely and handling the resulting right-censored observations can substantially improve the state of the art in model-based algorithm configuration.

</details>

<details>

<summary>2013-10-09 09:23:10 - Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</summary>

- *Jun Zhu, Xun Zheng, Bo Zhang*

- `1310.2408v1` - [abs](http://arxiv.org/abs/1310.2408v1) - [pdf](http://arxiv.org/pdf/1310.2408v1)

> Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.

</details>

<details>

<summary>2013-10-09 09:32:56 - Discriminative Relational Topic Models</summary>

- *Ning Chen, Jun Zhu, Fei Xia, Bo Zhang*

- `1310.2409v1` - [abs](http://arxiv.org/abs/1310.2409v1) - [pdf](http://arxiv.org/pdf/1310.2409v1)

> Many scientific and engineering fields involve analyzing network data. For document networks, relational topic models (RTMs) provide a probabilistic generative process to describe both the link structure and document contents, and they have shown promise on predicting network structures and discovering latent topic representations. However, existing RTMs have limitations in both the restricted model expressiveness and incapability of dealing with imbalanced network data. To expand the scope and improve the inference accuracy of RTMs, this paper presents three extensions: 1) unlike the common link likelihood with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference (RegBayes) with a regularization parameter to deal with the imbalanced link structure issue in common real networks and improve the discriminative ability of learned latent representations; and 3) instead of doing variational approximation with strict mean-field assumptions, we present collapsed Gibbs sampling algorithms for the generalized relational topic models by exploring data augmentation without making restricting assumptions. Under the generic RegBayes framework, we carefully investigate two popular discriminative loss functions, namely, the logistic log-loss and the max-margin hinge loss. Experimental results on several real network datasets demonstrate the significance of these extensions on improving the prediction performance, and the time efficiency can be dramatically improved with a simple fast approximation method.

</details>

<details>

<summary>2013-10-10 07:44:15 - MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster</summary>

- *S. L. Cotter, G. O. Roberts, A. M. Stuart, D. White*

- `1202.0709v3` - [abs](http://arxiv.org/abs/1202.0709v3) - [pdf](http://arxiv.org/pdf/1202.0709v3)

> Many problems arising in applications result in the need to probe a probability distribution for functions. Examples include Bayesian nonparametric statistics and conditioned diffusion processes. Standard MCMC algorithms typically become arbitrarily slow under the mesh refinement dictated by nonparametric description of the unknown function. We describe an approach to modifying a whole range of MCMC methods, applicable whenever the target measure has density with respect to a Gaussian process or Gaussian random field reference measure, which ensures that their speed of convergence is robust under mesh refinement. Gaussian processes or random fields are fields whose marginal distributions, when evaluated at any finite set of $N$ points, are $\mathbb{R}^N$-valued Gaussians. The algorithmic approach that we describe is applicable not only when the desired probability measure has density with respect to a Gaussian process or Gaussian random field reference measure, but also to some useful non-Gaussian reference measures constructed through random truncation. In the applications of interest the data is often sparse and the prior specification is an essential part of the overall modelling strategy. These Gaussian-based reference measures are a very flexible modelling tool, finding wide-ranging application. Examples are shown in density estimation, data assimilation in fluid mechanics, subsurface geophysics and image registration. The key design principle is to formulate the MCMC method so that it is, in principle, applicable for functions; this may be achieved by use of proposals based on carefully chosen time-discretizations of stochastic dynamical systems which exactly preserve the Gaussian reference measure. Taking this approach leads to many new algorithms which can be implemented via minor modification of existing algorithms, yet which show enormous speed-up on a wide range of applied problems.

</details>

<details>

<summary>2013-10-10 13:47:40 - Gibbs Max-margin Topic Models with Data Augmentation</summary>

- *Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang*

- `1310.2816v1` - [abs](http://arxiv.org/abs/1310.2816v1) - [pdf](http://arxiv.org/pdf/1310.2816v1)

> Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems. Furthermore, each step of the "augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors on binary, multi-class and multi-label classification tasks.

</details>

<details>

<summary>2013-10-11 08:14:52 - Two discussions of the paper "Bayesian measures of model complexity and fit" by D. Spiegelhalter et al., Read before The Royal Statistical Society at a meeting organized by the Research Section on Wednesday, March 13th, 2002</summary>

- *E. Moreno, F. -J. Vazquez-Polo, C. P. Robert*

- `1310.2905v2` - [abs](http://arxiv.org/abs/1310.2905v2) - [pdf](http://arxiv.org/pdf/1310.2905v2)

> These are the written discussions of the paper "Bayesian measures of model complexity and fit" by D. Spiegelhalter et al. (2002), following the discussions given at the Annual Meeting of the Royal Statistical Society in Newcastle-upon-Tyne on September 3rd, 2013.

</details>

<details>

<summary>2013-10-11 13:56:24 - A generalized Multiple-try Metropolis version of the Reversible Jump algorithm</summary>

- *S. Pandolfi, F. Bartolucci, N. Friel*

- `1006.0621v2` - [abs](http://arxiv.org/abs/1006.0621v2) - [pdf](http://arxiv.org/pdf/1006.0621v2)

> The Reversible Jump algorithm is one of the most widely used Markov chain Monte Carlo algorithms for Bayesian estimation and model selection. A generalized multiple-try version of this algorithm is proposed. The algorithm is based on drawing several proposals at each step and randomly choosing one of them on the basis of weights (selection probabilities) that may be arbitrary chosen. Among the possible choices, a method is employed which is based on selection probabilities depending on a quadratic approximation of the posterior distribution. Moreover, the implementation of the proposed algorithm for challenging model selection problems, in which the quadratic approximation is not feasible, is considered. The resulting algorithm leads to a gain in efficiency with respect to the Reversible Jump algorithm, and also in terms of computational effort. The performance of this approach is illustrated for real examples involving a logistic regression model and a latent class model.

</details>

<details>

<summary>2013-10-11 17:43:48 - A simple hierarchical Bayesian model for simultaneous inference of tournament graphs and informant error</summary>

- *Ben Hanowell*

- `1304.7817v4` - [abs](http://arxiv.org/abs/1304.7817v4) - [pdf](http://arxiv.org/pdf/1304.7817v4)

> The paper presents a hierarchical Bayesian model for simultaneous inference of tournament graphs and informant error. From multiple informant reports or measurement instrument outputs, the model estimates the structure of a criterion (i.e., true) tournament graph with possibly tied outcomes. Tournament graphs with possibly tied outcomes are graphs in which there are three possible states for each unordered pair of graph nodes: node i wins and node j loses; node j wins and node i loses; neither node wins (i.e., tied outcome). The model also estimates the rates at which individual informants (or instruments) mistake the winning and losing dyad members, falsely report a tied outcome, and falsely report a decisive outcome. The model was developed to infer social dominance structure from multiple informants' reports, but is potentially useful for inferring any structure that can be characterized by a tournament graph, and which is measured from multiple reports.

</details>

<details>

<summary>2013-10-11 20:32:00 - Quickest Change-Point Detection: A Bird's Eye View</summary>

- *Aleksey S. Polunchenko, Grigory Sokolov, Wenyu Du*

- `1310.3285v1` - [abs](http://arxiv.org/abs/1310.3285v1) - [pdf](http://arxiv.org/pdf/1310.3285v1)

> We provide a bird's eye view onto the area of sequential change-point detection. We focus on the discrete-time case with known pre- and post-change data distributions and offer a summary of the forefront asymptotic results established in each of the four major formulations of the underlying optimization problem: Bayesian, generalized Bayesian, minimax, and multi-cyclic.

</details>

<details>

<summary>2013-10-13 01:01:39 - Negative Binomial Process Count and Mixture Modeling</summary>

- *Mingyuan Zhou, Lawrence Carin*

- `1209.3442v3` - [abs](http://arxiv.org/abs/1209.3442v3) - [pdf](http://arxiv.org/pdf/1209.3442v3)

> The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural and computational advantages of the NB process. A variety of NB processes, including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters.

</details>

<details>

<summary>2013-10-14 12:38:08 - Approximate inference via variational sampling</summary>

- *Alexis Roche*

- `1105.1508v6` - [abs](http://arxiv.org/abs/1105.1508v6) - [pdf](http://arxiv.org/pdf/1105.1508v6)

> A new method called "variational sampling" is proposed to estimate integrals under probability distributions that can be evaluated up to a normalizing constant. The key idea is to fit the target distribution with an exponential family model by minimizing a strongly consistent empirical approximation to the Kullback-Leibler divergence computed using either deterministic or random sampling. It is shown how variational sampling differs conceptually from both quadrature and importance sampling and established that, in the case of random independence sampling, it may have much faster stochastic convergence than importance sampling under mild conditions. The variational sampling implementation presented in this paper requires a rough initial approximation to the target distribution, which may be found, e.g. using the Laplace method, and is shown to then have the potential to substantially improve over several existing approximate inference techniques to estimate moments of order up to two of nearly-Gaussian distributions, which occur frequently in Bayesian analysis. In particular, an application of variational sampling to Bayesian logistic regression in moderate dimension is presented.

</details>

<details>

<summary>2013-10-15 20:41:40 - Bayesian Low Rank and Sparse Covariance Matrix Decomposition</summary>

- *Lin Zhang, Abhra Sarkar, Bani K. Mallick*

- `1310.4195v1` - [abs](http://arxiv.org/abs/1310.4195v1) - [pdf](http://arxiv.org/pdf/1310.4195v1)

> We consider the problem of estimating high-dimensional covariance matrices of a particular structure, which is a summation of low rank and sparse matrices. This covariance structure has a wide range of applications including factor analysis and random effects models. We propose a Bayesian method of estimating the covariance matrices by representing the covariance model in the form of a factor model with unknown number of latent factors. We introduce binary indicators for factor selection and rank estimation for the low rank component combined with a Bayesian lasso method for the sparse component estimation. Simulation studies show that our method can recover the rank as well as the sparsity of the two components respectively. We further extend our method to a graphical factor model where the graphical model of the residuals as well as selecting the number of factors is of interest. We employ a hyper-inverse Wishart prior for modeling decomposable graphs of the residuals, and a Bayesian graphical lasso selection method for unrestricted graphs. We show through simulations that the extended models can recover both the number of latent factors and the graphical model of the residuals successfully when the sample size is sufficient relative to the dimension.

</details>

<details>

<summary>2013-10-16 07:04:04 - Supervised Heterogeneous Multiview Learning for Joint Association Study and Disease Diagnosis</summary>

- *Shandian Zhe, Zenglin Xu, Yuan Qi*

- `1304.7284v2` - [abs](http://arxiv.org/abs/1304.7284v2) - [pdf](http://arxiv.org/pdf/1304.7284v2)

> Given genetic variations and various phenotypical traits, such as Magnetic Resonance Imaging (MRI) features, we consider two important and related tasks in biomedical research: i)to select genetic and phenotypical markers for disease diagnosis and ii) to identify associations between genetic and phenotypical data. These two tasks are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. While a variety of sparse models have been applied for disease diagnosis and canonical correlation analysis and its extensions have bee widely used in association studies (e.g., eQTL analysis), these two tasks have been treated separately. To unify these two tasks, we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels based on Gaussian process ordinal regression; in return, the disease status is used to guide the discovery of relationships between the data sources. The sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. To learn the model from data, we develop an efficient variational expectation maximization algorithm. Simulation results demonstrate that our approach achieves higher accuracy in both predicting ordinal labels and discovering associations between data sources than alternative methods. We apply our approach to an imaging genetics dataset for the study of Alzheimer's Disease (AD). Our method identifies biologically meaningful relationships between genetic variations, MRI features, and AD status, and achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods.

</details>

<details>

<summary>2013-10-16 13:13:45 - Bayesian Information Sharing Between Noise And Regression Models Improves Prediction of Weak Effects</summary>

- *Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J Kangas, Pasi Soininen, Marjo-Riitta JÃ¤rvelin, Mika Ala-Korpela, Samuel Kaski*

- `1310.4362v1` - [abs](http://arxiv.org/abs/1310.4362v1) - [pdf](http://arxiv.org/pdf/1310.4362v1)

> We consider the prediction of weak effects in a multiple-output regression setup, when covariates are expected to explain a small amount, less than $\approx 1%$, of the variance of the target variables. To facilitate the prediction of the weak effects, we constrain our model structure by introducing a novel Bayesian approach of sharing information between the regression model and the noise model. Further reduction of the effective number of parameters is achieved by introducing an infinite shrinkage prior and group sparsity in the context of the Bayesian reduced rank regression, and using the Bayesian infinite factor model as a flexible low-rank noise model. In our experiments the model incorporating the novelties outperformed alternatives in genomic prediction of rich phenotype data. In particular, the information sharing between the noise and regression models led to significant improvement in prediction accuracy.

</details>

<details>

<summary>2013-10-17 11:09:19 - Thermodynamic assessment of probability distribution divergencies and Bayesian model comparison</summary>

- *Silia Vitoratou, Ioannis Ntzoufras*

- `1308.6753v2` - [abs](http://arxiv.org/abs/1308.6753v2) - [pdf](http://arxiv.org/pdf/1308.6753v2)

> Within path sampling framework, we show that probability distribution divergences, such as the Chernoff information, can be estimated via thermodynamic integration. The Boltzmann-Gibbs distribution pertaining to different Hamiltonians is implemented to derive tempered transitions along the path, linking the distributions of interest at the endpoints. Under this perspective, a geometric approach is feasible, which prompts intuition and facilitates tuning the error sources. Additionally, there are direct applications in Bayesian model evaluation. Existing marginal likelihood and Bayes factor estimators are reviewed here along with their stepping-stone sampling analogues. New estimators are presented and the use of compound paths is introduced.

</details>

<details>

<summary>2013-10-17 18:38:16 - A latent factor model with a mixture of sparse and dense factors to model gene expression data with confounding effects</summary>

- *Chuan Gao, Christopher D Brown, Barbara E Engelhardt*

- `1310.4792v1` - [abs](http://arxiv.org/abs/1310.4792v1) - [pdf](http://arxiv.org/pdf/1310.4792v1)

> One important problem in genome science is to determine sets of co-regulated genes based on measurements of gene expression levels across samples, where the quantification of expression levels includes substantial technical and biological noise. To address this problem, we developed a Bayesian sparse latent factor model that uses a three parameter beta prior to flexibly model shrinkage in the loading matrix. By applying three layers of shrinkage to the loading matrix (global, factor-specific, and element-wise), this model has non-parametric properties in that it estimates the appropriate number of factors from the data. We added a two-component mixture to model each factor loading as being generated from either a sparse or a dense mixture component; this allows dense factors that capture confounding noise, and sparse factors that capture local gene interactions. We developed two statistics to quantify the stability of the recovered matrices for both sparse and dense matrices. We tested our model on simulated data and found that we successfully recovered the true latent structure as compared to related models. We applied our model to a large gene expression study and found that we recovered known covariates and small groups of co-regulated genes. We validated these gene subsets by testing for associations between genotype data and these latent factors, and we found a substantial number of biologically important genetic regulators for the recovered gene subsets.

</details>

<details>

<summary>2013-10-17 20:03:03 - Fast MCMC sampling for Markov jump processes and extensions</summary>

- *Vinayak Rao, Yee Whye Teh*

- `1208.4818v3` - [abs](http://arxiv.org/abs/1208.4818v3) - [pdf](http://arxiv.org/pdf/1208.4818v3)

> Markov jump processes (or continuous-time Markov chains) are a simple and important class of continuous-time dynamical systems. In this paper, we tackle the problem of simulating from the posterior distribution over paths in these models, given partial and noisy observations. Our approach is an auxiliary variable Gibbs sampler, and is based on the idea of uniformization. This sets up a Markov chain over paths by alternately sampling a finite set of virtual jump times given the current path and then sampling a new path given the set of extant and virtual jump times using a standard hidden Markov model forward filtering-backward sampling algorithm. Our method is exact and does not involve approximations like time-discretization. We demonstrate how our sampler extends naturally to MJP-based models like Markov-modulated Poisson processes and continuous-time Bayesian networks and show significant computational benefits over state-of-the-art MCMC samplers for these models.

</details>

<details>

<summary>2013-10-17 21:31:40 - A Bayesian Nonparametric Meta-Analysis Model</summary>

- *George Karabatsos, Elizabeth Talbott, Stephen G. Walker*

- `1301.7703v3` - [abs](http://arxiv.org/abs/1301.7703v3) - [pdf](http://arxiv.org/pdf/1301.7703v3)

> In a meta-analysis, it is important to specify a model that adequately describes the effect-size distribution of the underlying population of studies. The conventional normal fixed-effect and normal random-effects models assume a normal effect-size population distribution, conditionally on parameters and covariates. For estimating the mean overall effect size, such models may be adequate, but for prediction they surely are not if the effect size distribution exhibits non-normal behavior. To address this issue, we propose a Bayesian nonparametric meta-analysis model, which can describe a wider range of effect-size distributions, including unimodal symmetric distributions, as well as skewed and more multimodal distributions. We demonstrate our model through the analysis of real meta-analytic data arising from behavioral-genetic research. We compare the predictive performance of the Bayesian nonparametric model against various conventional and more modern normal fixed-effects and random-effects models.

</details>

<details>

<summary>2013-10-20 16:58:57 - Bayesian Extensions of Kernel Least Mean Squares</summary>

- *Il Memming Park, Sohan Seth, Steven Van Vaerenbergh*

- `1310.5347v1` - [abs](http://arxiv.org/abs/1310.5347v1) - [pdf](http://arxiv.org/pdf/1310.5347v1)

> The kernel least mean squares (KLMS) algorithm is a computationally efficient nonlinear adaptive filtering method that "kernelizes" the celebrated (linear) least mean squares algorithm. We demonstrate that the least mean squares algorithm is closely related to the Kalman filtering, and thus, the KLMS can be interpreted as an approximate Bayesian filtering method. This allows us to systematically develop extensions of the KLMS by modifying the underlying state-space and observation models. The resulting extensions introduce many desirable properties such as "forgetting", and the ability to learn from discrete data, while retaining the computational simplicity and time complexity of the original algorithm.

</details>

<details>

<summary>2013-10-22 08:25:52 - PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification</summary>

- *Emilie Morvant, Sokol KoÃ§o, Liva Ralaivola*

- `1202.6228v6` - [abs](http://arxiv.org/abs/1202.6228v6) - [pdf](http://arxiv.org/pdf/1202.6228v6)

> In this work, we propose a PAC-Bayes bound for the generalization risk of the Gibbs classifier in the multi-class classification framework. The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate. Thanks to very recent and beautiful results on matrix concentration inequalities, we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper-bounded by its empirical risk plus a term depending on the number of training examples in each class. To the best of our knowledge, this is the first PAC-Bayes bounds based on confusion matrices.

</details>

<details>

<summary>2013-10-22 13:39:51 - Inducing Honest Reporting Without Observing Outcomes: An Application to the Peer-Review Process</summary>

- *Arthur Carvalho, Stanko Dimitrov, Kate Larson*

- `1309.3197v2` - [abs](http://arxiv.org/abs/1309.3197v2) - [pdf](http://arxiv.org/pdf/1309.3197v2)

> When eliciting opinions from a group of experts, traditional devices used to promote honest reporting assume that there is an observable future outcome. In practice, however, this assumption is not always reasonable. In this paper, we propose a scoring method built on strictly proper scoring rules to induce honest reporting without assuming observable outcomes. Our method provides scores based on pairwise comparisons between the reports made by each pair of experts in the group. For ease of exposition, we introduce our scoring method by illustrating its application to the peer-review process. In order to do so, we start by modeling the peer-review process using a Bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. Thereafter, we introduce our scoring method to evaluate the reported reviews. Under the assumptions that reviewers are Bayesian decision-makers and that they cannot influence the reviews of other reviewers, we show that risk-neutral reviewers strictly maximize their expected scores by honestly disclosing their reviews. We also show how the group's scores can be used to find a consensual review. Experimental results show that encouraging honest reporting through the proposed scoring method creates more accurate reviews than the traditional peer-review process.

</details>

<details>

<summary>2013-10-22 19:58:32 - An optimal three-way stable and monotonic spectrum of bounds on quantiles: a spectrum of coherent measures of financial risk and economic inequality</summary>

- *Iosif Pinelis*

- `1310.6025v1` - [abs](http://arxiv.org/abs/1310.6025v1) - [pdf](http://arxiv.org/pdf/1310.6025v1)

> A certain spectrum, indexed by a\in[0,\infty], of upper bounds P_a(X;x) on the tail probability P(X\geq x), with P_0(X;x)=P(X\geq x) and P_\infty(X;x) being the best possible exponential upper bound on P(X\geq x), is shown to be stable and monotonic in a, x, and X, where x is a real number and X is a random variable. The bounds P_a(X;x) are optimal values in certain minimization problems. The corresponding spectrum, also indexed by a\in[0,\infty], of upper bounds Q_a(X;p) on the (1-p)-quantile of X is stable and monotonic in a, p, and X, with Q_0(X;p) equal the largest (1-p)-quantile of X. In certain sense, the quantile bounds Q_a(X;p) are usually close enough to the true quantiles Q_0(X;p). Moreover, Q_a(X;p) is subadditive in X if a\geq 1, as well as positive-homogeneous and translation-invariant, and thus is a so-called coherent measure of risk. A number of other useful properties of the bounds P_a(X;x) and Q_a(X;p) are established. In particular, quite similarly to the bounds P_a(X;x) on the tail probabilities, the quantile bounds Q_a(X;p) are the optimal values in certain minimization problems. This allows for a comparatively easy incorporation of the bounds P_a(X;x) and Q_a(X;p) into more specialized optimization problems. It is shown that the minimization problems for which P_a(X;x) and Q_a(X;p) are the optimal values are in a certain sense dual to each other; in the case a=\infty this corresponds to the bilinear Legendre--Fenchel duality. In finance, the (1-p)-quantile Q_0(X;p) is known as the value-at-risk (VaR), whereas the value of Q_1(X;p) is known as the conditional value-at-risk (CVaR) and also as the expected shortfall (ES), average value-at-risk (AVaR), and expected tail loss (ETL). It is shown that the quantile bounds Q_a(X;p) can be used as measures of economic inequality. The spectrum parameter, a, may be considered an index of sensitivity to risk/inequality.

</details>

<details>

<summary>2013-10-23 14:24:09 - Besov regularity of functions with sparse random wavelet coefficients</summary>

- *Natalia Bochkina*

- `1310.3720v2` - [abs](http://arxiv.org/abs/1310.3720v2) - [pdf](http://arxiv.org/pdf/1310.3720v2)

> This paper addresses the problem of regularity properties of functions represented as an expansion in a wavelet basis with random coefficients in terms of finiteness of their Besov norm with probability 1. Such representations are used to specify a prior measure in Bayesian nonparametric wavelet regression. Investigating regularity of such functions is an important problem since the support of the posterior measure does not include functions that are not in the support of the prior measure, and therefore determines the functions that are possible to estimate using specified Bayesian model. We consider a more general parametrisation than has been studied previously which allows to study a priori regularity of functions under a wider class of priors. We also emphasise the difference between the abstract stochastic expansions that have been studied in the literature, and the expansions actually arising in nonparametric regression, and show that the latter cover a wider class of functions than the former. We also extend these results to stochastic expansions in an overcomplete wavelet dictionary.

</details>

<details>

<summary>2013-10-24 12:37:54 - Mean Field Bayes Backpropagation: scalable training of multilayer neural networks with binary weights</summary>

- *Daniel Soudry, Ron Meir*

- `1310.1867v4` - [abs](http://arxiv.org/abs/1310.1867v4) - [pdf](http://arxiv.org/pdf/1310.1867v4)

> Significant success has been reported recently using deep neural networks for classification. Such large networks can be computationally intensive, even after training is over. Implementing these trained networks in hardware chips with a limited precision of synaptic weights may improve their speed and energy efficiency by several orders of magnitude, thus enabling their integration into small and low-power electronic devices. With this motivation, we develop a computationally efficient learning algorithm for multilayer neural networks with binary weights, assuming all the hidden neurons have a fan-out of one. This algorithm, derived within a Bayesian probabilistic online setting, is shown to work well for both synthetic and real-world problems, performing comparably to algorithms with real-valued weights, while retaining computational tractability.

</details>

<details>

<summary>2013-10-24 14:15:39 - Active Learning of Linear Embeddings for Gaussian Processes</summary>

- *Roman Garnett, Michael A. Osborne, Philipp Hennig*

- `1310.6740v1` - [abs](http://arxiv.org/abs/1310.6740v1) - [pdf](http://arxiv.org/pdf/1310.6740v1)

> We propose an active learning method for discovering low-dimensional structure in high-dimensional Gaussian process (GP) tasks. Such problems are increasingly frequent and important, but have hitherto presented severe practical difficulties. We further introduce a novel technique for approximately marginalizing GP hyperparameters, yielding marginal predictions robust to hyperparameter mis-specification. Our method offers an efficient means of performing GP regression, quadrature, or Bayesian optimization in high-dimensional spaces.

</details>

<details>

<summary>2013-10-26 15:03:10 - Fast Bayesian parameter estimation for stochastic logistic growth models</summary>

- *Jonathan Heydari, Conor Lawless, David A. Lydall, Darren J. Wilkinson*

- `1310.5524v2` - [abs](http://arxiv.org/abs/1310.5524v2) - [pdf](http://arxiv.org/pdf/1310.5524v2)

> The transition density of a stochastic, logistic population growth model with multiplicative intrinsic noise is analytically intractable. Inferring model parameter values by fitting such stochastic differential equation (SDE) models to data therefore requires relatively slow numerical simulation. Where such simulation is prohibitively slow, an alternative is to use model approximations which do have an analytically tractable transition density, enabling fast inference. We introduce two such approximations, with either multiplicative or additive intrinsic noise, each derived from the linear noise approximation of the logistic growth SDE. After Bayesian inference we find that our fast LNA models, using Kalman filter recursion for computation of marginal likelihoods, give similar posterior distributions to slow arbitrarily exact models. We also demonstrate that simulations from our LNA models better describe the characteristics of the stochastic logistic growth models than a related approach. Finally, we demonstrate that our LNA model with additive intrinsic noise and measurement error best describes an example set of longitudinal observations of microbial population size taken from a typical, genome-wide screening experiment.

</details>

<details>

<summary>2013-10-26 21:35:44 - Bayesian Probabilistic Projection of International Migration Rates</summary>

- *Jonathan J. Azose, Adrian E. Raftery*

- `1310.7148v1` - [abs](http://arxiv.org/abs/1310.7148v1) - [pdf](http://arxiv.org/pdf/1310.7148v1)

> We propose a method for obtaining joint probabilistic projections of migration rates for all countries, broken down by age and sex. Joint trajectories for all countries are constrained to satisfy the requirement of zero global net migration. We evaluate our model using out-of-sample validation and compare point projections to the projected migration rates from a persistence model similar to the method used in the United Nations' World Population Prospects, and also to a state of the art gravity model. We also resolve an apparently paradoxical discrepancy between growth trends in the proportion of the world population migrating and the average absolute migration rate across countries.

</details>

<details>

<summary>2013-10-29 14:26:42 - Efficient and automatic methods for flexible regression on spatiotemporal data, with applications to groundwater monitoring</summary>

- *A. W. Bowman, L. Evers, D. Molinari, W. R. Jones, M. J. Spence*

- `1310.7815v1` - [abs](http://arxiv.org/abs/1310.7815v1) - [pdf](http://arxiv.org/pdf/1310.7815v1)

> Fitting statistical models to spatiotemporal data requires finding the right balance between imposing smoothness and following the data. In the context of p-splines, we propose a Bayesian framework for choosing the smoothing parameter which allows the construction of fully-automatic data-driven methods for fitting flexible models to spatiotemporal data. A computationally efficient implementation, exploiting the sparsity of the arising design and penalty matrices, is proposed. The findings are illustrated using a simulation and two examples, all concerned with the modelling of contaminants in groundwater, which suggest that the proposed strategy is more stable that competing strategies based on the use of criteria such as GCV and AIC.

</details>

<details>

<summary>2013-10-29 16:37:13 - Automatic Classification of Variable Stars in Catalogs with missing data</summary>

- *Karim Pichara, Pavlos Protopapas*

- `1310.7868v1` - [abs](http://arxiv.org/abs/1310.7868v1) - [pdf](http://arxiv.org/pdf/1310.7868v1)

> We present an automatic classification method for astronomical catalogs with missing data. We use Bayesian networks, a probabilistic graphical model, that allows us to perform inference to pre- dict missing values given observed data and dependency relationships between variables. To learn a Bayesian network from incomplete data, we use an iterative algorithm that utilises sampling methods and expectation maximization to estimate the distributions and probabilistic dependencies of variables from data with missing values. To test our model we use three catalogs with missing data (SAGE, 2MASS and UBVI) and one complete catalog (MACHO). We examine how classification accuracy changes when information from missing data catalogs is included, how our method compares to traditional missing data approaches and at what computational cost. Integrating these catalogs with missing data we find that classification of variable objects improves by few percent and by 15% for quasar detection while keeping the computational cost the same.

</details>

<details>

<summary>2013-10-31 06:49:32 - Nonparametric Bernstein-von Mises theorems in Gaussian white noise</summary>

- *IsmaÃ«l Castillo, Richard Nickl*

- `1208.3862v4` - [abs](http://arxiv.org/abs/1208.3862v4) - [pdf](http://arxiv.org/pdf/1208.3862v4)

> Bernstein-von Mises theorems for nonparametric Bayes priors in the Gaussian white noise model are proved. It is demonstrated how such results justify Bayes methods as efficient frequentist inference procedures in a variety of concrete nonparametric problems. Particularly Bayesian credible sets are constructed that have asymptotically exact $1-\alpha$ frequentist coverage level and whose $L^2$-diameter shrinks at the minimax rate of convergence (within logarithmic factors) over H\"{o}lder balls. Other applications include general classes of linear and nonlinear functionals and credible bands for auto-convolutions. The assumptions cover nonconjugate product priors defined on general orthonormal bases of $L^2$ satisfying weak conditions.

</details>


## 2013-11

<details>

<summary>2013-11-01 02:17:06 - Bayesian inference as iterated random functions with applications to sequential inference in graphical models</summary>

- *Arash A. Amini, XuanLong Nguyen*

- `1311.0072v1` - [abs](http://arxiv.org/abs/1311.0072v1) - [pdf](http://arxiv.org/pdf/1311.0072v1)

> We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.

</details>

<details>

<summary>2013-11-01 19:34:07 - There is a VaR beyond usual approximations</summary>

- *Marie Kratz*

- `1311.0270v1` - [abs](http://arxiv.org/abs/1311.0270v1) - [pdf](http://arxiv.org/pdf/1311.0270v1)

> Basel II and Solvency 2 both use the Value-at-Risk (VaR) as the risk measure to compute the Capital Requirements. In practice, to calibrate the VaR, a normal approximation is often chosen for the unknown distribution of the yearly log returns of financial assets. This is usually justified by the use of the Central Limit Theorem (CLT), when assuming aggregation of independent and identically distributed (iid) observations in the portfolio model. Such a choice of modeling, in particular using light tail distributions, has proven during the crisis of 2008/2009 to be an inadequate approximation when dealing with the presence of extreme returns; as a consequence, it leads to a gross underestimation of the risks. The main objective of our study is to obtain the most accurate evaluations of the aggregated risks distribution and risk measures when working on financial or insurance data under the presence of heavy tail and to provide practical solutions for accurately estimating high quantiles of aggregated risks. We explore a new method, called Normex, to handle this problem numerically as well as theoretically, based on properties of upper order statistics. Normex provides accurate results, only weakly dependent upon the sample size and the tail index. We compare it with existing methods.

</details>

<details>

<summary>2013-11-01 22:14:06 - Parsimonious Shifted Asymmetric Laplace Mixtures</summary>

- *Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne, Paula M. Murray*

- `1311.0317v1` - [abs](http://arxiv.org/abs/1311.0317v1) - [pdf](http://arxiv.org/pdf/1311.0317v1)

> A family of parsimonious shifted asymmetric Laplace mixture models is introduced. We extend the mixture of factor analyzers model to the shifted asymmetric Laplace distribution. Imposing constraints on the constitute parts of the resulting decomposed component scale matrices leads to a family of parsimonious models. An explicit two-stage parameter estimation procedure is described, and the Bayesian information criterion and the integrated completed likelihood are compared for model selection. This novel family of models is applied to real data, where it is compared to its Gaussian analogue within clustering and classification paradigms.

</details>

<details>

<summary>2013-11-02 21:38:34 - Inferring clonal evolution of tumors from single nucleotide somatic mutations</summary>

- *Wei Jiao, Shankar Vembu, Amit G. Deshwar, Lincoln Stein, Quaid Morris*

- `1210.3384v4` - [abs](http://arxiv.org/abs/1210.3384v4) - [pdf](http://arxiv.org/pdf/1210.3384v4)

> High-throughput sequencing allows the detection and quantification of frequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor cell populations. In some cases, the evolutionary history and population frequency of the subclonal lineages of tumor cells present in the sample can be reconstructed from these SNV frequency measurements. However, automated methods to do this reconstruction are not available and the conditions under which reconstruction is possible have not been described.   We describe the conditions under which the evolutionary history can be uniquely reconstructed from SNV frequencies from single or multiple samples from the tumor population and we introduce a new statistical model, PhyloSub, that infers the phylogeny and genotype of the major subclonal lineages represented in the population of cancer cells. It uses a Bayesian nonparametric prior over trees that groups SNVs into major subclonal lineages and automatically estimates the number of lineages and their ancestry. We sample from the joint posterior distribution over trees to identify evolutionary histories and cell population frequencies that have the highest probability of generating the observed SNV frequency data. When multiple phylogenies are consistent with a given set of SNV frequencies, PhyloSub represents the uncertainty in the tumor phylogeny using a partial order plot. Experiments on a simulated dataset and two real datasets comprising tumor samples from acute myeloid leukemia and chronic lymphocytic leukemia patients demonstrate that PhyloSub can infer both linear (or chain) and branching lineages and its inferences are in good agreement with ground truth, where it is available.

</details>

<details>

<summary>2013-11-03 16:46:12 - Bayesian inference for CoVaR</summary>

- *Mauro Bernardi, Ghislaine Gayraud, Lea Petrella*

- `1306.2834v3` - [abs](http://arxiv.org/abs/1306.2834v3) - [pdf](http://arxiv.org/pdf/1306.2834v3)

> Recent financial disasters emphasised the need to investigate the consequence associated with the tail co-movements among institutions; episodes of contagion are frequently observed and increase the probability of large losses affecting market participants' risk capital. Commonly used risk management tools fail to account for potential spillover effects among institutions because they provide individual risk assessment. We contribute to analyse the interdependence effects of extreme events providing an estimation tool for evaluating the Conditional Value-at-Risk (CoVaR) defined as the Value-at-Risk of an institution conditioned on another institution being under distress. In particular, our approach relies on Bayesian quantile regression framework. We propose a Markov chain Monte Carlo algorithm exploiting the Asymmetric Laplace distribution and its representation as a location-scale mixture of Normals. Moreover, since risk measures are usually evaluated on time series data and returns typically change over time, we extend the CoVaR model to account for the dynamics of the tail behaviour. Application on U.S. companies belonging to different sectors of the Standard and Poor's Composite Index (S&P500) is considered to evaluate the marginal contribution to the overall systemic risk of each individual institution

</details>

<details>

<summary>2013-11-03 20:49:07 - A Bayesian Residual-Based Test for Cointegration</summary>

- *Thomas Furmston, Stephen Hailes, A. Jennifer Morton*

- `1311.0524v1` - [abs](http://arxiv.org/abs/1311.0524v1) - [pdf](http://arxiv.org/pdf/1311.0524v1)

> Cointegration is an important concept in the analysis of non-stationary time-series, giving conditions under which a collection of non-stationary processes has an underlying stationary (cointegration) relationship. In this paper we present the first fully Bayesian residual-based test for cointegration, where we consider the whole space of possible cointegration relationships when testing for the presence of cointegration. We first demonstrate that such a test can be performed exactly in the case where the residual process follows a first-order autoregressive process. We then extend this test to include more complex residual processes, where we first consider a suitable cointegration test-statistic and then leverage Bayesian sampling techniques to perform the necessary inference. We empirically demonstrate that our Bayesian approach attains a superior classification accuracy than existing approaches, all of which use a point estimate of the cointegration relationship in their test. Finally, we demonstrate our approach on some real world financial time-series data.

</details>

<details>

<summary>2013-11-03 21:08:03 - Multivariate stochastic volatility modelling using Wishart autoregressive processes</summary>

- *K. Triantafyllopoulos*

- `1311.0530v1` - [abs](http://arxiv.org/abs/1311.0530v1) - [pdf](http://arxiv.org/pdf/1311.0530v1)

> A new multivariate stochastic volatility estimation procedure for financial time series is proposed. A Wishart autoregressive process is considered for the volatility precision covariance matrix, for the estimation of which a two step procedure is adopted. The first step is the conditional inference on the autoregressive parameters and the second step is the unconditional inference, based on a Newton-Raphson iterative algorithm. The proposed methodology, which is mostly Bayesian, is suitable for medium dimensional data and it bridges the gap between closed-form estimation and simulation-based estimation algorithms. An example, consisting of foreign exchange rates data, illustrates the proposed methodology.

</details>

<details>

<summary>2013-11-04 10:17:43 - Real-time covariance estimation for the local level model</summary>

- *K. Triantafyllopoulos*

- `1311.0634v1` - [abs](http://arxiv.org/abs/1311.0634v1) - [pdf](http://arxiv.org/pdf/1311.0634v1)

> This paper develops on-line inference for the multivariate local level model, with the focus being placed on covariance estimation of the innovations. We assess the application of the inverse Wishart prior distribution in this context and find it too restrictive since the serial correlation structure of the observation and state innovations is forced to be the same. We generalize the inverse Wishart distribution to allow for a more convenient correlation structure, but still retaining approximate conjugacy. We prove some relevant results for the new distribution and we develop approximate Bayesian inference, which allows simultaneous forecasting of time series data and estimation of the covariance of the innovations of the model. We provide results on the steady state of the level of the time series, which are deployed to achieve computational savings. Using Monte Carlo experiments, we compare the proposed methodology with existing estimation procedures. An example with real data consisting of production data from an industrial process is given.

</details>

<details>

<summary>2013-11-04 11:57:09 - Explaining the behavior of joint and marginal Monte Carlo estimators in latent variable models with independence assumptions</summary>

- *Silia Vitoratou, Ioannis Ntzoufras, Irini Moustaki*

- `1311.0656v1` - [abs](http://arxiv.org/abs/1311.0656v1) - [pdf](http://arxiv.org/pdf/1311.0656v1)

> In latent variable models the parameter estimation can be implemented by using the joint or the marginal likelihood, based on independence or conditional independence assumptions. The same dilemma occurs within the Bayesian framework with respect to the estimation of the Bayesian marginal (or integrated) likelihood, which is the main tool for model comparison and averaging. In most cases, the Bayesian marginal likelihood is a high dimensional integral that cannot be computed analytically and a plethora of methods based on Monte Carlo integration (MCI) are used for its estimation. In this work, it is shown that the joint MCI approach makes subtle use of the properties of the adopted model, leading to increased error and bias in finite settings. The sources and the components of the error associated with estimators under the two approaches are identified here and provided in exact forms. Additionally, the effect of the sample covariation on the Monte Carlo estimators is examined. In particular, even under independence assumptions the sample covariance will be close to (but not exactly) zero which surprisingly has a severe effect on the estimated values and their variability. To address this problem, an index of the sample's divergence from independence is introduced as a multivariate extension of covariance. The implications addressed here are important in the majority of practical problems appearing in Bayesian inference of multi-parameter models with analogous structures.

</details>

<details>

<summary>2013-11-04 12:04:42 - Identifying Clusters in Bayesian Disease Mapping</summary>

- *Craig Anderson, Duncan Lee, Nema Dean*

- `1311.0660v1` - [abs](http://arxiv.org/abs/1311.0660v1) - [pdf](http://arxiv.org/pdf/1311.0660v1)

> Disease mapping is the field of spatial epidemiology interested in estimating the spatial pattern in disease risk across $n$ areal units. One aim is to identify units exhibiting elevated disease risks, so that public health interventions can be made. Bayesian hierarchical models with a spatially smooth conditional autoregressive prior are used for this purpose, but they cannot identify the spatial extent of high-risk clusters. Therefore we propose a two stage solution to this problem, with the first stage being a spatially adjusted hierarchical agglomerative clustering algorithm. This algorithm is applied to data prior to the study period, and produces $n$ potential cluster structures for the disease data. The second stage fits a separate Poisson log-linear model to the study data for each cluster structure, which allows for step-changes in risk where two clusters meet. The most appropriate cluster structure is chosen by model comparison techniques, specifically by minimising the Deviance Information Criterion. The efficacy of the methodology is established by a simulation study, and is illustrated by a study of respiratory disease risk in Glasgow, Scotland.

</details>

<details>

<summary>2013-11-04 20:20:24 - Optimal designs for nonlinear regression models with respect to non-informative priors</summary>

- *Ina Burghaus, Holger Dette*

- `1311.0835v1` - [abs](http://arxiv.org/abs/1311.0835v1) - [pdf](http://arxiv.org/pdf/1311.0835v1)

> In nonlinear regression models the Fisher information depends on the parameters of the model. Consequently, optimal designs maximizing some functional of the information matrix cannot be implemented directly but require some preliminary knowledge about the unknown parameters. Bayesian optimality criteria provide an attractive solution to this problem. These criteria depend sensitively on a reasonable specification of a prior distribution for the model parameters which might not be available in all applications. In this paper we investigate Bayesian optimality criteria with non-informative prior dis- tributions. In particular, we study the Jeffreys and the Berger-Bernardo prior for which the corresponding optimality criteria are not necessarily concave. Several examples are investigated where optimal designs with respect to the new criteria are calculated and compared to Bayesian optimal designs based on a uniform and a functional uniform prior.

</details>

<details>

<summary>2013-11-08 08:04:28 - Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors</summary>

- *Junya Honda, Akimichi Takemura*

- `1311.1894v1` - [abs](http://arxiv.org/abs/1311.1894v1) - [pdf](http://arxiv.org/pdf/1311.1894v1)

> In stochastic bandit problems, a Bayesian policy called Thompson sampling (TS) has recently attracted much attention for its excellent empirical performance. However, the theoretical analysis of this policy is difficult and its asymptotic optimality is only proved for one-parameter models. In this paper we discuss the optimality of TS for the model of normal distributions with unknown means and variances as one of the most fundamental example of multiparameter models. First we prove that the expected regret of TS with the uniform prior achieves the theoretical bound, which is the first result to show that the asymptotic bound is achievable for the normal distribution model. Next we prove that TS with Jeffreys prior and reference prior cannot achieve the theoretical bound. Therefore the choice of priors is important for TS and non-informative priors are sometimes risky in cases of multiparameter models.

</details>

<details>

<summary>2013-11-08 08:13:14 - Inference for SDE models via Approximate Bayesian Computation</summary>

- *Umberto Picchini*

- `1204.5459v6` - [abs](http://arxiv.org/abs/1204.5459v6) - [pdf](http://arxiv.org/pdf/1204.5459v6)

> Models defined by stochastic differential equations (SDEs) allow for the representation of random variability in dynamical systems. The relevance of this class of models is growing in many applied research areas and is already a standard tool to model e.g. financial, neuronal and population growth dynamics. However inference for multidimensional SDE models is still very challenging, both computationally and theoretically. Approximate Bayesian computation (ABC) allow to perform Bayesian inference for models which are sufficiently complex that the likelihood function is either analytically unavailable or computationally prohibitive to evaluate. A computationally efficient ABC-MCMC algorithm is proposed, halving the running time in our simulations. Focus is on the case where the SDE describes latent dynamics in state-space models; however the methodology is not limited to the state-space framework. Simulation studies for a pharmacokinetics/pharmacodynamics model and for stochastic chemical reactions are considered and a MATLAB package implementing our ABC-MCMC algorithm is provided.

</details>

<details>

<summary>2013-11-08 11:07:02 - On posterior propriety for the Student-$t$ linear regression model under Jeffreys priors</summary>

- *Catalina A. Vallejos, Mark F. J. Steel*

- `1311.1454v2` - [abs](http://arxiv.org/abs/1311.1454v2) - [pdf](http://arxiv.org/pdf/1311.1454v2)

> Regression models with fat-tailed error terms are an increasingly popular choice to obtain more robust inference to the presence of outlying observations. This article focuses on Bayesian inference for the Student-$t$ linear regression model under objective priors that are based on the Jeffreys rule. Posterior propriety results presented in Fonseca et al. (2008) are revisited and corrected. In particular, it is shown that the standard Jeffreys-rule prior precludes the existence of a proper posterior distribution.

</details>

<details>

<summary>2013-11-08 23:27:12 - Bayesian registration of functions and curves</summary>

- *Wen Cheng, Ian L. Dryden, Xianzheng Huang*

- `1311.2105v1` - [abs](http://arxiv.org/abs/1311.2105v1) - [pdf](http://arxiv.org/pdf/1311.2105v1)

> Bayesian analysis of functions and curves is considered, where warping and other geometrical transformations are often required for meaningful comparisons. We focus on two applications involving the classification of mouse vertebrae shape outlines and the alignment of mass spectrometry data in proteomics. The functions and curves of interest are represented using the recently introduced square root velocity function, which enables a warping invariant elastic distance to be calculated in a straightforward manner. We distinguish between various spaces of interest: the original space, the ambient space after standardizing, and the quotient space after removing a group of transformations. Using Gaussian process models in the ambient space and Dirichlet priors for the warping functions, we explore Bayesian inference for curves and functions. Markov chain Monte Carlo algorithms are introduced for simulating from the posterior, including simulated tempering for multimodal posteriors. We also compare ambient and quotient space estimators for mean shape, and explain their frequent similarity in many practical problems using a Laplace approximation. A simulation study is carried out, as well as shape classification of the mouse vertebra outlines and practical alignment of the mass spectrometry functions.

</details>

<details>

<summary>2013-11-09 08:28:27 - Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse Signals</summary>

- *Jun Fang, Yanning Shen, Hongbin Li, Pu Wang*

- `1311.2150v1` - [abs](http://arxiv.org/abs/1311.2150v1) - [pdf](http://arxiv.org/pdf/1311.2150v1)

> We consider the problem of recovering block-sparse signals whose structures are unknown \emph{a priori}. Block-sparse signals with nonzero coefficients occurring in clusters arise naturally in many practical scenarios. However, the knowledge of the block structure is usually unavailable in practice. In this paper, we develop a new sparse Bayesian learning method for recovery of block-sparse signals with unknown cluster patterns. Specifically, a pattern-coupled hierarchical Gaussian prior model is introduced to characterize the statistical dependencies among coefficients, in which a set of hyperparameters are employed to control the sparsity of signal coefficients. Unlike the conventional sparse Bayesian learning framework in which each individual hyperparameter is associated independently with each coefficient, in this paper, the prior for each coefficient not only involves its own hyperparameter, but also the hyperparameters of its immediate neighbors. In doing this way, the sparsity patterns of neighboring coefficients are related to each other and the hierarchical model has the potential to encourage structured-sparse solutions. The hyperparameters, along with the sparse signal, are learned by maximizing their posterior probability via an expectation-maximization (EM) algorithm. Numerical results show that the proposed algorithm presents uniform superiority over other existing methods in a series of experiments.

</details>

<details>

<summary>2013-11-10 15:41:19 - Bayesian Analysis of ODE's: solver optimal accuracy and Bayes factors</summary>

- *Marcos CapistrÃ¡n, J. AndrÃ©s Christen, Sophie Donnet*

- `1311.2281v1` - [abs](http://arxiv.org/abs/1311.2281v1) - [pdf](http://arxiv.org/pdf/1311.2281v1)

> In most relevant cases in the Bayesian analysis of ODE inverse problems, a numerical solver needs to be used. Therefore, we cannot work with the exact theoretical posterior distribution but only with an approximate posterior deriving from the error in the numerical solver. To compare a numerical and the theoretical posterior distributions we propose to use Bayes Factors (BF), considering both of them as models for the data at hand. We prove that the theoretical vs a numerical posterior BF tends to 1, in the same order (of the step size used) as the numerical forward map solver does. For higher order solvers (eg. Runge-Kutta) the Bayes Factor is already nearly 1 for step sizes that would take far less computational effort. Considerable CPU time may be saved by using coarser solvers that nevertheless produce practically error free posteriors. Two examples are presented where nearly 90% CPU time is saved while all inference results are identical to using a solver with a much finer time step.

</details>

<details>

<summary>2013-11-11 10:52:24 - Exploiting correlation and budget constraints in Bayesian multi-armed bandit optimization</summary>

- *Matthew W. Hoffman, Bobak Shahriari, Nando de Freitas*

- `1303.6746v4` - [abs](http://arxiv.org/abs/1303.6746v4) - [pdf](http://arxiv.org/pdf/1303.6746v4)

> We address the problem of finding the maximizer of a nonlinear smooth function, that can only be evaluated point-wise, subject to constraints on the number of permitted function evaluations. This problem is also known as fixed-budget best arm identification in the multi-armed bandit literature. We introduce a Bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other Bayesian optimization methods. The Bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. As a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. This feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. The paper presents comprehensive comparisons of the proposed approach, Thompson sampling, classical Bayesian optimization techniques, more recent Bayesian bandit approaches, and state-of-the-art best arm identification methods. This is the first comparison of many of these methods in the literature and allows us to examine the relative merits of their different features.

</details>

<details>

<summary>2013-11-11 11:59:18 - Clustering Categorical Time Series into Unknown Number of Clusters: A Perfect Simulation based Approach</summary>

- *Sabyasachi Mukhopadhyay, Sourabh Bhattacharya*

- `1311.2422v1` - [abs](http://arxiv.org/abs/1311.2422v1) - [pdf](http://arxiv.org/pdf/1311.2422v1)

> Pamminger and Fruwirth-Schnatter (2010) considered a Bayesian approach to model-based clustering of categorical time series assuming a fixed number of clusters. But the popular methods for selecting the number of clusters, for example, the Bayes Information Criterion (BIC), turned out to have severe problems in the categorical time series context.   In this paper, we circumvent the difficulties of choosing the number of clusters by adopting the Bayesian semiparametric mixture model approach introduced by Bhattacharya (2008), who assume that the number of clusters is a random quantity, but is bounded above by a (possibly large) number of clusters. We adopt the perfect simulation approach of Mukhopadhyay and Bhattacharya (2012) for posterior simulation for completely solving the problems of convergence of the underlying Markov chain Monte Carlo (MCMC) approach. Importantly, within our main perfect simulation algorithm, there arose the necessity to simulate perfectly from the joint distribution of a set of continuous random variables with log-concave full conditional densities. We propose and develop a novel and efficient perfect simulation methodology for joint distributions with log-concave full conditionals. This perfect sampling methodology is of independent interest as well since in a very large and important class of Bayesian applications the full conditionals turn out to be log-concave.   We will consider application of our model and methodology to the Austrian wage mobility data, also analysed by Pamminger and Fruwirth-Schnatter (2010), and adopting the methods developed in Mukhopadhyay et al. (2011), Mukhopadhyay et al. (2012), will obtain the posterior modes of clusterings and also the desired highest posterior distribution credible regions of the posterior distribution of clusterings.

</details>

<details>

<summary>2013-11-12 11:56:45 - Quantile-based classifiers</summary>

- *Christian Hennig, Cinzia Viroli*

- `1303.1282v2` - [abs](http://arxiv.org/abs/1303.1282v2) - [pdf](http://arxiv.org/pdf/1303.1282v2)

> Quantile classifiers for potentially high-dimensional data are defined by classifying an observation according to a sum of appropriately weighted component-wise distances of the components of the observation to the within-class quantiles. An optimal percentage for the quantiles can be chosen by minimizing the misclassification error in the training sample.   It is shown that this is consistent, for $n \to \infty$, for the classification rule with asymptotically optimal quantile, and that, under some assumptions, for $p\to\infty$ the probability of correct classification converges to one. The role of skewness of the involved variables is discussed, which leads to an improved classifier.   The optimal quantile classifier performs very well in a comprehensive simulation study and a real data set from chemistry (classification of bioaerosols) compared to nine other classifiers, including the support vector machine and the recently proposed median-based classifier (Hall et al., 2009), which inspired the quantile classifier.

</details>

<details>

<summary>2013-11-12 18:18:34 - Moments and Root-Mean-Square Error of the Bayesian MMSE Estimator of Classification Error in the Gaussian Model</summary>

- *Amin Zollanvari, Edward R. Dougherty*

- `1310.1519v2` - [abs](http://arxiv.org/abs/1310.1519v2) - [pdf](http://arxiv.org/pdf/1310.1519v2)

> The most important aspect of any classifier is its error rate, because this quantifies its predictive capacity. Thus, the accuracy of error estimation is critical. Error estimation is problematic in small-sample classifier design because the error must be estimated using the same data from which the classifier has been designed. Use of prior knowledge, in the form of a prior distribution on an uncertainty class of feature-label distributions to which the true, but unknown, feature-distribution belongs, can facilitate accurate error estimation (in the mean-square sense) in circumstances where accurate completely model-free error estimation is impossible. This paper provides analytic asymptotically exact finite-sample approximations for various performance metrics of the resulting Bayesian Minimum Mean-Square-Error (MMSE) error estimator in the case of linear discriminant analysis (LDA) in the multivariate Gaussian model. These performance metrics include the first, second, and cross moments of the Bayesian MMSE error estimator with the true error of LDA, and therefore, the Root-Mean-Square (RMS) error of the estimator. We lay down the theoretical groundwork for Kolmogorov double-asymptotics in a Bayesian setting, which enables us to derive asymptotic expressions of the desired performance metrics. From these we produce analytic finite-sample approximations and demonstrate their accuracy via numerical examples. Various examples illustrate the behavior of these approximations and their use in determining the necessary sample size to achieve a desired RMS. The Supplementary Material contains derivations for some equations and added figures.

</details>

<details>

<summary>2013-11-13 02:23:34 - Informed Source Separation: A Bayesian Tutorial</summary>

- *Kevin H. Knuth*

- `1311.3001v1` - [abs](http://arxiv.org/abs/1311.3001v1) - [pdf](http://arxiv.org/pdf/1311.3001v1)

> Source separation problems are ubiquitous in the physical sciences; any situation where signals are superimposed calls for source separation to estimate the original signals. In this tutorial I will discuss the Bayesian approach to the source separation problem. This approach has a specific advantage in that it requires the designer to explicitly describe the signal model in addition to any other information or assumptions that go into the problem description. This leads naturally to the idea of informed source separation, where the algorithm design incorporates relevant information about the specific problem. This approach promises to enable researchers to design their own high-quality algorithms that are specifically tailored to the problem at hand.

</details>

<details>

<summary>2013-11-13 17:04:41 - Stochastic inference with deterministic spiking neurons</summary>

- *Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel, Karlheinz Meier*

- `1311.3211v1` - [abs](http://arxiv.org/abs/1311.3211v1) - [pdf](http://arxiv.org/pdf/1311.3211v1)

> The seemingly stochastic transient dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference. In vitro neurons, on the other hand, exhibit a highly deterministic response to various types of stimulation. We show that an ensemble of deterministic leaky integrate-and-fire neurons embedded in a spiking noisy environment can attain the correct firing statistics in order to sample from a well-defined target distribution. We provide an analytical derivation of the activation function on the single cell level; for recurrent networks, we examine convergence towards stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level.

</details>

<details>

<summary>2013-11-14 15:28:48 - Stratified Graphical Models - Context-Specific Independence in Graphical Models</summary>

- *Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander*

- `1309.6415v2` - [abs](http://arxiv.org/abs/1309.6415v2) - [pdf](http://arxiv.org/pdf/1309.6415v2)

> Theory of graphical models has matured over more than three decades to provide the backbone for several classes of models that are used in a myriad of applications such as genetic mapping of diseases, credit risk evaluation, reliability and computer security, etc. Despite of their generic applicability and wide adoptance, the constraints imposed by undirected graphical models and Bayesian networks have also been recognized to be unnecessarily stringent under certain circumstances. This observation has led to the proposal of several generalizations that aim at more relaxed constraints by which the models can impose local or context-specific dependence structures. Here we consider an additional class of such models, termed as stratified graphical models. We develop a method for Bayesian learning of these models by deriving an analytical expression for the marginal likelihood of data under a specific subclass of decomposable stratified models. A non-reversible Markov chain Monte Carlo approach is further used to identify models that are highly supported by the posterior distribution over the model space. Our method is illustrated and compared with ordinary graphical models through application to several real and synthetic datasets.

</details>

<details>

<summary>2013-11-14 15:31:46 - Flexible sampling of discrete data correlations without the marginal distributions</summary>

- *Alfredo Kalaitzis, Ricardo Silva*

- `1306.2685v3` - [abs](http://arxiv.org/abs/1306.2685v3) - [pdf](http://arxiv.org/pdf/1306.2685v3)

> Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.

</details>

<details>

<summary>2013-11-14 20:56:06 - Uniform random generation of large acyclic digraphs</summary>

- *Jack Kuipers, Giusi Moffa*

- `1202.6590v4` - [abs](http://arxiv.org/abs/1202.6590v4) - [pdf](http://arxiv.org/pdf/1202.6590v4)

> Directed acyclic graphs are the basic representation of the structure underlying Bayesian networks, which represent multivariate probability distributions. In many practical applications, such as the reverse engineering of gene regulatory networks, not only the estimation of model parameters but the reconstruction of the structure itself is of great interest. As well as for the assessment of different structure learning algorithms in simulation studies, a uniform sample from the space of directed acyclic graphs is required to evaluate the prevalence of certain structural features. Here we analyse how to sample acyclic digraphs uniformly at random through recursive enumeration, an approach previously thought too computationally involved. Based on complexity considerations, we discuss in particular how the enumeration directly provides an exact method, which avoids the convergence issues of the alternative Markov chain methods and is actually computationally much faster. The limiting behaviour of the distribution of acyclic digraphs then allows us to sample arbitrarily large graphs. Building on the ideas of recursive enumeration based sampling we also introduce a novel hybrid Markov chain with much faster convergence than current alternatives while still being easy to adapt to various restrictions. Finally we discuss how to include such restrictions in the combinatorial enumeration and the new hybrid Markov chain method for efficient uniform sampling of the corresponding graphs.

</details>

<details>

<summary>2013-11-15 15:48:27 - Spline approximations to conditional Archimedean copula</summary>

- *Philippe Lambert*

- `1311.3888v1` - [abs](http://arxiv.org/abs/1311.3888v1) - [pdf](http://arxiv.org/pdf/1311.3888v1)

> We propose a flexible copula model to describe changes with a covariate in the dependence structure of (conditionally exchangeable) random variables. The starting point is a spline approximation to the generator of an Archimedean copula. Changes in the dependence structure with a covariate $x$ are modelled by flexible regression of the spline coefficients on $x$. The performances and properties of the spline estimate of the reference generator and the abilities of these conditional models to approximate conditional copulas are studied through simulations. Inference is made using Bayesian arguments with posterior distributions explored using importance sampling or adaptive MCMC algorithms. The modelling strategy is illustrated with two examples.

</details>

<details>

<summary>2013-11-16 20:25:28 - Maximum likelihood versus likelihood-free quantum system identification in the atom maser</summary>

- *Catalin Catana, Theodore Kypraios, Madalin Guta*

- `1311.4091v1` - [abs](http://arxiv.org/abs/1311.4091v1) - [pdf](http://arxiv.org/pdf/1311.4091v1)

> We consider the system identification problem of estimating a dynamical parameter of a Markovian quantum open system (the atom maser), by performing continuous time measurements in the system's output (outgoing atoms). Two estimation methods are investigated and compared. On the one hand, the maximum likelihood estimator (MLE) takes into account the full measurement data and is asymptotically optimal in terms of its mean square error. On the other hand, the `likelihood-free' method of approximate Bayesian computation (ABC) produces an approximation of the posterior distribution for a given set of summary statistics, by sampling trajectories at different parameter values and comparing them with the measurement data via chosen statistics.   Our analysis is performed on the atom maser model, which exhibits interesting features such as bistability and dynamical phase transitions, and has connections with the classical theory of hidden Markov processes. Building on previous results which showed that atom counts are poor statistics for certain values of the Rabi angle, we apply MLE to the full measurement data and estimate its Fisher information. We then select several correlation statistics such as waiting times, distribution of successive identical detections, and use them as input of the ABC algorithm. The resulting posterior distribution follows closely the data likelihood, showing that the selected statistics contain `most' statistical information about the Rabi angle.

</details>

<details>

<summary>2013-11-17 05:27:56 - Parameter Estimation in Hidden Markov Models with Intractable Likelihoods Using Sequential Monte Carlo</summary>

- *Sinan Yildirim, Sumeetpal Singh, Thomas Dean, Ajay Jasra*

- `1311.4117v1` - [abs](http://arxiv.org/abs/1311.4117v1) - [pdf](http://arxiv.org/pdf/1311.4117v1)

> We propose sequential Monte Carlo based algorithms for maximum likelihood estimation of the static parameters in hidden Markov models with an intractable likelihood using ideas from approximate Bayesian computation. The static parameter estimation algorithms are gradient based and cover both offline and online estimation. We demonstrate their performance by estimating the parameters of three intractable models, namely the alpha-stable distribution, g-and-k distribution, and the stochastic volatility model with alpha-stable returns, using both real and synthetic data.

</details>

<details>

<summary>2013-11-18 21:00:10 - Estimating Functions of Distributions Defined over Spaces of Unknown Size</summary>

- *David H. Wolpert, Simon DeDeo*

- `1311.4548v1` - [abs](http://arxiv.org/abs/1311.4548v1) - [pdf](http://arxiv.org/pdf/1311.4548v1)

> We consider Bayesian estimation of information-theoretic quantities from data, using a Dirichlet prior. Acknowledging the uncertainty of the event space size $m$ and the Dirichlet prior's concentration parameter $c$, we treat both as random variables set by a hyperprior. We show that the associated hyperprior, $P(c, m)$, obeys a simple "Irrelevance of Unseen Variables" (IUV) desideratum iff $P(c, m) = P(c) P(m)$. Thus, requiring IUV greatly reduces the number of degrees of freedom of the hyperprior. Some information-theoretic quantities can be expressed multiple ways, in terms of different event spaces, e.g., mutual information. With all hyperpriors (implicitly) used in earlier work, different choices of this event space lead to different posterior expected values of these information-theoretic quantities. We show that there is no such dependence on the choice of event space for a hyperprior that obeys IUV. We also derive a result that allows us to exploit IUV to greatly simplify calculations, like the posterior expected mutual information or posterior expected multi-information. We also use computer experiments to favorably compare an IUV-based estimator of entropy to three alternative methods in common use. We end by discussing how seemingly innocuous changes to the formalization of an estimation problem can substantially affect the resultant estimates of posterior expectations.

</details>

<details>

<summary>2013-11-19 09:39:26 - Nonparametric Bayes dynamic modeling of relational data</summary>

- *Daniele Durante, David B. Dunson*

- `1311.4669v1` - [abs](http://arxiv.org/abs/1311.4669v1) - [pdf](http://arxiv.org/pdf/1311.4669v1)

> Symmetric binary matrices representing relations among entities are commonly collected in many areas. Our focus is on dynamically evolving binary relational matrices, with interest being in inference on the relationship structure and prediction. We propose a nonparametric Bayesian dynamic model, which reduces dimensionality in characterizing the binary matrix through a lower-dimensional latent space representation, with the latent coordinates evolving in continuous time via Gaussian processes. By using a logistic mapping function from the probability matrix space to the latent relational space, we obtain a flexible and computational tractable formulation. Employing P\`olya-Gamma data augmentation, an efficient Gibbs sampler is developed for posterior computation, with the dimension of the latent space automatically inferred. We provide some theoretical results on flexibility of the model, and illustrate performance via simulation experiments. We also consider an application to co-movements in world financial markets.

</details>

<details>

<summary>2013-11-19 18:46:59 - Domain Adaptation of Majority Votes via Perturbed Variation-based Label Transfer</summary>

- *Emilie Morvant*

- `1311.4833v1` - [abs](http://arxiv.org/abs/1311.4833v1) - [pdf](http://arxiv.org/pdf/1311.4833v1)

> We tackle the PAC-Bayesian Domain Adaptation (DA) problem. This arrives when one desires to learn, from a source distribution, a good weighted majority vote (over a set of classifiers) on a different target distribution. In this context, the disagreement between classifiers is known crucial to control. In non-DA supervised setting, a theoretical bound - the C-bound - involves this disagreement and leads to a majority vote learning algorithm: MinCq. In this work, we extend MinCq to DA by taking advantage of an elegant divergence between distribution called the Perturbed Varation (PV). Firstly, justified by a new formulation of the C-bound, we provide to MinCq a target sample labeled thanks to a PV-based self-labeling focused on regions where the source and target marginal distributions are closer. Secondly, we propose an original process for tuning the hyperparameters. Our framework shows very promising results on a toy problem.

</details>

<details>

<summary>2013-11-20 06:59:17 - Quasi-Bayesian analysis of nonparametric instrumental variables models</summary>

- *Kengo Kato*

- `1204.2108v5` - [abs](http://arxiv.org/abs/1204.2108v5) - [pdf](http://arxiv.org/pdf/1204.2108v5)

> This paper aims at developing a quasi-Bayesian analysis of the nonparametric instrumental variables model, with a focus on the asymptotic properties of quasi-posterior distributions. In this paper, instead of assuming a distributional assumption on the data generating process, we consider a quasi-likelihood induced from the conditional moment restriction, and put priors on the function-valued parameter. We call the resulting posterior quasi-posterior, which corresponds to ``Gibbs posterior'' in the literature. Here we focus on priors constructed on slowly growing finite-dimensional sieves. We derive rates of contraction and a nonparametric Bernstein-von Mises type result for the quasi-posterior distribution, and rates of convergence for the quasi-Bayes estimator defined by the posterior expectation. We show that, with priors suitably chosen, the quasi-posterior distribution (the quasi-Bayes estimator) attains the minimax optimal rate of contraction (convergence, resp.). These results greatly sharpen the previous related work.

</details>

<details>

<summary>2013-11-20 23:29:01 - Streaming Variational Bayes</summary>

- *Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, Michael I. Jordan*

- `1307.6769v2` - [abs](http://arxiv.org/abs/1307.6769v2) - [pdf](http://arxiv.org/pdf/1307.6769v2)

> We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation batch primitive. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI) by comparing the two after a single pass through a known amount of data---a case where SVI may be applied---and in the streaming setting, where SVI does not apply.

</details>

<details>

<summary>2013-11-21 10:51:26 - Nonparametric Bayesian models of hierarchical structure in complex networks</summary>

- *Mikkel N. Schmidt, Tue Herlau, Morten MÃ¸rup*

- `1311.1033v2` - [abs](http://arxiv.org/abs/1311.1033v2) - [pdf](http://arxiv.org/pdf/1311.1033v2)

> Analyzing and understanding the structure of complex relational data is important in many applications including analysis of the connectivity in the human brain. Such networks can have prominent patterns on different scales, calling for a hierarchically structured model. We propose two non-parametric Bayesian hierarchical network models based on Gibbs fragmentation tree priors, and demonstrate their ability to capture nested patterns in simulated networks. On real networks we demonstrate detection of hierarchical structure and show predictive performance on par with the state of the art. We envision that our methods can be employed in exploratory analysis of large scale complex networks for example to model human brain connectivity.

</details>

<details>

<summary>2013-11-21 22:16:00 - Compressive Measurement Designs for Estimating Structured Signals in Structured Clutter: A Bayesian Experimental Design Approach</summary>

- *Swayambhoo Jain, Akshay Soni, Jarvis Haupt*

- `1311.5599v1` - [abs](http://arxiv.org/abs/1311.5599v1) - [pdf](http://arxiv.org/pdf/1311.5599v1)

> This work considers an estimation task in compressive sensing, where the goal is to estimate an unknown signal from compressive measurements that are corrupted by additive pre-measurement noise (interference, or clutter) as well as post-measurement noise, in the specific setting where some (perhaps limited) prior knowledge on the signal, interference, and noise is available. The specific aim here is to devise a strategy for incorporating this prior information into the design of an appropriate compressive measurement strategy. Here, the prior information is interpreted as statistics of a prior distribution on the relevant quantities, and an approach based on Bayesian Experimental Design is proposed. Experimental results on synthetic data demonstrate that the proposed approach outperforms traditional random compressive measurement designs, which are agnostic to the prior information, as well as several other knowledge-enhanced sensing matrix designs based on more heuristic notions.

</details>

<details>

<summary>2013-11-21 22:40:08 - Estimation of Extreme Quantiles for Functions of Dependent Random Variables</summary>

- *Jinguo Gong, Yadong Li, Liang Peng, Qiwei Yao*

- `1311.5604v1` - [abs](http://arxiv.org/abs/1311.5604v1) - [pdf](http://arxiv.org/pdf/1311.5604v1)

> We propose a new method for estimating the extreme quantiles for a function of several dependent random variables. In contrast to the conventional approach based on extreme value theory, we do not impose the condition that the tail of the underlying distribution admits an approximate parametric form, and, furthermore, our estimation makes use of the full observed data. The proposed method is semiparametric as no parametric forms are assumed on all the marginal distributions. But we select appropriate bivariate copulas to model the joint dependence structure by taking the advantage of the recent development in constructing large dimensional vine copulas. Consequently a sample quantile resulted from a large bootstrap sample drawn from the fitted joint distribution is taken as the estimator for the extreme quantile. This estimator is proved to be consistent. The reliable and robust performance of the proposed method is further illustrated by simulation.

</details>

<details>

<summary>2013-11-22 12:02:54 - Estimation and approximation in multidimensional dynamics</summary>

- *Gianluca Frasso, Jonathan Jaeger, Philippe Lambert*

- `1311.5727v1` - [abs](http://arxiv.org/abs/1311.5727v1) - [pdf](http://arxiv.org/pdf/1311.5727v1)

> Differential equations (DEs) are commonly used to describe dynamic systems evolving in one (ordinary differential equations or ODEs) or in more than one dimensions (partial differential equations or PDEs). In real data applications the parameters involved in the DE models are usually unknown and need to be estimated from the available measurements together with the state function. In this paper, we present frequentist and Bayesian approaches for the joint estimation of the parameters and of the state functions involved in PDEs. We also propose two strategies to include differential (initial and/or boundary) conditions in the estimation procedure. We evaluate the performances of the proposed strategy on simulated and real data applications.

</details>

<details>

<summary>2013-11-22 16:30:34 - Semi-parametric Bayesian Partially Identified Models based on Support Function</summary>

- *Yuan Liao, Anna Simoni*

- `1212.3267v2` - [abs](http://arxiv.org/abs/1212.3267v2) - [pdf](http://arxiv.org/pdf/1212.3267v2)

> We provide a comprehensive semi-parametric study of Bayesian partially identified econometric models. While the existing literature on Bayesian partial identification has mostly focused on the structural parameter, our primary focus is on Bayesian credible sets (BCS's) of the unknown identified set and the posterior distribution of its support function. We construct a (two-sided) BCS based on the support function of the identified set. We prove the Bernstein-von Mises theorem for the posterior distribution of the support function. This powerful result in turn infers that, while the BCS and the frequentist confidence set for the partially identified parameter are asymptotically different, our constructed BCS for the identified set has an asymptotically correct frequentist coverage probability. Importantly, we illustrate that the constructed BCS for the identified set does not require a prior on the structural parameter. It can be computed efficiently for subset inference, especially when the target of interest is a sub-vector of the partially identified parameter, where projecting to a low-dimensional subset is often required. Hence, the proposed methods are useful in many applications.   The Bayesian partial identification literature has been assuming a known parametric likelihood function. However, econometric models usually only identify a set of moment inequalities, and therefore using an incorrect likelihood function may result in misleading inferences. In contrast, with a nonparametric prior on the unknown likelihood function, our proposed Bayesian procedure only requires a set of moment conditions, and can efficiently make inference about both the partially identified parameter and its identified set. This makes it widely applicable in general moment inequality models. Finally, the proposed method is illustrated in a financial asset pricing problem.

</details>

<details>

<summary>2013-11-25 21:21:57 - Online inference in Markov modulated nonlinear dynamic systems: a Rao-Blackwellized particle filtering approach</summary>

- *Saikat Saha, Gustaf Hendeby*

- `1311.6486v1` - [abs](http://arxiv.org/abs/1311.6486v1) - [pdf](http://arxiv.org/pdf/1311.6486v1)

> The Markov modulated (switching) state space is an important model paradigm in applied statistics. In this article, we specifically consider Markov modulated nonlinear state-space models and address the online Bayesian inference problem for such models. In particular, we propose a new Rao-Blackwellized particle filter for the inference task which is our main contribution here. The detailed descriptions including an algorithmic summary are subsequently presented.

</details>

<details>

<summary>2013-11-26 11:56:08 - Local and global asymptotic inference in smoothing spline models</summary>

- *Zuofeng Shang, Guang Cheng*

- `1212.6788v3` - [abs](http://arxiv.org/abs/1212.6788v3) - [pdf](http://arxiv.org/pdf/1212.6788v3)

> This article studies local and global inference for smoothing spline estimation in a unified asymptotic framework. We first introduce a new technical tool called functional Bahadur representation, which significantly generalizes the traditional Bahadur representation in parametric models, that is, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with this tool, we develop four interconnected procedures for inference: (i) pointwise confidence interval; (ii) local likelihood ratio testing; (iii) simultaneous confidence band; (iv) global likelihood ratio testing. In particular, our confidence intervals are proved to be asymptotically valid at any point in the support, and they are shorter on average than the Bayesian confidence intervals proposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150] and Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss a version of the Wilks phenomenon arising from local/global likelihood ratio testing. It is also worth noting that our simultaneous confidence bands are the first ones applicable to general quasi-likelihood models. Furthermore, issues relating to optimality and efficiency are carefully addressed. As a by-product, we discover a surprising relationship between periodic and nonperiodic smoothing splines in terms of inference.

</details>

<details>

<summary>2013-11-26 18:44:13 - Quantile tomography: using quantiles with multivariate data</summary>

- *Linglong Kong, Ivan Mizera*

- `0805.0056v2` - [abs](http://arxiv.org/abs/0805.0056v2) - [pdf](http://arxiv.org/pdf/0805.0056v2)

> The use of quantiles to obtain insights about multivariate data is addressed. It is argued that incisive insights can be obtained by considering directional quantiles, the quantiles of projections. Directional quantile envelopes are proposed as a way to condense this kind of information; it is demonstrated that they are essentially halfspace (Tukey) depth levels sets, coinciding for elliptic distributions (in particular multivariate normal) with density contours. Relevant questions concerning their indexing, the possibility of the reverse retrieval of directional quantile information, invariance with respect to affine transformations, and approximation/asymptotic properties are studied. It is argued that the analysis in terms of directional quantiles and their envelopes offers a straightforward probabilistic interpretation and thus conveys a concrete quantitative meaning; the directional definition can be adapted to elaborate frameworks, like estimation of extreme quantiles and directional quantile regression, the regression of depth contours on covariates. The latter facilitates the construction of multivariate growth charts---the question that motivated all the development.

</details>

<details>

<summary>2013-11-27 03:59:13 - Bayesian spike inference from calcium imaging data</summary>

- *Eftychios A. Pnevmatikakis, Josh Merel, Ari Pakman, Liam Paninski*

- `1311.6864v1` - [abs](http://arxiv.org/abs/1311.6864v1) - [pdf](http://arxiv.org/pdf/1311.6864v1)

> We present efficient Bayesian methods for extracting neuronal spiking information from calcium imaging data. The goal of our methods is to sample from the posterior distribution of spike trains and model parameters (baseline concentration, spike amplitude etc) given noisy calcium imaging data. We present discrete time algorithms where we sample the existence of a spike at each time bin using Gibbs methods, as well as continuous time algorithms where we sample over the number of spikes and their locations at an arbitrary resolution using Metropolis-Hastings methods for point processes. We provide Rao-Blackwellized extensions that (i) marginalize over several model parameters and (ii) provide smooth estimates of the marginal spike posterior distribution in continuous time. Our methods serve as complements to standard point estimates and allow for quantification of uncertainty in estimating the underlying spike train and model parameters.

</details>

<details>

<summary>2013-11-27 12:34:09 - Rate-optimal Bayesian intensity smoothing for inhomogeneous Poisson processes</summary>

- *Eduard Belitser, Paulo Serra, Harry van Zanten*

- `1304.6017v2` - [abs](http://arxiv.org/abs/1304.6017v2) - [pdf](http://arxiv.org/pdf/1304.6017v2)

> We apply nonparametric Bayesian methods to study the problem of estimating the intensity function of an inhomogeneous Poisson process. We exhibit a prior on intensities which both leads to a computationally feasible method and enjoys desirable theoretical optimality properties. The prior we use is based on B-spline expansions with free knots, adapted from well-established methods used in regression, for instance. We illustrate its practical use in the Poisson process setting by analyzing count data coming from a call centre. Theoretically we derive a new general theorem on contraction rates for posteriors in the setting of intensity function estimation. Practical choices that have to be made in the construction of our concrete prior, such as choosing the priors on the number and the locations of the spline knots, are based on these theoretical findings. The results assert that when properly constructed, our approach yields a rate-optimal procedure that automatically adapts to the regularity of the unknown intensity function.

</details>

<details>

<summary>2013-11-27 15:04:51 - Bayesian Analysis of Epidemics - Zombies, Influenza, and other Diseases</summary>

- *Caitlyn Witkowski, Brian Blais*

- `1311.6376v2` - [abs](http://arxiv.org/abs/1311.6376v2) - [pdf](http://arxiv.org/pdf/1311.6376v2)

> Mathematical models of epidemic dynamics offer significant insight into predicting and controlling infectious diseases. The dynamics of a disease model generally follow a susceptible, infected, and recovered (SIR) model, with some standard modifications. In this paper, we extend the work of Munz et.al (2009) on the application of disease dynamics to the so-called "zombie apocalypse", and then apply the identical methods to influenza dynamics. Unlike Munz et.al (2009), we include data taken from specific depictions of zombies in popular culture films and apply Markov Chain Monte Carlo (MCMC) methods on improved dynamical representations of the system. To demonstrate the usefulness of this approach, beyond the entertaining example, we apply the identical methodology to Google Trend data on influenza to establish infection and recovery rates. Finally, we discuss the use of the methods to explore hypothetical intervention policies regarding disease outbreaks.

</details>

<details>

<summary>2013-11-28 07:41:05 - Multilevel Bayesian framework for modeling the production, propagation and detection of ultra-high energy cosmic rays</summary>

- *Kunlaya Soiaporn, David Chernoff, Thomas Loredo, David Ruppert, Ira Wasserman*

- `1206.4569v2` - [abs](http://arxiv.org/abs/1206.4569v2) - [pdf](http://arxiv.org/pdf/1206.4569v2)

> Ultra-high energy cosmic rays (UHECRs) are atomic nuclei with energies over ten million times energies accessible to human-made particle accelerators. Evidence suggests that they originate from relatively nearby extragalactic sources, but the nature of the sources is unknown. We develop a multilevel Bayesian framework for assessing association of UHECRs and candidate source populations, and Markov chain Monte Carlo algorithms for estimating model parameters and comparing models by computing, via Chib's method, marginal likelihoods and Bayes factors. We demonstrate the framework by analyzing measurements of 69 UHECRs observed by the Pierre Auger Observatory (PAO) from 2004-2009, using a volume-complete catalog of 17 local active galactic nuclei (AGN) out to 15 megaparsecs as candidate sources. An early portion of the data ("period 1," with 14 events) was used by PAO to set an energy cut maximizing the anisotropy in period 1; the 69 measurements include this "tuned" subset, and subsequent "untuned" events with energies above the same cutoff. Also, measurement errors are approximately summarized. These factors are problematic for independent analyses of PAO data. Within the context of "standard candle" source models (i.e., with a common isotropic emission rate), and considering only the 55 untuned events, there is no significant evidence favoring association of UHECRs with local AGN vs. an isotropic background. The highest-probability associations are with the two nearest, adjacent AGN, Centaurus A and NGC 4945. If the association model is adopted, the fraction of UHECRs that may be associated is likely nonzero but is well below 50%. Our framework enables estimation of the angular scale for deflection of cosmic rays by cosmic magnetic fields; relatively modest scales of $\approx\!3^{\circ}$ to $30^{\circ}$ are favored. Models that assign a large fraction of UHECRs to a single nearby source (e.g., Centaurus A) are ruled out unless very large deflection scales are specified a priori, and even then they are disfavored. However, including the period 1 data alters the conclusions significantly, and a simulation study supports the idea that the period 1 data are anomalous, presumably due to the tuning. Accurate and optimal analysis of future data will likely require more complete disclosure of the data.

</details>

<details>

<summary>2013-11-28 08:43:52 - Bayesian clustering of replicated time-course gene expression data with weak signals</summary>

- *Audrey Qiuyan Fu, Steven Russell, Sarah J. Bray, Simon TavarÃ©*

- `1210.5029v3` - [abs](http://arxiv.org/abs/1210.5029v3) - [pdf](http://arxiv.org/pdf/1210.5029v3)

> To identify novel dynamic patterns of gene expression, we develop a statistical method to cluster noisy measurements of gene expression collected from multiple replicates at multiple time points, with an unknown number of clusters. We propose a random-effects mixture model coupled with a Dirichlet-process prior for clustering. The mixture model formulation allows for probabilistic cluster assignments. The random-effects formulation allows for attributing the total variability in the data to the sources that are consistent with the experimental design, particularly when the noise level is high and the temporal dependence is not strong. The Dirichlet-process prior induces a prior distribution on partitions and helps to estimate the number of clusters (or mixture components) from the data. We further tackle two challenges associated with Dirichlet-process prior-based methods. One is efficient sampling. We develop a novel Metropolis-Hastings Markov Chain Monte Carlo (MCMC) procedure to sample the partitions. The other is efficient use of the MCMC samples in forming clusters. We propose a two-step procedure for posterior inference, which involves resampling and relabeling, to estimate the posterior allocation probability matrix. This matrix can be directly used in cluster assignments, while describing the uncertainty in clustering. We demonstrate the effectiveness of our model and sampling procedure through simulated data. Applying our method to a real data set collected from Drosophila adult muscle cells after five-minute Notch activation, we identify 14 clusters of different transcriptional responses among 163 differentially expressed genes, which provides novel insights into underlying transcriptional mechanisms in the Notch signaling pathway. The algorithm developed here is implemented in the R package DIRECT, available on CRAN.

</details>

<details>

<summary>2013-11-28 09:31:53 - Assessing lack of common support in causal inference using Bayesian nonparametrics: Implications for evaluating the effect of breastfeeding on children's cognitive outcomes</summary>

- *Jennifer Hill, Yu-Sung Su*

- `1311.7244v1` - [abs](http://arxiv.org/abs/1311.7244v1) - [pdf](http://arxiv.org/pdf/1311.7244v1)

> Causal inference in observational studies typically requires making comparisons between groups that are dissimilar. For instance, researchers investigating the role of a prolonged duration of breastfeeding on child outcomes may be forced to make comparisons between women with substantially different characteristics on average. In the extreme there may exist neighborhoods of the covariate space where there are not sufficient numbers of both groups of women (those who breastfed for prolonged periods and those who did not) to make inferences about those women. This is referred to as lack of common support. Problems can arise when we try to estimate causal effects for units that lack common support, thus we may want to avoid inference for such units. If ignorability is satisfied with respect to a set of potential confounders, then identifying whether, or for which units, the common support assumption holds is an empirical question. However, in the high-dimensional covariate space often required to satisfy ignorability such identification may not be trivial. Existing methods used to address this problem often require reliance on parametric assumptions and most, if not all, ignore the information embedded in the response variable. We distinguish between the concepts of "common support" and common causal support." We propose a new approach for identifying common causal support that addresses some of the shortcomings of existing methods. We motivate and illustrate the approach using data from the National Longitudinal Survey of Youth to estimate the effect of breastfeeding at least nine months on reading and math achievement scores at age five or six. We also evaluate the comparative performance of this method in hypothetical examples and simulations where the true treatment effect is known.

</details>

<details>

<summary>2013-11-28 10:25:36 - Assessment of mortgage default risk via Bayesian state space models</summary>

- *Tevfik Aktekin, Refik Soyer, Feng Xu*

- `1311.7261v1` - [abs](http://arxiv.org/abs/1311.7261v1) - [pdf](http://arxiv.org/pdf/1311.7261v1)

> Managing risk at the aggregate level is crucial for banks and financial institutions as required by the Basel III framework. In this paper, we introduce discrete time Bayesian state space models with Poisson measurements to model aggregate mortgage default rate. We discuss parameter updating, filtering, smoothing, forecasting and estimation using Markov chain Monte Carlo methods. In addition, we investigate the dynamic behavior of the default rate and the effects of macroeconomic variables. We illustrate the use of the proposed models using actual U.S. residential mortgage data and discuss insights gained from Bayesian analysis.

</details>

<details>

<summary>2013-11-28 21:27:24 - On the Jeffreys-Lindley's paradox</summary>

- *Christian Robert*

- `1303.5973v3` - [abs](http://arxiv.org/abs/1303.5973v3) - [pdf](http://arxiv.org/pdf/1303.5973v3)

> This paper discusses the dual interpretation of the Jeffreys--Lindley's paradox associated with Bayesian posterior probabilities and Bayes factors, both as a differentiation between frequentist and Bayesian statistics and as a pointer to the difficulty of using improper priors while testing. We stress the considerable impact of this paradox on the foundations of both classical and Bayesian statistics. While assessing existing resolutions of the paradox, we focus on a critical viewpoint of the paradox discussed by Spanos (2013) in Philosophy of Science.

</details>

<details>

<summary>2013-11-29 14:50:36 - Bayesian nonparametric location-scale-shape mixtures</summary>

- *Antonio Canale, Bruno Scarpa*

- `1311.7582v1` - [abs](http://arxiv.org/abs/1311.7582v1) - [pdf](http://arxiv.org/pdf/1311.7582v1)

> Discrete mixture models are one of the most successful approaches for density estimation. Under a Bayesian nonparametric framework, Dirichlet process location-scale mixture of Gaussian kernels is the golden standard, both having nice theoretical properties and computational tractability. In this paper we explore the use of the skew-normal kernel, which can naturally accommodate several degrees of skewness by the use of a third parameter. The choice of this kernel function allows us to formulate nonparametric location-scale-shape mixture prior with large support and good performance in different applications. Asymptotically, we show that this modelling framework is consistent in frequentist sense. Efficient Gibbs sampling algorithms are also discussed and the performance of the methods are tested through simulations and applications to galaxy velocity and fertility data. Extensions to accommodate discrete data are also discussed.

</details>

<details>

<summary>2013-11-29 15:46:54 - A Bayesian framework for functional time series analysis</summary>

- *Giovanni Petris*

- `1311.0098v2` - [abs](http://arxiv.org/abs/1311.0098v2) - [pdf](http://arxiv.org/pdf/1311.0098v2)

> The paper introduces a general framework for statistical analysis of functional time series from a Bayesian perspective. The proposed approach, based on an extension of the popular dynamic linear model to Banach-space valued observations and states, is very flexible but also easy to implement in many cases. For many kinds of data, such as continuous functions, we show how the general theory of stochastic processes provides a convenient tool to specify priors and transition probabilities of the model. Finally, we show how standard Markov chain Monte Carlo methods for posterior simulation can be employed under consistent discretizations of the data.

</details>


## 2013-12

<details>

<summary>2013-12-02 01:56:17 - On the Equivalence between Bayesian and Classical Hypothesis Testing</summary>

- *Tom Shively, Stephen Walker*

- `1312.0302v1` - [abs](http://arxiv.org/abs/1312.0302v1) - [pdf](http://arxiv.org/pdf/1312.0302v1)

> For hypotheses of the type H_0:theta=theta_0 vs H_1:theta ne theta_0 we demonstrate the equivalence of a Bayesian hypothesis test using a Bayes factor and the corresponding classical test, for a large class of models, which are detailed in the paper. In particular, we show that the role of the prior and critical region for the Bayes factor test is only to specify the type I error. This is their only role since, as we show, the power function of the Bayes factor test coincides exactly with that of the classical test, once the type I error has been fixed.   For more complex tests involving nuisance parameters, we recover the classical test by using Jeffreys prior on the nuisance parameters, while the prior on the hypothesized parameters can be arbitrary up to a large class. On the other hand, we show that using proper priors on the nuisance parameters results in a test with uniformly lower power than the classical test.

</details>

<details>

<summary>2013-12-03 20:23:31 - Hierarchical Reverberation Mapping</summary>

- *Brendon J. Brewer, Tom M. Elliott*

- `1312.0919v1` - [abs](http://arxiv.org/abs/1312.0919v1) - [pdf](http://arxiv.org/pdf/1312.0919v1)

> Reverberation mapping (RM) is an important technique in studies of active galactic nuclei (AGN). The key idea of RM is to measure the time lag $\tau$ between variations in the continuum emission from the accretion disc and subsequent response of the broad line region (BLR). The measurement of $\tau$ is typically used to estimate the physical size of the BLR and is combined with other measurements to estimate the black hole mass $M_{\rm BH}$. A major difficulty with RM campaigns is the large amount of data needed to measure $\tau$. Recently, Fine et al (2012) introduced a new approach to RM where the BLR light curve is sparsely sampled, but this is counteracted by observing a large sample of AGN, rather than a single system. The results are combined to infer properties of the sample of AGN. In this letter we implement this method using a hierarchical Bayesian model and contrast this with the results from the previous stacked cross-correlation technique. We find that our inferences are more precise and allow for more straightforward interpretation than the stacked cross-correlation results.

</details>

<details>

<summary>2013-12-03 23:45:56 - Convergence rate to a lower tail dependence coefficient of a skew-t distribution</summary>

- *Thomas Fung, Eugene Seneta*

- `1312.0983v1` - [abs](http://arxiv.org/abs/1312.0983v1) - [pdf](http://arxiv.org/pdf/1312.0983v1)

> We examine the rate of decay to the limit of the tail dependence coefficient of a bivariate skew t distribution which always displays asymptotic tail dependence. It contains as a special case the usual bivariate symmetric t distribution, and hence is an appropriate (skew) extension. The rate is asymptotically power-law. The second-order structure of the univariate quantile function for such a skew-t distribution is a central issue.

</details>

<details>

<summary>2013-12-04 10:44:01 - Multiscale Dictionary Learning for Estimating Conditional Distributions</summary>

- *Francesca Petralia, Joshua Vogelstein, David B. Dunson*

- `1312.1099v1` - [abs](http://arxiv.org/abs/1312.1099v1) - [pdf](http://arxiv.org/pdf/1312.1099v1)

> Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.

</details>

<details>

<summary>2013-12-05 14:26:29 - Bayesian object classification of gold nanoparticles</summary>

- *Bledar A. Konomi, Soma S. Dhavala, Jianhua Z. Huang, Subrata Kundu, David Huitink, Hong Liang, Yu Ding, Bani K. Mallick*

- `1312.1560v1` - [abs](http://arxiv.org/abs/1312.1560v1) - [pdf](http://arxiv.org/pdf/1312.1560v1)

> The properties of materials synthesized with nanoparticles (NPs) are highly correlated to the sizes and shapes of the nanoparticles. The transmission electron microscopy (TEM) imaging technique can be used to measure the morphological characteristics of NPs, which can be simple circles or more complex irregular polygons with varying degrees of scales and sizes. A major difficulty in analyzing the TEM images is the overlapping of objects, having different morphological properties with no specific information about the number of objects present. Furthermore, the objects lying along the boundary render automated image analysis much more difficult. To overcome these challenges, we propose a Bayesian method based on the marked-point process representation of the objects. We derive models, both for the marks which parameterize the morphological aspects and the points which determine the location of the objects. The proposed model is an automatic image segmentation and classification procedure, which simultaneously detects the boundaries and classifies the NPs into one of the predetermined shape families. We execute the inference by sampling the posterior distribution using Markov chain Monte Carlo (MCMC) since the posterior is doubly intractable. We apply our novel method to several TEM imaging samples of gold NPs, producing the needed statistical characterization of their morphology.

</details>

<details>

<summary>2013-12-05 14:46:41 - Latent protein trees</summary>

- *Ricardo Henao, J. Will Thompson, M. Arthur Moseley, Geoffrey S. Ginsburg, Lawrence Carin, Joseph E. Lucas*

- `1108.2471v4` - [abs](http://arxiv.org/abs/1108.2471v4) - [pdf](http://arxiv.org/pdf/1108.2471v4)

> Unbiased, label-free proteomics is becoming a powerful technique for measuring protein expression in almost any biological sample. The output of these measurements after preprocessing is a collection of features and their associated intensities for each sample. Subsets of features within the data are from the same peptide, subsets of peptides are from the same protein, and subsets of proteins are in the same biological pathways, therefore, there is the potential for very complex and informative correlational structure inherent in these data. Recent attempts to utilize this data often focus on the identification of single features that are associated with a particular phenotype that is relevant to the experiment. However, to date, there have been no published approaches that directly model what we know to be multiple different levels of correlation structure. Here we present a hierarchical Bayesian model which is specifically designed to model such correlation structure in unbiased, label-free proteomics. This model utilizes partial identification information from peptide sequencing and database lookup as well as the observed correlation in the data to appropriately compress features into latent proteins and to estimate their correlation structure. We demonstrate the effectiveness of the model using artificial/benchmark data and in the context of a series of proteomics measurements of blood plasma from a collection of volunteers who were infected with two different strains of viral influenza.

</details>

<details>

<summary>2013-12-05 19:43:05 - A Random Field Model and its Application in Industrial Production</summary>

- *Julie Oger, Emmanuel Lesigne, Philippe Leduc*

- `1312.1653v1` - [abs](http://arxiv.org/abs/1312.1653v1) - [pdf](http://arxiv.org/pdf/1312.1653v1)

> In competitive industries, a reliable yield forecasting is a prime factor to accurately determine the production costs and therefore ensure profitability. Indeed, quantifying the risks long before the effective manufacturing process enables fact-based decision-making. From the development stage, improvement efforts can be early identified and prioritized. In order to measure the impact of industrial process fluctuations on the product performances, the construction of a failure risk probability estimator is presented in this article. The complex relationship between the process technology and the product design (non linearities, multi-modal features...) is handled via random process regression. A random field encodes, for each product configuration, the available information regarding the risk of non-compliance. After a brief presentation of the Gaussian model approach, we describe a Bayesian reasoning avoiding a priori choices of location and scale parameters. The Gaussian mixture prior, conditioned by measured (or calculated) data, yields a posterior characterized by a multivariate Student distribution. The probabilistic nature of the model is then operated to derive a failure risk probability, defined as a random variable. To do this, our approach is to consider as random all unknown, inaccessible or fluctuating data. In order to propagate uncertainties, a fuzzy set approach provides an appropriate framework for the implementation of a Bayesian model mimicking expert elicitation. The underlying leitmotiv is to insert minimal a priori information in the failure risk model. The relevancy of this concept is illustrated with theoretical examples.

</details>

<details>

<summary>2013-12-06 09:16:20 - Hierarchical Bayesian analysis of somatic mutation data in cancer</summary>

- *Jie Ding, Lorenzo Trippa, Xiaogang Zhong, Giovanni Parmigiani*

- `1312.1809v1` - [abs](http://arxiv.org/abs/1312.1809v1) - [pdf](http://arxiv.org/pdf/1312.1809v1)

> Identifying genes underlying cancer development is critical to cancer biology and has important implications across prevention, diagnosis and treatment. Cancer sequencing studies aim at discovering genes with high frequencies of somatic mutations in specific types of cancer, as these genes are potential driving factors (drivers) for cancer development. We introduce a hierarchical Bayesian methodology to estimate gene-specific mutation rates and driver probabilities from somatic mutation data and to shed light on the overall proportion of drivers among sequenced genes. Our methodology applies to different experimental designs used in practice, including one-stage, two-stage and candidate gene designs. Also, sample sizes are typically small relative to the rarity of individual mutations. Via a shrinkage method borrowing strength from the whole genome in assessing individual genes, we reinforce inference and address the selection effects induced by multistage designs. Our simulation studies show that the posterior driver probabilities provide a nearly unbiased false discovery rate estimate. We apply our methods to pancreatic and breast cancer data, contrast our results to previous estimates and provide estimated proportions of drivers for these two types of cancer.

</details>

<details>

<summary>2013-12-06 09:57:41 - Extreme value analysis for evaluating ozone control strategies</summary>

- *Brian Reich, Daniel Cooley, Kristen Foley, Sergey Napelenok, Benjamin Shaby*

- `1312.1816v1` - [abs](http://arxiv.org/abs/1312.1816v1) - [pdf](http://arxiv.org/pdf/1312.1816v1)

> Tropospheric ozone is one of six criteria pollutants regulated by the US EPA, and has been linked to respiratory and cardiovascular endpoints and adverse effects on vegetation and ecosystems. Regional photochemical models have been developed to study the impacts of emission reductions on ozone levels. The standard approach is to run the deterministic model under new emission levels and attribute the change in ozone concentration to the emission control strategy. However, running the deterministic model requires substantial computing time, and this approach does not provide a measure of uncertainty for the change in ozone levels. Recently, a reduced form model (RFM) has been proposed to approximate the complex model as a simple function of a few relevant inputs. In this paper, we develop a new statistical approach to make full use of the RFM to study the effects of various control strategies on the probability and magnitude of extreme ozone events. We fuse the model output with monitoring data to calibrate the RFM by modeling the conditional distribution of monitoring data given the RFM using a combination of flexible semiparametric quantile regression for the center of the distribution where data are abundant and a parametric extreme value distribution for the tail where data are sparse. Selected parameters in the conditional distribution are allowed to vary by the RFM value and the spatial location. Also, due to the simplicity of the RFM, we are able to embed the RFM in our Bayesian hierarchical framework to obtain a full posterior for the model input parameters, and propagate this uncertainty to the estimation of the effects of the control strategies. We use the new framework to evaluate three potential control strategies, and find that reducing mobile-source emissions has a larger impact than reducing point-source emissions or a combination of several emission sources.

</details>

<details>

<summary>2013-12-06 10:03:42 - Object-oriented Bayesian networks for a decision support system for antitrust enforcement</summary>

- *Julia Mortera, Paola Vicard, Cecilia Vergari*

- `1301.1444v2` - [abs](http://arxiv.org/abs/1301.1444v2) - [pdf](http://arxiv.org/pdf/1301.1444v2)

> We study an economic decision problem where the actors are two firms and the Antitrust Authority whose main task is to monitor and prevent firms' potential anti-competitive behaviour and its effect on the market. The Antitrust Authority's decision process is modelled using a Bayesian network where both the relational structure and the parameters of the model are estimated from a data set provided by the Authority itself. A number of economic variables that influence this decision process are also included in the model. We analyse how monitoring by the Antitrust Authority affects firms' strategies about cooperation. Firms' strategies are modelled as a repeated prisoner's dilemma using object-oriented Bayesian networks. We show how the integration of firms' decision process and external market information can be modelled in this way. Various decision scenarios and strategies are illustrated.

</details>

<details>

<summary>2013-12-06 10:55:33 - Bayesian Posterior Contraction Rates for Linear Severely Ill-posed Inverse Problems</summary>

- *Sergios Agapiou, Andrew M. Stuart, Yuan-Xiang Zhang*

- `1210.1563v3` - [abs](http://arxiv.org/abs/1210.1563v3) - [pdf](http://arxiv.org/pdf/1210.1563v3)

> We consider a class of linear ill-posed inverse problems arising from inversion of a compact operator with singular values which decay exponentially to zero. We adopt a Bayesian approach, assuming a Gaussian prior on the unknown function. If the observational noise is assumed to be Gaussian then this prior is conjugate to the likelihood so that the posterior distribution is also Gaussian. We study Bayesian posterior consistency in the small observational noise limit. We assume that the forward operator and the prior and noise covariance operators commute with one another. We show how, for given smoothness assumptions on the truth, the scale parameter of the prior can be adjusted to optimize the rate of posterior contraction to the truth, and we explicitly compute the logarithmic rate.

</details>

<details>

<summary>2013-12-06 12:20:34 - Bayesian alignment of similarity shapes</summary>

- *Kanti V. Mardia, Christopher J. Fallaize, Stuart Barber, Richard M. Jackson, Douglas L. Theobald*

- `1312.1840v1` - [abs](http://arxiv.org/abs/1312.1840v1) - [pdf](http://arxiv.org/pdf/1312.1840v1)

> We develop a Bayesian model for the alignment of two point configurations under the full similarity transformations of rotation, translation and scaling. Other work in this area has concentrated on rigid body transformations, where scale information is preserved, motivated by problems involving molecular data; this is known as form analysis. We concentrate on a Bayesian formulation for statistical shape analysis. We generalize the model introduced by Green and Mardia [Biometrika 93 (2006) 235-254] for the pairwise alignment of two unlabeled configurations to full similarity transformations by introducing a scaling factor to the model. The generalization is not straightforward, since the model needs to be reformulated to give good performance when scaling is included. We illustrate our method on the alignment of rat growth profiles and a novel application to the alignment of protein domains. Here, scaling is applied to secondary structure elements when comparing protein folds; additionally, we find that one global scaling factor is not in general sufficient to model these data and, hence, we develop a model in which multiple scale factors can be included to handle different scalings of shape components.

</details>

<details>

<summary>2013-12-06 13:34:04 - Bayesian nonparametric hierarchical modeling for multiple membership data in grouped attendance interventions</summary>

- *Terrance D. Savitsky, Susan M. Paddock*

- `1312.1856v1` - [abs](http://arxiv.org/abs/1312.1856v1) - [pdf](http://arxiv.org/pdf/1312.1856v1)

> We develop a dependent Dirichlet process (DDP) model for repeated measures multiple membership (MM) data. This data structure arises in studies under which an intervention is delivered to each client through a sequence of elements which overlap with those of other clients on different occasions. Our interest concentrates on study designs for which the overlaps of sequences occur for clients who receive an intervention in a shared or grouped fashion whose memberships may change over multiple treatment events. Our motivating application focuses on evaluation of the effectiveness of a group therapy intervention with treatment delivered through a sequence of cognitive behavioral therapy session blocks, called modules. An open-enrollment protocol permits entry of clients at the beginning of any new module in a manner that may produce unique MM sequences across clients. We begin with a model that composes an addition of client and multiple membership module random effect terms, which are assumed independent. Our MM DDP model relaxes the assumption of conditionally independent client and module random effects by specifying a collection of random distributions for the client effect parameters that are indexed by the unique set of module attendances. We demonstrate how this construction facilitates examining heterogeneity in the relative effectiveness of group therapy modules over repeated measurement occasions.

</details>

<details>

<summary>2013-12-06 14:33:25 - Travel time estimation for ambulances using Bayesian data augmentation</summary>

- *Bradford S. Westgate, Dawn B. Woodard, David S. Matteson, Shane G. Henderson*

- `1312.1873v1` - [abs](http://arxiv.org/abs/1312.1873v1) - [pdf](http://arxiv.org/pdf/1312.1873v1)

> We introduce a Bayesian model for estimating the distribution of ambulance travel times on each road segment in a city, using Global Positioning System (GPS) data. Due to sparseness and error in the GPS data, the exact ambulance paths and travel times on each road segment are unknown. We simultaneously estimate the paths, travel times, and parameters of each road segment travel time distribution using Bayesian data augmentation. To draw ambulance path samples, we use a novel reversible jump Metropolis-Hastings step. We also introduce two simpler estimation methods based on GPS speed data. We compare these methods to a recently published travel time estimation method, using simulated data and data from Toronto EMS. In both cases, out-of-sample point and interval estimates of ambulance trip times from the Bayesian method outperform estimates from the alternative methods. We also construct probability-of-coverage maps for ambulances. The Bayesian method gives more realistic maps than the recently published method. Finally, path estimates from the Bayesian method interpolate well between sparsely recorded GPS readings and are robust to GPS location errors.

</details>

<details>

<summary>2013-12-06 15:38:10 - Efficient Metropolis-Hastings Proposal Mechanisms for Bayesian Regression Tree Models</summary>

- *M. T. Pratola*

- `1312.1895v1` - [abs](http://arxiv.org/abs/1312.1895v1) - [pdf](http://arxiv.org/pdf/1312.1895v1)

> Bayesian regression trees are flexible non-parametric models that are well suited to many modern statistical regression problems. Many such tree models have been proposed, from the simple single- tree model to more complex tree ensembles. Their non-parametric formulation allows for effective and efficient modeling of datasets exhibiting complex non-linear relationships between the model pre- dictors and observations. However, the mixing behavior of the Markov Chain Monte Carlo (MCMC) sampler is sometimes poor. This is because the proposals in the sampler are typically local alterations of the tree structure, such as the birth/death of leaf nodes, which does not allow for efficient traversal of the model space. This poor mixing can lead to inferential problems, such as under-representing uncertainty. In this paper, we develop novel proposal mechanisms for efficient sampling. The first is a rule perturbation proposal while the second we call tree rotation. The perturbation proposal can be seen as an efficient variation of the change proposal found in existing literature. The novel tree rotation proposal is simple to implement as it only requires local changes to the regression tree structure, yet it efficiently traverses disparate regions of the model space along contours of equal probability. When combined with the classical birth/death proposal, the resulting MCMC sampler exhibits good acceptance rates and properly represents model uncertainty in the posterior samples. We implement this sampling algorithm in the Bayesian Additive Regression Tree (BART) model and demonstrate its effectiveness on a prediction problem from computer experiments and a test function where structural tree variability is needed to fully explore the posterior.

</details>

<details>

<summary>2013-12-07 23:42:55 - Sequential Monte Carlo Inference of Mixed Membership Stochastic Blockmodels for Dynamic Social Networks</summary>

- *Tomoki Kobayashi, Koji Eguchi*

- `1312.2154v1` - [abs](http://arxiv.org/abs/1312.2154v1) - [pdf](http://arxiv.org/pdf/1312.2154v1)

> Many kinds of data can be represented as a network or graph. It is crucial to infer the latent structure underlying such a network and to predict unobserved links in the network. Mixed Membership Stochastic Blockmodel (MMSB) is a promising model for network data. Latent variables and unknown parameters in MMSB have been estimated through Bayesian inference with the entire network; however, it is important to estimate them online for evolving networks. In this paper, we first develop online inference methods for MMSB through sequential Monte Carlo methods, also known as particle filters. We then extend them for time-evolving networks, taking into account the temporal dependency of the network structure. We demonstrate through experiments that the time-dependent particle filter outperformed several baselines in terms of prediction performance in an online condition.

</details>

<details>

<summary>2013-12-09 05:21:31 - Bayesian Structural Inference for Hidden Processes</summary>

- *Christopher C. Strelioff, James P. Crutchfield*

- `1309.1392v2` - [abs](http://arxiv.org/abs/1309.1392v2) - [pdf](http://arxiv.org/pdf/1309.1392v2)

> We introduce a Bayesian approach to discovering patterns in structurally complex processes. The proposed method of Bayesian Structural Inference (BSI) relies on a set of candidate unifilar HMM (uHMM) topologies for inference of process structure from a data series. We employ a recently developed exact enumeration of topological epsilon-machines. (A sequel then removes the topological restriction.) This subset of the uHMM topologies has the added benefit that inferred models are guaranteed to be epsilon-machines, irrespective of estimated transition probabilities. Properties of epsilon-machines and uHMMs allow for the derivation of analytic expressions for estimating transition probabilities, inferring start states, and comparing the posterior probability of candidate model topologies, despite process internal structure being only indirectly present in data. We demonstrate BSI's effectiveness in estimating a process's randomness, as reflected by the Shannon entropy rate, and its structure, as quantified by the statistical complexity. We also compare using the posterior distribution over candidate models and the single, maximum a posteriori model for point estimation and show that the former more accurately reflects uncertainty in estimated values. We apply BSI to in-class examples of finite- and infinite-order Markov processes, as well to an out-of-class, infinite-state hidden process.

</details>

<details>

<summary>2013-12-09 06:02:54 - Noisy Bayesian Active Learning</summary>

- *Mohammad Naghshvar, Tara Javidi, Kamalika Chaudhuri*

- `1312.2315v1` - [abs](http://arxiv.org/abs/1312.2315v1) - [pdf](http://arxiv.org/pdf/1312.2315v1)

> We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, a sample space $\mathcal{X}$, and a label set $\mathcal{L}$. One of the functions in $\mathcal{H}$ assigns labels to samples in $\mathcal{X}$. The goal is to identify the function that generates the labels even though the result of a label query on a sample is corrupted by independent noise. More precisely, the objective is to declare one of the functions in $\mathcal{H}$ as the true label generating function with high confidence using as few label queries as possible, by selecting the queries adaptively and in a strategic manner.   Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen-Shannon divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds.

</details>

<details>

<summary>2013-12-10 06:52:36 - A Bayesian linear model for the high-dimensional inverse problem of seismic tomography</summary>

- *Ran Zhang, Claudia Czado, Karin Sigloch*

- `1312.2686v1` - [abs](http://arxiv.org/abs/1312.2686v1) - [pdf](http://arxiv.org/pdf/1312.2686v1)

> We apply a linear Bayesian model to seismic tomography, a high-dimensional inverse problem in geophysics. The objective is to estimate the three-dimensional structure of the earth's interior from data measured at its surface. Since this typically involves estimating thousands of unknowns or more, it has always been treated as a linear(ized) optimization problem. Here we present a Bayesian hierarchical model to estimate the joint distribution of earth structural and earthquake source parameters. An ellipsoidal spatial prior allows to accommodate the layered nature of the earth's mantle. With our efficient algorithm we can sample the posterior distributions for large-scale linear inverse problems and provide precise uncertainty quantification in terms of parameter distributions and credible intervals given the data. We apply the method to a full-fledged tomography problem, an inversion for upper-mantle structure under western North America that involves more than 11,000 parameters. In studies on simulated and real data, we show that our approach retrieves the major structures of the earth's interior as well as classical least-squares minimization, while additionally providing uncertainty assessments.

</details>

<details>

<summary>2013-12-10 13:10:12 - Bankruptcy Prediction of Small and Medium Enterprises Using a Flexible Binary Generalized Extreme Value Model</summary>

- *Raffaella Calabrese, Giampiero Marra, Silvia Angela Osmetti*

- `1307.6081v2` - [abs](http://arxiv.org/abs/1307.6081v2) - [pdf](http://arxiv.org/pdf/1307.6081v2)

> We introduce a binary regression accounting-based model for bankruptcy prediction of small and medium enterprises (SMEs). The main advantage of the model lies in its predictive performance in identifying defaulted SMEs. Another advantage, which is especially relevant for banks, is that the relationship between the accounting characteristics of SMEs and response is not assumed a priori (e.g., linear, quadratic or cubic) and can be determined from the data. The proposed approach uses the quantile function of the generalized extreme value distribution as link function as well as smooth functions of accounting characteristics to flexibly model covariate effects. Therefore, the usual assumptions in scoring models of symmetric link function and linear or pre-specied covariate-response relationships are relaxed. Out-of-sample and out-of-time validation on Italian data shows that our proposal outperforms the commonly used (logistic) scoring model for different default horizons.

</details>

<details>

<summary>2013-12-10 18:15:44 - The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties</summary>

- *Roberto Imbuzeiro Oliveira*

- `1312.2903v1` - [abs](http://arxiv.org/abs/1312.2903v1) - [pdf](http://arxiv.org/pdf/1312.2903v1)

> Finite sample properties of random covariance-type matrices have been the subject of much research. In this paper we focus on the "lower tail" of such a matrix, and prove that it is subgaussian under a simple fourth moment assumption on the one-dimensional marginals of the random vectors. A similar result holds for more general sums of random positive semidefinite matrices, and the (relatively simple) proof uses a variant of the so-called PAC-Bayesian method for bounding empirical processes.   We give two applications of the main result. In the first one we obtain a new finite-sample bound for ordinary least squares estimator in linear regression with random design. Our result is model-free, requires fairly weak moment assumptions and is almost optimal. Our second application is to bounding restricted eigenvalue constants of certain random ensembles with "heavy tails". These constants are important in the analysis of problems in Compressed Sensing and High Dimensional Statistics, where one recovers a sparse vector from a small umber of linear measurements. Our result implies that heavy tails still allow for the fast recovery rates found in efficient methods such as the LASSO and the Dantzig selector. Along the way we strengthen, with a fairly short argument, a recent result of Rudelson and Zhou on the restricted eigenvalue property.

</details>

<details>

<summary>2013-12-10 19:05:09 - Analysis of Forensic DNA Mixtures with Artefacts</summary>

- *R. G. Cowell, T. Graversen, S. Lauritzen, J. Mortera*

- `1302.4404v2` - [abs](http://arxiv.org/abs/1302.4404v2) - [pdf](http://arxiv.org/pdf/1302.4404v2)

> DNA is now routinely used in criminal investigations and court cases, although DNA samples taken at crime scenes are of varying quality and therefore present challenging problems for their interpretation. We present a statistical model for the quantitative peak information obtained from an electropherogram (EPG) of a forensic DNA sample and illustrate its potential use for the analysis of criminal cases. In contrast to most previously used methods, we directly model the peak height information and incorporates important artefacts associated with the production of the EPG. Our model has a number of unknown parameters, and we show that these can be estimated by the method of maximum likelihood in the presence of multiple unknown contributors, and their approximate standard errors calculated; the computations exploit a Bayesian network representation of the model. A case example from a UK trial, as reported in the literature, is used to illustrate the efficacy and use of the model, both in finding likelihood ratios to quantify the strength of evidence, and in the deconvolution of mixtures for the purpose of finding likely profiles of one or more unknown contributors to a DNA sample. Our model is readily extended to simultaneous analysis of more than one mixture as illustrated in a case example. We show that combination of evidence from several samples may give an evidential strength close to that of a single source trace and thus modelling of peak height information provides for a potentially very efficient mixture analysis.

</details>

<details>

<summary>2013-12-10 19:13:55 - Basic statistics for probabilistic symbolic variables: a novel metric-based approach</summary>

- *Antonio Irpino, Rosanna Verde*

- `1110.2295v2` - [abs](http://arxiv.org/abs/1110.2295v2) - [pdf](http://arxiv.org/pdf/1110.2295v2)

> In data mining, it is usually to describe a set of individuals using some summaries (means, standard deviations, histograms, confidence intervals) that generalize individual descriptions into a typology description. In this case, data can be described by several values. In this paper, we propose an approach for computing basic statics for such data, and, in particular, for data described by numerical multi-valued variables (interval, histograms, discrete multi-valued descriptions). We propose to treat all numerical multi-valued variables as distributional data, i.e. as individuals described by distributions. To obtain new basic statistics for measuring the variability and the association between such variables, we extend the classic measure of inertia, calculated with the Euclidean distance, using the squared Wasserstein distance defined between probability measures. The distance is a generalization of the Wasserstein distance, that is a distance between quantile functions of two distributions. Some properties of such a distance are shown. Among them, we prove the Huygens theorem of decomposition of the inertia. We show the use of the Wasserstein distance and of the basic statistics presenting a k-means like clustering algorithm, for the clustering of a set of data described by modal numerical variables (distributional variables), on a real data set. Keywords: Wasserstein distance, inertia, dependence, distributional data, modal variables.

</details>

<details>

<summary>2013-12-11 10:09:55 - Nonasymptotic bounds on the estimation error of MCMC algorithms</summary>

- *Krzysztof ÅatuszyÅski, BÅaÅ¼ej Miasojedow, Wojciech Niemiro*

- `1106.4739v3` - [abs](http://arxiv.org/abs/1106.4739v3) - [pdf](http://arxiv.org/pdf/1106.4739v3)

> We address the problem of upper bounding the mean square error of MCMC estimators. Our analysis is nonasymptotic. We first establish a general result valid for essentially all ergodic Markov chains encountered in Bayesian computation and a possibly unbounded target function $f$. The bound is sharp in the sense that the leading term is exactly $\sigma_{\mathrm {as}}^2(P,f)/n$, where $\sigma_{\mathrm{as}}^2(P,f)$ is the CLT asymptotic variance. Next, we proceed to specific additional assumptions and give explicit computable bounds for geometrically and polynomially ergodic Markov chains under quantitative drift conditions. As a corollary, we provide results on confidence estimation.

</details>

<details>

<summary>2013-12-11 10:50:48 - Optimal exact tests for composite alternative hypotheses on cross tabulated data</summary>

- *Daniel Yekutieli*

- `1310.0275v2` - [abs](http://arxiv.org/abs/1310.0275v2) - [pdf](http://arxiv.org/pdf/1310.0275v2)

> We present methodology for constructing exact significance tests for cross tabulated data for "difficult" composite alternative hypotheses that have no natural test statistic. We construct a test for discovering Simpson's Paradox and a general test for discovering positive dependence between two ordinal variables. Our tests are Bayesian extensions of the likelihood ratio test, they are optimal with respect to the prior distribution, and are also closely related to Bayes factors and Bayesian FDR controlling testing procedures.

</details>

<details>

<summary>2013-12-11 11:26:14 - Marked empirical processes for non-stationary time series</summary>

- *Ngai Hang Chan, Rongmao Zhang*

- `1312.3120v1` - [abs](http://arxiv.org/abs/1312.3120v1) - [pdf](http://arxiv.org/pdf/1312.3120v1)

> Consider a first-order autoregressive process $X_i=\beta X_{i-1}+\varepsilon_i,$ where $\varepsilon_i=G(\eta_i,\eta_{i-1},\ldots)$ and $\eta_i,i\in\mathbb{Z}$ are i.i.d. random variables. Motivated by two important issues for the inference of this model, namely, the quantile inference for $H_0: \beta=1$, and the goodness-of-fit for the unit root model, the notion of the marked empirical process $\alpha_n(x)=\frac{1}{n}\sum_{i=1}^ng(X_i/a_n)I(\varepsilon_i\leq x),x\in\mathbb{R}$ is investigated in this paper. Herein, $g(\cdot)$ is a continuous function on $\mathbb{R}$ and $\{a_n\}$ is a sequence of self-normalizing constants. As the innovation $\{\varepsilon_i\}$ is usually not observable, the residual marked empirical process $\hat {\alpha}_n(x)=\frac{1}{n}\sum_{i=1}^ng(X_i/a_n)I(\hat{\varepsilon}_i\l eq x),x\in\mathbb{R},$ is considered instead, where $\hat{\varepsilon}_i=X_i-\hat{\beta}X_{i-1}$ and $\hat{\beta}$ is a consistent estimate of $\beta.$ In particular, via the martingale decomposition of stationary process and the stochastic integral result of Jakubowski (Ann. Probab. 24 (1996) 2141-2153), the limit distributions of $\alpha_n(x)$ and $\hat{\alpha}_n(x)$ are established when $\{\varepsilon_i\}$ is a short-memory process. Furthermore, by virtue of the results of Wu (Bernoulli 95 (2003) 809-831) and Ho and Hsing (Ann. Statist. 24 (1996) 992-1024) of empirical process and the integral result of Mikosch and Norvai\v{s}a (Bernoulli 6 (2000) 401-434) and Young (Acta Math. 67 (1936) 251-282), the limit distributions of $\alpha_n(x)$ and $\hat{\alpha}_n(x)$ are also derived when $\{\varepsilon_i\}$ is a long-memory process.

</details>

<details>

<summary>2013-12-11 12:04:47 - Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm</summary>

- *Leif T. Johnson, Charles J. Geyer*

- `1302.6741v2` - [abs](http://arxiv.org/abs/1302.6741v2) - [pdf](http://arxiv.org/pdf/1302.6741v2)

> A random-walk Metropolis sampler is geometrically ergodic if its equilibrium density is super-exponentially light and satisfies a curvature condition [Stochastic Process. Appl. 85 (2000) 341-361]. Many applications, including Bayesian analysis with conjugate priors of logistic and Poisson regression and of log-linear models for categorical data result in posterior distributions that are not super-exponentially light. We show how to apply the change-of-variable formula for diffeomorphisms to obtain new densities that do satisfy the conditions for geometric ergodicity. Sampling the new variable and mapping the results back to the old gives a geometrically ergodic sampler for the original variable. This method of obtaining geometric ergodicity has very wide applicability.

</details>

<details>

<summary>2013-12-11 12:36:02 - Quantile-adaptive model-free variable screening for high-dimensional heterogeneous data</summary>

- *Xuming He, Lan Wang, Hyokyoung Grace Hong*

- `1304.2186v2` - [abs](http://arxiv.org/abs/1304.2186v2) - [pdf](http://arxiv.org/pdf/1304.2186v2)

> We introduce a quantile-adaptive framework for nonlinear variable screening with high-dimensional heterogeneous data. This framework has two distinctive features: (1) it allows the set of active variables to vary across quantiles, thus making it more flexible to accommodate heterogeneity; (2) it is model-free and avoids the difficult task of specifying the form of a statistical model in a high dimensional space. Our nonlinear independence screening procedure employs spline approximations to model the marginal effects at a quantile level of interest. Under appropriate conditions on the quantile functions without requiring the existence of any moments, the new procedure is shown to enjoy the sure screening property in ultra-high dimensions. Furthermore, the quantile-adaptive framework can naturally handle censored data arising in survival analysis. We prove that the sure screening property remains valid when the response variable is subject to random right censoring. Numerical studies confirm the fine performance of the proposed method for various semiparametric models and its effectiveness to extract quantile-specific information from heteroscedastic data.

</details>

<details>

<summary>2013-12-11 21:26:16 - Asymptotic theory of sequential detection and identification in the hidden Markov models</summary>

- *Savas Dayanik, Kazutoshi Yamazaki*

- `1312.3352v1` - [abs](http://arxiv.org/abs/1312.3352v1) - [pdf](http://arxiv.org/pdf/1312.3352v1)

> We consider a unified framework of sequential change-point detection and hypothesis testing modeled by means of hidden Markov chains. One observes a sequence of random variables whose distributions are functionals of a hidden Markov chain. The objective is to detect quickly the event that the hidden Markov chain leaves a certain set of states, and to identify accurately the class of states into which it is absorbed. We propose computationally tractable sequential detection and identification strategies and obtain sufficient conditions for the asymptotic optimality in two Bayesian formulations. Numerical examples are provided to confirm the asymptotic optimality and to examine the rate of convergence.

</details>

<details>

<summary>2013-12-12 01:01:03 - Expectation Propagation for Nonlinear Inverse Problems -- with an Application to Electrical Impedance Tomography</summary>

- *Matthias Gehre, Bangti Jin*

- `1312.3378v1` - [abs](http://arxiv.org/abs/1312.3378v1) - [pdf](http://arxiv.org/pdf/1312.3378v1)

> In this paper, we study a fast approximate inference method based on expectation propagation for exploring the posterior probability distribution arising from the Bayesian formulation of nonlinear inverse problems. It is capable of efficiently delivering reliable estimates of the posterior mean and covariance, thereby providing an inverse solution together with quantified uncertainties. Some theoretical properties of the iterative algorithm are discussed, and the efficient implementation for an important class of problems of projection type is described. The method is illustrated with one typical nonlinear inverse problem, electrical impedance tomography with complete electrode model, under sparsity constraints. Numerical results for real experimental data are presented, and compared with that by Markov chain Monte Carlo. The results indicate that the method is accurate and computationally very efficient.

</details>

<details>

<summary>2013-12-12 13:47:05 - Bayesian transformation family selection: moving towards a transformed Gaussian universe</summary>

- *Efstratia Charitidou, Dimitris Fouskakis, Ioannis Ntzoufras*

- `1312.3482v1` - [abs](http://arxiv.org/abs/1312.3482v1) - [pdf](http://arxiv.org/pdf/1312.3482v1)

> The problem of transformation selection is thoroughly treated from a Bayesian perspective. Several families of transformations are considered with a view to achieving normality: the Box-Cox, the Modulus, the Yeo & Johnson and the Dual transformation. Markov chain Monte Carlo algorithms have been constructed in order to sample from the posterior distribution of the transformation parameter $\lambda_T$ associated with each competing family $T$. We investigate different approaches to constructing compatible prior distributions for $\lambda_T$ over alternative transformation families, using a unit-information power-prior approach and an alternative normal prior with approximate unit-information interpretation. Selection and discrimination between different transformation families is attained via posterior model probabilities. We demonstrate the efficiency of our approach using a variety of simulated datasets. Although there is no choice of transformation family that can be universally applied to all problems, empirical evidence suggests that some particular data structures are best treated by specific transformation families. For example, skewness is associated with the Box-Cox family while fat-tailed distributions are efficiently treated using the Modulus transformation.

</details>

<details>

<summary>2013-12-12 15:08:37 - An Improved Bayesian Semiparametric Model for Palaeoclimate Reconstruction: Cross-validation Based Model Assessment</summary>

- *Sabyasachi Mukhopadhyay, Sourabh Bhattacharya*

- `1310.7714v2` - [abs](http://arxiv.org/abs/1310.7714v2) - [pdf](http://arxiv.org/pdf/1310.7714v2)

> Fossil-based palaeoclimate reconstruction is an important area of ecological science that has gained momentum in the backdrop of the global climate change debate. The hierarchical Bayesian paradigm provides an interesting platform for studying such important scientific issue. However, our cross-validation based assessment of the existing Bayesian hierarchical models with respect to two modern proxy data sets based on chironomid and pollen, respectively, revealed that the models are inadequate for the data sets.   In this paper, we model the species assemblages (compositional data) by the zero-inflated multinomial distribution, while modelling the species response functions using Dirichlet process based Gaussian mixtures. This modelling strategy yielded significantly improved performances, and a formal Bayesian test of model adequacy, developed recently, showed that our new model is adequate for both the modern data sets. Furthermore, combining together the zero-inflated assumption, Importance Resampling Markov Chain Monte Carlo (IRMCMC) and the recently developed Transformation-based Markov Chain Monte Carlo (TMCMC), we develop a powerful and efficient computational methodology.

</details>

<details>

<summary>2013-12-13 10:38:52 - Comparison of BMA and EMOS statistical calibration methods for temperature and wind speed ensemble weather prediction</summary>

- *SÃ¡ndor Baran, AndrÃ¡s HorÃ¡nyi, DÃ³ra Nemoda*

- `1312.3763v1` - [abs](http://arxiv.org/abs/1312.3763v1) - [pdf](http://arxiv.org/pdf/1312.3763v1)

> The evolution of the weather can be described by deterministic numerical weather forecasting models. Multiple runs of these models with different initial conditions and/or model physics result in forecast ensembles which are used for estimating the distribution of future atmospheric variables. However, these ensembles are usually under-dispersive and uncalibrated, so post-processing is required.   In the present work we compare different versions of Bayesian Model Averaging (BMA) and Ensemble Model Output Statistics (EMOS) post-processing methods in order to calibrate 2m temperature and 10m wind speed forecasts of the operational ALADIN Limited Area Model Ensemble Prediction System of the Hungarian Meteorological Service. We show that compared to the raw ensemble both post-processing methods improve the calibration of probabilistic and accuracy of point forecasts and that the best BMA method slightly outperforms the EMOS technique.

</details>

<details>

<summary>2013-12-16 06:19:41 - A New Evolutionary Bayesian Approach Incorporating Additive Path Correction for Nonlinear Inverse Problems</summary>

- *M Venugopal, D Roy, R M Vasu*

- `1305.2702v2` - [abs](http://arxiv.org/abs/1305.2702v2) - [pdf](http://arxiv.org/pdf/1305.2702v2)

> An evolutionary form of a generalized Bayesian update method, which is strictly derivative- free yet directed through an additive update term based purely on the statistical moments of the design variables, is proposed for nonlinear inverse problems in general and applied in particular to an optical imaging problem, the ultrasound modulated optical tomography (UMOT). The additive update term, which bypasses most pitfalls of a conventional weight- based Bayesian update, results from a change of measures aimed at driving appropriately derived observation-prediction error terms or increments of cost functionals to zero-mean Brownian martingales. This constitutes a novel characterization corresponding to the extremization of the cost functional(s), where the design unknowns are represented as diffusion processes evolving with respect to a continuously parameterized iteration variable. This leads to a recursive prediction-update algorithm to implement the search. The scheme offers freedom from sample degeneracy and the accompanying divergence of the conventional weight-based Bayesian update schemes. We obtain the order of convergence of the conditioned process and also establish that the solutions are stable against tolerable variations in the regularizing noise terms, even as the original inverse problem remains severely ill-posed. Numerical evidence on solutions to the UMOT problem also confirms substantive improvements in the reconstruction efficacy through the proposed method vis-\`a- vis a Gauss-Newton approach, especially where the regularized quasi-Newton direction has low sensitivity to variations in the design unknowns.

</details>

<details>

<summary>2013-12-16 14:50:33 - Modelling Road Accident Blackspots Data with the Discrete Generalized Pareto distribution</summary>

- *Faustino Prieto, Emilio GÃ³mez-DÃ©niz, JosÃ© MarÃ­a Sarabia*

- `1312.4383v1` - [abs](http://arxiv.org/abs/1312.4383v1) - [pdf](http://arxiv.org/pdf/1312.4383v1)

> This study shows how road traffic networks events, in particular road accidents on blackspots, can be modelled with simple probabilistic distributions. We considered the number of accidents and the number of deaths on Spanish blackspots in the period 2003-2007, from Spanish General Directorate of Traffic (DGT). We modelled those datasets, respectively, with the discrete generalized Pareto distribution (a discrete parametric model with three parameters) and with the discrete Lomax distribution (a discrete parametric model with two parameters, and particular case of the previous model). For that, we analyzed the basic properties of both parametric models: cumulative distribution, survival, probability mass, quantile and hazard functions, genesis and rth-order moments; applied two estimation methods of their parameters: the $\mu$ and ($\mu+1$) frequency method and the maximum likelihood method; and used two goodness-of-fit tests: Chi-square test and discrete Kolmogorov-Smirnov test based on bootstrap resampling. We found that those probabilistic models can be useful to describe the road accident blackspots datasets analyzed.

</details>

<details>

<summary>2013-12-17 14:19:31 - Sensitivity analysis for Bayesian hierarchical models</summary>

- *Malgorzata Roos, Thiago G. Martins, Leonhard Held, Havard Rue*

- `1312.4797v1` - [abs](http://arxiv.org/abs/1312.4797v1) - [pdf](http://arxiv.org/pdf/1312.4797v1)

> Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its relevance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-runs of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis which is fast and accurate. It quantifies sensitivity without the need for a model re-run. We develop a ready-to-use priorSens package in R for routine prior sensitivity investigation by R-INLA. Throughout a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.

</details>

<details>

<summary>2013-12-17 16:10:24 - Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</summary>

- *Roger Frigola, Fredrik Lindsten, Thomas B. SchÃ¶n, Carl E. Rasmussen*

- `1306.2861v2` - [abs](http://arxiv.org/abs/1306.2861v2) - [pdf](http://arxiv.org/pdf/1306.2861v2)

> State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference \emph{and learning} (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity.

</details>

<details>

<summary>2013-12-17 16:32:54 - Identification of Gaussian Process State-Space Models with Particle Stochastic Approximation EM</summary>

- *Roger Frigola, Fredrik Lindsten, Thomas B. SchÃ¶n, Carl E. Rasmussen*

- `1312.4852v1` - [abs](http://arxiv.org/abs/1312.4852v1) - [pdf](http://arxiv.org/pdf/1312.4852v1)

> Gaussian process state-space models (GP-SSMs) are a very flexible family of models of nonlinear dynamical systems. They comprise a Bayesian nonparametric representation of the dynamics of the system and additional (hyper-)parameters governing the properties of this nonparametric representation. The Bayesian formalism enables systematic reasoning about the uncertainty in the system dynamics. We present an approach to maximum likelihood identification of the parameters in GP-SSMs, while retaining the full nonparametric description of the dynamics. The method is based on a stochastic approximation version of the EM algorithm that employs recent developments in particle Markov chain Monte Carlo for efficient identification.

</details>

<details>

<summary>2013-12-18 02:02:56 - Bayesian Reconstruction of Two-Sex Populations by Age: Estimating Sex Ratios at Birth and Sex Ratios of Mortality</summary>

- *Mark C. Wheldon, Adrian E. Raftery, Samuel J. Clark, Patrick Gerland*

- `1312.4594v2` - [abs](http://arxiv.org/abs/1312.4594v2) - [pdf](http://arxiv.org/pdf/1312.4594v2)

> The original version of Bayesian reconstruction, a method for estimating age-specific fertility, mortality, migration and population counts of the recent past with uncertainty, produced estimates for female-only populations. Here we show how two-sex populations can be similarly reconstructed and probabilistic estimates of various sex ratio quantities obtained. We demonstrate the method by reconstructing the populations of India from 1971 to 2001, Thailand from 1960 to 2000, and Laos from 1985 to 2005. We found evidence that in India, sex ratio at birth exceeded its conventional upper limit of 1.06, and, further, increased over the period of study, with posterior probability above 0.9. In addition, almost uniquely, we found evidence that life expectancy at birth was lower for females than for males in India (posterior probability for 1971--1976 0.79), although there was strong evidence for a narrowing of the gap through to 2001. In both Thailand and Laos, we found strong evidence for the more usual result that life expectancy at birth was greater for females and, in Thailand, that the difference increased over the period of study.

</details>

<details>

<summary>2013-12-18 07:35:55 - Bayesian Geoadditive Expectile Regression</summary>

- *Elisabeth Waldmann, Fabian Sobotka, Thomas Kneib*

- `1312.5054v1` - [abs](http://arxiv.org/abs/1312.5054v1) - [pdf](http://arxiv.org/pdf/1312.5054v1)

> Regression classes modeling more than the mean of the response have found a lot of attention in the last years. Expectile regression is a special and computationally convenient case of this family of models. Expectiles offer a quantile-like characterisation of a complete distribution and include the mean as a special case. In the frequentist framework the impact of a lot of covariates with very different structures have been made possible. We propose Bayesian expectile regression based on the asymmetric normal distribution. This renders possible incorporating for example linear, nonlinear, spatial and random effects in one model. Furthermore a detailed inference on the estimated parameters can be conducted. Proposal densities based on iterativly weighted least squares updates for the resulting Markov chain Monte Carlo (MCMC) simulation algorithm are proposed and the potential of the approach for extending the flexibility of expectile regression towards complex semiparametric regression specifications is discussed.

</details>

<details>

<summary>2013-12-18 11:45:49 - Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</summary>

- *Arthur Guez, David Silver, Peter Dayan*

- `1205.3109v4` - [abs](http://arxiv.org/abs/1205.3109v4) - [pdf](http://arxiv.org/pdf/1205.3109v4)

> Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.

</details>

<details>

<summary>2013-12-18 13:13:06 - On kernel smoothing for extremal quantile regression</summary>

- *Abdelaati Daouia, Laurent Gardes, StÃ©phane Girard*

- `1312.5123v1` - [abs](http://arxiv.org/abs/1312.5123v1) - [pdf](http://arxiv.org/pdf/1312.5123v1)

> Nonparametric regression quantiles obtained by inverting a kernel estimator of the conditional distribution of the response are long established in statistics. Attention has been, however, restricted to ordinary quantiles staying away from the tails of the conditional distribution. The purpose of this paper is to extend their asymptotic theory far enough into the tails. We focus on extremal quantile regression estimators of a response variable given a vector of covariates in the general setting, whether the conditional extreme-value index is positive, negative, or zero. Specifically, we elucidate their limit distributions when they are located in the range of the data or near and even beyond the sample boundary, under technical conditions that link the speed of convergence of their (intermediate or extreme) order with the oscillations of the quantile function and a von-Mises property of the conditional distribution. A simulation experiment and an illustration on real data were presented. The real data are the American electric data where the estimation of conditional extremes is found to be of genuine interest.

</details>

<details>

<summary>2013-12-18 13:37:50 - Semiparametric posterior limits under local asymptotic exponentiality</summary>

- *Bas Kleijn, Bartek Knapik*

- `1210.6204v3` - [abs](http://arxiv.org/abs/1210.6204v3) - [pdf](http://arxiv.org/pdf/1210.6204v3)

> Consider semiparametric models that display local asymptotic exponentiality (Ibragimov and Has'minskii (1981)), an asymptotic property of the likelihood associated with discontinuities of densities. Our interest goes to estimation of the location of such discontinuities while other aspects of the density form a nuisance parameter. It is shown that under certain conditions on model and prior, the posterior distribution displays Bernstein-von Mises-type asymptotic behaviour, with exponential distributions as the limiting sequence. In contrast to regular settings, the maximum likelihood estimator is inefficient under this form of irregularity. However, Bayesian point estimators based on the limiting posterior distribution attain the minimax risk. Therefore, the limiting behaviour of the posterior is used to advocate efficiency of Bayesian point estimation rather than compare it to frequentist estimation procedures based on the maximum likelihood estimator. Results are applied to semiparametric LAE location and scaling examples.

</details>

<details>

<summary>2013-12-18 13:49:12 - A conjugate class of random probability measures based on tilting and with its posterior analysis</summary>

- *John W. Lau*

- `1312.5137v1` - [abs](http://arxiv.org/abs/1312.5137v1) - [pdf](http://arxiv.org/pdf/1312.5137v1)

> This article constructs a class of random probability measures based on exponentially and polynomially tilting operated on the laws of completely random measures. The class is proved to be conjugate in that it covers both prior and posterior random probability measures in the Bayesian sense. Moreover, the class includes some common and widely used random probability measures, the normalized completely random measures (James (Poisson process partition calculus with applications to exchangeable models and Bayesian nonparametrics (2002) Preprint), Regazzini, Lijoi and Pr\"{u}nster (Ann. Statist. 31 (2003) 560-585), Lijoi, Mena and Pr\"{u}nster (J. Amer. Statist. Assoc. 100 (2005) 1278-1291)) and the Poisson-Dirichlet process (Pitman and Yor (Ann. Probab. 25 (1997) 855-900), Ishwaran and James (J. Amer. Statist. Assoc. 96 (2001) 161-173), Pitman (In Science and Statistics: A Festschrift for Terry Speed (2003) 1-34 IMS)), in a single construction. We describe an augmented version of the Blackwell-MacQueen P\'{o}lya urn sampling scheme (Blackwell and MacQueen (Ann. Statist. 1 (1973) 353-355)) that simplifies implementation and provide a simulation study for approximating the probabilities of partition sizes.

</details>

<details>

<summary>2013-12-19 17:03:09 - Exploring Multi-Modal Distributions with Nested Sampling</summary>

- *F. Feroz, J. Skilling*

- `1312.5638v1` - [abs](http://arxiv.org/abs/1312.5638v1) - [pdf](http://arxiv.org/pdf/1312.5638v1)

> In performing a Bayesian analysis, two difficult problems often emerge. First, in estimating the parameters of some model for the data, the resulting posterior distribution may be multi-modal or exhibit pronounced (curving) degeneracies. Secondly, in selecting between a set of competing models, calculation of the Bayesian evidence for each model is computationally expensive using existing methods such as thermodynamic integration. Nested Sampling is a Monte Carlo method targeted at the efficient calculation of the evidence, but also produces posterior inferences as a by-product and therefore provides means to carry out parameter estimation as well as model selection. The main challenge in implementing Nested Sampling is to sample from a constrained probability distribution. One possible solution to this problem is provided by the Galilean Monte Carlo (GMC) algorithm. We show results of applying Nested Sampling with GMC to some problems which have proven very difficult for standard Markov Chain Monte Carlo (MCMC) and down-hill methods, due to the presence of large number of local minima and/or pronounced (curving) degeneracies between the parameters. We also discuss the use of Nested Sampling with GMC in Bayesian object detection problems, which are inherently multi-modal and require the evaluation of Bayesian evidence for distinguishing between true and spurious detections.

</details>

<details>

<summary>2013-12-20 01:29:12 - Yet another application of marginals of multivariate Gibbs distributions</summary>

- *Annalisa Cerquetti*

- `1312.5789v1` - [abs](http://arxiv.org/abs/1312.5789v1) - [pdf](http://arxiv.org/pdf/1312.5789v1)

> We give yet another example of the usefulness of working with marginals of multivariate Gibbs distributions (Cerquetti, 2013) in deriving Bayesian nonparametric estimators under Gibbs priors in species sampling problems. Here in particular we substantially reduce length and complexity of the proofs in Bacallado et al. (2013, Th. 1, and Th. 2) for looking backward probabilities under incomplete information.

</details>

<details>

<summary>2013-12-20 11:09:33 - Non-parametric Bayesian modeling of complex networks</summary>

- *Mikkel N. Schmidt, Morten MÃ¸rup*

- `1312.5889v1` - [abs](http://arxiv.org/abs/1312.5889v1) - [pdf](http://arxiv.org/pdf/1312.5889v1)

> Modeling structure in complex networks using Bayesian non-parametrics makes it possible to specify flexible model structures and infer the adequate model complexity from the observed data. This paper provides a gentle introduction to non-parametric Bayesian modeling of complex networks: Using an infinite mixture model as running example we go through the steps of deriving the model as an infinite limit of a finite parametric model, inferring the model parameters by Markov chain Monte Carlo, and checking the model's fit and predictive performance. We explain how advanced non-parametric models for complex networks can be derived and point out relevant literature.

</details>

<details>

<summary>2013-12-20 11:57:58 - Modern Statistical Methods in Oceanography: A Hierarchical Perspective</summary>

- *Christopher K. Wikle, Ralph F. Milliff, Radu Herbei, William B. Leeds*

- `1312.5904v1` - [abs](http://arxiv.org/abs/1312.5904v1) - [pdf](http://arxiv.org/pdf/1312.5904v1)

> Processes in ocean physics, air-sea interaction and ocean biogeochemistry span enormous ranges in spatial and temporal scales, that is, from molecular to planetary and from seconds to millennia. Identifying and implementing sustainable human practices depend critically on our understandings of key aspects of ocean physics and ecology within these scale ranges. The set of all ocean data is distorted such that three- and four-dimensional (i.e., time-dependent) in situ data are very sparse, while observations of surface and upper ocean properties from space-borne platforms have become abundant in the past few decades. Precisions in observations of all types vary as well. In the face of these challenges, the interface between Statistics and Oceanography has proven to be a fruitful area for research and the development of useful models. With the recognition of the key importance of identifying, quantifying and managing uncertainty in data and models of ocean processes, a hierarchical perspective has become increasingly productive. As examples, we review a heterogeneous mix of studies from our own work demonstrating Bayesian hierarchical model applications in ocean physics, air-sea interaction, ocean forecasting and ocean ecosystem models. This review is by no means exhaustive and we have endeavored to identify hierarchical modeling work reported by others across the broad range of ocean-related topics reported in the statistical literature. We conclude by noting relevant ocean-statistics problems on the immediate research horizon, and some technical challenges they pose, for example, in terms of nonlinearity, dimensionality and computing.

</details>

<details>

<summary>2013-12-20 20:34:01 - Asymptotically Efficient Estimation of Weighted Average Derivatives with an Interval Censored Variable</summary>

- *Hiroaki Kaido*

- `1312.6102v1` - [abs](http://arxiv.org/abs/1312.6102v1) - [pdf](http://arxiv.org/pdf/1312.6102v1)

> This paper studies the identification and estimation of weighted average derivatives of conditional location functionals including conditional mean and conditional quantiles in settings where either the outcome variable or a regressor is interval-valued. Building on Manski and Tamer (2002) who study nonparametric bounds for mean regression with interval data, we characterize the identified set of weighted average derivatives of regression functions. Since the weighted average derivatives do not rely on parametric specifications for the regression functions, the identified set is well-defined without any parametric assumptions. Under general conditions, the identified set is compact and convex and hence admits characterization by its support function. Using this characterization, we derive the semiparametric efficiency bound of the support function when the outcome variable is interval-valued. We illustrate efficient estimation by constructing an efficient estimator of the support function for the case of mean regression with an interval censored outcome.

</details>

<details>

<summary>2013-12-22 16:01:32 - Equivariant and scale-free Tucker decomposition models</summary>

- *Peter David Hoff*

- `1312.6397v1` - [abs](http://arxiv.org/abs/1312.6397v1) - [pdf](http://arxiv.org/pdf/1312.6397v1)

> Analyses of array-valued datasets often involve reduced-rank array approximations, typically obtained via least-squares or truncations of array decompositions. However, least-squares approximations tend to be noisy in high-dimensional settings, and may not be appropriate for arrays that include discrete or ordinal measurements. This article develops methodology to obtain low-rank model-based representations of continuous, discrete and ordinal data arrays. The model is based on a parameterization of the mean array as a multilinear product of a reduced-rank core array and a set of index-specific orthogonal eigenvector matrices. It is shown how orthogonally equivariant parameter estimates can be obtained from Bayesian procedures under invariant prior distributions. Additionally, priors on the core array are developed that act as regularizers, leading to improved inference over the standard least-squares estimator, and providing robustness to misspecification of the array rank. This model-based approach is extended to accommodate discrete or ordinal data arrays using a semiparametric transformation model. The resulting low-rank representation is scale-free, in the sense that it is invariant to monotonic transformations of the data array. In an example analysis of a multivariate discrete network dataset, this scale-free approach provides a more complete description of data patterns.

</details>

<details>

<summary>2013-12-22 16:55:17 - A Tractable State-Space Model for Symmetric Positive-Definite Matrices</summary>

- *Jesse Windle, Carlos M. Carvalho*

- `1310.5951v2` - [abs](http://arxiv.org/abs/1310.5951v2) - [pdf](http://arxiv.org/pdf/1310.5951v2)

> Bayesian analysis of state-space models includes computing the posterior distribution of the system's parameters as well as filtering, smoothing, and predicting the system's latent states. When the latent states wander around $\mathbb{R}^n$ there are several well-known modeling components and computational tools that may be profitably combined to achieve these tasks. However, there are scenarios, like tracking an object in a video or tracking a covariance matrix of financial assets returns, when the latent states are restricted to a curve within $\mathbb{R}^n$ and these models and tools do not immediately apply. Within this constrained setting, most work has focused on filtering and less attention has been paid to the other aspects of Bayesian state-space inference, which tend to be more challenging. To that end, we present a state-space model whose latent states take values on the manifold of symmetric positive-definite matrices and for which one may easily compute the posterior distribution of the latent states and the system's parameters, in addition to filtered distributions and one-step ahead predictions. Deploying the model within the context of finance, we show how one can use realized covariance matrices as data to predict latent time-varying covariance matrices. This approach out-performs factor stochastic volatility.

</details>

<details>

<summary>2013-12-23 10:43:06 - Uncertainty Quantification in Complex Simulation Models Using Ensemble Copula Coupling</summary>

- *Roman Schefzik, Thordis L. Thorarinsdottir, Tilmann Gneiting*

- `1302.7149v2` - [abs](http://arxiv.org/abs/1302.7149v2) - [pdf](http://arxiv.org/pdf/1302.7149v2)

> Critical decisions frequently rely on high-dimensional output from complex computer simulation models that show intricate cross-variable, spatial and temporal dependence structures, with weather and climate predictions being key examples. There is a strongly increasing recognition of the need for uncertainty quantification in such settings, for which we propose and review a general multi-stage procedure called ensemble copula coupling (ECC), proceeding as follows: 1. Generate a raw ensemble, consisting of multiple runs of the computer model that differ in the inputs or model parameters in suitable ways. 2. Apply statistical postprocessing techniques, such as Bayesian model averaging or nonhomogeneous regression, to correct for systematic errors in the raw ensemble, to obtain calibrated and sharp predictive distributions for each univariate output variable individually. 3. Draw a sample from each postprocessed predictive distribution. 4. Rearrange the sampled values in the rank order structure of the raw ensemble to obtain the ECC postprocessed ensemble. The use of ensembles and statistical postprocessing have become routine in weather forecasting over the past decade. We show that seemingly unrelated, recent advances can be interpreted, fused and consolidated within the framework of ECC, the common thread being the adoption of the empirical copula of the raw ensemble. Depending on the use of Quantiles, Random draws or Transformations at the sampling stage, we distinguish the ECC-Q, ECC-R and ECC-T variants, respectively. We also describe relations to the Schaake shuffle and extant copula-based techniques. In a case study, the ECC approach is applied to predictions of temperature, pressure, precipitation and wind over Germany, based on the 50-member European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble.

</details>

<details>

<summary>2013-12-24 12:55:02 - A note on Bayesian convergence rates under local prior support conditions</summary>

- *Ryan Martin, Liang Hong, Stephen G. Walker*

- `1201.3102v7` - [abs](http://arxiv.org/abs/1201.3102v7) - [pdf](http://arxiv.org/pdf/1201.3102v7)

> Bounds on Bayesian posterior convergence rates, assuming the prior satisfies both local and global support conditions, are now readily available. In this paper we explore, in the context of density estimation, Bayesian convergence rates assuming only local prior support conditions. Our results give optimal rates under minimal conditions using very simple arguments.

</details>

<details>

<summary>2013-12-24 20:45:23 - Outlier robust system identification: a Bayesian kernel-based approach</summary>

- *Giulio Bottegal, Aleksandr Y. Aravkin, Hakan Hjalmarsson, Gianluigi Pillonetto*

- `1312.6317v2` - [abs](http://arxiv.org/abs/1312.6317v2) - [pdf](http://arxiv.org/pdf/1312.6317v2)

> In this paper, we propose an outlier-robust regularized kernel-based method for linear system identification. The unknown impulse response is modeled as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. To build robustness to outliers, we model the measurement noise as realizations of independent Laplacian random variables. The identification problem is cast in a Bayesian framework, and solved by a new Markov Chain Monte Carlo (MCMC) scheme. In particular, exploiting the representation of the Laplacian random variables as scale mixtures of Gaussians, we design a Gibbs sampler which quickly converges to the target distribution. Numerical simulations show a substantial improvement in the accuracy of the estimates over state-of-the-art kernel-based methods.

</details>

<details>

<summary>2013-12-25 13:11:04 - Model-based clustering and segmentation of time series with changes in regime</summary>

- *Allou SamÃ©, Faicel Chamroukhi, GÃ©rard Govaert, Patrice Aknin*

- `1312.6967v1` - [abs](http://arxiv.org/abs/1312.6967v1) - [pdf](http://arxiv.org/pdf/1312.6967v1)

> Mixture model-based clustering, usually applied to multidimensional data, has become a popular approach in many data analysis problems, both for its good statistical properties and for the simplicity of implementation of the Expectation-Maximization (EM) algorithm. Within the context of a railway application, this paper introduces a novel mixture model for dealing with time series that are subject to changes in regime. The proposed approach consists in modeling each cluster by a regression model in which the polynomial coefficients vary according to a discrete hidden process. In particular, this approach makes use of logistic functions to model the (smooth or abrupt) transitions between regimes. The model parameters are estimated by the maximum likelihood method solved by an Expectation-Maximization algorithm. The proposed approach can also be regarded as a clustering approach which operates by finding groups of time series having common changes in regime. In addition to providing a time series partition, it therefore provides a time series segmentation. The problem of selecting the optimal numbers of clusters and segments is solved by means of the Bayesian Information Criterion (BIC). The proposed approach is shown to be efficient using a variety of simulated time series and real-world time series of electrical power consumption from rail switching operations.

</details>

<details>

<summary>2013-12-28 19:28:55 - Bayesian prediction for stochastic processes. Theory and applications</summary>

- *Delphine Blanke, Denis Bosq*

- `1211.2300v2` - [abs](http://arxiv.org/abs/1211.2300v2) - [pdf](http://arxiv.org/pdf/1211.2300v2)

> In this paper, we adopt a Bayesian point of view for predicting real continuous-time processes. We give two equivalent definitions of a Bayesian predictor and study some properties: admissibility, prediction sufficiency, non-unbiasedness, comparison with efficient predictors. Prediction of Poisson process and prediction of Ornstein-Uhlenbeck process in the continuous and sampled situations are considered. Various simulations illustrate comparison with non-Bayesian predictors.

</details>

<details>

<summary>2013-12-31 14:10:34 - GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian Processes</summary>

- *Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham*

- `1310.5288v3` - [abs](http://arxiv.org/abs/1310.5288v3) - [pdf](http://arxiv.org/pdf/1310.5288v3)

> Gaussian processes are typically used for smoothing and interpolation on small datasets. We introduce a new Bayesian nonparametric framework -- GPatt -- enabling automatic pattern extrapolation with Gaussian processes on large multidimensional datasets. GPatt unifies and extends highly expressive kernels and fast exact inference techniques. Without human intervention -- no hand crafting of kernel features, and no sophisticated initialisation procedures -- we show that GPatt can solve large scale pattern extrapolation, inpainting, and kernel discovery problems, including a problem with 383400 training points. We find that GPatt significantly outperforms popular alternative scalable Gaussian process methods in speed and accuracy. Moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact inference are useful for modelling large scale multidimensional patterns.

</details>

<details>

<summary>2013-12-31 16:41:30 - Gaussian Process Kernels for Pattern Discovery and Extrapolation</summary>

- *Andrew Gordon Wilson, Ryan Prescott Adams*

- `1302.4245v3` - [abs](http://arxiv.org/abs/1302.4245v3) - [pdf](http://arxiv.org/pdf/1302.4245v3)

> Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.

</details>

