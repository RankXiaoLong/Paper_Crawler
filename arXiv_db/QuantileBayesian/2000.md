# 2000

## TOC

- [2000-08](#2000-08)
- [2000-09](#2000-09)

## 2000-08

<details>

<summary>2000-08-07 15:28:30 - Maximum entropy, fluctuations and priors</summary>

- *Ariel Caticha*

- `0008017v1` - [abs](http://arxiv.org/abs/0008017v1) - [pdf](http://arxiv.org/pdf/math-ph/0008017v1)

> The method of maximum entropy (ME) is extended to address the following problem: Once one accepts that the ME distribution is to be preferred over all others, the question is to what extent are distributions with lower entropy supposed to be ruled out. Two applications are given. The first is to the theory of thermodynamic fluctuations. The formulation is exact, covariant under changes of coordinates, and allows fluctuations of both the extensive and the conjugate intensive variables. The second application is to the construction of an objective prior for Bayesian inference. The prior obtained by following the ME method to its inevitable conclusion turns out to be a special case of what are currently known under the name of entropic priors.

</details>


## 2000-09

<details>

<summary>2000-09-13 09:58:36 - What is the question that MaxEnt answers? A probabilistic interpretation</summary>

- *Marian Grendar Jr, Marian Grendar*

- `0009020v1` - [abs](http://arxiv.org/abs/0009020v1) - [pdf](http://arxiv.org/pdf/math-ph/0009020v1)

> The Boltzmann-Wallis-Jaynes' multiplicity argument is taken up and elaborated. MaxEnt is proved and demonstrated to be just an asymptotic case of looking for such a vector of absolute frequencies in a feasible set, which has maximal probability of being generated by a uniform prior generator/pmf.

</details>

<details>

<summary>2000-09-13 10:50:41 - Minimax Entropy and Maximum Likelihood. Complementarity of tasks, identity of solutions</summary>

- *Marian Grendar, Jr, Marian Grendar*

- `0009129v1` - [abs](http://arxiv.org/abs/0009129v1) - [pdf](http://arxiv.org/pdf/math/0009129v1)

> Concept of exponential family is generalized by simple and general exponential form. Simple and general potential are introduced. Maximum Entropy and Maximum Likelihood tasks are defined. ML task on the simple exponential form and ME task on the simple potentials are proved to be complementary in set-up and identical in solutions. ML task on the general exponential form and ME task on the general potentials are weakly complementary, leading to the same necessary conditions. A hypothesis about complementarity of ML and MiniMax Entropy tasks and identity of their solutions, brought up by a special case analytical as well as several numerical investigations, is suggested in this case. MiniMax Ent can be viewed as a generalization of MaxEnt for parametric linear inverse problems, and its complementarity with ML as yet another argument in favor of Shannon's entropy criterion.

</details>

