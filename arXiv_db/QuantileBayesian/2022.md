# 2022

## TOC

- [2022-01](#2022-01)
- [2022-02](#2022-02)
- [2022-03](#2022-03)
- [2022-04](#2022-04)
- [2022-05](#2022-05)
- [2022-06](#2022-06)
- [2022-07](#2022-07)

## 2022-01

<details>

<summary>2022-01-01 07:32:22 - Model diagnostics for censored regression via randomized survival probabilities</summary>

- *Longhai Li, Tingxuan Wu, Cindy Feng*

- `1911.00198v4` - [abs](http://arxiv.org/abs/1911.00198v4) - [pdf](http://arxiv.org/pdf/1911.00198v4)

> Residuals in normal regression are used to assess a model's goodness-of-fit (GOF) and discover directions for improving the model. However, there is a lack of residuals with a characterized reference distribution for censored regression. In this paper, we propose to diagnose censored regression with normalized randomized survival probabilities (RSP). The key idea of RSP is to replace the survival probability of a censored failure time with a uniform random number between 0 and the survival probability of the censored time. We prove that RSPs always have the uniform distribution on $(0,1)$ under the true model with the true generating parameters. Therefore, we can transform RSPs into normally-distributed residuals with the normal quantile function. We call such residuals by normalized RSP (NRSP residuals). We conduct simulation studies to investigate the sizes and powers of statistical tests based on NRSP residuals in detecting the incorrect choice of distribution family and non-linear effect in covariates. Our simulation studies show that, although the GOF tests with NRSP residuals are not as powerful as a traditional GOF test method, a non-linear test based on NRSP residuals has significantly higher power in detecting non-linearity. We also compared these model diagnostics methods with a breast-cancer recurrent-free time dataset. The results show that the NRSP residual diagnostics successfully captures a subtle non-linear relationship in the dataset, which is not detected by the graphical diagnostics with CS residuals and existing GOF tests.

</details>

<details>

<summary>2022-01-01 08:40:17 - High-dimensional Bayesian Optimization Algorithm with Recurrent Neural Network for Disease Control Models in Time Series</summary>

- *Yuyang Chen, Kaiming Bi, Chih-Hang J. Wu, David Ben-Arieh, Ashesh Sinha*

- `2201.00147v1` - [abs](http://arxiv.org/abs/2201.00147v1) - [pdf](http://arxiv.org/pdf/2201.00147v1)

> Bayesian Optimization algorithm has become a promising approach for nonlinear global optimization problems and many machine learning applications. Over the past few years, improvements and enhancements have been brought forward and they have shown some promising results in solving the complex dynamic problems, systems of ordinary differential equations where the objective functions are computationally expensive to evaluate. Besides, the straightforward implementation of the Bayesian Optimization algorithm performs well merely for optimization problems with 10-20 dimensions. The study presented in this paper proposes a new high dimensional Bayesian Optimization algorithm combining Recurrent neural networks, which is expected to predict the optimal solution for the global optimization problems with high dimensional or time series decision models. The proposed RNN-BO algorithm can solve the optimal control problems in the lower dimension space and then learn from the historical data using the recurrent neural network to learn the historical optimal solution data and predict the optimal control strategy for any new initial system value setting. In addition, accurately and quickly providing the optimal control strategy is essential to effectively and efficiently control the epidemic spread while minimizing the associated financial costs. Therefore, to verify the effectiveness of the proposed algorithm, computational experiments are carried out on a deterministic SEIR epidemic model and a stochastic SIS optimal control model. Finally, we also discuss the impacts of different numbers of the RNN layers and training epochs on the trade-off between solution quality and related computational efforts.

</details>

<details>

<summary>2022-01-02 02:16:09 - Thinking inside the box: A tutorial on grey-box Bayesian optimization</summary>

- *Raul Astudillo, Peter I. Frazier*

- `2201.00272v1` - [abs](http://arxiv.org/abs/2201.00272v1) - [pdf](http://arxiv.org/pdf/2201.00272v1)

> Bayesian optimization (BO) is a framework for global optimization of expensive-to-evaluate objective functions. Classical BO methods assume that the objective function is a black box. However, internal information about objective function computation is often available. For example, when optimizing a manufacturing line's throughput with simulation, we observe the number of parts waiting at each workstation, in addition to the overall throughput. Recent BO methods leverage such internal information to dramatically improve performance. We call these "grey-box" BO methods because they treat objective computation as partially observable and even modifiable, blending the black-box approach with so-called "white-box" first-principles knowledge of objective function computation. This tutorial describes these methods, focusing on BO of composite objective functions, where one can observe and selectively evaluate individual constituents that feed into the overall objective; and multi-fidelity BO, where one can evaluate cheaper approximations of the objective function by varying parameters of the evaluation oracle.

</details>

<details>

<summary>2022-01-02 03:09:22 - Evidence synthesis with reconstructed survival data</summary>

- *Chenqi Fu, Shouhao Zhou, Xuelin Huang, Nicholas J. Short, Farhad Ravandi-Kashani, Donald A. Berry*

- `2201.00281v1` - [abs](http://arxiv.org/abs/2201.00281v1) - [pdf](http://arxiv.org/pdf/2201.00281v1)

> We present a general approach to synthesizing evidence of time-to-event endpoints in meta-analyses of aggregate data (AD). Our work goes beyond most previous meta-analytic research by using reconstructed survival data as a source of information. A Bayesian multilevel regression model, called the "meta-analysis of reconstructed survival data" (MARS), is introduced, by modeling and integrating reconstructed survival information with other types of summary data, to estimate the hazard ratio function and survival probabilities. The method attempts to reduce selection bias, and relaxes the presumption of proportional hazards in individual clinical studies from the conventional approaches restricted to hazard ratio estimates. Theoretically, we establish the asymptotic consistency of MARS, and investigate its relative efficiency with respect to the individual participant data (IPD) meta-analysis. In simulation studies, the MARS demonstrated comparable performance to IPD meta-analysis with minor deviation from the true values, suggesting great robustness and efficiency achievable in AD meta-analysis with finite sample. Finally, we applied MARS in a meta-analysis of acute myeloid leukemia to assess the association of minimal residual disease with survival, to help respond to FDA's emerging concerns on translational use of surrogate biomarker in drug development of hematologic malignancies.

</details>

<details>

<summary>2022-01-02 21:52:04 - Latent structure blockmodels for Bayesian spectral graph clustering</summary>

- *Francesco Sanna Passino, Nicholas A. Heard*

- `2107.01734v2` - [abs](http://arxiv.org/abs/2107.01734v2) - [pdf](http://arxiv.org/pdf/2107.01734v2)

> Spectral embedding of network adjacency matrices often produces node representations living approximately around low-dimensional submanifold structures. In particular, hidden substructure is expected to arise when the graph is generated from a latent position model. Furthermore, the presence of communities within the network might generate community-specific submanifold structures in the embedding, but this is not explicitly accounted for in most statistical models for networks. In this article, a class of models called latent structure block models (LSBM) is proposed to address such scenarios, allowing for graph clustering when community-specific one dimensional manifold structure is present. LSBMs focus on a specific class of latent space model, the random dot product graph (RDPG), and assign a latent submanifold to the latent positions of each community. A Bayesian model for the embeddings arising from LSBMs is discussed, and shown to have a good performance on simulated and real world network data. The model is able to correctly recover the underlying communities living in a one-dimensional manifold, even when the parametric form of the underlying curves is unknown, achieving remarkable results on a variety of real data.

</details>

<details>

<summary>2022-01-03 05:50:10 - Bayesian Pyramids: Identifiable Multilayer Discrete Latent Structure Models for Discrete Data</summary>

- *Yuqi Gu, David B. Dunson*

- `2101.10373v2` - [abs](http://arxiv.org/abs/2101.10373v2) - [pdf](http://arxiv.org/pdf/2101.10373v2)

> High dimensional categorical data are routinely collected in biomedical and social sciences. It is of great importance to build interpretable parsimonious models that perform dimension reduction and uncover meaningful latent structures from such discrete data. Identifiability is a fundamental requirement for valid modeling and inference in such scenarios, yet is challenging to address when there are complex latent structures. In this article, we propose a class of identifiable multilayer (potentially deep) discrete latent structure models for discrete data, termed Bayesian pyramids. We establish the identifiability of Bayesian pyramids by developing novel transparent conditions on the pyramid-shaped deep latent directed graph. The proposed identifiability conditions can ensure Bayesian posterior consistency under suitable priors. As an illustration, we consider the two-latent-layer model and propose a Bayesian shrinkage estimation approach. Simulation results for this model corroborate the identifiability and estimability of model parameters. Applications of the methodology to DNA nucleotide sequence data uncover useful discrete latent features that are highly predictive of sequence types. The proposed framework provides a recipe for interpretable unsupervised learning of discrete data, and can be a useful alternative to popular machine learning methods.

</details>

<details>

<summary>2022-01-03 08:37:30 - Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users</summary>

- *Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, Mohammed Bennamoun*

- `2007.06823v3` - [abs](http://arxiv.org/abs/2007.06823v3) - [pdf](http://arxiv.org/pdf/2007.06823v3)

> Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian Neural Networks, i.e. Stochastic Artificial Neural Networks trained using Bayesian methods.

</details>

<details>

<summary>2022-01-03 19:18:25 - Deriving discriminative classifiers from generative models</summary>

- *Elie Azeraf, Emmanuel Monfrini, Wojciech Pieczynski*

- `2201.00844v1` - [abs](http://arxiv.org/abs/2201.00844v1) - [pdf](http://arxiv.org/pdf/2201.00844v1)

> We deal with Bayesian generative and discriminative classifiers. Given a model distribution $p(x, y)$, with the observation $y$ and the target $x$, one computes generative classifiers by firstly considering $p(x, y)$ and then using the Bayes rule to calculate $p(x | y)$. A discriminative model is directly given by $p(x | y)$, which is used to compute discriminative classifiers. However, recent works showed that the Bayesian Maximum Posterior classifier defined from the Naive Bayes (NB) or Hidden Markov Chain (HMC), both generative models, can also match the discriminative classifier definition. Thus, there are situations in which dividing classifiers into "generative" and "discriminative" is somewhat misleading. Indeed, such a distinction is rather related to the way of computing classifiers, not to the classifiers themselves. We present a general theoretical result specifying how a generative classifier induced from a generative model can also be computed in a discriminative way from the same model. Examples of NB and HMC are found again as particular cases, and we apply the general result to two original extensions of NB, and two extensions of HMC, one of which being original. Finally, we shortly illustrate the interest of the new discriminative way of computing classifiers in the Natural Language Processing (NLP) framework.

</details>

<details>

<summary>2022-01-04 03:24:34 - A Statistical Approach to Estimating Adsorption-Isotherm Parameters in Gradient-Elution Preparative Liquid Chromatography</summary>

- *Jiaji Su, Zhigang Yao, Cheng Li, Ye Zhang*

- `2201.00958v1` - [abs](http://arxiv.org/abs/2201.00958v1) - [pdf](http://arxiv.org/pdf/2201.00958v1)

> Determining the adsorption isotherms is an issue of significant importance in preparative chromatography. A modern technique for estimating adsorption isotherms is to solve an inverse problem so that the simulated batch separation coincides with actual experimental results. However, due to the ill-posedness, the high non-linearity, and the uncertainty quantification of the corresponding physical model, the existing deterministic inversion methods are usually inefficient in real-world applications. To overcome these difficulties and study the uncertainties of the adsorption-isotherm parameters, in this work, based on the Bayesian sampling framework, we propose a statistical approach for estimating the adsorption isotherms in various chromatography systems. Two modified Markov chain Monte Carlo algorithms are developed for a numerical realization of our statistical approach. Numerical experiments with both synthetic and real data are conducted and described to show the efficiency of the proposed new method.

</details>

<details>

<summary>2022-01-04 11:55:45 - Marginal likelihood computation for model selection and hypothesis testing: an extensive review</summary>

- *Fernando Llorente, Luca Martino, David Delgado, Javier Lopez-Santiago*

- `2005.08334v4` - [abs](http://arxiv.org/abs/2005.08334v4) - [pdf](http://arxiv.org/pdf/2005.08334v4)

> This is an up-to-date introduction to, and overview of, marginal likelihood computation for model selection and hypothesis testing. Computing normalizing constants of probability models (or ratio of constants) is a fundamental issue in many applications in statistics, applied mathematics, signal processing and machine learning. This article provides a comprehensive study of the state-of-the-art of the topic. We highlight limitations, benefits, connections and differences among the different techniques. Problems and possible solutions with the use of improper priors are also described. Some of the most relevant methodologies are compared through theoretical comparisons and numerical experiments.

</details>

<details>

<summary>2022-01-04 18:54:09 - Flexible marked spatio-temporal point processes with applications to event sequences from association football</summary>

- *Santhosh Narayanan, Ioannis Kosmidis, Petros Dellaportas*

- `2103.04647v2` - [abs](http://arxiv.org/abs/2103.04647v2) - [pdf](http://arxiv.org/pdf/2103.04647v2)

> We develop a new family of marked point processes by focusing the characteristic properties of marked Hawkes processes exclusively to the space of marks, providing the freedom to specify a different model for the occurrence times. This is possible through the decomposition of the joint distribution of marks and times that allows to separately specify the conditional distribution of marks given the filtration of the process and the current time. We develop a Bayesian framework for the inference and prediction from this family of marked point processes that can naturally accommodate process and point-specific covariate information to drive cross-excitations, offering wide flexibility and applicability in the modelling of real-world processes. The framework is used here for the modelling of in-game event sequences from association football, resulting not only in inferences about previously unquantified characteristics of the game dynamics and extraction of event-specific team abilities, but also in predictions for the occurrence of events of interest, such as goals, corners or fouls, in a specified interval of time.

</details>

<details>

<summary>2022-01-04 21:35:01 - A hybrid approach to targeting social assistance</summary>

- *Lendie Follett, Heath Henderson*

- `2201.01356v1` - [abs](http://arxiv.org/abs/2201.01356v1) - [pdf](http://arxiv.org/pdf/2201.01356v1)

> Proxy means testing (PMT) and community-based targeting (CBT) are two of the leading methods for targeting social assistance in developing countries. In this paper, we present a hybrid targeting method that incorporates CBT's emphasis on local information and preferences with PMT's reliance on verifiable indicators. Specifically, we outline a Bayesian framework for targeting that resembles PMT in that beneficiary selection is based on a weighted sum of sociodemographic characteristics. We nevertheless propose calibrating the weights to preference rankings from community targeting exercises, implying that the weights used by our method reflect how potential beneficiaries themselves substitute sociodemographic features when making targeting decisions. We discuss several practical extensions to the model, including a generalization to multiple rankings per community, an adjustment for elite capture, a method for incorporating auxiliary information on potential beneficiaries, and a dynamic updating procedure. We further provide an empirical illustration using data from Burkina Faso and Indonesia.

</details>

<details>

<summary>2022-01-04 21:38:56 - Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis</summary>

- *Max Goplerud, Kosuke Imai, Nicole E. Pashley*

- `2201.01357v1` - [abs](http://arxiv.org/abs/2201.01357v1) - [pdf](http://arxiv.org/pdf/2201.01357v1)

> Estimation of heterogeneous treatment effects is an active area of research in causal inference. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach is based on a Bayesian mixture of regularized regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling cluster membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.

</details>

<details>

<summary>2022-01-05 06:40:28 - RobustCalibration: Robust Calibration of Computer Models in R</summary>

- *Mengyang Gu*

- `2201.01476v1` - [abs](http://arxiv.org/abs/2201.01476v1) - [pdf](http://arxiv.org/pdf/2201.01476v1)

> Two fundamental research tasks in science and engineering are forward predictions and data inversion. This article introduces a recent R package RobustCalibration for Bayesian data inversion and model calibration by experiments and field observations. Mathematical models for forward predictions are often written in computer code, and they can be computationally expensive slow to run. To overcome the computational bottleneck from the simulator, we implemented a statistical emulator from the RobustGaSP package for emulating both scalar-valued or vector-valued computer model outputs. Both posterior sampling and maximum likelihood approach are implemented in the RobustCalibration package for parameter estimation. For imperfect computer models, we implement Gaussian stochastic process and the scaled Gaussian stochastic process for modeling the discrepancy function between the reality and mathematical model. This package is applicable to various types of field observations, such as repeated experiments and multiple sources of measurements. We discuss numerical examples of calibrating mathematical models that have closed-form expressions, and differential equations solved by numerical methods.

</details>

<details>

<summary>2022-01-05 10:56:58 - Inverse Extended Kalman Filter</summary>

- *Himali Singh, Arpan Chattopadhyay, Kumar Vijay Mishra*

- `2201.01539v1` - [abs](http://arxiv.org/abs/2201.01539v1) - [pdf](http://arxiv.org/pdf/2201.01539v1)

> Recent advances in counter-adversarial systems have garnered significant research interest in inverse filtering from a Bayesian perspective. For example, interest in estimating the adversary's Kalman filter tracked estimate with the purpose of predicting the adversary's future steps has led to recent formulations of inverse Kalman filter (I-KF). In this context of inverse filtering, we address the key challenges of nonlinear process dynamics and unknown input to the forward filter by proposing inverse extended Kalman filter (I-EKF). We derive I-EKF with and without an unknown input by considering nonlinearity in both forward and inverse state-space models. In the process, I-KF-with-unknown-input is also obtained. We then provide theoretical stability guarantees using both bounded nonlinearity and unknown matrix approaches. We further generalize these formulations and results to the case of higher-order, Gaussian-sum, and dithered I-EKFs. Numerical experiments validate our methods for various proposed inverse filters using the recursive Cram\'er-Rao lower bound as a benchmark.

</details>

<details>

<summary>2022-01-05 17:11:09 - Inferring the sources of HIV infection in Africa from deep sequence data with semi-parametric Bayesian Poisson flow models</summary>

- *Xiaoyue Xi, Simon EF Spencer, Matthew Hall, M Kate Grabowski, Joseph Kagaayi, Oliver Ratmann*

- `2110.12273v4` - [abs](http://arxiv.org/abs/2110.12273v4) - [pdf](http://arxiv.org/pdf/2110.12273v4)

> Pathogen deep-sequencing is an increasingly routinely used technology in infectious disease surveillance. We present a semi-parametric Bayesian Poisson model to exploit these emerging data for inferring infectious disease transmission flows and the sources of infection at the population level. The framework is computationally scalable in high dimensional flow spaces thanks to Hilbert Space Gaussian process approximations, allows for sampling bias adjustments, and estimation of gender- and age-specific transmission flows at finer resolution than previously possible. We apply the approach to densely sampled, population-based HIV deep-sequence data from Rakai, Uganda, and find substantive evidence that adolescent and young women are predominantly infected through age-disparate relationships.

</details>

<details>

<summary>2022-01-05 17:22:35 - Bridging disconnected networks of first and second lines of biologic therapies in rheumatoid arthritis with registry data: Bayesian evidence synthesis with target trial emulation</summary>

- *Sylwia Bujkiewicz, Janharpreet Singh, Lorna Wheaton, David Jenkins, Reynaldo Martina, Kimme Hyrich, Keith R. Abrams*

- `2201.01720v1` - [abs](http://arxiv.org/abs/2201.01720v1) - [pdf](http://arxiv.org/pdf/2201.01720v1)

> Objective: We aim to utilise real world data in evidence synthesis to optimise an evidence base for the effectiveness of biologic therapies in rheumatoid arthritis in order to allow for evidence on first-line therapies to inform second-line effectiveness estimates. Study design and setting: We use data from the British Society for Rheumatology Biologics Register for Rheumatoid Arthritis (BSRBR-RA) to supplement RCT evidence obtained from the literature, by emulating target trials of treatment sequences to estimate treatment effects in each line of therapy. Treatment effects estimates from the target trials inform a bivariate network meta-analysis (NMA) of first and second-line treatments. Results: Summary data were obtained from 21 trials of biologic therapies including 2 for second-line treatment and results from six emulated target trials of both treatment lines. Bivariate NMA resulted in a decrease in uncertainty around the effectiveness estimates of the second-line therapies, when compared to the results of univariate NMA, and allowed for predictions of treatment effects not evaluated in second-line RCTs. Conclusion: Bivariate NMA provides effectiveness estimates for all treatments in first- and second-line, including predicted effects in second-line where these estimates did not exist in the data. This novel methodology may have further applications, for example for bridging networks of trials in children and adults.

</details>

<details>

<summary>2022-01-06 02:55:55 - Ambiguity and Partial Bayesian Updating</summary>

- *Matthew Kovach*

- `2102.11429v2` - [abs](http://arxiv.org/abs/2102.11429v2) - [pdf](http://arxiv.org/pdf/2102.11429v2)

> Models of updating a set of priors either do not allow a decision maker to make inference about her priors (full bayesian updating or FB) or require an extreme degree of selection (maximum likelihood updating or ML). I characterize a general method for updating a set of priors, partial bayesian updating (PB), in which the decision maker (i) utilizes an event-dependent threshold to determine whether a prior is likely enough, conditional on observed information, and then (ii) applies Bayes' rule to the sufficiently likely priors. I show that PB nests FB and ML and explore its behavioral properties.

</details>

<details>

<summary>2022-01-06 03:42:50 - Risk Loadings in Classification Ratemaking</summary>

- *Liang Yang, Zhengxiao Li, Shengwang Meng*

- `2002.01798v2` - [abs](http://arxiv.org/abs/2002.01798v2) - [pdf](http://arxiv.org/pdf/2002.01798v2)

> The risk premium of a policy is the sum of the pure premium and the risk loading. In the classification ratemaking process, generalized linear models are usually used to calculate pure premiums, and various premium principles are applied to derive the risk loadings. No matter which premium principle is used, some risk loading parameters should be given in advance subjectively. To overcome this subjective problem and calculate the risk premium more reasonably and objectively, we propose a top-down method to calculate these risk loading parameters. First, we implement the bootstrap method to calculate the total risk premium of the portfolio. Then, under the constraint that the portfolio's total risk premium should equal the sum of the risk premiums of each policy, the risk loading parameters are determined. During this process, besides using generalized linear models, three kinds of quantile regression models are also applied, namely, traditional quantile regression model, fully parametric quantile regression model, and quantile regression model with coefficient functions. The empirical result shows that the risk premiums calculated by the method proposed in this study can reasonably differentiate the heterogeneity of different risk classes.

</details>

<details>

<summary>2022-01-06 10:14:15 - Machine learning assisted Bayesian model comparison: learnt harmonic mean estimator</summary>

- *Jason D. McEwen, Christopher G. R. Wallis, Matthew A. Price, Matthew M. Docherty*

- `2111.12720v2` - [abs](http://arxiv.org/abs/2111.12720v2) - [pdf](http://arxiv.org/pdf/2111.12720v2)

> We resurrect the infamous harmonic mean estimator for computing the marginal likelihood (Bayesian evidence) and solve its problematic large variance. The marginal likelihood is a key component of Bayesian model selection since it is required to evaluate model posterior probabilities; however, its computation is challenging. The original harmonic mean estimator, first proposed in 1994 by Newton and Raftery, involves computing the harmonic mean of the likelihood given samples from the posterior. It was immediately realised that the original estimator can fail catastrophically since its variance can become very large and may not be finite. A number of variants of the harmonic mean estimator have been proposed to address this issue although none have proven fully satisfactory. We present the learnt harmonic mean estimator, a variant of the original estimator that solves its large variance problem. This is achieved by interpreting the harmonic mean estimator as importance sampling and introducing a new target distribution. The new target distribution is learned to approximate the optimal but inaccessible target, while minimising the variance of the resulting estimator. Since the estimator requires samples of the posterior only it is agnostic to the strategy used to generate posterior samples. We validate the estimator on a variety of numerical experiments, including a number of pathological examples where the original harmonic mean estimator fails catastrophically. In all cases our learnt harmonic mean estimator is shown to be highly accurate. The estimator is computationally scalable and can be applied to problems of dimension $\mathcal{O}(10^3)$ and beyond. Code implementing the learnt harmonic mean estimator is made publicly available.

</details>

<details>

<summary>2022-01-06 11:24:09 - Causal Analysis at Extreme Quantiles with Application to London Traffic Flow Data</summary>

- *Kaushik Jana, Prajamitra Bhuyan, Emma J. McCoy*

- `2108.10215v3` - [abs](http://arxiv.org/abs/2108.10215v3) - [pdf](http://arxiv.org/pdf/2108.10215v3)

> Transport engineers employ various interventions to enhance traffic-network performance. Recent emphasises on cycling as a sustainable travel mode aims to reduce traffic congestion. Quantifying the impacts of Cycle Superhighways is complicated due to the non-random assignment of such intervention over the transport network and heavy-tailed distribution of traffic flow. Treatment effects on asymmetric and heavy tailed distributions are better reflected at extreme tails rather than at averages or intermediate quantiles. In such situations, standard methods for estimating quantile treatment effects at the extremes can provide misleading inference due to the high variability of estimates. In this work, we propose a novel method which incorporates a heavy tailed component in the outcome distribution to estimate the extreme tails and simultaneously employs quantile regression to model the bulk part of the distribution utilising a state-of-the-art technique. Simulation results show the superiority of the proposed method over existing estimators for quantile causal effects at extremes in the case of heavy tailed distributions. The analysis of London transport data utilising the proposed method indicates that the traffic flow increased substantially after the Cycle Superhighway came into operation. The findings can assist government agencies in effective decision making to avoid high consequence events and improve network performance.

</details>

<details>

<summary>2022-01-06 12:05:40 - Combine Statistical Thinking With Open Scientific Practice: A Protocol of a Bayesian Research Project</summary>

- *Alexandra Sarafoglou, Anna van der Heijden, Tim Draws, Joran Cornelisse, Eric-Jan Wagenmakers, Maarten Marsman*

- `1810.07496v4` - [abs](http://arxiv.org/abs/1810.07496v4) - [pdf](http://arxiv.org/pdf/1810.07496v4)

> Current developments in the statistics community suggest that modern statistics education should be structured holistically, that is, by allowing students to work with real data and to answer concrete statistical questions, but also by educating them about alternative frameworks, such as Bayesian inference. In this article, we describe how we incorporated such a holistic structure in a Bayesian research project on ordered binomial probabilities. The project was conducted with a group of three undergraduate psychology students who had basic knowledge of Bayesian statistics and programming, but lacked formal mathematical training. The research project aimed to (1) convey the basic mathematical concepts of Bayesian inference; (2) have students experience the entire empirical cycle including collection, analysis, and interpretation of data and (3) teach students open science practices.

</details>

<details>

<summary>2022-01-06 12:58:23 - Bayesian Regression Approach for Building and Stacking Predictive Models in Time Series Analytics</summary>

- *Bohdan M. Pavlyshenko*

- `2201.02034v1` - [abs](http://arxiv.org/abs/2201.02034v1) - [pdf](http://arxiv.org/pdf/2201.02034v1)

> The paper describes the use of Bayesian regression for building time series models and stacking different predictive models for time series. Using Bayesian regression for time series modeling with nonlinear trend was analyzed. This approach makes it possible to estimate an uncertainty of time series prediction and calculate value at risk characteristics. A hierarchical model for time series using Bayesian regression has been considered. In this approach, one set of parameters is the same for all data samples, other parameters can be different for different groups of data samples. Such an approach allows using this model in the case of short historical data for specified time series, e.g. in the case of new stores or new products in the sales prediction problem. In the study of predictive models stacking, the models ARIMA, Neural Network, Random Forest, Extra Tree were used for the prediction on the first level of model ensemble. On the second level, time series predictions of these models on the validation set were used for stacking by Bayesian regression. This approach gives distributions for regression coefficients of these models. It makes it possible to estimate the uncertainty contributed by each model to stacking result. The information about these distributions allows us to select an optimal set of stacking models, taking into account the domain knowledge. The probabilistic approach for stacking predictive models allows us to make risk assessment for the predictions that are important in a decision-making process.

</details>

<details>

<summary>2022-01-07 01:56:49 - Location-Scale and Compensated Effects in Unconditional Quantile Regressions</summary>

- *Julian Martinez-Iriarte, Gabriel Montes-Rojas, Yixiao Sun*

- `2201.02292v1` - [abs](http://arxiv.org/abs/2201.02292v1) - [pdf](http://arxiv.org/pdf/2201.02292v1)

> This paper proposes an extension of the unconditional quantile regression analysis to (i) location-scale shifts, and (ii) compensated shifts. The first case is intended to study a counterfactual policy analysis aimed at increasing not only the mean or location of a covariate but also its dispersion or scale. The compensated shift refers to a situation where a shift in a covariate is compensated at a certain rate by another covariate. Not accounting for these possible scale or compensated effects will result in an incorrect assessment of the potential policy effects on the quantiles of an outcome variable. More general interventions and compensated shifts are also considered. The unconditional policy parameters are estimated with simple semi-parametric estimators, for which asymptotic properties are studied. Monte Carlo simulations are implemented to study their finite sample performances, and the proposed approach is applied to a Mincer equation to study the effects of a location-scale shift in education on the unconditional quantiles of wages.

</details>

<details>

<summary>2022-01-07 02:26:45 - New designs for Bayesian adaptive cluster-randomized trials</summary>

- *Junwei Shen, Shirin Golchi, Erica E. M. Moodie, David Benrimoh*

- `2201.02301v1` - [abs](http://arxiv.org/abs/2201.02301v1) - [pdf](http://arxiv.org/pdf/2201.02301v1)

> Adaptive approaches, allowing for more flexible trial design, have been proposed for individually randomized trials to save time or reduce sample size. However, adaptive designs for cluster-randomized trials in which groups of participants rather than individuals are randomized to treatment arms are less common. Motivated by a cluster-randomized trial designed to assess the effectiveness of a machine-learning based clinical decision support system for physicians treating patients with depression, two Bayesian adaptive designs for cluster-randomized trials are proposed to allow for early stopping for efficacy at pre-planned interim analyses. The difference between the two designs lies in the way that participants are sequentially recruited. Given a maximum number of clusters as well as maximum cluster size allowed in the trial, one design sequentially recruits clusters with the given maximum cluster size, while the other recruits all clusters at the beginning of the trial but sequentially enrolls individual participants until the trial is stopped early for efficacy or the final analysis has been reached. The design operating characteristics are explored via simulations for a variety of scenarios and two outcome types for the two designs. The simulation results show that for different outcomes the design choice may be different. We make recommendations for designs of Bayesian adaptive cluster-randomized trial based on the simulation results.

</details>

<details>

<summary>2022-01-07 02:51:39 - Time Series Anomaly Detection for Cyber-Physical Systems via Neural System Identification and Bayesian Filtering</summary>

- *Cheng Feng, Pengwei Tian*

- `2106.07992v2` - [abs](http://arxiv.org/abs/2106.07992v2) - [pdf](http://arxiv.org/pdf/2106.07992v2)

> Recent advances in AIoT technologies have led to an increasing popularity of utilizing machine learning algorithms to detect operational failures for cyber-physical systems (CPS). In its basic form, an anomaly detection module monitors the sensor measurements and actuator states from the physical plant, and detects anomalies in these measurements to identify abnormal operation status. Nevertheless, building effective anomaly detection models for CPS is rather challenging as the model has to accurately detect anomalies in presence of highly complicated system dynamics and unknown amount of sensor noise. In this work, we propose a novel time series anomaly detection method called Neural System Identification and Bayesian Filtering (NSIBF) in which a specially crafted neural network architecture is posed for system identification, i.e., capturing the dynamics of CPS in a dynamical state-space model; then a Bayesian filtering algorithm is naturally applied on top of the "identified" state-space model for robust anomaly detection by tracking the uncertainty of the hidden state of the system recursively over time. We provide qualitative as well as quantitative experiments with the proposed method on a synthetic and three real-world CPS datasets, showing that NSIBF compares favorably to the state-of-the-art methods with considerable improvements on anomaly detection in CPS.

</details>

<details>

<summary>2022-01-07 04:44:25 - Bayesian Online Change Point Detection for Baseline Shifts</summary>

- *Ginga Yoshizawa*

- `2201.02325v1` - [abs](http://arxiv.org/abs/2201.02325v1) - [pdf](http://arxiv.org/pdf/2201.02325v1)

> In time series data analysis, detecting change points on a real-time basis (online) is of great interest in many areas, such as finance, environmental monitoring, and medicine. One promising means to achieve this is the Bayesian online change point detection (BOCPD) algorithm, which has been successfully adopted in particular cases in which the time series of interest has a fixed baseline. However, we have found that the algorithm struggles when the baseline irreversibly shifts from its initial state. This is because with the original BOCPD algorithm, the sensitivity with which a change point can be detected is degraded if the data points are fluctuating at locations relatively far from the original baseline. In this paper, we not only extend the original BOCPD algorithm to be applicable to a time series whose baseline is constantly shifting toward unknown values but also visualize why the proposed extension works. To demonstrate the efficacy of the proposed algorithm compared to the original one, we examine these algorithms on two real-world data sets and six synthetic data sets.

</details>

<details>

<summary>2022-01-07 10:59:13 - Measurement Error Models for Spatial Network Lattice Data: Analysis of Car Crashes in Leeds</summary>

- *Andrea Gilardi, Riccardo Borgoni, Luca Presicce, Jorge Mateu*

- `2201.02394v1` - [abs](http://arxiv.org/abs/2201.02394v1) - [pdf](http://arxiv.org/pdf/2201.02394v1)

> Road casualties represent an alarming concern for modern societies, especially in poor and developing countries. In the last years, several authors developed sophisticated statistical approaches to help local authorities implement new policies and mitigate the problem. These models are typically developed taking into account a set of socio-economic or demographic variables, such as population density and traffic volumes. However, they usually ignore that the external factors may be suffering from measurement errors, which can severely bias the statistical inference. This paper presents a Bayesian hierarchical model to analyse car crashes occurrences at the network lattice level taking into account measurement error in the spatial covariates. The suggested methodology is exemplified considering all road collisions in the road network of Leeds (UK) from 2011 to 2019. Traffic volumes are approximated at the street segment level using an extensive set of road counts obtained from mobile devices, and the estimates are corrected using a measurement error model. Our results show that omitting measurement error considerably worsens the model's fit and attenuates the effects of imprecise covariates.

</details>

<details>

<summary>2022-01-07 13:28:35 - Functional Data Analysis for Extracting the Intrinsic Dimensionality of Spectra: Application to Chemical Homogeneity in the Open Cluster M67</summary>

- *Aarya A. Patil, Jo Bovy, Gwendolyn Eadie, Sebastian Jaimungal*

- `2109.10891v2` - [abs](http://arxiv.org/abs/2109.10891v2) - [pdf](http://arxiv.org/pdf/2109.10891v2)

> High-resolution spectroscopic surveys of the Milky Way have entered the Big Data regime and have opened avenues for solving outstanding questions in Galactic archaeology. However, exploiting their full potential is limited by complex systematics, whose characterization has not received much attention in modern spectroscopic analyses. In this work, we present a novel method to disentangle the component of spectral data space intrinsic to the stars from that due to systematics. Using functional principal component analysis on a sample of $18,933$ giant spectra from APOGEE, we find that the intrinsic structure above the level of observational uncertainties requires ${\approx}$10 functional principal components (FPCs). Our FPCs can reduce the dimensionality of spectra, remove systematics, and impute masked wavelengths, thereby enabling accurate studies of stellar populations. To demonstrate the applicability of our FPCs, we use them to infer stellar parameters and abundances of 28 giants in the open cluster M67. We employ Sequential Neural Likelihood, a simulation-based Bayesian inference method that learns likelihood functions using neural density estimators, to incorporate non-Gaussian effects in spectral likelihoods. By hierarchically combining the inferred abundances, we limit the spread of the following elements in M67: $\mathrm{Fe} \lesssim 0.02$ dex; $\mathrm{C} \lesssim 0.03$ dex; $\mathrm{O}, \mathrm{Mg}, \mathrm{Si}, \mathrm{Ni} \lesssim 0.04$ dex; $\mathrm{Ca} \lesssim 0.05$ dex; $\mathrm{N}, \mathrm{Al} \lesssim 0.07$ dex (at 68% confidence). Our constraints suggest a lack of self-pollution by core-collapse supernovae in M67, which has promising implications for the future of chemical tagging to understand the star formation history and dynamical evolution of the Milky Way.

</details>

<details>

<summary>2022-01-07 14:58:01 - PAC-Bayesian Matrix Completion with a Spectral Scaled Student Prior</summary>

- *The Tien Mai*

- `2104.08191v2` - [abs](http://arxiv.org/abs/2104.08191v2) - [pdf](http://arxiv.org/pdf/2104.08191v2)

> We study the problem of matrix completion in this paper. A spectral scaled Student prior is exploited to favour the underlying low-rank structure of the data matrix. We provide a thorough theoretical investigation for our approach through PAC-Bayesian bounds. More precisely, our PAC-Bayesian approach enjoys a minimax-optimal oracle inequality which guarantees that our method works well under model misspecification and under general sampling distribution. Interestingly, we also provide efficient gradient-based sampling implementations for our approach by using Langevin Monte Carlo. More specifically, we show that our algorithms are significantly faster than Gibbs sampler in this problem. To illustrate the attractive features of our inference strategy, some numerical simulations are conducted and an application to image inpainting is demonstrated.

</details>

<details>

<summary>2022-01-07 21:37:00 - Bayesian Cumulative Probability Models for Continuous and Mixed Outcomes</summary>

- *Nathan T. James, Frank E. Harrell Jr., Bryan E. Shepherd*

- `2102.00330v2` - [abs](http://arxiv.org/abs/2102.00330v2) - [pdf](http://arxiv.org/pdf/2102.00330v2)

> Ordinal cumulative probability models (CPMs) -- also known as cumulative link models -- such as the proportional odds regression model are typically used for discrete ordered outcomes, but can accommodate both continuous and mixed discrete/continuous outcomes since these are also ordered. Recent papers describe ordinal CPMs in this setting using non-parametric maximum likelihood estimation. We formulate a Bayesian CPM for continuous or mixed outcome data. Bayesian CPMs inherit many of the benefits of frequentist CPMs and have advantages with regard to interpretation, flexibility, and exact inference (within simulation error) for parameters and functions of parameters. We explore characteristics of the Bayesian CPM through simulations and a case study using HIV biomarker data. In addition, we provide the package 'bayesCPM' which implements Bayesian CPM models using the R interface to the Stan probabilistic programing language. The Bayesian CPM for continuous outcomes can be implemented with only minor modifications to the prior specification and, despite some limitations, has generally good statistical performance with moderate or large sample sizes.

</details>

<details>

<summary>2022-01-07 22:51:11 - An Improved Mathematical Model of Sepsis: Modeling, Bifurcation Analysis, and Optimal Control Study for Complex Nonlinear Infectious Disease System</summary>

- *Yuyang Chen, Kaiming Bi, Chih-Hang J. Wu, David Ben-Arieh, Ashesh Sinha*

- `2201.02702v1` - [abs](http://arxiv.org/abs/2201.02702v1) - [pdf](http://arxiv.org/pdf/2201.02702v1)

> Sepsis is a life-threatening medical emergency, which is a major cause of death worldwide and the second highest cause of mortality in the United States. Researching the optimal control treatment or intervention strategy on the comprehensive sepsis system is key in reducing mortality. For this purpose, first, this paper improves a complex nonlinear sepsis model proposed in our previous work. Then, bifurcation analyses are conducted for each sepsis subsystem to study the model behaviors under some system parameters. The bifurcation analysis results also further indicate the necessity of control treatment and intervention therapy. If the sepsis system is without adding any control under some parameter and initial system value settings, the system will perform persistent inflammation outcomes as time goes by. Therefore, we develop our complex improved nonlinear sepsis model into a sepsis optimal control model, and then use some effective biomarkers recommended in existing clinic practices as optimization objective function to measure the development of sepsis. Besides that, a Bayesian optimization algorithm by combining Recurrent neural network (RNN-BO algorithm) is introduced to predict the optimal control strategy for the studied sepsis optimal control system. The difference between the RNN-BO algorithm from other optimization algorithms is that once given any new initial system value setting (initial value is associated with the initial conditions of patients), the RNN-BO algorithm is capable of quickly predicting a corresponding time-series optimal control based on the historical optimal control data for any new sepsis patient. To demonstrate the effectiveness and efficiency of the RNN-BO algorithm on solving the optimal control solution on the complex nonlinear sepsis system, some numerical simulations are implemented by comparing with other optimization algorithms in this paper.

</details>

<details>

<summary>2022-01-08 02:26:56 - Bayesian Changepoint Estimation for Spatially Indexed Functional Time Series</summary>

- *Mengchen Wang, Trevor Harris, Bo Li*

- `2201.02742v1` - [abs](http://arxiv.org/abs/2201.02742v1) - [pdf](http://arxiv.org/pdf/2201.02742v1)

> We propose a Bayesian hierarchical model to simultaneously estimate mean based changepoints in spatially correlated functional time series. Unlike previous methods that assume a shared changepoint at all spatial locations or ignore spatial correlation, our method treats changepoints as a spatial process. This allows our model to respect spatial heterogeneity and exploit spatial correlations to improve estimation. Our method is derived from the ubiquitous cumulative sum (CUSUM) statistic that dominates changepoint detection in functional time series. However, instead of directly searching for the maximum of the CUSUM based processes, we build spatially correlated two-piece linear models with appropriate variance structure to locate all changepoints at once. The proposed linear model approach increases the robustness of our method to variability in the CUSUM process, which, combined with our spatial correlation model, improves changepoint estimation near the edges. We demonstrate through extensive simulation studies that our method outperforms existing functional changepoint estimators in terms of both estimation accuracy and uncertainty quantification, under either weak and strong spatial correlation, and weak and strong change signals. Finally, we demonstrate our method using a temperature data set and a coronavirus disease 2019 (COVID-19) study.

</details>

<details>

<summary>2022-01-08 20:05:40 - Introduction to Multi-Armed Bandits</summary>

- *Aleksandrs Slivkins*

- `1904.07272v7` - [abs](http://arxiv.org/abs/1904.07272v7) - [pdf](http://arxiv.org/pdf/1904.07272v7)

> Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.   The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.   The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.

</details>

<details>

<summary>2022-01-09 05:03:12 - Reducing bias and variance in quantile estimates with an exponential model</summary>

- *Rohit Pandey*

- `2201.01421v2` - [abs](http://arxiv.org/abs/2201.01421v2) - [pdf](http://arxiv.org/pdf/2201.01421v2)

> Percentiles and more generally, quantiles are commonly used in various contexts to summarize data. For most distributions, there is exactly one quantile that is unbiased. For distributions like the Gaussian that have the same mean and median, that becomes the medians. There are different ways to estimate quantiles from finite samples described in the literature and implemented in statistics packages. It is possible to leverage the memory-less property of the exponential distribution and design high quality estimators that are unbiased and have low variance and mean squared errors. Naturally, these estimators out-perform the ones in statistical packages when the underlying distribution is exponential. But, they also happen to generalize well when that assumption is violated.

</details>

<details>

<summary>2022-01-09 10:04:28 - Algorithms for Inference in SVARs Identified with Sign and Zero Restrictions</summary>

- *Matthew Read*

- `2109.10676v2` - [abs](http://arxiv.org/abs/2109.10676v2) - [pdf](http://arxiv.org/pdf/2109.10676v2)

> I develop algorithms to facilitate Bayesian inference in structural vector autoregressions that are set-identified with sign and zero restrictions by showing that the system of restrictions is equivalent to a system of sign restrictions in a lower-dimensional space. Consequently, algorithms applicable under sign restrictions can be extended to allow for zero restrictions. Specifically, I extend algorithms proposed in Amir-Ahmadi and Drautzburg (2021) to check whether the identified set is nonempty and to sample from the identified set without rejection sampling. I compare the new algorithms to alternatives by applying them to variations of the model considered by Arias et al. (2019), who estimate the effects of US monetary policy using sign and zero restrictions on the monetary policy reaction function. The new algorithms are particularly useful when a rich set of sign restrictions substantially truncates the identified set given the zero restrictions.

</details>

<details>

<summary>2022-01-09 18:55:12 - The role of exchangeability in causal inference</summary>

- *Olli Saarela, David A. Stephens, Erica E. M. Moodie*

- `2006.01799v3` - [abs](http://arxiv.org/abs/2006.01799v3) - [pdf](http://arxiv.org/pdf/2006.01799v3)

> The notion of exchangeability has been recognized in the causal inference literature in various guises, but only rarely in the original meaning as a symmetry property of probability distributions. Since the latter is a standard ingredient in Bayesian inference, we argue that in Bayesian causal inference it is natural to link the causal model, including the notion of confounding and definition of causal contrasts of interest, to the concept of exchangeability. Here we propose a probabilistic between-group exchangeability property as an identifying condition for causal effects, relate it to alternative conditions for unconfounded inferences, commonly stated using potential outcomes, and define causal contrasts in the presence of exchangeability in terms of posterior predictive expectations for further exchangeable units. While our main focus is in a point treatment setting, we also investigate how this reasoning carries over to longitudinal settings.

</details>

<details>

<summary>2022-01-09 19:44:54 - Information Borrowing in Regression Models</summary>

- *Amy Zhang, Le Bao, Michael J. Daniels*

- `2201.03077v1` - [abs](http://arxiv.org/abs/2201.03077v1) - [pdf](http://arxiv.org/pdf/2201.03077v1)

> Model development often takes data structure, subject matter considerations, model assumptions, and goodness of fit into consideration. To diagnose issues with any of these factors, it can be helpful to understand regression model estimates at a more granular level. We propose a new method for decomposing point estimates from a regression model via weights placed on data clusters. The weights are informed only by the model specification and data availability and thus can be used to explicitly link the effects of data imbalance and model assumptions to actual model estimates. The weight matrix has been understood in linear models as the hat matrix in the existing literature. We extend it to Bayesian hierarchical regression models that incorporate prior information and complicated dependence structures through the covariance among random effects. We show that the model weights, which we call borrowing factors, generalize shrinkage and information borrowing to all regression models. In contrast, the focus of the hat matrix has been mainly on the diagonal elements indicating the amount of leverage. We also provide metrics that summarize the borrowing factors and are practically useful. We present the theoretical properties of the borrowing factors and associated metrics and demonstrate their usage in two examples. By explicitly quantifying borrowing and shrinkage, researchers can better incorporate domain knowledge and evaluate model performance and the impacts of data properties such as data imbalance or influential points.

</details>

<details>

<summary>2022-01-10 01:42:28 - Loss-calibrated expectation propagation for approximate Bayesian decision-making</summary>

- *Michael J. Morais, Jonathan W. Pillow*

- `2201.03128v1` - [abs](http://arxiv.org/abs/2201.03128v1) - [pdf](http://arxiv.org/pdf/2201.03128v1)

> Approximate Bayesian inference methods provide a powerful suite of tools for finding approximations to intractable posterior distributions. However, machine learning applications typically involve selecting actions, which -- in a Bayesian setting -- depend on the posterior distribution only via its contribution to expected utility. A growing body of work on loss-calibrated approximate inference methods has therefore sought to develop posterior approximations sensitive to the influence of the utility function. Here we introduce loss-calibrated expectation propagation (Loss-EP), a loss-calibrated variant of expectation propagation. This method resembles standard EP with an additional factor that "tilts" the posterior towards higher-utility decisions. We show applications to Gaussian process classification under binary utility functions with asymmetric penalties on False Negative and False Positive errors, and show how this asymmetry can have dramatic consequences on what information is "useful" to capture in an approximation.

</details>

<details>

<summary>2022-01-10 06:22:30 - Non-Asymptotic Guarantees for Robust Statistical Learning under $(1+\varepsilon)$-th Moment Assumption</summary>

- *Lihu Xu, Fang Yao, Qiuran Yao, Huiming Zhang*

- `2201.03182v1` - [abs](http://arxiv.org/abs/2201.03182v1) - [pdf](http://arxiv.org/pdf/2201.03182v1)

> There has been a surge of interest in developing robust estimators for models with heavy-tailed data in statistics and machine learning. This paper proposes a log-truncated M-estimator for a large family of statistical regressions and establishes its excess risk bound under the condition that the data have $(1+\varepsilon)$-th moment with $\varepsilon \in (0,1]$. With an additional assumption on the associated risk function, we obtain an $\ell_2$-error bound for the estimation. Our theorems are applied to establish robust M-estimators for concrete regressions. Besides convex regressions such as quantile regression and generalized linear models, many non-convex regressions can also be fit into our theorems, we focus on robust deep neural network regressions, which can be solved by the stochastic gradient descent algorithms. Simulations and real data analysis demonstrate the superiority of log-truncated estimations over standard estimations.

</details>

<details>

<summary>2022-01-10 06:32:31 - Optimal Experimental Design for Staggered Rollouts</summary>

- *Ruoxuan Xiong, Susan Athey, Mohsen Bayati, Guido Imbens*

- `1911.03764v3` - [abs](http://arxiv.org/abs/1911.03764v3) - [pdf](http://arxiv.org/pdf/1911.03764v3)

> In this paper, we study the problem of designing experiments that are conducted on a set of units such as users or groups of users in an online marketplace, for multiple time periods such as weeks or months. These experiments are particularly useful to study the treatments that have causal effects on both current and future outcomes (instantaneous and lagged effects). The design problem involves selecting a treatment time for each unit, before or during the experiment, in order to most precisely estimate the instantaneous and lagged effects, post experimentation. This optimization of the treatment decisions can directly minimize the opportunity cost of the experiment by reducing its sample size requirement. The optimization is an NP-hard integer program for which we provide a near-optimal solution, when the design decisions are performed all at the beginning (fixed-sample-size designs). Next, we study sequential experiments that allow adaptive decisions during the experiments, and also potentially early stop the experiments, further reducing their cost. However, the sequential nature of these experiments complicates both the design phase and the estimation phase. We propose a new algorithm, PGAE, that addresses these challenges by adaptively making treatment decisions, estimating the treatment effects, and drawing valid post-experimentation inference. PGAE combines ideas from Bayesian statistics, dynamic programming, and sample splitting. Using synthetic experiments on real data sets from multiple domains, we demonstrate that our proposed solutions for fixed-sample-size and sequential experiments reduce the opportunity cost of the experiments by over 50% and 70%, respectively, compared to benchmarks.

</details>

<details>

<summary>2022-01-10 07:15:42 - Gaussian Process Regression in the Flat Limit</summary>

- *Simon Barthelm, Pierre-Olivier Amblard, Nicolas Tremblay, Konstantin Usevich*

- `2201.01074v2` - [abs](http://arxiv.org/abs/2201.01074v2) - [pdf](http://arxiv.org/pdf/2201.01074v2)

> Gaussian process (GP) regression is a fundamental tool in Bayesian statistics. It is also known as kriging and is the Bayesian counterpart to the frequentist kernel ridge regression. Most of the theoretical work on GP regression has focused on a large-$n$ asymptotics, characterising the behaviour of GP regression as the amount of data increases. Fixed-sample analysis is much more difficult outside of simple cases, such as locations on a regular grid. In this work we perform a fixed-sample analysis that was first studied in the context of approximation theory by Driscoll & Fornberg (2002), called the "flat limit". In flat-limit asymptotics, the goal is to characterise kernel methods as the length-scale of the kernel function tends to infinity, so that kernels appear flat over the range of the data. Surprisingly, this limit is well-defined, and displays interesting behaviour: Driscoll & Fornberg showed that radial basis interpolation converges in the flat limit to polynomial interpolation, if the kernel is Gaussian. Leveraging recent results on the spectral behaviour of kernel matrices in the flat limit, we study the flat limit of Gaussian process regression. Results show that Gaussian process regression tends in the flat limit to (multivariate) polynomial regression, or (polyharmonic) spline regression, depending on the kernel. Importantly, this holds for both the predictive mean and the predictive variance, so that the posterior predictive distributions become equivalent. Our results have practical consequences: for instance, they show that optimal GP predictions in the sense of leave-one-out loss may occur at very large length-scales, which would be invisible to current implementations because of numerical difficulties.

</details>

<details>

<summary>2022-01-10 08:31:21 - Manifold Markov chain Monte Carlo methods for Bayesian inference in diffusion models</summary>

- *Matthew M. Graham, Alexandre H. Thiery, Alexandros Beskos*

- `1912.02982v3` - [abs](http://arxiv.org/abs/1912.02982v3) - [pdf](http://arxiv.org/pdf/1912.02982v3)

> Bayesian inference for nonlinear diffusions, observed at discrete times, is a challenging task that has prompted the development of a number of algorithms, mainly within the computational statistics community. We propose a new direction, and accompanying methodology, borrowing ideas from statistical physics and computational chemistry, for inferring the posterior distribution of latent diffusion paths and model parameters, given observations of the process. Joint configurations of the underlying process noise and of parameters, mapping onto diffusion paths consistent with observations, form an implicitly defined manifold. Then, by making use of a constrained Hamiltonian Monte Carlo algorithm on the embedded manifold, we are able to perform computationally efficient inference for a class of discretely observed diffusion models. Critically, in contrast with other approaches proposed in the literature, our methodology is highly automated, requiring minimal user intervention and applying alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. Exploiting Markovianity, we propose a variant of the method with complexity that scales linearly in the resolution of path discretisation and the number of observation times. Python code reproducing the results is available at https://doi.org/10.5281/zenodo.5796148

</details>

<details>

<summary>2022-01-10 12:24:35 - A nonBayesian view of Hempel's paradox of the ravens</summary>

- *Yudi Pawitan*

- `2107.02522v3` - [abs](http://arxiv.org/abs/2107.02522v3) - [pdf](http://arxiv.org/pdf/2107.02522v3)

> In Hempel's paradox of the ravens, seeing a red pencil is considered as supporting evidence that all ravens are black. Also known as the Paradox of Confirmation, the paradox and its many resolutions indicate that we cannot underestimate the logical and statistical elements needed in the assessment of evidence in support of a hypothesis. Most of the previous analyses of the paradox are within the Bayesian framework. These analyses and Hempel himself generally accept the paradoxical conclusion; it feels paradoxical supposedly because the amount of evidence is extremely small. Here I describe a nonBayesian analysis of various statistical models with an accompanying likelihood-based reasoning. The analysis shows that the paradox feels paradoxical because there are natural models where observing a red pencil has no relevance to the color of ravens. In general the value of the evidence depends crucially on the sampling scheme and on the assumption about the underlying parameters of the relevant model.

</details>

<details>

<summary>2022-01-10 12:28:26 - AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms</summary>

- *Albert Podusenko, Bart van Erp, Magnus Koudahl, Bert de Vries*

- `2112.13366v2` - [abs](http://arxiv.org/abs/2112.13366v2) - [pdf](http://arxiv.org/pdf/2112.13366v2)

> In this paper we present AIDA, which is an active inference-based agent that iteratively designs a personalized audio processing algorithm through situated interactions with a human client. The target application of AIDA is to propose on-the-spot the most interesting alternative values for the tuning parameters of a hearing aid (HA) algorithm, whenever a HA client is not satisfied with their HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of optimal (acoustic) context-aware Bayesian trial design. In computational terms, AIDA is realized as an active inference-based agent with an Expected Free Energy criterion for trial design. This type of architecture is inspired by neuro-economic models on efficient (Bayesian) trial design in brains and implies that AIDA comprises generative probabilistic models for acoustic signals and user responses. We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive filters and a user response model based on a Gaussian Process Classifier. The full AIDA agent has been implemented in a factor graph for the generative model and all tasks (parameter learning, acoustic context classification, trial design, etc.) are realized by variational message passing on the factor graph. All verification and validation experiments and demonstrations are freely accessible at our GitHub repository.

</details>

<details>

<summary>2022-01-10 14:26:03 - A Tutorial on Learning With Bayesian Networks</summary>

- *David Heckerman*

- `2002.00269v3` - [abs](http://arxiv.org/abs/2002.00269v3) - [pdf](http://arxiv.org/pdf/2002.00269v3)

> A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with Bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.

</details>

<details>

<summary>2022-01-10 16:39:25 - Bayesian Consistency with the Supremum Metric</summary>

- *Nhat Ho, Stephen G. Walker*

- `2201.03447v1` - [abs](http://arxiv.org/abs/2201.03447v1) - [pdf](http://arxiv.org/pdf/2201.03447v1)

> We present simple conditions for Bayesian consistency in the supremum metric. The key to the technique is a triangle inequality which allows us to explicitly use weak convergence, a consequence of the standard Kullback--Leibler support condition for the prior. A further condition is to ensure that smoothed versions of densities are not too far from the original density, thus dealing with densities which could track the data too closely. A key result of the paper is that we demonstrate supremum consistency using weaker conditions compared to those currently used to secure $\mathbb{L}_1$ consistency.

</details>

<details>

<summary>2022-01-10 18:11:48 - The How and Why of Bayesian Nonparametric Causal Inference</summary>

- *Antonio R. Linero, Joseph L. Antonelli*

- `2111.03897v2` - [abs](http://arxiv.org/abs/2111.03897v2) - [pdf](http://arxiv.org/pdf/2111.03897v2)

> Spurred on by recent successes in causal inference competitions, Bayesian nonparametric (and high-dimensional) methods have recently seen increased attention in the causal inference literature. In this paper, we present a comprehensive overview of Bayesian nonparametric applications to causal inference. Our aims are to (i) introduce the fundamental Bayesian nonparametric toolkit; (ii) discuss how to determine which tool is most appropriate for a given problem; and (iii) show how to avoid common pitfalls in applying Bayesian nonparametric methods in high-dimensional settings. Unlike standard fixed-dimensional parametric problems, where outcome modeling alone can sometimes be effective, we argue that most of the time it is necessary to model both the selection and outcome processes.

</details>

<details>

<summary>2022-01-10 23:29:10 - A Horseshoe mixture model for Bayesian screening with an application to light sheet fluorescence microscopy in brain imaging</summary>

- *Francesco Denti, Ricardo Azevedo, Chelsie Lo, Damian Wheeler, Sunil P. Gandhi, Michele Guindani, Babak Shahbaba*

- `2106.08281v2` - [abs](http://arxiv.org/abs/2106.08281v2) - [pdf](http://arxiv.org/pdf/2106.08281v2)

> Most classical screening methods partition the units (e.g., brain regions) into two classes: significant and non-significant. In our context, some binary grouping methods lead to overly simplistic discoveries by filtering out weak but important signals, that could be adulterated by the noise present in the data. To overcome this limitation, we introduce a new Bayesian approach that can classify the brain regions into several tiers with varying degrees of relevance. Our approach is based on a combination of shrinkage priors - widely used in regression and multiple hypothesis testing problems - and mixture models - commonly used in model-based clustering. In contrast to the existing regularizing prior distributions, which either use the spike-and-slab prior or continuous scale mixtures, our class of priors is based on a discrete mixture of continuous scale mixtures and As a result, our approach provides a more general setting for Bayesian sparse estimation, drastically reduces the number of shrinkage parameters needed, and creates a framework for sharing information across units of interest. This approach leads to more biologically meaningful and interpretable results in our brain imaging problem, allowing the discrimination between active and inactive regions, while ranking the discoveries into clusters representing tiers of similar importance.

</details>

<details>

<summary>2022-01-11 08:55:38 - Bambi: A simple interface for fitting Bayesian linear models in Python</summary>

- *Toms Capretto, Camen Piho, Ravin Kumar, Jacob Westfall, Tal Yarkoni, Osvaldo A. Martin*

- `2012.10754v4` - [abs](http://arxiv.org/abs/2012.10754v4) - [pdf](http://arxiv.org/pdf/2012.10754v4)

> The popularity of Bayesian statistical methods has increased dramatically in recent years across many research areas and industrial applications. This is the result of a variety of methodological advances with faster and cheaper hardware as well as the development of new software tools. Here we introduce an open source Python package named Bambi (BAyesian Model Building Interface) that is built on top of the PyMC probabilistic programming framework and the ArviZ package for exploratory analysis of Bayesian models. Bambi makes it easy to specify complex generalized linear hierarchical models using a formula notation similar to those found in R. We demonstrate Bambi's versatility and ease of use with a few examples spanning a range of common statistical models including multiple regression, logistic regression, and mixed-effects modeling with crossed group specific effects. Additionally we discuss how automatic priors are constructed. Finally, we conclude with a discussion of our plans for the future development of Bambi.

</details>

<details>

<summary>2022-01-11 13:54:39 - Robust Generalised Bayesian Inference for Intractable Likelihoods</summary>

- *Takuo Matsubara, Jeremias Knoblauch, Franois-Xavier Briol, Chris. J. Oates*

- `2104.07359v3` - [abs](http://arxiv.org/abs/2104.07359v3) - [pdf](http://arxiv.org/pdf/2104.07359v3)

> Generalised Bayesian inference updates prior beliefs using a loss function, rather than a likelihood, and can therefore be used to confer robustness against possible mis-specification of the likelihood. Here we consider generalised Bayesian inference with a Stein discrepancy as a loss function, motivated by applications in which the likelihood contains an intractable normalisation constant. In this context, the Stein discrepancy circumvents evaluation of the normalisation constant and produces generalised posteriors that are either closed form or accessible using standard Markov chain Monte Carlo. On a theoretical level, we show consistency, asymptotic normality, and bias-robustness of the generalised posterior, highlighting how these properties are impacted by the choice of Stein discrepancy. Then, we provide numerical experiments on a range of intractable distributions, including applications to kernel-based exponential family models and non-Gaussian graphical models.

</details>

<details>

<summary>2022-01-11 13:56:34 - The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks</summary>

- *Takuo Matsubara, Chris J. Oates, Franois-Xavier Briol*

- `2010.08488v4` - [abs](http://arxiv.org/abs/2010.08488v4) - [pdf](http://arxiv.org/pdf/2010.08488v4)

> Bayesian neural networks attempt to combine the strong predictive performance of neural networks with formal quantification of uncertainty associated with the predictive output in the Bayesian framework. However, it remains unclear how to endow the parameters of the network with a prior distribution that is meaningful when lifted into the output space of the network. A possible solution is proposed that enables the user to posit an appropriate Gaussian process covariance function for the task at hand. Our approach constructs a prior distribution for the parameters of the network, called a ridgelet prior, that approximates the posited Gaussian process in the output space of the network. In contrast to existing work on the connection between neural networks and Gaussian processes, our analysis is non-asymptotic, with finite sample-size error bounds provided. This establishes the universality property that a Bayesian neural network can approximate any Gaussian process whose covariance function is sufficiently regular. Our experimental assessment is limited to a proof-of-concept, where we demonstrate that the ridgelet prior can out-perform an unstructured prior on regression problems for which a suitable Gaussian process prior can be provided.

</details>

<details>

<summary>2022-01-11 14:56:11 - Meta-Learning Reliable Priors in the Function Space</summary>

- *Jonas Rothfuss, Dominique Heyn, Jinfan Chen, Andreas Krause*

- `2106.03195v2` - [abs](http://arxiv.org/abs/2106.03195v2) - [pdf](http://arxiv.org/pdf/2106.03195v2)

> When data are scarce meta-learning can improve a learner's accuracy by harnessing previous experience from related learning tasks. However, existing methods have unreliable uncertainty estimates which are often overconfident. Addressing these shortcomings, we introduce a novel meta-learning framework, called F-PACOH, that treats meta-learned priors as stochastic processes and performs meta-level regularization directly in the function space. This allows us to directly steer the probabilistic predictions of the meta-learner towards high epistemic uncertainty in regions of insufficient meta-training data and, thus, obtain well-calibrated uncertainty estimates. Finally, we showcase how our approach can be integrated with sequential decision making, where reliable uncertainty quantification is imperative. In our benchmark study on meta-learning for Bayesian Optimization (BO), F-PACOH significantly outperforms all other meta-learners and standard baselines.

</details>

<details>

<summary>2022-01-11 19:15:10 - Bayesian Scalable Precision Factor Analysis for Massive Sparse Gaussian Graphical Models</summary>

- *Noirrit Kiran Chandra, Peter Mueller, Abhra Sarkar*

- `2107.11316v3` - [abs](http://arxiv.org/abs/2107.11316v3) - [pdf](http://arxiv.org/pdf/2107.11316v3)

> We propose a novel approach to estimating the precision matrix of multivariate Gaussian data that relies on decomposing them into a low-rank and a diagonal component. Such decompositions are very popular for modeling large covariance matrices as they admit a latent factor based representation that allows easy inference. The same is however not true for precision matrices due to the lack of computationally convenient representations which restricts inference to low-to-moderate dimensional problems. We address this remarkable gap in the literature by building on a latent variable representation for such decomposition for precision matrices. The construction leads to an efficient Gibbs sampler that scales very well to high-dimensional problems far beyond the limits of the current state-of-the-art. The ability to efficiently explore the full posterior space also allows the model uncertainty to be easily assessed. The decomposition crucially additionally allows us to adapt sparsity inducing priors to shrink the insignificant entries of the precision matrix toward zero, making the approach adaptable to high-dimensional small-sample-size sparse settings. Exact zeros in the matrix encoding the underlying conditional independence graph are then determined via a novel posterior false discovery rate control procedure. A near minimax optimal posterior concentration rate for estimating precision matrices is attained by our method under mild regularity assumptions. We evaluate the method's empirical performance through synthetic experiments and illustrate its practical utility in data sets from two different application domains.

</details>

<details>

<summary>2022-01-11 19:15:22 - Escaping the curse of dimensionality in Bayesian model based clustering</summary>

- *Noirrit Kiran Chandra, Antonio Canale, David B. Dunson*

- `2006.02700v4` - [abs](http://arxiv.org/abs/2006.02700v4) - [pdf](http://arxiv.org/pdf/2006.02700v4)

> Bayesian mixture models are widely used for clustering of high-dimensional data with appropriate uncertainty quantification, but as the dimension increases, posterior inference often tends to favor too many or too few clusters. This article explains this behavior by studying the random partition posterior in a non-standard setting with a fixed sample size and increasing data dimensionality. We show conditions under which the finite sample posterior tends to either assign every observation to a different cluster or all observations to the same cluster as the dimension grows. Interestingly, the conditions do not depend on the choice of clustering prior, as long as all possible partitions of observations into clusters have positive prior probabilities, and hold irrespective of the true data-generating model. We then propose a class of latent mixtures for Bayesian clustering (Lamb) on a set of low-dimensional latent variables inducing a partition on the observed data. The model is amenable to scalable posterior inference and we show that it avoids the pitfalls of high-dimensionality under reasonable and mild assumptions. The proposed approach is shown to have good performance in simulation studies and an application to inferring cell types based on scRNAseq.

</details>

<details>

<summary>2022-01-11 23:37:40 - An Embedded Model Estimator for Non-Stationary Random Functions using Multiple Secondary Variables</summary>

- *Colin Daly*

- `2011.04116v4` - [abs](http://arxiv.org/abs/2011.04116v4) - [pdf](http://arxiv.org/pdf/2011.04116v4)

> An algorithm for non-stationary spatial modelling using multiple secondary variables is developed. It combines Geostatistics with Quantile Random Forests to give a new interpolation and stochastic simulation algorithm. This paper introduces the method and shows that it has consistency results that are similar in nature to those applying to geostatistical modelling and to Quantile Random Forests. The method allows for embedding of simpler interpolation techniques, such as Kriging, to further condition the model. The algorithm works by estimating a conditional distribution for the target variable at each target location. The family of such distributions is called the envelope of the target variable. From this, it is possible to obtain spatial estimates, quantiles and uncertainty. An algorithm to produce conditional simulations from the envelope is also developed. As they sample from the envelope, realizations are therefore locally influenced by relative changes of importance of secondary variables, trends and variability.

</details>

<details>

<summary>2022-01-12 09:33:47 - Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie</summary>

- *Rmi Laumont, Valentin de Bortoli, Andrs Almansa, Julie Delon, Alain Durmus, Marcelo Pereyra*

- `2103.04715v6` - [abs](http://arxiv.org/abs/2103.04715v6) - [pdf](http://arxiv.org/pdf/2103.04715v6)

> Since the seminal work of Venkatakrishnan et al. in 2013, Plug & Play (PnP) methods have become ubiquitous in Bayesian imaging. These methods derive Minimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for inverse problems in imaging by combining an explicit likelihood function with a prior that is implicitly defined by an image denoising algorithm. The PnP algorithms proposed in the literature mainly differ in the iterative schemes they use for optimisation or for sampling. In the case of optimisation schemes, some recent works guarantee the convergence to a fixed point, albeit not necessarily a MAP estimate. In the case of sampling schemes, to the best of our knowledge, there is no known proof of convergence. There also remain important open questions regarding whether the underlying Bayesian models and estimators are well defined, well-posed, and have the basic regularity properties required to support these numerical schemes. To address these limitations, this paper develops theory, methods, and provably convergent algorithms for performing Bayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA (Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference; and 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent results on the quantitative convergence of Markov chains, we establish detailed convergence guarantees for these two algorithms under realistic assumptions on the denoising operators used, with special attention to denoisers based on deep neural networks. We also show that these algorithms approximately target a decision-theoretically optimal Bayesian model that is well-posed. The proposed algorithms are demonstrated on several canonical problems such as image deblurring, inpainting, and denoising, where they are used for point estimation as well as for uncertainty visualisation and quantification.

</details>

<details>

<summary>2022-01-12 18:08:42 - Representation of Context-Specific Causal Models with Observational and Interventional Data</summary>

- *Eliana Duarte, Liam Solus*

- `2101.09271v3` - [abs](http://arxiv.org/abs/2101.09271v3) - [pdf](http://arxiv.org/pdf/2101.09271v3)

> We consider the problem of representing causal models that encode context-specific information for discrete data using a proper subclass of staged tree models which we call CStrees. We show that the context-specific information encoded by a CStree can be equivalently expressed via a collection of DAGs. As not all staged tree models admit this property, CStrees are a subclass that provides a transparent, intuitive and compact representation of context-specific causal information. We prove that CStrees admit a global Markov property which yields a graphical criterion for model equivalence generalizing that of Verma and Pearl for DAG models. These results extend to the general interventional model setting, making CStrees the first family of context-specific models admitting a characterization of interventional model equivalence. We also provide a closed-form formula for the maximum likelihood estimator of a CStree and use it to show that the Bayesian information criterion is a locally consistent score function for this model class. The performance of CStrees is analyzed on both simulated and real data, where we see that modeling with CStrees instead of general staged trees does not result in a significant loss of predictive accuracy, while affording DAG representations of context-specific causal information.

</details>

<details>

<summary>2022-01-12 19:27:18 - Geometric Conditions for the Discrepant Posterior Phenomenon and Connections to Simpson's Paradox</summary>

- *Yang Chen, Ruobin Gong, Min-ge Xie*

- `2001.08336v2` - [abs](http://arxiv.org/abs/2001.08336v2) - [pdf](http://arxiv.org/pdf/2001.08336v2)

> The discrepant posterior phenomenon (DPP) is a counter-intuitive phenomenon that can frequently occur in a Bayesian analysis of multivariate parameters. It refers to the phenomenon that a parameter estimate based on a posterior is more extreme than both of those inferred based on either the prior or the likelihood alone. Inferential claims that exhibit DPP defy the common intuition that the posterior is a prior-data compromise, and the phenomenon can be surprisingly ubiquitous in well-behaved Bayesian models. In this paper we revisit this phenomenon and, using point estimation as an example, derive conditions under which the DPP occurs in Bayesian models with exponential quadratic likelihoods and conjugate multivariate Gaussian priors. The family of exponential quadratic likelihood models includes Gaussian models and those models with local asymptotic normality property. We provide an intuitive geometric interpretation of the phenomenon and show that there exists a nontrivial space of marginal directions such that the DPP occurs. We further relate the phenomenon to the Simpson's paradox and discover their deep-rooted connection that is associated with marginalization. We also draw connections with Bayesian computational algorithms when difficult geometry exists. Our discovery demonstrates that DPP is more prevalent than previously understood and anticipated. Theoretical results are complemented by numerical illustrations. Scenarios covered in this study have implications for parameterization, sensitivity analysis, and prior choice for Bayesian modeling.

</details>

<details>

<summary>2022-01-12 21:25:38 - An integrated abundance model for estimating county-level prevalence of opioid misuse in Ohio</summary>

- *Staci A. Hepler, David Kline, Andrea Bonny, Erin McKnight, Lance A. Waller*

- `2101.01235v3` - [abs](http://arxiv.org/abs/2101.01235v3) - [pdf](http://arxiv.org/pdf/2101.01235v3)

> Opioid misuse is a national epidemic and a significant drug related threat to the United States. While the scale of the problem is undeniable, estimates of the local prevalence of opioid misuse are lacking, despite their importance to policy-making and resource allocation. This is due, in part, to the challenge of directly measuring opioid misuse at a local level. In this paper, we develop a Bayesian hierarchical spatio-temporal abundance model that integrates indirect county-level data on opioid-related outcomes with state-level survey estimates on prevalence of opioid misuse to estimate the latent county-level prevalence and counts of people who misuse opioids. A simulation study shows that our integrated model accurately recovers the latent counts and prevalence. We apply our model to county-level surveillance data on opioid overdose deaths and treatment admissions from the state of Ohio. Our proposed framework can be applied to other applications of small area estimation for hard to reach populations, which is a common occurrence with many health conditions such as those related to illicit behaviors.

</details>

<details>

<summary>2022-01-12 22:34:31 - Correlates of the country differences in the infection and mortality rates during the first wave of the COVID-19 pandemic: Evidence from Bayesian model averaging</summary>

- *Viktor Stojkoski, Zoran Utkovski, Petar Jolakoski, Dragan Tevdovski, Ljupco Kocarev*

- `2004.07947v9` - [abs](http://arxiv.org/abs/2004.07947v9) - [pdf](http://arxiv.org/pdf/2004.07947v9)

> In the initial wave of the COVID-19 pandemic we observed great discrepancies in both infection and mortality rates between countries. Besides the biological and epidemiological factors, a multitude of social and economic criteria also influence the extent to which these discrepancies appear. Consequently, there is an active debate regarding the critical socio-economic and health factors that correlate with the infection and mortality rates outcome of the pandemic. Here, we leverage Bayesian model averaging techniques and country level data to investigate the potential of 28 variables, describing a diverse set of health and socio-economic characteristics, in being correlates of the final number of infections and deaths during the first wave of the coronavirus pandemic. We show that only few variables are able to robustly correlate with these outcomes. To understand the relationship between the potential correlates in explaining the infection and death rates, we create a Jointness Space. Using this space, we conclude that the extent to which each variable is able to provide a credible explanation for the COVID-19 infections/mortality outcome varies between countries because of their heterogeneous features.

</details>

<details>

<summary>2022-01-13 11:57:17 - Assessing the Performance of Diagnostic Classification Models in Small Sample Contexts with Different Estimation Methods</summary>

- *Motonori Oka, Kensuke Okada*

- `2104.10975v2` - [abs](http://arxiv.org/abs/2104.10975v2) - [pdf](http://arxiv.org/pdf/2104.10975v2)

> Fueled by the call for formative assessments, diagnostic classification models (DCMs) have recently gained popularity in psychometrics. Despite their potential for providing diagnostic information that aids in classroom instruction and students' learning, empirical applications of DCMs to classroom assessments have been highly limited. This is partly because how DCMs with different estimation methods perform in small sample contexts is not yet well-explored. Hence, this study aims to investigate the performance of respondent classification and item parameter estimation with a comprehensive simulation design that resembles classroom assessments using different estimation methods. The key findings are the following: (1) although the marked difference in respondent classification accuracy was not observed among the maximum likelihood (ML), Bayesian, and nonparametric methods, the Bayesian method provided slightly more accurate respondent classification in parsimonious DCMs than the ML method, and in complex DCMs, the ML method yielded the slightly better result than the Bayesian method; (2) while item parameter recovery was poor in both Bayesian and ML methods, the Bayesian method exhibited unstable slip values owing to the multimodality of their posteriors under complex DCMs, and the ML method produced irregular estimates that appear to be well-estimated due to a boundary problem under parsimonious DCMs.

</details>

<details>

<summary>2022-01-13 12:32:23 - Hypothesis testing and confidence sets: why Bayesian not frequentist, and how to set a prior with a regulatory authority</summary>

- *Roger Sewell*

- `2112.10685v3` - [abs](http://arxiv.org/abs/2112.10685v3) - [pdf](http://arxiv.org/pdf/2112.10685v3)

> We marshall the arguments for preferring Bayesian hypothesis testing and confidence sets to frequentist ones. We define admissible solutions to inference problems, noting that Bayesian solutions are admissible. We give six weaker common-sense criteria for solutions to inference problems, all failed by these frequentist methods but satisfied by any admissible method. We note that pseudo-Bayesian methods made by handicapping Bayesian methods to satisfy criteria on type I error rate makes them frequentist not Bayesian in nature.   We give four examples showing the differences between Bayesian and frequentist methods; the first to be accessible to those with no calculus, the second to illustrate dramatically in abstract what is wrong with these frequentist methods, the third to show that the same problems arise, albeit to a lesser extent, in everyday statistical problems, and the fourth to illustrate how on some real-life inference problems Bayesian methods require less data than fixed sample-size (resp. pseudo-Bayesian) frequentist hypothesis testing by factors exceeding 3000 (resp 300) without recourse to informative priors.   To address the issue of different parties with opposing interests reaching agreement on a prior, we illustrate the beneficial effects of a Bayesian "Let the data decide" policy both on results under a wide variety of conditions and on motivation to reach a common prior by consent.   We show that in general the frequentist confidence level contains less relevant Shannon information than the Bayesian posterior, and give an example where no deterministic frequentist critical regions give any relevant information even though the Bayesian posterior contains up to the maximum possible amount. In contrast use of the Bayesian prior allows construction of non-deterministic critical regions for which the Bayesian posterior can be recovered from the frequentist confidence.

</details>

<details>

<summary>2022-01-13 14:45:35 - Bayesian Estimation of Multivariate Hawkes Processes with Inhibition and Sparsity</summary>

- *Isabella Deutsch, Gordon J. Ross*

- `2201.05009v1` - [abs](http://arxiv.org/abs/2201.05009v1) - [pdf](http://arxiv.org/pdf/2201.05009v1)

> Hawkes processes are point processes that model data where events occur in clusters through the self-exciting property of the intensity function. We consider a multivariate setting where multiple dimensions can influence each other with intensity function to allow for excitation and inhibition, both within and across dimensions. We discuss how such a model can be implemented and highlight challenges in the estimation procedure induced by a potentially negative intensity function. Furthermore, we introduce a new, stronger condition for stability that encompasses current approaches established in the literature. Finally, we examine the total number of offsprings to reparametrise the model and subsequently use Normal and sparsity-inducing priors in a Bayesian estimation procedure on simulated data.

</details>

<details>

<summary>2022-01-13 16:02:35 - Multi-task longitudinal forecasting with missing values on Alzheimer's Disease</summary>

- *Carlos Sevilla-Salcedo, Vandad Imani, Pablo M. Olmos, Vanessa Gmez-Verdejo, Jussi Tohka*

- `2201.05040v1` - [abs](http://arxiv.org/abs/2201.05040v1) - [pdf](http://arxiv.org/pdf/2201.05040v1)

> Machine learning techniques typically applied to dementia forecasting lack in their capabilities to jointly learn several tasks, handle time dependent heterogeneous data and missing values. In this paper, we propose a framework using the recently presented SSHIBA model for jointly learning different tasks on longitudinal data with missing values. The method uses Bayesian variational inference to impute missing values and combine information of several views. This way, we can combine different data-views from different time-points in a common latent space and learn the relations between each time-point while simultaneously modelling and predicting several output variables. We apply this model to predict together diagnosis, ventricle volume, and clinical scores in dementia. The results demonstrate that SSHIBA is capable of learning a good imputation of the missing values and outperforming the baselines while simultaneously predicting three different tasks.

</details>

<details>

<summary>2022-01-13 20:53:29 - Bayes posterior convergence for loss functions via almost additive Thermodynamic Formalism</summary>

- *Artur O. Lopes, Silvia R. C. Lopes, Paulo Varandas*

- `2012.05601v2` - [abs](http://arxiv.org/abs/2012.05601v2) - [pdf](http://arxiv.org/pdf/2012.05601v2)

> Statistical inference can be seen as information processing involving input information and output information that updates belief about some unknown parameters. We consider the Bayesian framework for making inferences about dynamical systems from ergodic observations, where the Bayesian procedure is based on the Gibbs posterior inference, a decision process generalization of standard Bayesian inference where the likelihood is replaced by the exponential of a loss function. In the case of direct observation and almost-additive loss functions, we prove an exponential convergence of the a posteriori measures a limit measure. Our estimates on the Bayes posterior convergence for direct observation are related and extend those in a recent paper by K. McGoff, S. Mukherjee and A. Nobel. Our approach makes use of non-additive thermodynamic formalism and large deviation properties instead of joinings.

</details>

<details>

<summary>2022-01-14 00:55:53 - Hypergraph reconstruction from network data</summary>

- *Jean-Gabriel Young, Giovanni Petri, Tiago P. Peixoto*

- `2008.04948v4` - [abs](http://arxiv.org/abs/2008.04948v4) - [pdf](http://arxiv.org/pdf/2008.04948v4)

> Networks can describe the structure of a wide variety of complex systems by specifying which pairs of entities in the system are connected. While such pairwise representations are flexible, they are not necessarily appropriate when the fundamental interactions involve more than two entities at the same time. Pairwise representations nonetheless remain ubiquitous, because higher-order interactions are often not recorded explicitly in network data. Here, we introduce a Bayesian approach to reconstruct latent higher-order interactions from ordinary pairwise network data. Our method is based on the principle of parsimony and only includes higher-order structures when there is sufficient statistical evidence for them. We demonstrate its applicability to a wide range of datasets, both synthetic and empirical.

</details>

<details>

<summary>2022-01-14 01:57:48 - Eikonal depth: an optimal control approach to statistical depths</summary>

- *Martin Molina-Fructuoso, Ryan Murray*

- `2201.05274v1` - [abs](http://arxiv.org/abs/2201.05274v1) - [pdf](http://arxiv.org/pdf/2201.05274v1)

> Statistical depths provide a fundamental generalization of quantiles and medians to data in higher dimensions. This paper proposes a new type of globally defined statistical depth, based upon control theory and eikonal equations, which measures the smallest amount of probability density that has to be passed through in a path to points outside the support of the distribution: for example spatial infinity. This depth is easy to interpret and compute, expressively captures multi-modal behavior, and extends naturally to data that is non-Euclidean. We prove various properties of this depth, and provide discussion of computational considerations. In particular, we demonstrate that this notion of depth is robust under an aproximate isometrically constrained adversarial model, a property which is not enjoyed by the Tukey depth. Finally we give some illustrative examples in the context of two-dimensional mixture models and MNIST.

</details>

<details>

<summary>2022-01-14 02:30:30 - Nested sampling for frequentist computation: fast estimation of small $p$-values</summary>

- *Andrew Fowlie, Sebastian Hoof, Will Handley*

- `2105.13923v2` - [abs](http://arxiv.org/abs/2105.13923v2) - [pdf](http://arxiv.org/pdf/2105.13923v2)

> We propose a novel method for computing $p$-values based on nested sampling (NS) applied to the sampling space rather than the parameter space of the problem, in contrast to its usage in Bayesian computation. The computational cost of NS scales as $\log^2{1/p}$, which compares favorably to the $1/p$ scaling for Monte Carlo (MC) simulations. For significances greater than about $4\sigma$ in both a toy problem and a simplified resonance search, we show that NS requires orders of magnitude fewer simulations than ordinary MC estimates. This is particularly relevant for high-energy physics, which adopts a $5\sigma$ gold standard for discovery. We conclude with remarks on new connections between Bayesian and frequentist computation and possibilities for tuning NS implementations for still better performance in this setting.

</details>

<details>

<summary>2022-01-14 10:55:54 - k-parametric Dynamic Generalized Linear Models: a sequential approach via Information Geometry</summary>

- *Rara Marotta, Mariane Branco Alves, Helio S. Migon*

- `2201.05387v1` - [abs](http://arxiv.org/abs/2201.05387v1) - [pdf](http://arxiv.org/pdf/2201.05387v1)

> Dynamic generalized linear models may be seen simultaneously as an extension to dynamic linear models and to generalized linear models, formally treating serial auto-correlation inherent to responses observed through time. The present work revisits inference methods for this class, proposing an approach based on information geometry, focusing on the $k$- parametric exponential family. Among others, the proposed method accommodates multinomial and can be adapted to accommodate compositional responses on $k=d+1$ categories, while preserving the sequential aspect of the Bayesian inferential procedure, producing real-time inference. The updating scheme benefits from the conjugate structure in the exponential family, assuring computational efficiency. Concepts such as Kullback-Leibler divergence and the projection theorem are used in the development of the method, placing it close to recent approaches on variational inference. Applications to real data are presented, demonstrating the computational efficiency of the method, favorably comparing to alternative approaches, as well as its flexibility to quickly accommodate new information when strategically needed, preserving aspects of monitoring and intervention analysis, as well as discount factors, which are usual in sequential analyzes.

</details>

<details>

<summary>2022-01-14 14:05:30 - Bayesian sense of time in biological and artificial brains</summary>

- *Zafeirios Fountas, Alexey Zakharov*

- `2201.05464v1` - [abs](http://arxiv.org/abs/2201.05464v1) - [pdf](http://arxiv.org/pdf/2201.05464v1)

> Enquiries concerning the underlying mechanisms and the emergent properties of a biological brain have a long history of theoretical postulates and experimental findings. Today, the scientific community tends to converge to a single interpretation of the brain's cognitive underpinnings -- that it is a Bayesian inference machine. This contemporary view has naturally been a strong driving force in recent developments around computational and cognitive neurosciences. Of particular interest is the brain's ability to process the passage of time -- one of the fundamental dimensions of our experience. How can we explain empirical data on human time perception using the Bayesian brain hypothesis? Can we replicate human estimation biases using Bayesian models? What insights can the agent-based machine learning models provide for the study of this subject? In this chapter, we review some of the recent advancements in the field of time perception and discuss the role of Bayesian processing in the construction of temporal models.

</details>

<details>

<summary>2022-01-14 14:09:36 - Knowledge Gradient for Selection with Covariates: Consistency and Computation</summary>

- *Liang Ding, L. Jeff Hong, Haihui Shen, Xiaowei Zhang*

- `1906.05098v7` - [abs](http://arxiv.org/abs/1906.05098v7) - [pdf](http://arxiv.org/pdf/1906.05098v7)

> Knowledge gradient is a design principle for developing Bayesian sequential sampling policies to solve optimization problems. In this paper we consider the ranking and selection problem in the presence of covariates, where the best alternative is not universal but depends on the covariates. In this context, we prove that under minimal assumptions, the sampling policy based on knowledge gradient is consistent, in the sense that following the policy the best alternative as a function of the covariates will be identified almost surely as the number of samples grows. We also propose a stochastic gradient ascent algorithm for computing the sampling policy and demonstrate its performance via numerical experiments.

</details>

<details>

<summary>2022-01-14 14:10:10 - On Preference Learning Based on Sequential Bayesian Optimization with Pairwise Comparison</summary>

- *Tanya Ignatenko, Kirill Kondrashov, Marco Cox, Bert de Vries*

- `2103.13192v2` - [abs](http://arxiv.org/abs/2103.13192v2) - [pdf](http://arxiv.org/pdf/2103.13192v2)

> User preference learning is generally a hard problem. Individual preferences are typically unknown even to users themselves, while the space of choices is infinite. Here we study user preference learning from information-theoretic perspective. We model preference learning as a system with two interacting sub-systems, one representing a user with his/her preferences and another one representing an agent that has to learn these preferences. The user with his/her behaviour is modeled by a parametric preference function. To efficiently learn the preferences and reduce search space quickly, we propose the agent that interacts with the user to collect the most informative data for learning. The agent presents two proposals to the user for evaluation, and the user rates them based on his/her preference function. We show that the optimum agent strategy for data collection and preference learning is a result of maximin optimization of the normalized weighted Kullback-Leibler (KL) divergence between true and agent-assigned predictive user response distributions. The resulting value of KL-divergence, which we also call remaining system uncertainty (RSU), provides an efficient performance metric in the absence of the ground truth. This metric characterises how well the agent can predict user and, thus, the quality of the underlying learned user (preference) model. Our proposed agent comprises sequential mechanisms for user model inference and proposal generation. To infer the user model (preference function), Bayesian approximate inference is used in the agent. The data collection strategy is to generate proposals, responses to which help resolving uncertainty associated with prediction of the user responses the most. The efficiency of our approach is validated by numerical simulations.

</details>

<details>

<summary>2022-01-14 16:40:26 - Spatiotemporal Clustering with Neyman-Scott Processes via Connections to Bayesian Nonparametric Mixture Models</summary>

- *Yixin Wang, Anthony Degleris, Alex H. Williams, Scott W. Linderman*

- `2201.05044v2` - [abs](http://arxiv.org/abs/2201.05044v2) - [pdf](http://arxiv.org/pdf/2201.05044v2)

> Neyman-Scott processes (NSPs) are point process models that generate clusters of points in time or space. They are natural models for a wide range of phenomena, ranging from neural spike trains to document streams. The clustering property is achieved via a doubly stochastic formulation: first, a set of latent events is drawn from a Poisson process; then, each latent event generates a set of observed data points according to another Poisson process. This construction is similar to Bayesian nonparametric mixture models like the Dirichlet process mixture model (DPMM) in that the number of latent events (i.e. clusters) is a random variable, but the point process formulation makes the NSP especially well suited to modeling spatiotemporal data. While many specialized algorithms have been developed for DPMMs, comparatively fewer works have focused on inference in NSPs. Here, we present novel connections between NSPs and DPMMs, with the key link being a third class of Bayesian mixture models called mixture of finite mixture models (MFMMs). Leveraging this connection, we adapt the standard collapsed Gibbs sampling algorithm for DPMMs to enable scalable Bayesian inference on NSP models. We demonstrate the potential of Neyman-Scott processes on a variety of applications including sequence detection in neural spike trains and event detection in document streams.

</details>

<details>

<summary>2022-01-14 18:41:30 - Nonparametric Bayesian inference for reversible multi-dimensional diffusions</summary>

- *Matteo Giordano, Kolyan Ray*

- `2012.12083v3` - [abs](http://arxiv.org/abs/2012.12083v3) - [pdf](http://arxiv.org/pdf/2012.12083v3)

> We study nonparametric Bayesian models for reversible multi-dimensional diffusions with periodic drift. For continuous observation paths, reversibility is exploited to prove a general posterior contraction rate theorem for the drift gradient vector field under approximation-theoretic conditions on the induced prior for the invariant measure. The general theorem is applied to Gaussian priors and $p$-exponential priors, which are shown to converge to the truth at the minimax optimal rate over Sobolev smoothness classes in any dimension

</details>

<details>

<summary>2022-01-14 19:49:42 - Bayesian Spatial Binary Regression for Label Fusion in Structural Neuroimaging</summary>

- *D. Andrew Brown, Christopher S. McMahan, Russell T. Shinohara, Kristin A. Linn*

- `1710.10351v3` - [abs](http://arxiv.org/abs/1710.10351v3) - [pdf](http://arxiv.org/pdf/1710.10351v3)

> Alzheimer's disease is a neurodegenerative condition that accelerates cognitive decline relative to normal aging. It is of critical scientific importance to gain a better understanding of early disease mechanisms in the brain to facilitate effective, targeted therapies. The volume of the hippocampus is often used in diagnosis and monitoring of the disease. Measuring this volume via neuroimaging is difficult since each hippocampus must either be manually identified or automatically delineated, a task referred to as segmentation. Automatic hippocampal segmentation often involves mapping a previously manually segmented image to a new brain image and propagating the labels to obtain an estimate of where each hippocampus is located in the new image. A more recent approach to this problem is to propagate labels from multiple manually segmented atlases and combine the results using a process known as label fusion. To date, most label fusion algorithms employ voting procedures with voting weights assigned directly or estimated via optimization. We propose using a fully Bayesian spatial regression model for label fusion that facilitates direct incorporation of covariate information while making accessible the entire posterior distribution. Our results suggest that incorporating tissue classification (e.g, gray matter) into the label fusion procedure can greatly improve segmentation when relatively homogeneous, healthy brains are used as atlases for diseased brains. The fully Bayesian approach also produces meaningful uncertainty measures about hippocampal volumes, information which can be leveraged to detect significant, scientifically meaningful differences between healthy and diseased populations, improving the potential for early detection and tracking of the disease.

</details>

<details>

<summary>2022-01-14 20:09:54 - A generalized likelihood based Bayesian approach for scalable joint regression and covariance selection in high dimensions</summary>

- *Srijata Samanta, Kshitij Khare, George Michailidis*

- `2201.05653v1` - [abs](http://arxiv.org/abs/2201.05653v1) - [pdf](http://arxiv.org/pdf/2201.05653v1)

> The paper addresses joint sparsity selection in the regression coefficient matrix and the error precision (inverse covariance) matrix for high-dimensional multivariate regression models in the Bayesian paradigm. The selected sparsity patterns are crucial to help understand the network of relationships between the predictor and response variables, as well as the conditional relationships among the latter. While Bayesian methods have the advantage of providing natural uncertainty quantification through posterior inclusion probabilities and credible intervals, current Bayesian approaches either restrict to specific sub-classes of sparsity patterns and/or are not scalable to settings with hundreds of responses and predictors. Bayesian approaches which only focus on estimating the posterior mode are scalable, but do not generate samples from the posterior distribution for uncertainty quantification. Using a bi-convex regression based generalized likelihood and spike-and-slab priors, we develop an algorithm called Joint Regression Network Selector (JRNS) for joint regression and covariance selection which (a) can accommodate general sparsity patterns, (b) provides posterior samples for uncertainty quantification, and (c) is scalable and orders of magnitude faster than the state-of-the-art Bayesian approaches providing uncertainty quantification. We demonstrate the statistical and computational efficacy of the proposed approach on synthetic data and through the analysis of selected cancer data sets. We also establish high-dimensional posterior consistency for one of the developed algorithms.

</details>

<details>

<summary>2022-01-14 20:52:30 - Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions</summary>

- *Ignavier Ng, Yujia Zheng, Jiji Zhang, Kun Zhang*

- `2201.05666v1` - [abs](http://arxiv.org/abs/2201.05666v1) - [pdf](http://arxiv.org/pdf/2201.05666v1)

> Many of the causal discovery methods rely on the faithfulness assumption to guarantee asymptotic correctness. However, the assumption can be approximately violated in many ways, leading to sub-optimal solutions. Although there is a line of research in Bayesian network structure learning that focuses on weakening the assumption, such as exact search methods with well-defined score functions, they do not scale well to large graphs. In this work, we introduce several strategies to improve the scalability of exact score-based methods in the linear Gaussian setting. In particular, we develop a super-structure estimation method based on the support of inverse covariance matrix which requires assumptions that are strictly weaker than faithfulness, and apply it to restrict the search space of exact search. We also propose a local search strategy that performs exact search on the local clusters formed by each variable and its neighbors within two hops in the super-structure. Numerical experiments validate the efficacy of the proposed procedure, and demonstrate that it scales up to hundreds of nodes with a high accuracy.

</details>

<details>

<summary>2022-01-14 23:36:38 - Expectile-based hydrological modelling for uncertainty estimation: Life after mean</summary>

- *Hristos Tyralis, Georgia Papacharalampous, Sina Khatami*

- `2201.05712v1` - [abs](http://arxiv.org/abs/2201.05712v1) - [pdf](http://arxiv.org/pdf/2201.05712v1)

> Predictions of hydrological models should be probabilistic in nature. Our aim is to introduce a method that estimates directly the uncertainty of hydrological simulations using expectiles, thus complementing previous quantile-based direct approaches. Expectiles are new risk measures in hydrology. They are least square analogues of quantiles and can characterize the probability distribution in much the same way as quantiles do. To this end, we propose calibrating hydrological models using the expectile loss function, which is consistent for expectiles. We apply our method to 511 basins in contiguous US and deliver predictive expectiles of hydrological simulations with the GR4J, GR5J and GR6J hydrological models at expectile levels 0.500, 0.900, 0.950 and 0.975. An honest assessment empirically proves that the GR6J model outperforms the other two models at all expectile levels. Great opportunities are offered for moving beyond the mean in hydrological modelling by simply adjusting the objective function.

</details>

<details>

<summary>2022-01-14 23:56:43 - Datasets for Online Controlled Experiments</summary>

- *C. H. Bryan Liu, ngelo Cardoso, Paul Couturier, Emma J. McCoy*

- `2111.10198v2` - [abs](http://arxiv.org/abs/2111.10198v2) - [pdf](http://arxiv.org/pdf/2111.10198v2)

> Online Controlled Experiments (OCE) are the gold standard to measure impact and guide decisions for digital products and services. Despite many methodological advances in this area, the scarcity of public datasets and the lack of a systematic review and categorization hinder its development. We present the first survey and taxonomy for OCE datasets, which highlight the lack of a public dataset to support the design and running of experiments with adaptive stopping, an increasingly popular approach to enable quickly deploying improvements or rolling back degrading changes. We release the first such dataset, containing daily checkpoints of decision metrics from multiple, real experiments run on a global e-commerce platform. The dataset design is guided by a broader discussion on data requirements for common statistical tests used in digital experimentation. We demonstrate how to use the dataset in the adaptive stopping scenario using sequential and Bayesian hypothesis tests and learn the relevant parameters for each approach.

</details>

<details>

<summary>2022-01-15 11:07:44 - $Z$-value Directional False Discovery Rate Control with Data Masking</summary>

- *Dennis Leung*

- `2201.05828v1` - [abs](http://arxiv.org/abs/2201.05828v1) - [pdf](http://arxiv.org/pdf/2201.05828v1)

> We revisit the fundamental "normal-means" problem, where independent normal test statistics $z_i \sim N(\mu_i, 1)$, $i = 1, \dots, m$, also known as "$z$-values", are observed for the mean effects $\mu_1, \dots, \mu_m$. While there is extensive literature on testing the point null hypotheses $\mu_i = 0$ with false discovery rate control, the problem of declaring whether $\mu_i$ is positive or negative with directional false discovery rate (dFDR) control has been much underserved. Leveraging the recently invented "data masking" technique, we propose a computationally efficient testing algorithm, called ZDIRECT, that seeks to mimic the optimal discovery procedure motivated by a Bayesian perspective yet provides exact dFDR control under minimal pure frequentist assumptions. In our simulation studies, ZDIRECT has demonstrated an apparent power advantage over the directional Benjamini and Hochberg procedure, which, to the best of our knowledge, is the only other existing procedure that offers dFDR control for the normal-means problem.

</details>

<details>

<summary>2022-01-15 12:02:33 - Tight Regret Bounds for Noisy Optimization of a Brownian Motion</summary>

- *Zexin Wang, Vincent Y. F. Tan, Jonathan Scarlett*

- `2001.09327v2` - [abs](http://arxiv.org/abs/2001.09327v2) - [pdf](http://arxiv.org/pdf/2001.09327v2)

> We consider the problem of Bayesian optimization of a one-dimensional Brownian motion in which the $T$ adaptively chosen observations are corrupted by Gaussian noise. We show that as the smallest possible expected cumulative regret and the smallest possible expected simple regret scale as $\Omega(\sigma\sqrt{T / \log (T)}) \cap \mathcal{O}(\sigma\sqrt{T} \cdot \log T)$ and $\Omega(\sigma / \sqrt{T \log (T)}) \cap \mathcal{O}(\sigma\log T / \sqrt{T})$ respectively, where $\sigma^2$ is the noise variance. Thus, our upper and lower bounds are tight up to a factor of $\mathcal{O}( (\log T)^{1.5} )$. The upper bound uses an algorithm based on confidence bounds and the Markov property of Brownian motion (among other useful properties), and the lower bound is based on a reduction to binary hypothesis testing.

</details>

<details>

<summary>2022-01-15 17:15:07 - Robust uncertainty estimates with out-of-distribution pseudo-inputs training</summary>

- *Pierre Segonne, Yevgen Zainchkovskyy, Sren Hauberg*

- `2201.05890v1` - [abs](http://arxiv.org/abs/2201.05890v1) - [pdf](http://arxiv.org/pdf/2201.05890v1)

> Probabilistic models often use neural networks to control their predictive uncertainty. However, when making out-of-distribution (OOD)} predictions, the often-uncontrollable extrapolation properties of neural networks yield poor uncertainty predictions. Such models then don't know what they don't know, which directly limits their robustness w.r.t unexpected inputs. To counter this, we propose to explicitly train the uncertainty predictor where we are not given data to make it reliable. As one cannot train without data, we provide mechanisms for generating pseudo-inputs in informative low-density regions of the input space, and show how to leverage these in a practical Bayesian framework that casts a prior distribution over the model uncertainty. With a holistic evaluation, we demonstrate that this yields robust and interpretable predictions of uncertainty while retaining state-of-the-art performance on diverse tasks such as regression and generative modelling

</details>

<details>

<summary>2022-01-16 01:03:16 - Novel Bayesian method for simultaneous detection of activation signatures and background connectivity for task fMRI data</summary>

- *Michelle F. Miranda, Jeffrey S. Morris*

- `2109.00160v2` - [abs](http://arxiv.org/abs/2109.00160v2) - [pdf](http://arxiv.org/pdf/2109.00160v2)

> In this paper, we introduce a new Bayesian approach for analyzing task fMRI data that simultaneously detects activation signatures and background connectivity. Our modeling involves a new hybrid tensor spatial-temporal basis strategy that enables scalable computing yet captures nearby and distant intervoxel correlation and long-memory temporal correlation. The spatial basis involves a composite hybrid transform with two levels: the first accounts for within-ROI correlation, and second between-ROI distant correlation. We demonstrate in simulations how our basis space regression modeling strategy increases sensitivity for identifying activation signatures, partly driven by the induced background connectivity that itself can be summarized to reveal biological insights. This strategy leads to computationally scalable fully Bayesian inference at the voxel or ROI level that adjusts for multiple testing. We apply this model to Human Connectome Project data to reveal insights into brain activation patterns and background connectivity related to working memory tasks.

</details>

<details>

<summary>2022-01-16 17:37:03 - Constrained inference through posterior projections</summary>

- *Deborshee Sen, Sayan Patra, David Dunson*

- `1812.05741v4` - [abs](http://arxiv.org/abs/1812.05741v4) - [pdf](http://arxiv.org/pdf/1812.05741v4)

> Bayesian approaches are appealing for constrained inference problems by allowing a probabilistic characterization of uncertainty, while providing a computational machinery for incorporating complex constraints in hierarchical models. However, the usual Bayesian strategy of placing a prior on the constrained space and conducting posterior computation with Markov chain Monte Carlo algorithms is often intractable. An alternative is to conduct inference for a less constrained posterior and project samples to the constrained space through a minimal distance mapping. We formalize and provide a unifying framework for such posterior projections. For theoretical tractability, we initially focus on constrained parameter spaces corresponding to closed and convex subsets of the original space. We then consider non-convex Stiefel manifolds. We provide a general formulation of projected posteriors in a Bayesian decision-theoretic framework. We show that asymptotic properties of the unconstrained posterior are transferred to the projected posterior, leading to asymptotically correct credible intervals. We demonstrate numerically that projected posteriors can have better performance that competitor approaches in real data examples.

</details>

<details>

<summary>2022-01-16 20:50:08 - On Maximum-a-Posteriori estimation with Plug & Play priors and stochastic gradient descent</summary>

- *Rmi Laumont, Valentin de Bortoli, Andrs Almansa, Julie Delon, Alain Durmus, Marcelo Pereyra*

- `2201.06133v1` - [abs](http://arxiv.org/abs/2201.06133v1) - [pdf](http://arxiv.org/pdf/2201.06133v1)

> Bayesian methods to solve imaging inverse problems usually combine an explicit data likelihood function with a prior distribution that explicitly models expected properties of the solution. Many kinds of priors have been explored in the literature, from simple ones expressing local properties to more involved ones exploiting image redundancy at a non-local scale. In a departure from explicit modelling, several recent works have proposed and studied the use of implicit priors defined by an image denoising algorithm. This approach, commonly known as Plug & Play (PnP) regularisation, can deliver remarkably accurate results, particularly when combined with state-of-the-art denoisers based on convolutional neural networks. However, the theoretical analysis of PnP Bayesian models and algorithms is difficult and works on the topic often rely on unrealistic assumptions on the properties of the image denoiser. This papers studies maximum-a-posteriori (MAP) estimation for Bayesian models with PnP priors. We first consider questions related to existence, stability and well-posedness, and then present a convergence proof for MAP computation by PnP stochastic gradient descent (PnP-SGD) under realistic assumptions on the denoiser used. We report a range of imaging experiments demonstrating PnP-SGD as well as comparisons with other PnP schemes.

</details>

<details>

<summary>2022-01-17 10:25:32 - Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty</summary>

- *Niko Hauzenberger, Florian Huber, Massimiliano Marcellino, Nico Petz*

- `2112.01995v2` - [abs](http://arxiv.org/abs/2112.01995v2) - [pdf](http://arxiv.org/pdf/2112.01995v2)

> We develop a non-parametric multivariate time series model that remains agnostic on the precise relationship between a (possibly) large set of macroeconomic time series and their lagged values. The main building block of our model is a Gaussian Process prior on the functional relationship that determines the conditional mean of the model, hence the name of Gaussian Process Vector Autoregression (GP-VAR). We control for changes in the error variances by introducing a stochastic volatility specification. To facilitate computation in high dimensions and to introduce convenient statistical properties tailored to match stylized facts commonly observed in macro time series, we assume that the covariance of the Gaussian Process is scaled by the latent volatility factors. We illustrate the use of the GP-VAR by analyzing the effects of macroeconomic uncertainty, with a particular emphasis on time variation and asymmetries in the transmission mechanisms. Using US data, we find that uncertainty shocks have time-varying effects, they are less persistent during recessions but their larger size in these specific periods causes more than proportional effects on real growth and employment.

</details>

<details>

<summary>2022-01-17 10:39:30 - Debiased Inference on Heterogeneous Quantile Treatment Effects with Regression Rank-Scores</summary>

- *Alexander Giessing, Jingshen Wang*

- `2102.01753v3` - [abs](http://arxiv.org/abs/2102.01753v3) - [pdf](http://arxiv.org/pdf/2102.01753v3)

> Understanding treatment effect heterogeneity in observational studies is of great practical importance to many scientific fields. Quantile regression provides a natural framework for modeling such heterogeneity. In this paper, we propose a new method for inference on heterogeneous quantile treatment effects in the presence of high-dimensional covariates. Our estimator combines a $\ell_1$-penalized regression adjustment with a quantile-specific bias correction scheme based on quantile regression rank scores. We present a comprehensive study of the theoretical properties of this estimator, including weak convergence of the heterogeneous quantile treatment effect process to a Gaussian process. We illustrate the finite-sample performance of our approach through Monte Carlo experiments and an empirical example, dealing with the differential effect of statin usage for lowering low-density lipoprotein cholesterol levels for the Alzheimer's disease patients who participated in the UK Biobank study.

</details>

<details>

<summary>2022-01-17 13:43:38 - Tolerance and Prediction Intervals for Non-normal Models</summary>

- *Geoffrey S Johnson*

- `2011.11583v5` - [abs](http://arxiv.org/abs/2011.11583v5) - [pdf](http://arxiv.org/pdf/2011.11583v5)

> A prediction interval covers a future observation from a random process in repeated sampling, and is typically constructed by identifying a pivotal quantity that is also an ancillary statistic. Analogously, a tolerance interval covers a population percentile in repeated sampling and is often based on a pivotal quantity. One approach we consider in non-normal models leverages a link function resulting in a pivotal quantity that is approximately normally distributed. In settings where this normal approximation does not hold we consider a second approach for tolerance and prediction based on a confidence interval for the mean. These methods are intuitive, simple to implement, have proper operating characteristics, and are computationally efficient compared to Bayesian, re-sampling, and machine learning methods. This is demonstrated in the context of multi-site clinical trial recruitment with staggered site initiation, real-world time on treatment, and end-of-study success for a clinical endpoint.

</details>

<details>

<summary>2022-01-17 15:11:15 - Systematic evaluation of variability detection methods for eROSITA</summary>

- *Johannes Buchner, Thomas Boller, David Bogensberger, Adam Malyali, Kirpal Nandra, Joern Wilms, Tom Dwelly, Teng Liu*

- `2106.14529v2` - [abs](http://arxiv.org/abs/2106.14529v2) - [pdf](http://arxiv.org/pdf/2106.14529v2)

> The reliability of detecting source variability in sparsely and irregularly sampled X-ray light curves is investigated. This is motivated by the unprecedented survey capabilities of eROSITA onboard SRG, providing light curves for many thousand sources in its final-depth equatorial deep field survey. Four methods for detecting variability are evaluated: excess variance, amplitude maximum deviations, Bayesian blocks and a new Bayesian formulation of the excess variance. We judge the false detection rate of variability based on simulated Poisson light curves of constant sources, and calibrate significance thresholds. Simulations with flares injected favour the amplitude maximum deviation as most sensitive at low false detections. Simulations with white and red stochastic source variability favour Bayesian methods. The results are applicable also for the million sources expected in eROSITA's all-sky survey.

</details>

<details>

<summary>2022-01-17 15:51:16 - Estimators for covariate-adjusted ROC curves with missing biomarkers values</summary>

- *Ana M. Bianco, Graciela Boente, Wenceslao Gonzlez-Manteiga, Ana Prez-Gonzlez*

- `2201.06483v1` - [abs](http://arxiv.org/abs/2201.06483v1) - [pdf](http://arxiv.org/pdf/2201.06483v1)

> In this paper, we present three estimators of the ROC curve when missing observations arise among the biomarkers. Two of the procedures assume that we have covariates that allow to estimate the propensity and the estimators are obtained using an inverse probability weighting method or a smoothed version of it. The other one assumes that the covariates are related to the biomarkers through a regression model which enables us to construct convolution--based estimators of the distribution and quantile functions. Consistency results are obtained under mild conditions. Through a numerical study we evaluate the finite sample performance of the different proposals. A real data set is also analysed.

</details>

<details>

<summary>2022-01-18 00:57:25 - Antimodes and Graphical Anomaly Exploration via Depth Quantile Functions</summary>

- *Gabriel Chandler, Wolfgang Polonik*

- `2201.06682v1` - [abs](http://arxiv.org/abs/2201.06682v1) - [pdf](http://arxiv.org/pdf/2201.06682v1)

> Depth quantile functions (DQF) encode geometric information about a point cloud via functions of a single variable, whereas each observation in a data set can be associated with a single function. These functions can then be easily plotted. This is true regardless of the dimension of the data, and in fact holds for object data as well, provided a mapping to an RKHS exists. This visualization aspect proves valuable in the case of anomaly detection, where a universal definition of what constitutes an anomaly is lacking. A relationship drawn between anomalies and antimodes provides a strategy for identifying anomalous observations through visual examination of the DQF plot. The DQF in one dimension is explored, providing intuition for its behavior generally and connections to several existing methodologies are made clear. For higher dimensions and object data, the adaptive DQF is introduced and explored on several data sets with promising results.

</details>

<details>

<summary>2022-01-18 01:58:56 - Improved probabilistic seismic demand-intensity relationship: heteroskedastic approachs</summary>

- *Libo Chen*

- `2201.06698v1` - [abs](http://arxiv.org/abs/2201.06698v1) - [pdf](http://arxiv.org/pdf/2201.06698v1)

> As an integral part of assessing the seismic performance of structures, the probabilistic seismic demand-intensity relationship has been widely studied. In this study, the phenomenon of heteroscedasticity in probabilistic seismic demand models was systematically investigated. A brief review of the definition, diagnosis, and conventional treatment of heteroscedasticity is presented herein, and based on that, two more generalized methods for both univariate and multivariate cases are proposed. For a typical four-span simply supported girder bridge, a series of nonlinear time history analyses were performed through multiple stripe analysis to determine its seismic demand-intensity that can be employed as a sample set. For both univariate and multivariate cases, probabilistic seismic demand models were developed based on the two aforementioned methods under the Bayesian regression framework, and the fitted results were compared and analyzed with the conventional models using linear regression approaches. In the presence of probabilistic seismic demand considering heteroscedasticity, the patterns of non-constant variance or covariance can be characterized effectively, and a better-calibrated prediction region than that of homoscedastic models can be provided. The causes of the heteroscedasticity phenomenon and subsequent solutions are thoroughly discussed. The analysis procedures can be further embedded in seismic fragility and risk assessment, thus providing a more accurate basis for aseismic decision-making.

</details>

<details>

<summary>2022-01-18 11:27:56 - Bayesian calibration of Arterial Windkessel Model</summary>

- *Michail Spitieris, Ingelin Steinsland, Emma Ingestrom*

- `2201.06883v1` - [abs](http://arxiv.org/abs/2201.06883v1) - [pdf](http://arxiv.org/pdf/2201.06883v1)

> This work is motivated by personalized digital twins based on observations and physical models for treatment and prevention of Hypertension. The models commonly used are simplification of the real process and the aim is to make inference about physically interpretable parameters. To account for model discrepancy we propose to set up the estimation problem in a Bayesian calibration framework. This naturally solves the inverse problem accounting for and quantifying the uncertainty in the model formulation, in the parameter estimates and predictions. We focus on the inverse problem, i.e. to estimate the physical parameters given observations. The models we consider are the two and three parameters Windkessel models (WK2 and WK3). These models simulate the blood pressure waveform given the blood inflow and a set of physically interpretable calibration parameters. The third parameter in WK3 function as a tuning parameter. The WK2 model offers physical interpretable parameters and therefore we adopt it as a computer model choice in a Bayesian calibration formulation. In a synthetic simulation study, we simulate noisy data from the WK3 model. We estimate the model parameters using conventional methods, i.e. least squares optimization and through the Bayesian calibration framework. It is demonstrated that our formulation can reconstruct the blood pressure waveform of the complex model, but most importantly can learn the parameters according to known mathematical connections between the two models. We also successfully apply this formulation to a real case study, where data was obtained from a pilot randomized controlled trial study. Our approach is successful for both the simulation study and the real cases.

</details>

<details>

<summary>2022-01-18 11:56:40 - Lottery paradox, DNA evidence and other stories: How to accept uncertain statements</summary>

- *Yudi Pawitan*

- `2108.03971v2` - [abs](http://arxiv.org/abs/2108.03971v2) - [pdf](http://arxiv.org/pdf/2108.03971v2)

> I think we can agree that dealing with uncertainty is not easy. Probability is the main tool for dealing with uncertainty, and we know there are many probability-related puzzles and paradoxes. Here I describe a rather idiosyncratic selection that highlights the problem of accepting uncertain statements. Without going into a formal decision theory, there are simple intuitive rational bases for doing that, for instance based on high probability alone. The lottery paradox shows the logical problem of accepting uncertain statements based on high probability. The DNA evidence story is an example of the use probabilistic reasoning in court, where philosophical differences between the schools of inference -- the frequentist, Bayesian and likelihood schools -- lead to substantial differences in the quantification of evidence.

</details>

<details>

<summary>2022-01-18 13:59:38 - Flexible clustering via hidden hierarchical Dirichlet priors</summary>

- *Antonio Lijoi, Igor Prnster, Giovanni Rebaudo*

- `2201.06994v1` - [abs](http://arxiv.org/abs/2201.06994v1) - [pdf](http://arxiv.org/pdf/2201.06994v1)

> The Bayesian approach to inference stands out for naturally allowing borrowing information across heterogeneous populations, with different samples possibly sharing the same distribution. A popular Bayesian nonparametric model for clustering probability distributions is the nested Dirichlet process, which however has the drawback of grouping distributions in a single cluster when ties are observed across samples. With the goal of achieving a flexible and effective clustering method for both samples and observations, we investigate a nonparametric prior that arises as the composition of two different discrete random structures and derive a closed-form expression for the induced distribution of the random partition, the fundamental tool regulating the clustering behavior of the model. On the one hand, this allows to gain a deeper insight into the theoretical properties of the model and, on the other hand, it yields an MCMC algorithm for evaluating Bayesian inferences of interest. Moreover, we single out limitations of this algorithm when working with more than two populations and, consequently, devise an alternative more efficient sampling scheme, which as a by-product, allows testing homogeneity between different populations. Finally, we perform a comparison with the nested Dirichlet process and provide illustrative examples of both synthetic and real data.

</details>

<details>

<summary>2022-01-18 14:40:56 - Precision Matrix Estimation under the Horseshoe-like Prior-Penalty Dual</summary>

- *Ksheera Sagar, Sayantan Banerjee, Jyotishka Datta, Anindya Bhadra*

- `2104.10750v2` - [abs](http://arxiv.org/abs/2104.10750v2) - [pdf](http://arxiv.org/pdf/2104.10750v2)

> Precision matrix estimation in a multivariate Gaussian model is fundamental to network estimation. Although there exist both Bayesian and frequentist approaches to this, it is difficult to obtain good Bayesian and frequentist properties under the same prior--penalty dual. To bridge this gap, our contribution is a novel prior--penalty dual that closely approximates the graphical horseshoe prior and penalty, and performs well in both Bayesian and frequentist senses. A chief difficulty with the horseshoe prior is a lack of closed form expression of the density function, which we overcome in this article. In terms of theory, we establish posterior convergence rate of the precision matrix that matches the oracle rate, in addition to the frequentist consistency of the MAP estimator. In addition, our results also provide theoretical justifications for previously developed approaches that have been unexplored so far, e.g. for the graphical horseshoe prior. Computationally efficient EM and MCMC algorithms are developed respectively for the penalized likelihood and fully Bayesian estimation problems. In numerical experiments, the horseshoe-based approaches echo their superior theoretical properties by comprehensively outperforming the competing methods. A protein--protein interaction network estimation in B-cell lymphoma is considered to validate the proposed methodology.

</details>

<details>

<summary>2022-01-18 15:03:31 - ciftiTools: A package for reading, writing, visualizing and manipulating CIFTI files in R</summary>

- *Damon Pham, John Muschelli, Amanda Mejia*

- `2106.11338v3` - [abs](http://arxiv.org/abs/2106.11338v3) - [pdf](http://arxiv.org/pdf/2106.11338v3)

> There is significant interest in adopting surface- and grayordinate-based analysis of MR data for a number of reasons, including improved whole-cortex visualization, the ability to perform surface smoothing to avoid issues associated with volumetric smoothing, improved inter-subject alignment, and reduced dimensionality. The CIFTI grayordinate file format introduced by the Human Connectome Project further advances grayordinate-based analysis by combining gray matter data from the left and right cortical hemispheres with gray matter data from the subcortex and cerebellum into a single file. Analyses performed in grayordinate space are well-suited to leverage information shared across the brain and across subjects through both traditional analysis techniques and more advanced statistical methods, including Bayesian methods. The R statistical environment facilitates use of advanced statistical techniques, yet little support for grayordinates analysis has been previously available in R. Indeed, few comprehensive programmatic tools for working with CIFTI files have been available in any language. Here, we present the ciftiTools R package, which provides a unified environment for reading, writing, visualizing, and manipulating CIFTI files and related data formats. We illustrate ciftiTools' convenient and user-friendly suite of tools for working with grayordinates and surface geometry data in R, and we describe how ciftiTools is being utilized to advance the statistical analysis of grayordinate-based functional MRI data.

</details>

<details>

<summary>2022-01-18 15:49:47 - The Time-Varying Multivariate Autoregressive Index Model</summary>

- *G. Cubadda, S. Grassi, B. Guardabascio*

- `2201.07069v1` - [abs](http://arxiv.org/abs/2201.07069v1) - [pdf](http://arxiv.org/pdf/2201.07069v1)

> Many economic variables feature changes in their conditional mean and volatility, and Time Varying Vector Autoregressive Models are often used to handle such complexity in the data. Unfortunately, when the number of series grows, they present increasing estimation and interpretation problems. This paper tries to address this issue proposing a new Multivariate Autoregressive Index model that features time varying means and volatility. Technically, we develop a new estimation methodology that mix switching algorithms with the forgetting factors strategy of Koop and Korobilis (2012). This substantially reduces the computational burden and allows to select or weight, in real time, the number of common components and other features of the data using Dynamic Model Selection or Dynamic Model Averaging without further computational cost. Using USA macroeconomic data, we provide a structural analysis and a forecasting exercise that demonstrates the feasibility and usefulness of this new model.   Keywords: Large datasets, Multivariate Autoregressive Index models, Stochastic volatility, Bayesian VARs.

</details>

<details>

<summary>2022-01-18 18:13:03 - Bayesian inference of spatial and temporal relations in AI patents for EU countries</summary>

- *Krzysztof Rusek, Agnieszka Kleszcz, Albert Cabellos-Aparicio*

- `2201.07168v1` - [abs](http://arxiv.org/abs/2201.07168v1) - [pdf](http://arxiv.org/pdf/2201.07168v1)

> In the paper, we propose two models of Artificial Intelligence (AI) patents in European Union (EU) countries addressing spatial and temporal behaviour. In particular, the models can quantitatively describe the interaction between countries or explain the rapidly growing trends in AI patents. For spatial analysis Poisson regression is used to explain collaboration between a pair of countries measured by the number of common patents. Through Bayesian inference, we estimated the strengths of interactions between countries in the EU and the rest of the world. In particular, a significant lack of cooperation has been identified for some pairs of countries.   Alternatively, an inhomogeneous Poisson process combined with the logistic curve growth accurately models the temporal behaviour by an accurate trend line. Bayesian analysis in the time domain revealed an upcoming slowdown in patenting intensity.

</details>

<details>

<summary>2022-01-18 18:29:30 - Fine-grained network traffic prediction from coarse data</summary>

- *Krzysztof Rusek, Mathias Drton*

- `2201.07179v1` - [abs](http://arxiv.org/abs/2201.07179v1) - [pdf](http://arxiv.org/pdf/2201.07179v1)

> ICT systems provide detailed information on computer network traffic. However, due to storage limitations, some of the information on past traffic is often only retained in an aggregated form. In this paper we show that   Linear Gaussian State Space Models yield simple yet effective methods to make predictions based on time series at different aggregation levels. The models link coarse-grained and fine-grained time series to a single model that is able to provide fine-grained predictions. Our numerical experiments show up to 3.7 times improvement in expected mean absolute forecast error when forecasts are made using, instead of ignoring, additional coarse-grained observations. The forecasts are obtained in a Bayesian formulation of the model, which allows for provisioning of a traffic prediction service with highly informative priors obtained from coarse-grained historical data.

</details>

<details>

<summary>2022-01-18 19:03:54 - Physics Informed Deep Kernel Learning</summary>

- *Zheng Wang, Wei Xing, Robert Kirby, Shandian Zhe*

- `2006.04976v2` - [abs](http://arxiv.org/abs/2006.04976v2) - [pdf](http://arxiv.org/pdf/2006.04976v2)

> Deep kernel learning is a promising combination of deep neural networks and nonparametric function learning. However, as a data driven approach, the performance of deep kernel learning can still be restricted by scarce or insufficient data, especially in extrapolation tasks. To address these limitations, we propose Physics Informed Deep Kernel Learning (PI-DKL) that exploits physics knowledge represented by differential equations with latent sources. Specifically, we use the posterior function sample of the Gaussian process as the surrogate for the solution of the differential equation, and construct a generative component to integrate the equation in a principled Bayesian hybrid framework. For efficient and effective inference, we marginalize out the latent variables in the joint probability and derive a collapsed model evidence lower bound (ELBO), based on which we develop a stochastic model estimation algorithm. Our ELBO can be viewed as a nice, interpretable posterior regularization objective. On synthetic datasets and real-world applications, we show the advantage of our approach in both prediction accuracy and uncertainty quantification.

</details>

<details>

<summary>2022-01-18 20:07:54 - The Pseudo-Lindley Alpha Power transformed distribution, mathematical characterizations and asymptotic properties</summary>

- *Modou Ngom, Moumouni Diallo, Adja Mbarka Fall, Gane Samb Lo*

- `2201.07292v1` - [abs](http://arxiv.org/abs/2201.07292v1) - [pdf](http://arxiv.org/pdf/2201.07292v1)

> We introduce a new generalization of the Pseudo-Lindley distribution by applying alpha power transformation. The obtained distribution is referred as the Pseudo-Lindley alpha power transformed distribution (\textit{PL-APT}). Some tractable mathematical properties of the \textit{PL-APT} distribution as reliability, hazard rate, order statistics and entropies are provided. The maximum likelihood method is used to obtain the parameters' estimation of the \textit{PL-APT} distribution. The asymptotic properties of the proposed distribution are discussed. Also, a simulation study is performed to compare the modeling capability and flexibility of \textit{PL-APT} with Lindley and Pseudo-Lindley distributions. The \textit{PL-APT} provides a good fit as the Lindley and the Pseudo-Lindley distribution. The extremal domain of attraction of \textit{PL-APT} is found and its quantile and extremal quantile functions studied. Finally, the extremal value index is estimated by the double-indexed Hill's estimator (Ngom and Lo, 2016) and related asymptotic statistical tests are provided and characterized.

</details>

<details>

<summary>2022-01-18 21:24:52 - Asymptotic properties of Bayesian inference in linear regression with a structural break</summary>

- *Kenichi Shimizu*

- `2201.07319v1` - [abs](http://arxiv.org/abs/2201.07319v1) - [pdf](http://arxiv.org/pdf/2201.07319v1)

> This paper studies large sample properties of a Bayesian approach to inference about slope parameters $\gamma$ in linear regression models with a structural break. In contrast to the conventional approach to inference about $\gamma$ that does not take into account the uncertainty of the unknown break location $\tau$, the Bayesian approach that we consider incorporates such uncertainty. Our main theoretical contribution is a Bernstein-von Mises type theorem (Bayesian asymptotic normality) for $\gamma$ under a wide class of priors, which essentially indicates an asymptotic equivalence between the conventional frequentist and Bayesian inference. Consequently, a frequentist researcher could look at credible intervals of $\gamma$ to check robustness with respect to the uncertainty of $\tau$. Simulation studies show that the conventional confidence intervals of $\gamma$ tend to undercover in finite samples whereas the credible intervals offer more reasonable coverages in general. As the sample size increases, the two methods coincide, as predicted from our theoretical conclusion. Using data from Paye and Timmermann (2006) on stock return prediction, we illustrate that the traditional confidence intervals on $\gamma$ might underrepresent the true sampling uncertainty.

</details>

<details>

<summary>2022-01-18 23:54:07 - A Survey of Uncertainty in Deep Neural Networks</summary>

- *Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, Xiao Xiang Zhu*

- `2107.03342v3` - [abs](http://arxiv.org/abs/2107.03342v3) - [pdf](http://arxiv.org/pdf/2107.03342v3)

> Due to their increasing spread, confidence in neural network predictions became more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over or under confidence. Many researchers have been working on understanding and quantifying uncertainty in a neural network's prediction. As a result, different types and sources of uncertainty have been identified and a variety of approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. A comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and not reducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks, ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for the calibration of neural networks and give an overview of existing baselines and implementations. Different examples from the wide spectrum of challenges in different fields give an idea of the needs and challenges regarding uncertainties in practical applications. Additionally, the practical limitations of current methods for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.

</details>

<details>

<summary>2022-01-19 09:31:57 - Fitting Double Hierarchical Models with the Integrated Nested Laplace Approximation</summary>

- *Mabel Morales-Otero, Virgilio Gmez-Rubio, Vicente Nez-Antn*

- `2201.07493v1` - [abs](http://arxiv.org/abs/2201.07493v1) - [pdf](http://arxiv.org/pdf/2201.07493v1)

> Double hierarchical generalized linear models (DHGLM) are a family of models that are flexible enough as to model hierarchically the mean and scale parameters. In a Bayesian framework, fitting highly parameterized hierarchical models is challenging when this problem is addressed using typical Markov chain Monte Carlo (MCMC) methods due to the potential high correlation between different parameters and effects in the model. The integrated nested Laplace approximation (INLA) could be considered instead to avoid dealing with these problems. However, DHGLM do not fit within the latent Gaussian Markov random field (GMRF) models that INLA can fit.   In this paper we show how to fit DHGLM with INLA by combining INLA and importance sampling (IS) algorithms. In particular, we will illustrate how to split DHGLM into submodels that can be fitted with INLA so that the remainder of the parameters are fit using adaptive multiple IS (AMIS) with the aid of the graphical representation of the hierarchical model. This is illustrated using a simulation study on three different types of models and two real data examples.

</details>

<details>

<summary>2022-01-19 17:50:41 - Bayesian Data Synthesis and the Utility-Risk Trade-Off for Mixed Epidemiological Data</summary>

- *Joseph Feldman, Daniel Kowal*

- `2102.08255v5` - [abs](http://arxiv.org/abs/2102.08255v5) - [pdf](http://arxiv.org/pdf/2102.08255v5)

> Much of the micro data used for epidemiological studies contain sensitive measurements on real individuals. As a result, such micro data cannot be published out of privacy concerns, rendering any published statistical analyses on them nearly impossible to reproduce. To promote the dissemination of key datasets for analysis without jeopardizing the privacy of individuals, we introduce a cohesive Bayesian framework for the generation of fully synthetic, high dimensional micro datasets of mixed categorical, binary, count, and continuous variables. This process centers around a joint Bayesian model that is simultaneously compatible with all of these data types, enabling the creation of mixed synthetic datasets through posterior predictive sampling. Furthermore, a focal point of epidemiological data analysis is the study of conditional relationships between various exposures and key outcome variables through regression analysis. We design a modified data synthesis strategy to target and preserve these conditional relationships, including both nonlinearities and interactions. The proposed techniques are deployed to create a synthetic version of a confidential dataset containing dozens of health, cognitive, and social measurements on nearly 20,000 North Carolina children.

</details>

<details>

<summary>2022-01-19 18:28:59 - The Elliptical Potential Lemma for General Distributions with an Application to Linear Thompson Sampling</summary>

- *Nima Hamidi, Mohsen Bayati*

- `2102.07987v3` - [abs](http://arxiv.org/abs/2102.07987v3) - [pdf](http://arxiv.org/pdf/2102.07987v3)

> In this note, we introduce a general version of the well-known elliptical potential lemma that is a widely used technique in the analysis of algorithms in sequential learning and decision-making problems. We consider a stochastic linear bandit setting where a decision-maker sequentially chooses among a set of given actions, observes their noisy rewards, and aims to maximize her cumulative expected reward over a decision-making horizon. The elliptical potential lemma is a key tool for quantifying uncertainty in estimating parameters of the reward function, but it requires the noise and the prior distributions to be Gaussian. Our general elliptical potential lemma relaxes this Gaussian requirement which is a highly non-trivial extension for a number of reasons; unlike the Gaussian case, there is no closed-form solution for the covariance matrix of the posterior distribution, the covariance matrix is not a deterministic function of the actions, and the covariance matrix is not decreasing with respect to the semidefinite inequality. While this result is of broad interest, we showcase an application of it to prove an improved Bayesian regret bound for the well-known Thompson sampling algorithm in stochastic linear bandits with changing action sets where prior and noise distributions are general. This bound is minimax optimal up to constants.

</details>

<details>

<summary>2022-01-19 22:07:43 - Bayesian Prediction with Covariates Subject to Detection Limits</summary>

- *Caroline Svahn, Mattias Villani*

- `2201.07874v1` - [abs](http://arxiv.org/abs/2201.07874v1) - [pdf](http://arxiv.org/pdf/2201.07874v1)

> Missing values in covariates due to censoring by signal interference or lack of sensitivity in the measuring devices are common in industrial problems. We propose a full Bayesian solution to the prediction problem with an efficient Markov Chain Monte Carlo (MCMC) algorithm that updates all the censored covariate values jointly in a random scan Gibbs sampler. We show that the joint updating of missing covariate values can be at least two orders of magnitude more efficient than univariate updating. This increased efficiency is shown to be crucial for quickly learning the missing covariate values and their uncertainty in a real-time decision making context, in particular when there is substantial correlation in the posterior for the missing values. The approach is evaluated on simulated data and on data from the telecom sector. Our results show that the proposed Bayesian imputation gives substantially more accurate predictions than na\"ive imputation, and that the use of auxiliary variables in the imputation gives additional predictive power.

</details>

<details>

<summary>2022-01-20 08:36:36 - Bayesian Fused Lasso Modeling via Horseshoe Prior</summary>

- *Yuko Kakikawa, Kaito Shimamura, Shuichi Kawano*

- `2201.08053v1` - [abs](http://arxiv.org/abs/2201.08053v1) - [pdf](http://arxiv.org/pdf/2201.08053v1)

> Bayesian fused lasso is one of the sparse Bayesian methods, which shrinks both regression coefficients and their successive differences simultaneously. In this paper, we propose a Bayesian fused lasso modeling via horseshoe prior. By assuming a horseshoe prior on the difference of successive regression coefficients, the proposed method enables us to prevent over-shrinkage of those differences. We also propose a Bayesian hexagonal operator for regression with shrinkage and equality selection (HORSES) with horseshoe prior, which imposes priors on all combinations of differences of regression coefficients. Simulation studies and an application to real data show that the proposed method gives better performance than existing methods.

</details>

<details>

<summary>2022-01-20 10:30:56 - Statistical Depth Functions for Ranking Distributions: Definitions, Statistical Learning and Applications</summary>

- *Morgane Goibert, Stphan Clmenon, Ekhine Irurozki, Pavlo Mozharovskyi*

- `2201.08105v1` - [abs](http://arxiv.org/abs/2201.08105v1) - [pdf](http://arxiv.org/pdf/2201.08105v1)

> The concept of median/consensus has been widely investigated in order to provide a statistical summary of ranking data, i.e. realizations of a random permutation $\Sigma$ of a finite set, $\{1,\; \ldots,\; n\}$ with $n\geq 1$ say. As it sheds light onto only one aspect of $\Sigma$'s distribution $P$, it may neglect other informative features. It is the purpose of this paper to define analogs of quantiles, ranks and statistical procedures based on such quantities for the analysis of ranking data by means of a metric-based notion of depth function on the symmetric group. Overcoming the absence of vector space structure on $\mathfrak{S}_n$, the latter defines a center-outward ordering of the permutations in the support of $P$ and extends the classic metric-based formulation of consensus ranking (medians corresponding then to the deepest permutations). The axiomatic properties that ranking depths should ideally possess are listed, while computational and generalization issues are studied at length. Beyond the theoretical analysis carried out, the relevance of the novel concepts and methods introduced for a wide variety of statistical tasks are also supported by numerous numerical experiments.

</details>

<details>

<summary>2022-01-20 12:50:43 - Bayesian Nonparametric Mixtures of Exponential Random Graph Models for Ensembles of Networks</summary>

- *Sa Ren, Xue Wang, Peng Liu, Jian Zhang*

- `2201.08153v1` - [abs](http://arxiv.org/abs/2201.08153v1) - [pdf](http://arxiv.org/pdf/2201.08153v1)

> Ensembles of networks arise in various fields where multiple independent networks are observed on the same set of nodes, for example, a collection of brain networks constructed on the same brain regions for different individuals. However, there are few models that describe both the variations and characteristics of networks in an ensemble at the same time. In this paper, we propose to model the ensemble of networks using a Dirichlet Process Mixture of Exponential Random Graph Models (DPM-ERGMs), which divides the ensemble into different clusters and models each cluster of networks using a separate Exponential Random Graph Model (ERGM). By employing a Dirichlet process mixture, the number of clusters can be determined automatically and changed adaptively with the data provided. Moreover, in order to perform full Bayesian inference for DPM-ERGMs, we employ the intermediate importance sampling technique inside the Metropolis-within-slice sampling scheme, which addressed the problem of sampling from the intractable ERGMs on an infinite sample space. We also demonstrate the performance of DPM-ERGMs with both simulated and real datasets.

</details>

<details>

<summary>2022-01-20 13:25:53 - Practical Transfer Learning for Bayesian Optimization</summary>

- *Matthias Feurer, Benjamin Letham, Frank Hutter, Eytan Bakshy*

- `1802.02219v3` - [abs](http://arxiv.org/abs/1802.02219v3) - [pdf](http://arxiv.org/pdf/1802.02219v3)

> When hyperparameter optimization of a machine learning algorithm is repeated for multiple datasets it is possible to transfer knowledge to an optimization run on a new dataset. We develop a new hyperparameter-free ensemble model for Bayesian optimization that is a generalization of two existing transfer learning extensions to Bayesian optimization and establish a worst-case bound compared to vanilla Bayesian optimization. Using a large collection of hyperparameter optimization benchmark problems, we demonstrate that our contributions substantially reduce optimization time compared to standard Gaussian process-based Bayesian optimization and improve over the current state-of-the-art for transfer hyperparameter optimization.

</details>

<details>

<summary>2022-01-20 13:56:35 - Sequential Bayesian Inference for Uncertain Nonlinear Dynamic Systems: A Tutorial</summary>

- *Konstantinos E. Tatsis, Vasilis K. Dertimanis, Eleni N. Chatzi*

- `2201.08180v1` - [abs](http://arxiv.org/abs/2201.08180v1) - [pdf](http://arxiv.org/pdf/2201.08180v1)

> In this article, an overview of Bayesian methods for sequential simulation from posterior distributions of nonlinear and non-Gaussian dynamic systems is presented. The focus is mainly laid on sequential Monte Carlo methods, which are based on particle representations of probability densities and can be seamlessly generalized to any state-space representation. Within this context, a unified framework of the various Particle Filter (PF) alternatives is presented for the solution of state, state-parameter and input-state-parameter estimation problems on the basis of sparse measurements. The algorithmic steps of each filter are thoroughly presented and a simple illustrative example is utilized for the inference of i) unobserved states, ii) unknown system parameters and iii) unmeasured driving inputs.

</details>

<details>

<summary>2022-01-20 14:01:24 - Has EU accession boosted patents performance in the EU-13? -- A critical evaluation using causal impact analysis with Bayesian structural time-series models</summary>

- *Agnieszka Kleszcz, Krzysztof Rusek*

- `2201.09878v1` - [abs](http://arxiv.org/abs/2201.09878v1) - [pdf](http://arxiv.org/pdf/2201.09878v1)

> Nowadays innovation is one of the main determinants of economic development. Patents are a key measure of innovation output, as patent indicators reflect the inventive performance of countries, technologies and firms. This paper provides new insights on the causal effects of the enlargement of the European Union (EU) by investigating the patents performance within the new EU member states (EU-13). The empirical results based on data collected from the OECD database from 1985-2017 and causal impact using a Bayesian structural time-series model (proposed by Google) point towards a conclusion that joining the EU has had a significant impact on patents performance in Romania, Estonia, Poland, Czech Republic, Croatia and Lithuania, although in the latter two countries the impact was negative. For the rest of the EU-13 countries there is no significant effect on patent performance. Whether the EU accession effect is significant or not, the EU-13 are far behind the EU-15 (countries which entered the EU before 2004) in terms of patent performance. The majority of patents (98.66\%) are assigned to the EU-15, with just 1.34\% of assignees belonging to the EU-13.

</details>

<details>

<summary>2022-01-20 21:50:38 - Evaluation of data imputation strategies in complex, deeply-phenotyped data sets: the case of the EU-AIMS Longitudinal European Autism Project</summary>

- *A. Llera, M. Brammer, B. Oakley, J. Tillmann, M. Zabihi, T. Mei, T. Charman, C. Ecker, F. Dell Acqua, T. Banaschewski, C. Moessnang, S. Baron-Cohen, R. Holt, S. Durston, D. Murphy, E. Loth, J. K. Buitelaar, D. L. Floris, C. F. Beckmann*

- `2201.09753v1` - [abs](http://arxiv.org/abs/2201.09753v1) - [pdf](http://arxiv.org/pdf/2201.09753v1)

> An increasing number of large-scale multi-modal research initiatives has been conducted in the typically developing population, as well as in psychiatric cohorts. Missing data is a common problem in such datasets due to the difficulty of assessing multiple measures on a large number of participants. The consequences of missing data accumulate when researchers aim to explore relationships between multiple measures. Here we aim to evaluate different imputation strategies to fill in missing values in clinical data from a large (total N=764) and deeply characterised (i.e. range of clinical and cognitive instruments administered) sample of N=453 autistic individuals and N=311 control individuals recruited as part of the EU-AIMS Longitudinal European Autism Project (LEAP) consortium. In particular we consider a total of 160 clinical measures divided in 15 overlapping subsets of participants. We use two simple but common univariate strategies, mean and median imputation, as well as a Round Robin regression approach involving four independent multivariate regression models including a linear model, Bayesian Ridge regression, as well as several non-linear models, Decision Trees, Extra Trees and K-Neighbours regression. We evaluate the models using the traditional mean square error towards removed available data, and consider in addition the KL divergence between the observed and the imputed distributions. We show that all of the multivariate approaches tested provide a substantial improvement compared to typical univariate approaches. Further, our analyses reveal that across all 15 data-subsets tested, an Extra Trees regression approach provided the best global results. This allows the selection of a unique model to impute missing data for the LEAP project and deliver a fixed set of imputed clinical data to be used by researchers working with the LEAP dataset in the future.

</details>

<details>

<summary>2022-01-21 00:47:58 - Curved factor analysis with the Ellipsoid-Gaussian distribution</summary>

- *Hanyu Song, David B. Dunson*

- `2201.08502v1` - [abs](http://arxiv.org/abs/2201.08502v1) - [pdf](http://arxiv.org/pdf/2201.08502v1)

> There is a need for new models for characterizing dependence in multivariate data. The multivariate Gaussian distribution is routinely used, but cannot characterize nonlinear relationships in the data. Most non-linear extensions tend to be highly complex; for example, involving estimation of a non-linear regression model in latent variables. In this article, we propose a relatively simple class of Ellipsoid-Gaussian multivariate distributions, which are derived by using a Gaussian linear factor model involving latent variables having a von Mises-Fisher distribution on a unit hyper-sphere. We show that the Ellipsoid-Gaussian distribution can flexibly model curved relationships among variables with lower-dimensional structures. Taking a Bayesian approach, we propose a hybrid of gradient-based geodesic Monte Carlo and adaptive Metropolis for posterior sampling. We derive basic properties and illustrate the utility of the Ellipsoid-Gaussian distribution on a variety of simulated and real data applications.

</details>

<details>

<summary>2022-01-21 02:10:13 - Metropolis Augmented Hamiltonian Monte Carlo</summary>

- *Guangyao Zhou*

- `2201.08044v2` - [abs](http://arxiv.org/abs/2201.08044v2) - [pdf](http://arxiv.org/pdf/2201.08044v2)

> Hamiltonian Monte Carlo (HMC) is a powerful Markov Chain Monte Carlo (MCMC) method for sampling from complex high-dimensional continuous distributions. However, in many situations it is necessary or desirable to combine HMC with other Metropolis-Hastings (MH) samplers. The common HMC-within-Gibbs strategy implies a trade-off between long HMC trajectories and more frequent other MH updates. Addressing this trade-off has been the focus of several recent works. In this paper we propose Metropolis Augmented Hamiltonian Monte Carlo (MAHMC), an HMC variant that allows MH updates within HMC and eliminates this trade-off. Experiments on two representative examples demonstrate MAHMC's efficiency and ease of use when compared with within-Gibbs alternatives.

</details>

<details>

<summary>2022-01-21 07:45:58 - Estimating the Lasso's Effective Noise</summary>

- *Johannes Lederer, Michael Vogt*

- `2004.11554v2` - [abs](http://arxiv.org/abs/2004.11554v2) - [pdf](http://arxiv.org/pdf/2004.11554v2)

> Much of the theory for the lasso in the linear model $Y = X \beta^* + \varepsilon$ hinges on the quantity $2 \| X^\top \varepsilon \|_{\infty} / n$, which we call the lasso's effective noise. Among other things, the effective noise plays an important role in finite-sample bounds for the lasso, the calibration of the lasso's tuning parameter, and inference on the parameter vector $\beta^*$. In this paper, we develop a bootstrap-based estimator of the quantiles of the effective noise. The estimator is fully data-driven, that is, does not require any additional tuning parameters. We equip our estimator with finite-sample guarantees and apply it to tuning parameter calibration for the lasso and to high-dimensional inference on the parameter vector $\beta^*$.

</details>

<details>

<summary>2022-01-21 12:03:45 - Unity Smoothing for Handling Inconsistent Evidence in Bayesian Networks and Unity Propagation for Faster Inference</summary>

- *Mads Lindskou, Torben Tvedebrink, Poul Svante Eriksen, Sren Hjsgaard, Niels Morling*

- `2201.08659v1` - [abs](http://arxiv.org/abs/2201.08659v1) - [pdf](http://arxiv.org/pdf/2201.08659v1)

> We propose Unity Smoothing (US) for handling inconsistencies between a Bayesian network model and new unseen observations. We show that prediction accuracy, using the junction tree algorithm with US is comparable to that of Laplace smoothing. Moreover, in applications were sparsity of the data structures is utilized, US outperforms Laplace smoothing in terms of memory usage. Furthermore, we detail how to avoid redundant calculations that must otherwise be performed during the message passing scheme in the junction tree algorithm which we refer to as Unity Propagation (UP). Experimental results shows that it is always faster to exploit UP on top of the Lauritzen-Spigelhalter message passing scheme for the junction tree algorithm.

</details>

<details>

<summary>2022-01-21 18:17:40 - An axiomatization of $$-quantiles</summary>

- *Fabio Bellini, Ilaria Peri*

- `2109.02360v2` - [abs](http://arxiv.org/abs/2109.02360v2) - [pdf](http://arxiv.org/pdf/2109.02360v2)

> We give an axiomatic foundation to $\Lambda$-quantiles, a family of generalized quantiles introduced by Frittelli et al. (2014) under the name of Lambda Value at Risk. Under mild assumptions, we show that these functionals are characterized by a property that we call "locality", that means that any change in the distribution of the probability mass that arises entirely above or below the value of the $\Lambda$-quantile does not modify its value. We compare with a related axiomatization of the usual quantiles given by Chambers (2009), based on the stronger property of "ordinal covariance", that means that quantiles are covariant with respect to increasing transformations. Further, we present a systematic treatment of the properties of $\Lambda$-quantiles, refining some of the results of Frittelli et al. (2014) and Burzoni et al. (2017) and showing that in the case of a nonincreasing $\Lambda$ the properties of $\Lambda$-quantiles closely resemble those of the usual quantiles.

</details>

<details>

<summary>2022-01-21 21:29:52 - Bayesian Kernel Two-Sample Testing</summary>

- *Qinyi Zhang, Veit Wild, Sarah Filippi, Seth Flaxman, Dino Sejdinovic*

- `2002.05550v2` - [abs](http://arxiv.org/abs/2002.05550v2) - [pdf](http://arxiv.org/pdf/2002.05550v2)

> In modern data analysis, nonparametric measures of discrepancies between random variables are particularly important. The subject is well-studied in the frequentist literature, while the development in the Bayesian setting is limited where applications are often restricted to univariate cases. Here, we propose a Bayesian kernel two-sample testing procedure based on modelling the difference between kernel mean embeddings in the reproducing kernel Hilbert space utilising the framework established by Flaxman et al (2016). The use of kernel methods enables its application to random variables in generic domains beyond the multivariate Euclidean spaces. The proposed procedure results in a posterior inference scheme that allows an automatic selection of the kernel parameters relevant to the problem at hand. In a series of synthetic experiments and two real data experiments (i.e. testing network heterogeneity from high-dimensional data and six-membered monocyclic ring conformation comparison), we illustrate the advantages of our approach.

</details>

<details>

<summary>2022-01-22 12:17:54 - Sample Size Considerations for Bayesian Multilevel Hidden Markov Models: A Simulation Study on Multivariate Continuous Data with highly overlapping Component Distributions based on Sleep Data</summary>

- *Jasper Ginn, Sebastian Mildiner Moraga, Emmeke Aarts*

- `2201.09033v1` - [abs](http://arxiv.org/abs/2201.09033v1) - [pdf](http://arxiv.org/pdf/2201.09033v1)

> Spurred in part by the ever-growing number of sensors and web-based methods of collecting data, the use of Intensive Longitudinal Data (ILD) is becoming more common in the social and behavioural sciences. The ILD collected in this field are often hypothesised to be the result of latent states (e.g. behaviour, emotions), and the promise of ILD lies in its ability to capture the dynamics of these states as they unfold in time. In particular, by collecting data for multiple subjects, researchers can observe how such dynamics differ between subjects. The Bayesian Multilevel Hidden Markov Model (mHMM) is a relatively novel model that is suited to model the ILD of this kind while taking into account heterogeneity between subjects. While the mHMM has been applied in a variety of settings, large-scale studies that examine the required sample size for this model are lacking. In this paper, we address this research gap by conducting a simulation study to evaluate the effect of changing (1) the number of subjects, (2) the number of occasions, and (3) the between subjects variability on parameter estimates obtained by the mHMM. We frame this simulation study in the context of sleep research, which consists of multivariate continuous data that displays considerable overlap in the state dependent component distributions. In addition, we generate a set of baseline scenarios with more general data properties. Overall, the number of subjects has the largest effect on model performance. However, the number of occasions is important to adequately model latent state transitions. We discuss how the characteristics of the data influence parameter estimation and provide recommendations to researchers seeking to apply the mHMM to their own data.

</details>

<details>

<summary>2022-01-23 09:03:22 - Weight Expansion: A New Perspective on Dropout and Generalization</summary>

- *Gaojie Jin, Xinping Yi, Pengfei Yang, Lijun Zhang, Sven Schewe, Xiaowei Huang*

- `2201.09209v1` - [abs](http://arxiv.org/abs/2201.09209v1) - [pdf](http://arxiv.org/pdf/2201.09209v1)

> While dropout is known to be a successful regularization technique, insights into the mechanisms that lead to this success are still lacking. We introduce the concept of \emph{weight expansion}, an increase in the signed volume of a parallelotope spanned by the column or row vectors of the weight covariance matrix, and show that weight expansion is an effective means of increasing the generalization in a PAC-Bayesian setting. We provide a theoretical argument that dropout leads to weight expansion and extensive empirical support for the correlation between dropout and weight expansion. To support our hypothesis that weight expansion can be regarded as an \emph{indicator} of the enhanced generalization capability endowed by dropout, and not just as a mere by-product, we have studied other methods that achieve weight expansion (resp.\ contraction), and found that they generally lead to an increased (resp.\ decreased) generalization ability. This suggests that dropout is an attractive regularizer, because it is a computationally cheap method for obtaining weight expansion. This insight justifies the role of dropout as a regularizer, while paving the way for identifying regularizers that promise improved generalization through weight expansion.

</details>

<details>

<summary>2022-01-23 12:08:47 - Ensemble Kalman filter based Sequential Monte Carlo Sampler for sequential Bayesian inference</summary>

- *Jiangqi Wu, Linjie Wen, Peter L Green, Jinglai Li, Simon Maskell*

- `2012.08848v2` - [abs](http://arxiv.org/abs/2012.08848v2) - [pdf](http://arxiv.org/pdf/2012.08848v2)

> Many real-world problems require one to estimate parameters of interest, in a Bayesian framework, from data that are collected sequentially in time. Conventional methods for sampling from posterior distributions, such as {Markov Chain Monte Carlo} can not efficiently address such problems as they do not take advantage of the data's sequential structure. To this end, sequential methods which seek to update the posterior distribution whenever a new collection of data become available are often used to solve these types of problems. Two popular choices of sequential method are the Ensemble Kalman filter (EnKF) and the sequential Monte Carlo sampler (SMCS). While EnKF only computes a Gaussian approximation of the posterior distribution, SMCS can draw samples directly from the posterior. Its performance, however, depends critically upon the kernels that are used. In this work, we present a method that constructs the kernels of SMCS using an EnKF formulation, and we demonstrate the performance of the method with numerical examples.

</details>

<details>

<summary>2022-01-23 13:53:23 - Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey</summary>

- *Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley*

- `2201.09267v1` - [abs](http://arxiv.org/abs/2201.09267v1) - [pdf](http://arxiv.org/pdf/2201.09267v1)

> This is a tutorial and survey paper on metric learning. Algorithms are divided into spectral, probabilistic, and deep metric learning. We first start with the definition of distance metric, Mahalanobis distance, and generalized Mahalanobis distance. In spectral methods, we start with methods using scatters of data, including the first spectral metric learning, relevant methods to Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric learning, locally linear metric adaptation, and adversarial metric learning are covered. We also explain several kernel spectral methods for metric learning in the feature space. We also introduce geometric metric learning methods on the Riemannian manifolds. In probabilistic methods, we start with collapsing classes in both input and feature spaces and then explain the neighborhood component analysis methods, Bayesian metric learning, information theoretic methods, and empirical risk minimization in metric learning. In deep learning methods, we first introduce reconstruction autoencoders and supervised loss functions for metric learning. Then, Siamese networks and its various loss functions, triplet mining, and triplet sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning, geometric metric learning by neural networks, and few-shot metric learning.

</details>

<details>

<summary>2022-01-23 21:37:45 - Probabilistic Learning of Treatment Trees in Cancer</summary>

- *Tsung-Hung Yao, Zhenke Wu, Karthik Bharath, Jinju Li, Veerabhadran Baladandayuthapan*

- `2201.09371v1` - [abs](http://arxiv.org/abs/2201.09371v1) - [pdf](http://arxiv.org/pdf/2201.09371v1)

> Accurate identification of synergistic treatment combinations and their underlying biological mechanisms is critical across many disease domains, especially cancer. In translational oncology research, preclinical systems such as patient-derived xenografts (PDX) have emerged as a unique study design evaluating multiple treatments administered to samples from the same human tumor implanted into genetically identical mice. In this paper, we propose a novel Bayesian probabilistic tree-based framework for PDX data to investigate the hierarchical relationships between treatments by inferring treatment cluster trees, referred to as treatment trees (Rx-tree). The framework motivates a new metric of mechanistic similarity between two or more treatments accounting for inherent uncertainty in tree estimation; treatments with a high estimated similarity have potentially high mechanistic synergy. Building upon Dirichlet Diffusion Trees, we derive a closed-form marginal likelihood encoding the tree structure, which facilitates computationally efficient posterior inference via a new two-stage algorithm. Simulation studies demonstrate superior performance of the proposed method in recovering the tree structure and treatment similarities. Our analyses of a recently collated PDX dataset produce treatment similarity estimates that show a high degree of concordance with known biological mechanisms across treatments in five different cancers. More importantly, we uncover new and potentially effective combination therapies that confer synergistic regulation of specific downstream biological pathways for future clinical investigations. Our accompanying code, data, and shiny application for visualization of results are available at: https://github.com/bayesrx/RxTree.

</details>

<details>

<summary>2022-01-24 02:36:02 - Clustering of heterogeneous populations of networks</summary>

- *Jean-Gabriel Young, Alec Kirkley, M. E. J. Newman*

- `2107.07489v2` - [abs](http://arxiv.org/abs/2107.07489v2) - [pdf](http://arxiv.org/pdf/2107.07489v2)

> Statistical methods for reconstructing networks from repeated measurements typically assume that all measurements are generated from the same underlying network structure. This need not be the case, however. People's social networks might be different on weekdays and weekends, for instance. Brain networks may differ between healthy patients and those with dementia or other conditions. Here we describe a Bayesian analysis framework for such data that allows for the fact that network measurements may be reflective of multiple possible structures. We define a finite mixture model of the measurement process and derive a fast Gibbs sampling procedure that samples exactly from the full posterior distribution of model parameters. The end result is a clustering of the measured networks into groups with similar structure. We demonstrate the method on both real and synthetic network populations.

</details>

<details>

<summary>2022-01-24 05:13:58 - Probability Distribution on Rooted Trees</summary>

- *Yuta Nakahara, Shota Saito, Akira Kamatsuka, Toshiyasu Matsushima*

- `2201.09460v1` - [abs](http://arxiv.org/abs/2201.09460v1) - [pdf](http://arxiv.org/pdf/2201.09460v1)

> The hierarchical and recursive expressive capability of rooted trees is applicable to represent statistical models in various areas, such as data compression, image processing, and machine learning. On the other hand, such hierarchical expressive capability causes a problem in tree selection to avoid overfitting. One unified approach to solve this is a Bayesian approach, on which the rooted tree is regarded as a random variable and a direct loss function can be assumed on the selected model or the predicted value for a new data point. However, all the previous studies on this approach are based on the probability distribution on full trees, to the best of our knowledge. In this paper, we propose a generalized probability distribution for any rooted trees in which only the maximum number of child nodes and the maximum depth are fixed. Furthermore, we derive recursive methods to evaluate the characteristics of the probability distribution without any approximations.

</details>

<details>

<summary>2022-01-24 10:52:09 - A General Framework for Treatment Effect Estimation in Semi-Supervised and High Dimensional Settings</summary>

- *Abhishek Chakrabortty, Guorong Dai, Eric Tchetgen Tchetgen*

- `2201.00468v2` - [abs](http://arxiv.org/abs/2201.00468v2) - [pdf](http://arxiv.org/pdf/2201.00468v2)

> In this article, we aim to provide a general and complete understanding of semi-supervised (SS) causal inference for treatment effects. Specifically, we consider two such estimands: (a) the average treatment effect and (b) the quantile treatment effect, as prototype cases, in an SS setting, characterized by two available data sets: (i) a labeled data set of size $n$, providing observations for a response and a set of high dimensional covariates, as well as a binary treatment indicator; and (ii) an unlabeled data set of size $N$, much larger than $n$, but without the response observed. Using these two data sets, we develop a family of SS estimators which are ensured to be: (1) more robust and (2) more efficient than their supervised counterparts based on the labeled data set only. Beyond the 'standard' double robustness results (in terms of consistency) that can be achieved by supervised methods as well, we further establish root-n consistency and asymptotic normality of our SS estimators whenever the propensity score in the model is correctly specified, without requiring specific forms of the nuisance functions involved. Such an improvement of robustness arises from the use of the massive unlabeled data, so it is generally not attainable in a purely supervised setting. In addition, our estimators are shown to be semi-parametrically efficient as long as all the nuisance functions are correctly specified. Moreover, as an illustration of the nuisance estimators, we consider inverse-probability-weighting type kernel smoothing estimators involving unknown covariate transformation mechanisms, and establish in high dimensional scenarios novel results on their uniform convergence rates, which should be of independent interest. Numerical results on both simulated and real data validate the advantage of our methods over their supervised counterparts with respect to both robustness and efficiency.

</details>

<details>

<summary>2022-01-24 11:33:35 - Quantile based modelling of diurnal temperature range with the five-parameter lambda distribution</summary>

- *Silius M. Vandeskog, Thordis L. Thorarinsdottir, Ingelin Steinsland, Finn Lindgren*

- `2109.11180v2` - [abs](http://arxiv.org/abs/2109.11180v2) - [pdf](http://arxiv.org/pdf/2109.11180v2)

> Diurnal temperature range is an important variable in climate science that can provide information regarding climate variability and climate change. Changes in diurnal temperature range can have implications for hydrology, human health and ecology, among others. Yet, the statistical literature on modelling diurnal temperature range is lacking. In this paper we propose to model the distribution of diurnal temperature range using the five-parameter lambda (FPL) distribution. Additionally, in order to model diurnal temperature range with explanatory variables, we propose a distributional quantile regression model that combines quantile regression with marginal modelling using the FPL distribution. Inference is performed using the method of quantiles. The models are fitted to 30 years of daily observations of diurnal temperature range from 112 weather stations in the southern part of Norway. The flexible FPL distribution shows great promise as a model for diurnal temperature range, and performs well against competing models. The distributional quantile regression model is fitted to diurnal temperature range data using geographic, orographic and climatological explanatory variables. It performs well and captures much of the spatial variation in the distribution of diurnal temperature range in Norway.

</details>

<details>

<summary>2022-01-24 11:41:38 - On the identifiability of Bayesian factor analytic models</summary>

- *Panagiotis Papastamoulis, Ioannis Ntzoufras*

- `2004.05105v2` - [abs](http://arxiv.org/abs/2004.05105v2) - [pdf](http://arxiv.org/pdf/2004.05105v2)

> A well known identifiability issue in factor analytic models is the invariance with respect to orthogonal transformations. This problem burdens the inference under a Bayesian setup, where Markov chain Monte Carlo (MCMC) methods are used to generate samples from the posterior distribution. We introduce a post-processing scheme in order to deal with rotation, sign and permutation invariance of the MCMC sample. The exact version of the contributed algorithm requires to solve $2^q$ assignment problems per (retained) MCMC iteration, where $q$ denotes the number of factors of the fitted model. For large numbers of factors two approximate schemes based on simulated annealing are also discussed. We demonstrate that the proposed method leads to interpretable posterior distributions using synthetic and publicly available data from typical factor analytic models as well as mixtures of factor analyzers. An R package is available online at CRAN web-page.

</details>

<details>

<summary>2022-01-24 13:27:11 - Multivariate sensitivity analysis for a large-scale climate impact and adaptation model</summary>

- *Oluwole Oyebamiji, Christopher Nemeth, Paula Harrison, Rob Dunford, George Cojocaru*

- `2201.09681v1` - [abs](http://arxiv.org/abs/2201.09681v1) - [pdf](http://arxiv.org/pdf/2201.09681v1)

> We develop a new efficient methodology for Bayesian global sensitivity analysis for large-scale multivariate data. The focus is on computationally demanding models with correlated variables. A multivariate Gaussian process is used as a surrogate model to replace the expensive computer model. To improve the computational efficiency and performance of the model, compactly supported correlation functions are used. The goal is to generate sparse matrices, which give crucial advantages when dealing with large datasets, where we use cross-validation to determine the optimal degree of sparsity. This method was combined with a robust adaptive Metropolis algorithm coupled with a parallel implementation to speed up the convergence to the target distribution. The method was applied to a multivariate dataset from the IMPRESSIONS Integrated Assessment Platform (IAP2), an extension of the CLIMSAVE IAP, which has been widely applied in climate change impact, adaptation and vulnerability assessments. Our empirical results on synthetic and IAP2 data show that the proposed methods are efficient and accurate for global sensitivity analysis of complex models.

</details>

<details>

<summary>2022-01-24 14:01:26 - An Infinite-Feature Extension for Bayesian ReLU Nets That Fixes Their Asymptotic Overconfidence</summary>

- *Agustinus Kristiadi, Matthias Hein, Philipp Hennig*

- `2010.02709v5` - [abs](http://arxiv.org/abs/2010.02709v5) - [pdf](http://arxiv.org/pdf/2010.02709v5)

> A Bayesian treatment can mitigate overconfidence in ReLU nets around the training data. But far away from them, ReLU Bayesian neural networks (BNNs) can still underestimate uncertainty and thus be asymptotically overconfident. This issue arises since the output variance of a BNN with finitely many features is quadratic in the distance from the data region. Meanwhile, Bayesian linear models with ReLU features converge, in the infinite-width limit, to a particular Gaussian process (GP) with a variance that grows cubically so that no asymptotic overconfidence can occur. While this may seem of mostly theoretical interest, in this work, we show that it can be used in practice to the benefit of BNNs. We extend finite ReLU BNNs with infinite ReLU features via the GP and show that the resulting model is asymptotically maximally uncertain far away from the data while the BNNs' predictive power is unaffected near the data. Although the resulting model approximates a full GP posterior, thanks to its structure, it can be applied \emph{post-hoc} to any pre-trained ReLU BNN at a low cost.

</details>

<details>

<summary>2022-01-24 14:21:19 - Valid belief updates for prequentially additive loss functions arising in Semi-Modular Inference</summary>

- *Geoff K. Nicholls, Jeong Eun Lee, Chieh-Hsi Wu, Chris U. Carmona*

- `2201.09706v1` - [abs](http://arxiv.org/abs/2201.09706v1) - [pdf](http://arxiv.org/pdf/2201.09706v1)

> Model-based Bayesian evidence combination leads to models with multiple parameteric modules. In this setting the effects of model misspecification in one of the modules may in some cases be ameliorated by cutting the flow of information from the misspecified module. Semi-Modular Inference (SMI) is a framework allowing partial cuts which modulate but do not completely cut the flow of information between modules. We show that SMI is part of a family of inference procedures which implement partial cuts. It has been shown that additive losses determine an optimal, valid and order-coherent belief update. The losses which arise in Cut models and SMI are not additive. However, like the prequential score function, they have a kind of prequential additivity which we define. We show that prequential additivity is sufficient to determine the optimal valid and order-coherent belief update and that this belief update coincides with the belief update in each of our SMI schemes.

</details>

<details>

<summary>2022-01-24 16:23:38 - Inferring taxonomic placement from DNA barcoding allowing discovery of new taxa</summary>

- *Alessandro Zito, Tommaso Rigon, David B. Dunson*

- `2201.09782v1` - [abs](http://arxiv.org/abs/2201.09782v1) - [pdf](http://arxiv.org/pdf/2201.09782v1)

> In ecology it has become common to apply DNA barcoding to biological samples leading to datasets containing a large number of nucleotide sequences. The focus is then on inferring the taxonomic placement of each of these sequences by leveraging on existing databases containing reference sequences having known taxa. This is highly challenging because i) sequencing is typically only available for a relatively small region of the genome due to cost considerations; ii) many of the sequences are from organisms that are either unknown to science or for which there are no reference sequences available. These issues can lead to substantial classification uncertainty, particularly in inferring new taxa. To address these challenges, we propose a new class of Bayesian nonparametric taxonomic classifiers, BayesANT, which use species sampling model priors to allow new taxa to be discovered at each taxonomic rank. Using a simple product multinomial likelihood with conjugate Dirichlet priors at the lowest rank, a highly efficient algorithm is developed to provide a probabilistic prediction of the taxa placement of each sequence at each rank. BayesANT is shown to have excellent performance in real data, including when many sequences in the test set belong to taxa unobserved in training.

</details>

<details>

<summary>2022-01-25 03:40:49 - Spatial meshing for general Bayesian multivariate models</summary>

- *Michele Peruzzi, David B. Dunson*

- `2201.10080v1` - [abs](http://arxiv.org/abs/2201.10080v1) - [pdf](http://arxiv.org/pdf/2201.10080v1)

> Quantifying spatial and/or temporal associations in multivariate geolocated data of different types is achievable via spatial random effects in a Bayesian hierarchical model, but severe computational bottlenecks arise when spatial dependence is encoded as a latent Gaussian process (GP) in the increasingly common large scale data settings on which we focus. The scenario worsens in non-Gaussian models because the reduced analytical tractability leads to additional hurdles to computational efficiency. In this article, we introduce Bayesian models of spatially referenced data in which the likelihood or the latent process (or both) are not Gaussian. First, we exploit the advantages of spatial processes built via directed acyclic graphs, in which case the spatial nodes enter the Bayesian hierarchy and lead to posterior sampling via routine Markov chain Monte Carlo (MCMC) methods. Second, motivated by the possible inefficiencies of popular gradient-based sampling approaches in the multivariate contexts on which we focus, we introduce the simplified manifold preconditioner adaptation (SiMPA) algorithm which uses second order information about the target but avoids expensive matrix operations. We demostrate the performance and efficiency improvements of our methods relative to alternatives in extensive synthetic and real world remote sensing and community ecology applications with large scale data at up to hundreds of thousands of spatial locations and up to tens of outcomes. Software for the proposed methods is part of R package 'meshed', available on CRAN.

</details>

<details>

<summary>2022-01-25 07:30:07 - A Feature Weighted Mixed Naive Bayes Model for Monitoring Anomalies in the Fan System of a Thermal Power Plant</summary>

- *Min Wang, Li Sheng, Donghua Zhou, Maoyin Chen*

- `2012.07230v4` - [abs](http://arxiv.org/abs/2012.07230v4) - [pdf](http://arxiv.org/pdf/2012.07230v4)

> With the increasing intelligence and integration, a great number of two-valued variables (generally stored in the form of 0 or 1 value) often exist in large-scale industrial processes. However, these variables cannot be effectively handled by traditional monitoring methods such as LDA, PCA and PLS. Recently, a mixed hidden naive Bayesian model (MHNBM) is developed for the first time to utilize both two-valued and continuous variables for abnormality monitoring. Although MHNBM is effective, it still has some shortcomings that need to be improved. For MHNBM, the variables with greater correlation to other variables have greater weights, which cannot guarantee greater weights are assigned to the more discriminating variables. In addition, the conditional probability must be computed based on the historical data. When the training data is scarce, the conditional probability between continuous variables tends to be uniformly distributed, which affects the performance of MHNBM. Here a novel feature weighted mixed naive Bayes model (FWMNBM) is developed to overcome the above shortcomings. For FWMNBM, the variables that are more correlated to the class have greater weights, which makes the more discriminating variables contribute more to the model. At the same time, FWMNBM does not have to calculate the conditional probability between variables, thus it is less restricted by the number of training data samples. Compared with MHNBM, FWMNBM has better performance, and its effectiveness is validated through the numerical cases of a simulation example and a practical case of Zhoushan thermal power plant (ZTPP), China.

</details>

<details>

<summary>2022-01-25 10:02:23 - Semi-Supervised Quantile Estimation: Robust and Efficient Inference in High Dimensional Settings</summary>

- *Abhishek Chakrabortty, Guorong Dai, Raymond J. Carroll*

- `2201.10208v1` - [abs](http://arxiv.org/abs/2201.10208v1) - [pdf](http://arxiv.org/pdf/2201.10208v1)

> We consider quantile estimation in a semi-supervised setting, characterized by two available data sets: (i) a small or moderate sized labeled data set containing observations for a response and a set of possibly high dimensional covariates, and (ii) a much larger unlabeled data set where only the covariates are observed. We propose a family of semi-supervised estimators for the response quantile(s) based on the two data sets, to improve the estimation accuracy compared to the supervised estimator, i.e., the sample quantile from the labeled data. These estimators use a flexible imputation strategy applied to the estimating equation along with a debiasing step that allows for full robustness against misspecification of the imputation model. Further, a one-step update strategy is adopted to enable easy implementation of our method and handle the complexity from the non-linear nature of the quantile estimating equation. Under mild assumptions, our estimators are fully robust to the choice of the nuisance imputation model, in the sense of always maintaining root-n consistency and asymptotic normality, while having improved efficiency relative to the supervised estimator. They also attain semi-parametric optimality if the relation between the response and the covariates is correctly specified via the imputation model. As an illustration of estimating the nuisance imputation function, we consider kernel smoothing type estimators on lower dimensional and possibly estimated transformations of the high dimensional covariates, and we establish novel results on their uniform convergence rates in high dimensions, involving responses indexed by a function class and usage of dimension reduction techniques. These results may be of independent interest. Numerical results on both simulated and real data confirm our semi-supervised approach's improved performance, in terms of both estimation and inference.

</details>

<details>

<summary>2022-01-25 11:56:53 - A Class of Conjugate Priors for Multinomial Probit Models which Includes the Multivariate Normal One</summary>

- *Augusto Fasano, Daniele Durante*

- `2007.06944v2` - [abs](http://arxiv.org/abs/2007.06944v2) - [pdf](http://arxiv.org/pdf/2007.06944v2)

> Multinomial probit models are routinely-implemented representations for learning how the class probabilities of categorical response data change with p observed predictors. Although several frequentist methods have been developed for estimation, inference and classification within such a class of models, Bayesian inference is still lagging behind. This is due to the apparent absence of a tractable class of conjugate priors, that may facilitate posterior inference on the multinomial probit coefficients. Such an issue has motivated increasing efforts toward the development of effective Markov chain Monte Carlo methods, but state-of-the-art solutions still face severe computational bottlenecks, especially in high dimensions. In this article, we show that the entire class of unified skew-normal (SUN) distributions is conjugate to several multinomial probit models. Leveraging this result and the SUN properties, we improve upon state-of-the-art solutions for posterior inference and classification both in terms of closed-form results for several functionals of interest, and also by developing novel computational methods relying either on independent and identically distributed samples from the exact posterior or on scalable and accurate variational approximations based on blocked partially-factorized representations. As illustrated in simulations and in a gastrointestinal lesions application, the magnitude of the improvements relative to current methods is particularly evident, in practice, when the focus is on high-dimensional studies.

</details>

<details>

<summary>2022-01-25 18:54:59 - A deep mixture density network for outlier-corrected interpolation of crowd-sourced weather data</summary>

- *Charlie Kirkwood, Theo Economou, Henry Odbert, Nicolas Pugeault*

- `2201.10544v1` - [abs](http://arxiv.org/abs/2201.10544v1) - [pdf](http://arxiv.org/pdf/2201.10544v1)

> As the costs of sensors and associated IT infrastructure decreases - as exemplified by the Internet of Things - increasing volumes of observational data are becoming available for use by environmental scientists. However, as the number of available observation sites increases, so too does the opportunity for data quality issues to emerge, particularly given that many of these sensors do not have the benefit of official maintenance teams. To realise the value of crowd sourced 'Internet of Things' type observations for environmental modelling, we require approaches that can automate the detection of outliers during the data modelling process so that they do not contaminate the true distribution of the phenomena of interest. To this end, here we present a Bayesian deep learning approach for spatio-temporal modelling of environmental variables with automatic outlier detection. Our approach implements a Gaussian-uniform mixture density network whose dual purposes - modelling the phenomenon of interest, and learning to classify and ignore outliers - are achieved simultaneously, each by specifically designed branches of our neural network. For our example application, we use the Met Office's Weather Observation Website data, an archive of observations from around 1900 privately run and unofficial weather stations across the British Isles. Using data on surface air temperature, we demonstrate how our deep mixture model approach enables the modelling of a highly skilled spatio-temporal temperature distribution without contamination from spurious observations. We hope that adoption of our approach will help unlock the potential of incorporating a wider range of observation sources, including from crowd sourcing, into future environmental models.

</details>

<details>

<summary>2022-01-25 19:37:29 - Bayesian graphical modeling for heterogeneous causal effects</summary>

- *Federico Castelletti, Guido Consonni*

- `2106.03252v2` - [abs](http://arxiv.org/abs/2106.03252v2) - [pdf](http://arxiv.org/pdf/2106.03252v2)

> Our motivation stems from current medical research aiming at personalized treatment using a molecular-based approach. The broad goal is to develop a more precise and targeted decision making process, relative to traditional treatments based primarily on clinical diagnoses. Specifically, we consider patients affected by Acute Myeloid Leukemia (AML), an hematological cancer characterized by uncontrolled proliferation of hematopoietic stem cells in the bone marrow. Because AML responds poorly to chemoterapeutic treatments, the development of targeted therapies is essential to improve patients' prospects. In particular, the dataset we analyze contains the levels of proteins involved in cell cycle regulation and linked to the progression of the disease. We analyse treatment effects within a causal framework represented by a Directed Acyclic Graph (DAG) model, whose vertices are the protein levels in the network. A major obstacle in implementing the above program is however represented by individual heterogeneity. We address this issue through a Dirichlet Process (DP) mixture of Gaussian DAG-models where both the graphical structure as well as the allied model parameters are regarded as uncertain. Our procedure determines a clustering structure of the units reflecting the underlying heterogeneity, and produces subject-specific estimates of causal effects based on Bayesian model averaging. With reference to the AML dataset, we identify different effects of protein regulation among individuals; moreover, our method clusters patients into groups that exhibit only mild similarities with traditional categories based on morphological features.

</details>

<details>

<summary>2022-01-25 20:24:39 - Bayesian Covariance Structure Modeling of Multi-Way Nested Data</summary>

- *Stef Baas, Richard J. Boucherie, Jean-Paul Fox*

- `2201.10612v1` - [abs](http://arxiv.org/abs/2201.10612v1) - [pdf](http://arxiv.org/pdf/2201.10612v1)

> A Bayesian multivariate model with a structured covariance matrix for multi-way nested data is proposed. This flexible modeling framework allows for positive and for negative associations among clustered observations, and generalizes the well-known dependence structure implied by random effects. A conjugate shifted-inverse gamma prior is proposed for the covariance parameters which ensures that the covariance matrix remains positive definite under posterior analysis. A numerically efficient Gibbs sampling procedure is defined for balanced nested designs, and is validated using two simulation studies. For a top-layer unbalanced nested design, the procedure requires an additional data augmentation step. The proposed data augmentation procedure facilitates sampling latent variables from (truncated) univariate normal distributions, and avoids numerical computation of the inverse of the structured covariance matrix. The Bayesian multivariate (linear transformation) model is applied to two-way nested interval-censored event times to analyze differences in adverse events between three groups of patients, who were randomly allocated to treatment with different stents (BIO-RESORT). The parameters of the structured covariance matrix represent unobserved heterogeneity in treatment effects and are examined to detect differential treatment effects.

</details>

<details>

<summary>2022-01-25 21:12:07 - System identification using Bayesian neural networks with nonparametric noise models</summary>

- *Christos Merkatas, Simo Srkk*

- `2104.12119v3` - [abs](http://arxiv.org/abs/2104.12119v3) - [pdf](http://arxiv.org/pdf/2104.12119v3)

> System identification is of special interest in science and engineering. This article is concerned with a system identification problem arising in stochastic dynamic systems, where the aim is to estimate the parameters of a system along with its unknown noise processes. In particular, we propose a Bayesian nonparametric approach for system identification in discrete time nonlinear random dynamical systems assuming only the order of the Markov process is known. The proposed method replaces the assumption of Gaussian distributed error components with a highly flexible family of probability density functions based on Bayesian nonparametric priors. Additionally, the functional form of the system is estimated by leveraging Bayesian neural networks which also leads to flexible uncertainty quantification. Asymptotically on the number of hidden neurons, the proposed model converges to full nonparametric Bayesian regression model. A Gibbs sampler for posterior inference is proposed and its effectiveness is illustrated on simulated and real time series.

</details>

<details>

<summary>2022-01-25 23:29:29 - Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian Nonparametrics</summary>

- *Ryan Giordano, Runjing Liu, Michael I. Jordan, Tamara Broderick*

- `1810.06587v3` - [abs](http://arxiv.org/abs/1810.06587v3) - [pdf](http://arxiv.org/pdf/1810.06587v3)

> Bayesian models based on the Dirichlet process and other stick-breaking priors have been proposed as core ingredients for clustering, topic modeling, and other unsupervised learning tasks. However, due to the flexibility of these models, the consequences of prior choices can be opaque. And so prior specification can be relatively difficult. At the same time, prior choice can have a substantial effect on posterior inferences. Thus, considerations of robustness need to go hand in hand with nonparametric modeling. In the current paper, we tackle this challenge by exploiting the fact that variational Bayesian methods, in addition to having computational advantages in fitting complex nonparametric models, also yield sensitivities with respect to parametric and nonparametric aspects of Bayesian models. In particular, we demonstrate how to assess the sensitivity of conclusions to the choice of concentration parameter and stick-breaking distribution for inferences under Dirichlet process mixtures and related mixture models. We provide both theoretical and empirical support for our variational approach to Bayesian sensitivity analysis.

</details>

<details>

<summary>2022-01-26 08:03:04 - Variational multiple shooting for Bayesian ODEs with Gaussian processes</summary>

- *Pashupati Hegde, aatay Yldz, Harri Lhdesmki, Samuel Kaski, Markus Heinonen*

- `2106.10905v2` - [abs](http://arxiv.org/abs/2106.10905v2) - [pdf](http://arxiv.org/pdf/2106.10905v2)

> Recent machine learning advances have proposed black-box estimation of unknown continuous-time system dynamics directly from data. However, earlier works are based on approximative ODE solutions or point estimates. We propose a novel Bayesian nonparametric model that uses Gaussian processes to infer posteriors of unknown ODE systems directly from data. We derive sparse variational inference with decoupled functional sampling to represent vector field posteriors. We also introduce a probabilistic shooting augmentation to enable efficient inference from arbitrarily long trajectories. The method demonstrates the benefit of computing vector field posteriors, with predictive uncertainty scores outperforming alternative methods on multiple ODE learning tasks.

</details>

<details>

<summary>2022-01-26 09:00:09 - Explaining Hyperparameter Optimization via Partial Dependence Plots</summary>

- *Julia Moosbauer, Julia Herbinger, Giuseppe Casalicchio, Marius Lindauer, Bernd Bischl*

- `2111.04820v2` - [abs](http://arxiv.org/abs/2111.04820v2) - [pdf](http://arxiv.org/pdf/2111.04820v2)

> Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models. However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance. This lack of explainability makes it difficult to trust and understand the automated HPO process and its results. We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO with Bayesian optimization (BO). BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias. Hence, many IML techniques, such as the partial dependence plot (PDP), carry the risk of generating biased interpretations. By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands. We propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions. In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.

</details>

<details>

<summary>2022-01-26 09:02:50 - Spying on the prior of the number of data clusters and the partition distribution in Bayesian cluster analysis</summary>

- *Jan Greve, Bettina Grn, Gertraud Malsiner-Walli, Sylvia Frhwirth-Schnatter*

- `2012.12337v2` - [abs](http://arxiv.org/abs/2012.12337v2) - [pdf](http://arxiv.org/pdf/2012.12337v2)

> Cluster analysis aims at partitioning data into groups or clusters. In applications, it is common to deal with problems where the number of clusters is unknown. Bayesian mixture models employed in such applications usually specify a flexible prior that takes into account the uncertainty with respect to the number of clusters. However, a major empirical challenge involving the use of these models is in the characterisation of the induced prior on the partitions. This work introduces an approach to compute descriptive statistics of the prior on the partitions for three selected Bayesian mixture models developed in the areas of Bayesian finite mixtures and Bayesian nonparametrics. The proposed methodology involves computationally efficient enumeration of the prior on the number of clusters in-sample (termed as ``data clusters'') and determining the first two prior moments of symmetric additive statistics characterising the partitions. The accompanying reference implementation is made available in the R package 'fipp'. Finally, we illustrate the proposed methodology through comparisons and also discuss the implications for prior elicitation in applications.

</details>

<details>

<summary>2022-01-26 10:40:55 - Visualizing the diversity of representations learned by Bayesian neural networks</summary>

- *Dennis Grinwald, Kirill Bykov, Shinichi Nakajima, Marina M. -C. Hhne*

- `2201.10859v1` - [abs](http://arxiv.org/abs/2201.10859v1) - [pdf](http://arxiv.org/pdf/2201.10859v1)

> Explainable artificial intelligence (XAI) aims to make learning machines less opaque, and offers researchers and practitioners various tools to reveal the decision-making strategies of neural networks. In this work, we investigate how XAI methods can be used for exploring and visualizing the diversity of feature representations learned by Bayesian neural networks (BNNs). Our goal is to provide a global understanding of BNNs by making their decision-making strategies a) visible and tangible through feature visualizations and b) quantitatively measurable with a distance measure learned by contrastive learning. Our work provides new insights into the posterior distribution in terms of human-understandable feature information with regard to the underlying decision-making strategies. Our main findings are the following: 1) global XAI methods can be applied to explain the diversity of decision-making strategies of BNN instances, 2) Monte Carlo dropout exhibits increased diversity in feature representations compared to the multimodal posterior approximation of MultiSWAG, 3) the diversity of learned feature representations highly correlates with the uncertainty estimates, and 4) the inter-mode diversity of the multimodal posterior decreases as the network width increases, while the intra-mode diversity increases. Our findings are consistent with the recent deep neural networks theory, providing additional intuitions about what the theory implies in terms of humanly understandable concepts.

</details>

<details>

<summary>2022-01-26 15:08:23 - Approximate Reference Prior for Gaussian Random Fields</summary>

- *Victor De Oliveira, Zifei Han*

- `2201.10993v1` - [abs](http://arxiv.org/abs/2201.10993v1) - [pdf](http://arxiv.org/pdf/2201.10993v1)

> Reference priors are theoretically attractive for the analysis of geostatistical data since they enable automatic Bayesian analysis and have desirable Bayesian and frequentist properties. But their use is hindered by computational hurdles that make their application in practice challenging. In this work, we derive a new class of default priors that approximate reference priors for the parameters of some Gaussian random fields. It is based on an approximation to the integrated likelihood of the covariance parameters derived from the spectral approximation of stationary random fields. This prior depends on the structure of the mean function and the spectral density of the model evaluated at a set of spectral points associated with an auxiliary regular grid. In addition to preserving the desirable Bayesian and frequentist properties, these approximate reference priors are more stable, and their computations are much less onerous than those of exact reference priors. Unlike exact reference priors, the marginal approximate reference prior of correlation parameter is always proper, regardless of the mean function or the smoothness of the correlation function. This property has important consequences for covariance model selection. An illustration comparing default Bayesian analyses is provided with a data set of lead pollution in Galicia, Spain.

</details>

<details>

<summary>2022-01-26 20:01:30 - Sequential Bayesian Inference for Factor Analysis</summary>

- *Konstantinos Vamvourellis, Konstantinos Kalogeropoulos, Irini Moustaki*

- `2201.11163v1` - [abs](http://arxiv.org/abs/2201.11163v1) - [pdf](http://arxiv.org/pdf/2201.11163v1)

> We develop an efficient Bayesian sequential inference framework for factor analysis models observed via various data types, such as continuous, binary and ordinal data. In the continuous data case, where it is possible to marginalise over the latent factors, the proposed methodology tailors the Iterated Batch Importance Sampling (IBIS) of Chopin (2002) to handle such models and we incorporate Hamiltonian Markov Chain Monte Carlo. For binary and ordinal data, we develop an efficient IBIS scheme to handle the parameter and latent factors, combining with Laplace or Variational Bayes approximations. The methodology can be used in the context of sequential hypothesis testing via Bayes factors, which are known to have advantages over traditional null hypothesis testing. Moreover, the developed sequential framework offers multiple benefits even in non-sequential cases, by providing posterior distribution, model evidence and scoring rules (under the prequential framework) in one go, and by offering a more robust alternative computational scheme to Markov Chain Monte Carlo that can be useful in problematic target distributions.

</details>

<details>

<summary>2022-01-27 01:43:23 - A Comprehensive Toolbox to Facilitate Quantitative Decision Science in Drug Development: A web-based R shiny application GOahead</summary>

- *Bo Wei, Weibin Zhong, Rong Liu, Alan Wu, Alan Chiang, Michael Branson, Nanxiang Ge*

- `2201.11267v1` - [abs](http://arxiv.org/abs/2201.11267v1) - [pdf](http://arxiv.org/pdf/2201.11267v1)

> Decision-making is critical at each stage of drug development and making informed and transparent Go/No-Go decisions require a sound quantitative decision framework. We designed and implemented GOahead, a comprehensive web-based tool to improve how statisticians and collaborators could prospectively plan and implement the selected Go/No-Go decision approach in real-time. In the paper, we conducted a comprehensive overview of dual-criterion and confidence interval-based approaches to enable quantitative decision-making. illustrative examples are demonstrated for single and two arms designs in both Bayesian and frequentist frameworks, multiple arms design with MCP-MOD is also demonstrated. GOahead can be found on shinyapps server.

</details>

<details>

<summary>2022-01-27 13:26:11 - Selection and the Distribution of Female Hourly Wages in the U.S</summary>

- *Ivn Fernndez-Val, Franco Peracchi, Aico van Vuuren, Francis Vella*

- `1901.00419v5` - [abs](http://arxiv.org/abs/1901.00419v5) - [pdf](http://arxiv.org/pdf/1901.00419v5)

> We analyze the role of selection bias in generating the changes in the observed distribution of female hourly wages in the United States using CPS data for the years 1975 to 2020. We account for the selection bias from the employment decision by modeling the distribution of the number of working hours and estimating a nonseparable model of wages. We decompose changes in the wage distribution into composition, structural and selection effects. Composition effects have increased wages at all quantiles while the impact of the structural effects varies by time period and quantile. Changes in the role of selection only appear at the lower quantiles of the wage distribution. The evidence suggests that there is positive selection in the 1970s which diminishes until the later 1990s. This reduces wages at lower quantiles and increases wage inequality. Post 2000 there appears to be an increase in positive sorting which reduces the selection effects on wage inequality.

</details>

<details>

<summary>2022-01-27 13:52:34 - Bayesian nonparametric analysis for the detection of spikes in noisy calcium imaging data</summary>

- *Laura D'Angelo, Antonio Canale, Zhaoxia Yu, Michele Guindani*

- `2102.09403v2` - [abs](http://arxiv.org/abs/2102.09403v2) - [pdf](http://arxiv.org/pdf/2102.09403v2)

> Recent advancements in miniaturized fluorescence microscopy have made it possible to investigate neuronal responses to external stimuli in awake behaving animals through the analysis of intra-cellular calcium signals. An on-going challenge is deconvolving the temporal signals to extract the spike trains from the noisy calcium signals' time-series. In this manuscript, we propose a nested Bayesian finite mixture specification that allows the estimation of spiking activity and, simultaneously, reconstructing the distributions of the calcium transient spikes' amplitudes under different experimental conditions. The proposed model leverages two nested layers of random discrete mixture priors to borrow information between experiments and discover similarities in the distributional patterns of neuronal responses to different stimuli. Furthermore, the spikes' intensity values are also clustered within and between experimental conditions to determine the existence of common (recurring) response amplitudes. Simulation studies and the analysis of a data set from the Allen Brain Observatory show the effectiveness of the method in clustering and detecting neuronal activities.

</details>

<details>

<summary>2022-01-27 14:15:09 - Transfer Portal: Accurately Forecasting the Impact of a Player Transfer in Soccer</summary>

- *Daniel Dinsdale, Joe Gallagher*

- `2201.11533v1` - [abs](http://arxiv.org/abs/2201.11533v1) - [pdf](http://arxiv.org/pdf/2201.11533v1)

> One of the most important and challenging problems in football is predicting future player performance when transferred to another club within and between different leagues. In addition to being the most valuable prediction a team makes, it is also the most complex analytics task to perform as it needs to take into consideration: a) differences in playing style between the player's current team and target team, b) differences in style and ability of other players on each team, c) differences in league quality and style, and d) the role the player is desired to play. In this paper, we present a method which addresses these issues and enables us to make accurate predictions of future performance. Our Transfer Portal model utilizes a personalized neural network accounting for both stylistic and ability level input representations for players, teams, and leagues to simulate future player performance at any chosen club. Furthermore, we use a Bayesian updating framework to dynamically modify player and team representations over time which enables us to generate predictions for rising stars with small amounts of data.

</details>

<details>

<summary>2022-01-27 17:46:33 - Bayesian network mediation analysis with application to brain functional connectome</summary>

- *Yize Zhao, Tianqi Chen, Jiachen Cai, Sarah Lichenstein, Marc Potenza, Sarah Yip*

- `2201.11695v1` - [abs](http://arxiv.org/abs/2201.11695v1) - [pdf](http://arxiv.org/pdf/2201.11695v1)

> Brain functional connectome, the collection of interconnected neural circuits along functional networks, is one of the most cutting edge neuroimaging traits, and has a potential to play a mediating role within the effect pathway between an exposure and an outcome. While existing mediation analytic approaches are capable of providing insight into complex processes, they mainly focus on a univariate mediator or mediator vector, without considering network-variate mediators. To fill the methodological gap and accomplish this exciting and urgent application, in the paper, we propose an integrative mediation analysis under a Bayesian paradigm with networks entailing the mediation effect. To parameterize the network measurements, we introduce individually specified stochastic block models with unknown block allocation, and naturally bridge effect elements through the latent network mediators induced by the connectivity weights across network modules. To enable the identification of truly active mediating components, we simultaneously impose a feature selection across network mediators. We show the superiority of our model in estimating different effect components and selecting active mediating network structures. As a practical illustration of this approach's application to network neuroscience, we characterize the relationship between a therapeutic intervention and opioid abstinence as mediated by brain functional sub-networks.

</details>

<details>

<summary>2022-01-27 18:39:16 - Evaluating Federal Policies Using Bayesian Time Series Models: Estimating the Causal Impact of the Hospital Readmissions Reduction Program</summary>

- *Georgia Papadogeorgou, Fiammetta Menchetti, Christine Choirat, Jason H. Wasfy, Corwin M. Zigler, Fabrizia Mealli*

- `1809.09590v2` - [abs](http://arxiv.org/abs/1809.09590v2) - [pdf](http://arxiv.org/pdf/1809.09590v2)

> Researchers are often faced with evaluating the effect of a policy or program that was simultaneously initiated across an entire population of units at a single point in time, and its effects over the targeted population can manifest at any time period afterwards. In the presence of data measured over time, Bayesian time series models have been used to impute what would have happened after the policy was initiated, had the policy not taken place, in order to estimate causal effects. However, the considerations regarding the definition of the target estimands, the underlying assumptions, the plausibility of such assumptions, and the choice of an appropriate model have not been thoroughly investigated. In this paper, we establish useful estimands for the evaluation of large-scale policies. We discuss that imputation of missing potential outcomes relies on an assumption which, even though untestable, can be partially evaluated using observed data. We illustrate an approach to evaluate this key causal assumption and facilitate model elicitation based on data from the time interval before policy initiation and using classic statistical techniques. As an illustration, we study the Hospital Readmissions Reduction Program (HRRP), a US federal intervention aiming to improve health outcomes for patients with pneumonia, acute myocardial infraction, or congestive failure admitted to a hospital. We evaluate the effect of the HRRP on population mortality across the US and in four geographic subregions, and at different time windows. We find that the HRRP increased mortality from the three targeted conditions across most scenarios considered, and is likely to have had a detrimental effect on public health.

</details>

<details>

<summary>2022-01-27 19:12:53 - Estimating Potential Outcome Distributions with Collaborating Causal Networks</summary>

- *Tianhui Zhou, David Carlson*

- `2110.01664v2` - [abs](http://arxiv.org/abs/2110.01664v2) - [pdf](http://arxiv.org/pdf/2110.01664v2)

> Many causal inference approaches have focused on identifying an individual's outcome change due to a potential treatment, or the individual treatment effect (ITE), from observational studies. Rather than only estimating the ITE, we propose Collaborating Causal Networks (CCN) to estimate the full potential outcome distributions. This modification facilitates estimating the utility of each treatment and allows for individual variation in utility functions (e.g., variability in risk tolerance). We show that CCN learns distributions that asymptotically capture the correct potential outcome distributions under standard causal inference assumptions. Furthermore, we develop a new adjustment approach that is empirically effective in alleviating sample imbalance between treatment groups in observational studies. We evaluate CCN by extensive empirical experiments and demonstrate improved distribution estimates compared to existing Bayesian and Generative Adversarial Network-based methods. Additionally, CCN empirically improves decisions over a variety of utility functions.

</details>

<details>

<summary>2022-01-28 00:55:58 - Local Latent Space Bayesian Optimization over Structured Inputs</summary>

- *Natalie Maus, Haydn T. Jones, Juston S. Moore, Matt J. Kusner, John Bradshaw, Jacob R. Gardner*

- `2201.11872v1` - [abs](http://arxiv.org/abs/2201.11872v1) - [pdf](http://arxiv.org/pdf/2201.11872v1)

> Bayesian optimization over the latent spaces of deep autoencoder models (DAEs) has recently emerged as a promising new approach for optimizing challenging black-box functions over structured, discrete, hard-to-enumerate search spaces (e.g., molecules). Here the DAE dramatically simplifies the search space by mapping inputs into a continuous latent space where familiar Bayesian optimization tools can be more readily applied. Despite this simplification, the latent space typically remains high-dimensional. Thus, even with a well-suited latent space, these approaches do not necessarily provide a complete solution, but may rather shift the structured optimization problem to a high-dimensional one. In this paper, we propose LOL-BO, which adapts the notion of trust regions explored in recent work on high-dimensional Bayesian optimization to the structured setting. By reformulating the encoder to function as both an encoder for the DAE globally and as a deep kernel for the surrogate model within a trust region, we better align the notion of local optimization in the latent space with local optimization in the input space. LOL-BO achieves as much as 20 times improvement over state-of-the-art latent space Bayesian optimization methods across six real-world benchmarks, demonstrating that improvement in optimization strategies is as important as developing better DAE models.

</details>

<details>

<summary>2022-01-28 04:20:07 - Simplifying deflation for non-convex optimization with applications in Bayesian inference and topology optimization</summary>

- *Mohamed Tarek, Yijiang Huang*

- `2201.11926v1` - [abs](http://arxiv.org/abs/2201.11926v1) - [pdf](http://arxiv.org/pdf/2201.11926v1)

> Non-convex optimization problems have multiple local optimal solutions. Non-convex optimization problems are commonly found in numerous applications. One of the methods recently proposed to efficiently explore multiple local optimal solutions without random re-initialization relies on the concept of deflation. In this paper, different ways to use deflation in non-convex optimization and nonlinear system solving are discussed. A simple, general and novel deflation constraint is proposed to enable the use of deflation together with existing nonlinear programming solvers or nonlinear system solvers. The connection between the proposed deflation constraint and a minimum distance constraint is presented. Additionally, a number of variations of deflation constraints and their limitations are discussed. Finally, a number of applications of the proposed methodology in the fields of approximate Bayesian inference and topology optimization are presented.

</details>

<details>

<summary>2022-01-28 09:30:32 - BCDAG: An R package for Bayesian structure and Causal learning of Gaussian DAGs</summary>

- *Federico Castelletti, Alessandro Mascaro*

- `2201.12003v1` - [abs](http://arxiv.org/abs/2201.12003v1) - [pdf](http://arxiv.org/pdf/2201.12003v1)

> Directed Acyclic Graphs (DAGs) provide a powerful framework to model causal relationships among variables in multivariate settings; in addition, through the do-calculus theory, they allow for the identification and estimation of causal effects between variables also from pure observational data. In this setting, the process of inferring the DAG structure from the data is referred to as causal structure learning or causal discovery. We introduce BCDAG, an R package for Bayesian causal discovery and causal effect estimation from Gaussian observational data, implementing the Markov chain Monte Carlo (MCMC) scheme proposed by Castelletti & Mascaro (2021). Our implementation scales efficiently with the number of observations and, whenever the DAGs are sufficiently sparse, with the number of variables in the dataset. The package also provides functions for convergence diagnostics and for visualizing and summarizing posterior inference. In this paper, we present the key features of the underlying methodology along with its implementation in BCDAG. We then illustrate the main functions and algorithms on both real and simulated datasets.

</details>

<details>

<summary>2022-01-28 10:28:04 - Differential Privacy of Dirichlet Posterior Sampling</summary>

- *Donlapark Ponnoprat*

- `2110.01984v2` - [abs](http://arxiv.org/abs/2110.01984v2) - [pdf](http://arxiv.org/pdf/2110.01984v2)

> Besides the Laplace distribution and the Gaussian distribution, there are many more probability distributions that are not well-understood in terms of privacy-preserving property -- one of which is the Dirichlet distribution. In this work, we study the inherent privacy of releasing a single draw from a Dirichlet posterior distribution (the Dirichlet posterior sampling). As our main result, we provide a simple privacy guarantee of the Dirichlet posterior sampling with the framework of R\'enyi Differential Privacy (RDP). Consequently, the RDP guarantee allows us to derive a simpler form of the $(\varepsilon,\delta)$-differential privacy guarantee compared to those from the previous work. As an application, we use the RDP guarantee to derive a utility guarantee of the Dirichlet posterior sampling for privately releasing a normalized histogram, which is confirmed by our experimental results. Moreover, we demonstrate that the RDP guarantee can be used to track the privacy loss in Bayesian reinforcement learning.

</details>

<details>

<summary>2022-01-28 10:31:14 - Conditional Deep Inverse Rosenblatt Transports</summary>

- *Tiangang Cui, Sergey Dolgov, Olivier Zahm*

- `2106.04170v2` - [abs](http://arxiv.org/abs/2106.04170v2) - [pdf](http://arxiv.org/pdf/2106.04170v2)

> We present a novel offline-online method to mitigate the computational burden of Bayesian inference, particularly in the regime where the posterior densities are computationally demanding to evaluate while real-time inference results are needed. In the offline phase, the proposed method learns the joint law of the parameter random variables and the observable random variables in the tensor-train (TT) format. Then, in the online phase, the resulting order-preserving transport can be conditioned on newly observed data to characterize the posterior random variables in real-time. Compared with the state-of-the-art normalizing flows techniques, our proposed method relies on function approximation, for which we can provide a thorough performance analysis. The function approximation perspective allows us to significantly improve the capability of transport maps in challenging problems with high-dimensional observations and high-dimensional parameters. Capitalizing on this, we present novel heuristics to either reorder or reparametrize the variables to enhance the approximation power of TT. We then integrate the TT-based transport maps and the parameter reordering/reparametrization into a layered composite map to further improve the performance of the resulting inference. We demonstrate the efficiency of the proposed method on various statistical learning tasks involving ordinary differential equations (ODEs) and partial differential equations (PDEs).

</details>

<details>

<summary>2022-01-28 10:33:04 - Certified dimension reduction in nonlinear Bayesian inverse problems</summary>

- *Olivier Zahm, Tiangang Cui, Kody Law, Alessio Spantini, Youssef Marzouk*

- `1807.03712v4` - [abs](http://arxiv.org/abs/1807.03712v4) - [pdf](http://arxiv.org/pdf/1807.03712v4)

> We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the Kullback--Leibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.

</details>

<details>

<summary>2022-01-28 11:17:12 - A loss discounting framework for model averaging and selection in time series models</summary>

- *Dawid Bernaciak, Jim E. Griffin*

- `2201.12045v1` - [abs](http://arxiv.org/abs/2201.12045v1) - [pdf](http://arxiv.org/pdf/2201.12045v1)

> We introduce a Loss Discounting Framework for forecast combination which generalises and combines Bayesian model synthesis and generalized Bayes methodologies. The framework allows large scale model averaging/selection and is also suitable for handling sudden regime changes. This novel and simple model synthesis framework is compared to both established methodologies and state of the art methods for a number of macroeconomic forecasting examples. We find that the proposed method offers an attractive, computationally efficient alternative to the benchmark methodologies and often outperforms more complex techniques.

</details>

<details>

<summary>2022-01-28 13:14:20 - Stochastic Consensus and the Shadow of Doubt</summary>

- *Emilien Macault*

- `2201.12100v1` - [abs](http://arxiv.org/abs/2201.12100v1) - [pdf](http://arxiv.org/pdf/2201.12100v1)

> We propose a stochastic model of opinion exchange in networks. A finite set of agents is organized in a fixed network structure. There is a binary state of the world and each agent receives a private signal on the state. We model beliefs as urns where red balls represent one possible value of the state and blue balls the other value. The model revolves purely around communication and beliefs dynamics. Communication happens in discrete time and, at each period, agents draw and display one ball from their urn with replacement. Then, they reinforce their urns by adding balls of the colors drawn by their neighbors. We show that for any network structure, this process converges almost-surely to a stable state. Futher, we show that if the communication network is connected, this stable state is such that all urns have the same proportion of balls. This result strengthens the main convergence properties of non-Bayesian learning models. Yet, contrary to those models, we show that this limit proportion is a full-support random variable. This implies that an arbitrarily small proportion of misinformed agents can substantially change the value of the limit consensus. We propose a set of conjectures on the distribution of this limit proportion based on simulations. In particular, we show evidence that the limit belief follows a beta distribution and that its average value is independent from the network structure.

</details>

<details>

<summary>2022-01-28 16:27:26 - Wasserstein posterior contraction rates in non-dominated Bayesian nonparametric models</summary>

- *Federico Camerlenghi, Emanuele Dolera, Stefano Favaro, Edoardo Mainini*

- `2201.12225v1` - [abs](http://arxiv.org/abs/2201.12225v1) - [pdf](http://arxiv.org/pdf/2201.12225v1)

> Posterior contractions rates (PCRs) strengthen the notion of Bayesian consistency, quantifying the speed at which the posterior distribution concentrates on arbitrarily small neighborhoods of the true model, with probability tending to 1 or almost surely, as the sample size goes to infinity. Under the Bayesian nonparametric framework, a common assumption in the study of PCRs is that the model is dominated for the observations; that is, it is assumed that the posterior can be written through the Bayes formula. In this paper, we consider the problem of establishing PCRs in Bayesian nonparametric models where the posterior distribution is not available through the Bayes formula, and hence models that are non-dominated for the observations. By means of the Wasserstein distance and a suitable sieve construction, our main result establishes PCRs in Bayesian nonparametric models where the posterior is available through a more general disintegration than the Bayes formula. To the best of our knowledge, this is the first general approach to provide PCRs in non-dominated Bayesian nonparametric models, and it relies on minimal modeling assumptions and on a suitable continuity assumption for the posterior distribution. Some refinements of our result are presented under additional assumptions on the prior distribution, and applications are given with respect to the Dirichlet process prior and the normalized extended Gamma process prior.

</details>

<details>

<summary>2022-01-28 18:36:33 - The curved exponential family of a staged tree</summary>

- *Christiane Grgen, Manuele Leonelli, Orlando Marigliano*

- `2010.15515v3` - [abs](http://arxiv.org/abs/2010.15515v3) - [pdf](http://arxiv.org/pdf/2010.15515v3)

> Staged tree models are a discrete generalization of Bayesian networks. We show that these form curved exponential families and derive their natural parameters, sufficient statistic, and cumulant-generating function as functions of their graphical representation. We give necessary and sufficient graphical criteria for classifying regular subfamilies and discuss implications for model selection.

</details>

<details>

<summary>2022-01-28 18:43:41 - Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein</summary>

- *Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, Olivier Teboul*

- `2201.12324v1` - [abs](http://arxiv.org/abs/2201.12324v1) - [pdf](http://arxiv.org/pdf/2201.12324v1)

> Optimal transport tools (OTT-JAX) is a Python toolbox that can solve optimal transport problems between point clouds and histograms. The toolbox builds on various JAX features, such as automatic and custom reverse mode differentiation, vectorization, just-in-time compilation and accelerators support. The toolbox covers elementary computations, such as the resolution of the regularized OT problem, and more advanced extensions, such as barycenters, Gromov-Wasserstein, low-rank solvers, estimation of convex maps, differentiable generalizations of quantiles and ranks, and approximate OT between Gaussian mixtures. The toolbox code is available at \texttt{https://github.com/ott-jax/ott}

</details>

<details>

<summary>2022-01-28 19:47:33 - Causal Discovery with Heterogeneous Observational Data</summary>

- *Fangting Zhou, Kejun He, Yang Ni*

- `2201.12392v1` - [abs](http://arxiv.org/abs/2201.12392v1) - [pdf](http://arxiv.org/pdf/2201.12392v1)

> We consider the problem of causal discovery (structure learning) from heterogeneous observational data. Most existing methods assume a homogeneous sampling scheme, which leads to misleading conclusions when violated in many applications. To this end, we propose a novel approach that exploits data heterogeneity to infer possibly cyclic causal structures from causally insufficient systems. The core idea is to model the direct causal effects as functions of exogenous covariates that properly explain data heterogeneity. We investigate structure identifiability properties of the proposed model. Structure learning is carried out in a fully Bayesian fashion, which provides natural uncertainty quantification. We demonstrate its utility through extensive simulations and a real-world application.

</details>

<details>

<summary>2022-01-28 20:20:59 - A penalized complexity prior for deep Bayesian transfer learning with application to materials informatics</summary>

- *Mohamed A. Abba, Jonathan P Williams, Brian J Reich*

- `2201.12401v1` - [abs](http://arxiv.org/abs/2201.12401v1) - [pdf](http://arxiv.org/pdf/2201.12401v1)

> A key task in the emerging field of materials informatics is to use machine learning to predict a material's properties and functions. A fast and accurate predictive model allows researchers to more efficiently identify or construct a material with desirable properties. As in many fields, deep learning is one of the state-of-the art approaches, but fully training a deep learning model is not always feasible in materials informatics due to limitations on data availability, computational resources, and time. Accordingly, there is a critical need in the application of deep learning to materials informatics problems to develop efficient transfer learning algorithms. The Bayesian framework is natural for transfer learning because the model trained from the source data can be encoded in the prior distribution for the target task of interest. However, the Bayesian perspective on transfer learning is relatively unaccounted for in the literature, and is complicated for deep learning because the parameter space is large and the interpretations of individual parameters are unclear. Therefore, rather than subjective prior distributions for individual parameters, we propose a new Bayesian transfer learning approach based on the penalized complexity prior on the Kullback-Leibler divergence between the predictive models of the source and target tasks. We show via simulations that the proposed method outperforms other transfer learning methods across a variety of settings. The new method is then applied to a predictive materials science problem where we show improved precision for estimating the band gap of a material based on its structural properties.

</details>

<details>

<summary>2022-01-28 21:10:36 - Automating Control of Overestimation Bias for Reinforcement Learning</summary>

- *Arsenii Kuznetsov, Alexander Grishin, Artem Tsypin, Arsenii Ashukha, Artur Kadurin, Dmitry Vetrov*

- `2110.13523v2` - [abs](http://arxiv.org/abs/2110.13523v2) - [pdf](http://arxiv.org/pdf/2110.13523v2)

> Overestimation bias control techniques are used by the majority of high-performing off-policy reinforcement learning algorithms. However, most of these techniques rely on pre-defined bias correction policies that are either not flexible enough or require environment-specific tuning of hyperparameters. In this work, we present a general data-driven approach for the automatic selection of bias control hyperparameters. We demonstrate its effectiveness on three algorithms: Truncated Quantile Critics, Weighted Delayed DDPG, and Maxmin Q-learning. The proposed technique eliminates the need for an extensive hyperparameter search. We show that it leads to a significant reduction of the actual number of interactions while preserving the performance.

</details>

<details>

<summary>2022-01-28 21:28:44 - A comparison of nonlinear extensions to the ensemble Kalman filter: Gaussian Anamorphosis and Two-Step Ensemble Filters</summary>

- *Ian Grooms*

- `2107.07475v2` - [abs](http://arxiv.org/abs/2107.07475v2) - [pdf](http://arxiv.org/pdf/2107.07475v2)

> Ensemble Kalman filters are based on a Gaussian assumption, which can limit their performance in some non-Gaussian settings. This paper reviews two nonlinear, non-Gaussian extensions of the Ensemble Kalman Filter: Gaussian anamorphosis (GA) methods and two-step updates, of which the rank histogram filter (RHF) is a prototypical example. GA-EnKF methods apply univariate transforms to the state and observation variables to make their distribution more Gaussian before applying an EnKF. The two-step methods use a scalar Bayesian update for the first step, followed by linear regression for the second step. The connection of the two-step framework to the full Bayesian problem is made, which opens the door to more advanced two-step methods in the full Bayesian setting. A new method for the first part of the two-step framework is proposed, with a similar form to the RHF but a different motivation, called the `improved RHF' (iRHF). A suite of experiments with the Lorenz-`96 model demonstrate situations where the GA-EnKF methods are similar to EnKF, and where they outperform EnKF. The experiments also strongly support the accuracy of the RHF and iRHF filters for nonlinear and non-Gaussian observations; these methods uniformly beat the EnKF and GA-EnKF methods in the experiments reported here. The new iRHF method is only more accurate than RHF at small ensemble sizes in the experiments reported here.

</details>

<details>

<summary>2022-01-29 09:34:38 - Data-Driven Parameter Estimation</summary>

- *George V. Moustakides*

- `2201.12539v1` - [abs](http://arxiv.org/abs/2201.12539v1) - [pdf](http://arxiv.org/pdf/2201.12539v1)

> Optimum parameter estimation methods require knowledge of a parametric probability density that statistically describes the available observations. In this work we examine Bayesian and non-Bayesian parameter estimation problems under a data-driven formulation where the necessary parametric probability density is replaced by available data. We present various data-driven versions that either result in neural network approximations of the optimum estimators or in well defined optimization problems that can be solved numerically. In particular, for the data-driven equivalent of non-Bayesian estimation we end up with optimization problems similar to the ones encountered for the design of generative networks.

</details>

<details>

<summary>2022-01-29 12:10:13 - Posterior contraction in group sparse logit models for categorical responses</summary>

- *Seonghyun Jeong*

- `2010.03513v3` - [abs](http://arxiv.org/abs/2010.03513v3) - [pdf](http://arxiv.org/pdf/2010.03513v3)

> This paper studies posterior contraction rates in multi-category logit models with priors incorporating group sparse structures. We consider a general class of logit models that includes the well-known multinomial logit models as a special case. Group sparsity is useful when predictor variables are naturally clustered and particularly useful for variable selection in the multinomial logit models. We provide a unified platform for posterior contraction rates of group-sparse logit models that include binary logistic regression under individual sparsity. No size restriction is directly imposed on the true signal in this study. In addition to establishing the first-ever contraction properties for multi-category logit models under group sparsity, this work also refines recent findings on the Bayesian theory of binary logistic regression.

</details>

<details>

<summary>2022-01-29 14:36:00 - Global Bias-Corrected Divide-and-Conquer by Quantile-Matched Composite for General Nonparametric Regressions</summary>

- *Yan Chen, Lu Lin*

- `2201.12597v1` - [abs](http://arxiv.org/abs/2201.12597v1) - [pdf](http://arxiv.org/pdf/2201.12597v1)

> The issues of bias-correction and robustness are crucial in the strategy of divide-and-conquer (DC), especially for asymmetric nonparametric models with massive data. It is known that quantile-based methods can achieve the robustness, but the quantile estimation for nonparametric regression has non-ignorable bias when the error distribution is asymmetric. This paper explores a global bias-corrected DC by quantile-matched composite for nonparametric regressions with general error distributions. The proposed strategies can achieve the bias-correction and robustness, simultaneously. Unlike common DC quantile estimations that use an identical quantile level to construct a local estimator by each local machine, in the new methodologies, the local estimators are obtained at various quantile levels for different data batches, and then the global estimator is elaborately constructed as a weighted sum of the local estimators. In the weighted sum, the weights and quantile levels are well-matched such that the bias of the global estimator is corrected significantly, especially for the case where the error distribution is asymmetric. Based on the asymptotic properties of the global estimator, the optimal weights are attained, and the corresponding algorithms are then suggested. The behaviors of the new methods are further illustrated by various numerical examples from simulation experiments and real data analyses. Compared with the competitors, the new methods have the favorable features of estimation accuracy, robustness, applicability and computational efficiency.

</details>

<details>

<summary>2022-01-29 21:26:56 - Bidders' Responses to Auction Format Change in Internet Display Advertising Auctions</summary>

- *Shumpei Goke, Gabriel Y. Weintraub, Ralph Mastromonaco, Sam Seljan*

- `2110.13814v2` - [abs](http://arxiv.org/abs/2110.13814v2) - [pdf](http://arxiv.org/pdf/2110.13814v2)

> We study actual bidding behavior when a new auction format gets introduced into the marketplace. More specifically, we investigate this question using a novel dataset on internet display advertising auctions that exploits a staggered adoption by different publishers (sellers) of first-price auctions (FPAs), instead of the traditional second-price auctions (SPAs). Event study regression estimates indicate that, immediately after the auction format change, the revenue per sold impression (price) jumped considerably for the treated publishers relative to the control publishers, ranging from 35% to 75% of the pre-treatment price level of the treatment group. Further, we observe that in later auction format changes the increase in the price levels under FPAs relative to price levels under SPAs dissipates over time, reminiscent of the celebrated revenue equivalence theorem. We take this as evidence of initially insufficient bid shading after the format change rather than an immediate shift to a new Bayesian Nash equilibrium. Prices then went down as bidders learned to shade their bids. We also show that bidders' sophistication impacted their response to the auction format change. Our work constitutes one of the first field studies on bidders' responses to auction format changes, providing an important complement to theoretical model predictions. As such, it provides valuable information to auction designers when considering the implementation of different formats.

</details>

<details>

<summary>2022-01-30 02:22:33 - Structured Stochastic Gradient MCMC</summary>

- *Antonios Alexos, Alex Boyd, Stephan Mandt*

- `2107.09028v3` - [abs](http://arxiv.org/abs/2107.09028v3) - [pdf](http://arxiv.org/pdf/2107.09028v3)

> Stochastic gradient Markov Chain Monte Carlo (SGMCMC) is considered the gold standard for Bayesian inference in large-scale models, such as Bayesian neural networks. Since practitioners face speed versus accuracy tradeoffs in these models, variational inference (VI) is often the preferable option. Unfortunately, VI makes strong assumptions on both the factorization and functional form of the posterior. In this work, we propose a new non-parametric variational approximation that makes no assumptions about the approximate posterior's functional form and allows practitioners to specify the exact dependencies the algorithm should respect or break. The approach relies on a new Langevin-type algorithm that operates on a modified energy function, where parts of the latent variables are averaged over samples from earlier iterations of the Markov chain. This way, statistical dependencies can be broken in a controlled way, allowing the chain to mix faster. This scheme can be further modified in a "dropout" manner, leading to even more scalability. We test our scheme for ResNet-20 on CIFAR-10, SVHN, and FMNIST. In all cases, we find improvements in convergence speed and/or final accuracy compared to SG-MCMC and VI.

</details>

<details>

<summary>2022-01-30 14:34:37 - Causal inference under mis-specification: adjustment based on the propensity score</summary>

- *David A. Stephens, Widemberg S. Nobre, Erica E. M. Moodie, Alexandra M. Schmidt*

- `2201.12831v1` - [abs](http://arxiv.org/abs/2201.12831v1) - [pdf](http://arxiv.org/pdf/2201.12831v1)

> We study Bayesian approaches to causal inference via propensity score regression. Much of the Bayesian literature on propensity score methods have relied on approaches that cannot be viewed as fully Bayesian in the context of conventional `likelihood times prior' posterior inference; in addition, most methods rely on parametric and distributional assumptions, and presumed correct specification. We emphasize that causal inference is typically carried out in settings of mis-specification, and develop strategies for fully Bayesian inference that reflect this. We focus on methods based on decision-theoretic arguments, and show how inference based on loss-minimization can give valid and fully Bayesian inference. We propose a computational approach to inference based on the Bayesian bootstrap which has good Bayesian and frequentist properties.

</details>

<details>

<summary>2022-01-30 15:09:10 - On the proof of posterior contraction for sparse generalized linear models with multivariate responses</summary>

- *Shao-Hsuan Wang, Ray Bai, Hsin-Hsiung Huang*

- `2201.12839v1` - [abs](http://arxiv.org/abs/2201.12839v1) - [pdf](http://arxiv.org/pdf/2201.12839v1)

> In recent years, the literature on Bayesian high-dimensional variable selection has rapidly grown. It is increasingly important to understand whether these Bayesian methods can consistently estimate the model parameters. To this end, shrinkage priors are useful for identifying relevant signals in high-dimensional data. For multivariate linear regression models with Gaussian response variables, Bai and Ghosh (2018) proposed a multivariate Bayesian model with shrinkage priors (MBSP) for estimation and variable selection in high-dimensional settings. However, the proofs of posterior consistency for the MBSP method (Theorems 3 and 4 of Bai and Ghosh (2018) were incorrect. In this paper, we provide a corrected proof of Theorems 3 and 4 of Bai and Ghosh (2018). We leverage these new proofs to extend the MBSP model to multivariate generalized linear models (GLMs). Under our proposed model (MBSP-GLM), multiple responses belonging to the exponential family are simultaneously modeled and mixed-type responses are allowed. We show that the MBSP-GLM model achieves strong posterior consistency when $p$ grows at a subexponential rate with $n$. Furthermore, we quantify the posterior contraction rate at which the posterior shrinks around the true regression coefficients and allow the dimension of the responses $q$ to grow as $n$ grows. Thus, we strengthen the previous results on posterior consistency, which did not provide rate results. This greatly expands the scope of the MBSP model to include response variables of many data types, including binary and count data. To the best of our knowledge, this is the first posterior contraction result for multivariate Bayesian GLMs.

</details>

<details>

<summary>2022-01-30 15:35:21 - Deep Non-Crossing Quantiles through the Partial Derivative</summary>

- *Axel Brando, Joan Gimeno, Jose A. Rodrguez-Serrano, Jordi Vitri*

- `2201.12848v1` - [abs](http://arxiv.org/abs/2201.12848v1) - [pdf](http://arxiv.org/pdf/2201.12848v1)

> Quantile Regression (QR) provides a way to approximate a single conditional quantile. To have a more informative description of the conditional distribution, QR can be merged with deep learning techniques to simultaneously estimate multiple quantiles. However, the minimisation of the QR-loss function does not guarantee non-crossing quantiles, which affects the validity of such predictions and introduces a critical issue in certain scenarios. In this article, we propose a generic deep learning algorithm for predicting an arbitrary number of quantiles that ensures the quantile monotonicity constraint up to the machine precision and maintains its modelling performance with respect to alternative models. The presented method is evaluated over several real-world datasets obtaining state-of-the-art results as well as showing that it scales to large-size data sets.

</details>

<details>

<summary>2022-01-30 16:24:52 - Extremal Random Forests</summary>

- *Nicola Gnecco, Edossa Merga Terefe, Sebastian Engelke*

- `2201.12865v1` - [abs](http://arxiv.org/abs/2201.12865v1) - [pdf](http://arxiv.org/pdf/2201.12865v1)

> Classical methods for quantile regression fail in cases where the quantile of interest is extreme and only few or no training data points exceed it. Asymptotic results from extreme value theory can be used to extrapolate beyond the range of the data, and several approaches exist that use linear regression, kernel methods or generalized additive models. Most of these methods break down if the predictor space has more than a few dimensions or if the regression function of extreme quantiles is complex. We propose a method for extreme quantile regression that combines the flexibility of random forests with the theory of extrapolation. Our extremal random forest (ERF) estimates the parameters of a generalized Pareto distribution, conditional on the predictor vector, by maximizing a local likelihood with weights extracted from a quantile random forest. Under certain assumptions, we show consistency of the estimated parameters. Furthermore, we penalize the shape parameter in this likelihood to regularize its variability in the predictor space. Simulation studies show that our ERF outperforms both classical quantile regression methods and existing regression approaches from extreme value theory. We apply our methodology to extreme quantile prediction for U.S. wage data.

</details>

<details>

<summary>2022-01-30 16:46:01 - Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations</summary>

- *Winnie Xu, Ricky T. Q. Chen, Xuechen Li, David Duvenaud*

- `2102.06559v4` - [abs](http://arxiv.org/abs/2102.06559v4) - [pdf](http://arxiv.org/pdf/2102.06559v4)

> We perform scalable approximate inference in continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer gives hidden units that follow a stochastic differential equation. We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior over weights approaches the true posterior. This approach brings continuous-depth Bayesian neural nets to a competitive comparison against discrete-depth alternatives, while inheriting the memory-efficient training and tunable precision of Neural ODEs.

</details>

<details>

<summary>2022-01-30 18:49:41 - Variational Inference as Iterative Projection in a Bayesian Hilbert Space</summary>

- *Timothy D. Barfoot, Gabriele M. T. D'Eleuterio*

- `2005.07275v2` - [abs](http://arxiv.org/abs/2005.07275v2) - [pdf](http://arxiv.org/pdf/2005.07275v2)

> Variational Bayesian inference is an important machine-learning tool that finds application from statistics to robotics. The goal is to find an approximate probability density function (PDF) from a chosen family that is in some sense 'closest' to the full Bayesian posterior. Closeness is typically defined through the selection of an appropriate loss functional such as the Kullback-Leibler (KL) divergence. In this paper, we explore a new formulation of variational inference by exploiting the fact that (most) PDFs are members of a Bayesian Hilbert space under careful definitions of vector addition, scalar multiplication and an inner product. We show that variational inference based on KL divergence then amounts to an iterative projection, in the Euclidean sense, of the Bayesian posterior onto a subspace corresponding to the selected approximation family. We work through the details of this general framework for the specific case of the Gaussian approximation family and show the equivalence to another Gaussian variational inference approach. We furthermore discuss the implications for systems that exhibit sparsity, which is handled naturally in Bayesian space, and give an example of a high-dimensional robotic state estimation problem that can be handled as a result. Finally, we provide some preliminary examples of how the approach could be applied to non-Gaussian inference.

</details>

<details>

<summary>2022-01-30 19:56:51 - Joint Quantile Disease Mapping with Application to Malaria and G6PD Deficiency</summary>

- *Hanan Alahmadi, Hvard Rue, Janet van Niekerk*

- `2201.12902v1` - [abs](http://arxiv.org/abs/2201.12902v1) - [pdf](http://arxiv.org/pdf/2201.12902v1)

> Statistical analysis based on quantile regression methods is more comprehensive, flexible, and less sensitive to outliers when compared to mean regression methods. When the link between different diseases are of interest, joint disease mapping is useful for measuring directional correlation between them. Most studies study this link through multiple correlated mean regressions. In this paper we propose a joint quantile regression framework for multiple diseases where different quantile levels can be considered. We are motivated by the theorized link between the presence of Malaria and the gene deficiency G6PD, where medical scientist have anecdotally discovered a possible link between high levels of G6PD and lower than expected levels of Malaria initially pointing towards the occurrence of G6PD inhibiting the occurrence of Malaria. This link cannot be investigated with mean regressions and thus the need for flexible joint quantile regression in a disease mapping framework. Our joint quantile disease mapping model can be used for linear and non-linear effects of covariates by stochastic splines, since we define it as a latent Gaussian model. We perform Bayesian inference of this model using the INLA framework embedded in the R software package INLA. Finally, we illustrate the applicability of model by analyzing the malaria and G6PD deficiency incidences in 21 African countries using linked quantiles of different levels.

</details>

<details>

<summary>2022-01-30 21:27:35 - Rapid Convergence of Informed Importance Tempering</summary>

- *Quan Zhou, Aaron Smith*

- `2107.10827v2` - [abs](http://arxiv.org/abs/2107.10827v2) - [pdf](http://arxiv.org/pdf/2107.10827v2)

> Informed Markov chain Monte Carlo (MCMC) methods have been proposed as scalable solutions to Bayesian posterior computation on high-dimensional discrete state spaces, but theoretical results about their convergence behavior in general settings are lacking. In this article, we propose a class of MCMC schemes called informed importance tempering (IIT), which combine importance sampling and informed local proposals, and derive generally applicable spectral gap bounds for IIT estimators. Our theory shows that IIT samplers have remarkable scalability when the target posterior distribution concentrates on a small set. Further, both our theory and numerical experiments demonstrate that the informed proposal should be chosen with caution: the performance of some proposals may be very sensitive to the shape of the target distribution. We find that the "square-root proposal weighting" scheme tends to perform well in most settings.

</details>

<details>

<summary>2022-01-31 11:10:24 - Estimation of World Seroprevalence of SARS-CoV-2 antibodies</summary>

- *Kwangmin Lee, Seongmin Kim, Seongil Jo, Jaeyong Lee*

- `2201.13124v1` - [abs](http://arxiv.org/abs/2201.13124v1) - [pdf](http://arxiv.org/pdf/2201.13124v1)

> In this paper, we estimate the seroprevalence against COVID-19 by country and derive the seroprevalence over the world. To estimate seroprevalence, we use serological surveys (also called the serosurveys) conducted within each country. When the serosurveys are incorporated to estimate world seroprevalence, there are two issues. First, there are countries in which a serological survey has not been conducted. Second, the sample collection dates differ from country to country. We attempt to tackle these problems using the vaccination data, confirmed cases data, and national statistics. We construct Bayesian models to estimate the numbers of people who have antibodies produced by infection or vaccination separately. For the number of people with antibodies due to infection, we develop a hierarchical model for combining the information included in both confirmed cases data and national statistics. At the same time, we propose regression models to estimate missing values in the vaccination data. As of 31st of July 2021, using the proposed methods, we obtain the 95% credible interval of the world seroprevalence as [38.6%, 59.2%].

</details>

<details>

<summary>2022-01-31 13:05:20 - A subsampling approach for Bayesian model selection</summary>

- *Jon Lachmann, Geir Storvik, Florian Frommlet, Aliaksadr Hubin*

- `2201.13198v1` - [abs](http://arxiv.org/abs/2201.13198v1) - [pdf](http://arxiv.org/pdf/2201.13198v1)

> It is common practice to use Laplace approximations to compute marginal likelihoods in Bayesian versions of generalised linear models (GLM). Marginal likelihoods combined with model priors are then used in different search algorithms to compute the posterior marginal probabilities of models and individual covariates. This allows performing Bayesian model selection and model averaging. For large sample sizes, even the Laplace approximation becomes computationally challenging because the optimisation routine involved needs to evaluate the likelihood on the full set of data in multiple iterations. As a consequence, the algorithm is not scalable for large datasets. To address this problem, we suggest using a version of a popular batch stochastic gradient descent (BSGD) algorithm for estimating the marginal likelihood of a GLM by subsampling from the data. We further combine the algorithm with Markov chain Monte Carlo (MCMC) based methods for Bayesian model selection and provide some theoretical results on the convergence of the estimates. Finally, we report results from experiments illustrating the performance of the proposed algorithm.

</details>

<details>

<summary>2022-01-31 17:43:43 - Deep Learning Macroeconomics</summary>

- *Rafael R. S. Guimaraes*

- `2201.13380v1` - [abs](http://arxiv.org/abs/2201.13380v1) - [pdf](http://arxiv.org/pdf/2201.13380v1)

> Limited datasets and complex nonlinear relationships are among the challenges that may emerge when applying econometrics to macroeconomic problems. This research proposes deep learning as an approach to transfer learning in the former case and to map relationships between variables in the latter case. Although macroeconomists already apply transfer learning when assuming a given a priori distribution in a Bayesian context, estimating a structural VAR with signal restriction and calibrating parameters based on results observed in other models, to name a few examples, advance in a more systematic transfer learning strategy in applied macroeconomics is the innovation we are introducing. We explore the proposed strategy empirically, showing that data from different but related domains, a type of transfer learning, helps identify the business cycle phases when there is no business cycle dating committee and to quick estimate a economic-based output gap. Next, since deep learning methods are a way of learning representations, those that are formed by the composition of multiple non-linear transformations, to yield more abstract representations, we apply deep learning for mapping low-frequency from high-frequency variables. The results obtained show the suitability of deep learning models applied to macroeconomic problems. First, models learned to classify United States business cycles correctly. Then, applying transfer learning, they were able to identify the business cycles of out-of-sample Brazilian and European data. Along the same lines, the models learned to estimate the output gap based on the U.S. data and obtained good performance when faced with Brazilian data. Additionally, deep learning proved adequate for mapping low-frequency variables from high-frequency data to interpolate, distribute, and extrapolate time series by related series.

</details>

<details>

<summary>2022-01-31 18:07:35 - Score Matched Neural Exponential Families for Likelihood-Free Inference</summary>

- *Lorenzo Pacchiardi, Ritabrata Dutta*

- `2012.10903v4` - [abs](http://arxiv.org/abs/2012.10903v4) - [pdf](http://arxiv.org/pdf/2012.10903v4)

> Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distributions for stochastic models with intractable likelihood, by relying on model simulations. In Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to the observation in order to sample from an approximate posterior, whose form depends on the chosen statistics. In this work, we introduce a new way to learn ABC statistics: we first generate parameter-simulation pairs from the model independently on the observation; then, we use Score Matching to train a neural conditional exponential family to approximate the likelihood. The exponential family is the largest class of distributions with fixed-size sufficient statistics; thus, we use them in ABC, which is intuitively appealing and has state-of-the-art performance. In parallel, we insert our likelihood approximation in an MCMC for doubly intractable distributions to draw posterior samples. We can repeat that for any number of observations with no additional model simulations, with performance comparable to related approaches. We validate our methods on toy models with known likelihood and a large-dimensional time-series model.

</details>


## 2022-02

<details>

<summary>2022-02-01 03:33:27 - Multilevel Longitudinal Analysis of Social Networks</summary>

- *Johan Koskinen, Tom A. B. Snijders*

- `2201.12713v2` - [abs](http://arxiv.org/abs/2201.12713v2) - [pdf](http://arxiv.org/pdf/2201.12713v2)

> Stochastic actor-oriented models (SAOM) are a broadly applied modelling framework for analysing network dynamics using network panel data. They have been extended to address co-evolution of multiple networks as well as networks and behaviour. This paper extends the SAOM to the analysis of multiple network panels through a random coefficient multilevel model, estimated with a Bayesian approach. This is illustrated by a study of the dynamic interdependence of friendship and minor delinquency, represented by the combination of a one-mode and a two-mode network, using a sample of 81 school classes in the first year of secondary school.

</details>

<details>

<summary>2022-02-01 06:21:31 - Bayesian Generalized Additive Model Selection Including a Fast Variational Option</summary>

- *Virginia X. He, Matt P. Wand*

- `2201.00412v2` - [abs](http://arxiv.org/abs/2201.00412v2) - [pdf](http://arxiv.org/pdf/2201.00412v2)

> We use Bayesian model selection paradigms, such as group least absolute shrinkage and selection operator priors, to facilitate generalized additive model selection. Our approach allows for the effects of continuous predictors to be categorized as either zero, linear or non-linear. Employment of carefully tailored auxiliary variables results in Gibbsian Markov chain Monte Carlo schemes for practical implementation of the approach. In addition, mean field variational algorithms with closed form updates are obtained. Whilst not as accurate, this fast variational option enhances scalability to very large data sets. A package in the R language aids use in practice.

</details>

<details>

<summary>2022-02-01 11:14:29 - Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer Treatment-Effects from Observational Data</summary>

- *Andrew Jesson, Panagiotis Tigas, Joost van Amersfoort, Andreas Kirsch, Uri Shalit, Yarin Gal*

- `2111.02275v2` - [abs](http://arxiv.org/abs/2111.02275v2) - [pdf](http://arxiv.org/pdf/2111.02275v2)

> Estimating personalized treatment effects from high-dimensional observational data is essential in situations where experimental designs are infeasible, unethical, or expensive. Existing approaches rely on fitting deep models on outcomes observed for treated and control populations. However, when measuring individual outcomes is costly, as is the case of a tumor biopsy, a sample-efficient strategy for acquiring each result is required. Deep Bayesian active learning provides a framework for efficient data acquisition by selecting points with high uncertainty. However, existing methods bias training data acquisition towards regions of non-overlapping support between the treated and control populations. These are not sample-efficient because the treatment effect is not identifiable in such regions. We introduce causal, Bayesian acquisition functions grounded in information theory that bias data acquisition towards regions with overlapping support to maximize sample efficiency for learning personalized treatment effects. We demonstrate the performance of the proposed acquisition strategies on synthetic and semi-synthetic datasets IHDP and CMNIST and their extensions, which aim to simulate common dataset biases and pathologies.

</details>

<details>

<summary>2022-02-01 15:40:53 - Cross Validation for Rare Events</summary>

- *Anass Aghbalou, Patrice Bertail, Franois Portier, Anne Sabourin*

- `2202.00488v1` - [abs](http://arxiv.org/abs/2202.00488v1) - [pdf](http://arxiv.org/pdf/2202.00488v1)

> We derive sanity-check bounds for the cross-validation (CV) estimate of the generalization risk for learning algorithms dedicated to extreme or rare events. We consider classification on extreme regions of the covariate space, a problem analyzed in Jalalzai et al. 2018. The risk is then a probability of error conditional to the norm of the covariate vector exceeding a high quantile. Establishing sanity-check bounds consist in recovering bounds regarding the CV estimate that are of the same nature as the ones regarding the empirical risk. We achieve this goal both for K-fold CV with an exponential bound and for leave-p-out CV with a polynomial bound, thus extending the state-of-the-art results to the modified version of the risk which is adapted to extreme value analysis.

</details>

<details>

<summary>2022-02-01 15:45:38 - Spectral Clustering, Spanning Forest, and Bayesian Forest Process</summary>

- *Leo L. Duan, Arkaprava Roy*

- `2202.00493v1` - [abs](http://arxiv.org/abs/2202.00493v1) - [pdf](http://arxiv.org/pdf/2202.00493v1)

> Spectral clustering algorithms are very popular. Starting from a pairwise similarity matrix, spectral clustering gives a partition of data that approximately minimizes the total similarity scores across clusters. Since there is no need to model how data are distributed within each cluster, such a method enjoys algorithmic simplicity and robustness in clustering non-Gaussian data such as those near manifolds. Nevertheless, several important questions are unaddressed, such as how to estimate the similarity scores and cluster assignment probabilities, as important uncertainty estimates in clustering. In this article, we propose to solve these problems with a discovered generative modeling counterpart. Our clustering model is based on a spanning forest graph that consists of several disjoint spanning trees, with each tree corresponding to a cluster. Taking a Bayesian approach, we assign proper densities on the root and leaf nodes, and we prove that the posterior mode is almost the same as spectral clustering estimates. Further, we show that the associated generative process, named "forest process", is a continuous extension to the classic urn process, hence inheriting many nice properties such as having unbounded support for the number of clusters and being amenable to existing partition probability function; at the same time, we carefully characterize their differences. We demonstrate a novel application in joint clustering of multiple-subject functional magnetic resonance imaging scans of the human brain.

</details>

<details>

<summary>2022-02-01 15:59:43 - MetaStan: An R package for Bayesian (model-based) meta-analysis using Stan</summary>

- *Burak Krsad Gnhan, Christian Rver, Tim Friede*

- `2202.00502v1` - [abs](http://arxiv.org/abs/2202.00502v1) - [pdf](http://arxiv.org/pdf/2202.00502v1)

> Meta-analysis methods are used to combine evidence from multiple studies. Meta-regression as well as model-based meta-analysis are extensions of standard pairwise meta-analysis in which information about study-level covariates and (arm-level) dosing amount or exposure may be taken into account. A Bayesian approach to inference is very attractive in this context, especially when a meta-analysis is based on few studies only or rare events. In this article, we present the R package MetaStan which implements a wide range of pairwise and model-based meta-analysis models.   A generalised linear mixed model (GLMM) framework is used to describe the pairwise meta-analysis, meta-regression and model-based meta-analysis models. Within the GLMM framework, the likelihood and link functions are adapted to reflect the nature of the data. For example, a binomial likelihood with a logit link is used to perform a meta-analysis based on datasets with dichotomous endpoints. Bayesian computations are conducted using Stan via the rstan interface. Stan uses a Hamiltonian Monte Carlo sampler which belongs to the family of Markov chain Monte Carlo methods. Stan implementations are done by using suitable parametrizations to ease computations.   The user-friendly R package MetaStan, available on CRAN, supports a wide range of pairwise and model-based meta-analysis models. MetaStan provides fitting functions for pairwise meta-analysis with the option of including covariates and model-based meta-analysis. The supported outcome types are continuous, binary, and count. Forest plots for the pairwise meta-analysis and dose-response plots for the model-based meta-analysis can be obtained from the package. The use of MetaStan is demonstrated through clinical examples.

</details>

<details>

<summary>2022-02-01 16:29:28 - Adaptive Conditional Distribution Estimation with Bayesian Decision Tree Ensembles</summary>

- *Yinpu Li, Antonio R. Linero, Jared S. Murray*

- `2005.02490v2` - [abs](http://arxiv.org/abs/2005.02490v2) - [pdf](http://arxiv.org/pdf/2005.02490v2)

> We present a Bayesian nonparametric model for conditional distribution estimation using Bayesian additive regression trees (BART). The generative model we use is based on rejection sampling from a base model. Typical of BART models, our model is flexible, has a default prior specification, and is computationally convenient. To address the distinguished role of the response in the BART model we propose, we further introduce an approach to targeted smoothing which is possibly of independent interest for BART models. We study the proposed model theoretically and provide sufficient conditions for the posterior distribution to concentrate at close to the minimax optimal rate adaptively over smoothness classes in the high-dimensional regime in which many predictors are irrelevant. To fit our model we propose a data augmentation algorithm which allows for existing BART samplers to be extended with minimal effort. We illustrate the performance of our methodology on simulated data and use it to study the relationship between education and body mass index using data from the medical expenditure panel survey (MEPS).

</details>

<details>

<summary>2022-02-01 17:28:43 - A Statistical Model of Serve Return Impact Patterns in Professional Tennis</summary>

- *Stephanie A. Kovalchik, Jim Albert*

- `2202.00583v1` - [abs](http://arxiv.org/abs/2202.00583v1) - [pdf](http://arxiv.org/pdf/2202.00583v1)

> The spread in the use of tracking systems in sport has made fine-grained spatiotemporal analysis a primary focus of an emerging sports analytics industry. Recently publicized tracking data for men's professional tennis allows for the first detailed spatial analysis of return impact. Mixture models are an appealing model-based framework for spatial analysis in sport, where latent variable discovery is often of primary interest. Although finite mixture models have the advantages of interpretability and scalability, most implementations assume standard parametric distributions for outcomes conditioned on latent variables. In this paper, we present a more flexible alternative that allows the latent conditional distribution to be a mixed member of finite Gaussian mixtures. Our model was motivated by our efforts to describe common styles of return impact location of professional tennis players and is the reason we name the approach a 'latent style allocation' model. In a fully Bayesian implementation, we apply the model to 142,803 return points played by 141 top players at Association of Tennis Professional events between 2018 and 2020 and show that the latent style allocation improves predictive performance over a finite Gaussian mixture model and identifies six unique impact styles on the first and second serve return.

</details>

<details>

<summary>2022-02-01 17:49:00 - The Effect of Sample Size and Missingness on Inference with Missing Data</summary>

- *Julian Morimoto*

- `2112.09275v4` - [abs](http://arxiv.org/abs/2112.09275v4) - [pdf](http://arxiv.org/pdf/2112.09275v4)

> When are inferences (whether Direct-Likelihood, Bayesian, or Frequentist) obtained from partial data valid? This paper answers this question by offering a new asymptotic theory about inference with missing data that is more general than existing theories. By using more powerful tools from real analysis and probability theory than those used in previous research, it proves that as the sample size increases and the extent of missingness decreases, the average-loglikelihood function generated by partial data and that ignores the missingness mechanism will almost surely converge uniformly to that which would have been generated by complete data; and if the data are Missing at Random, this convergence depends only on sample size. Thus, inferences from partial data, such as posterior modes, uncertainty estimates, confidence intervals, likelihood ratios, test statistics, and indeed, all quantities or features derived from the partial-data loglikelihood function, will be consistently estimated. They will approximate their complete-data analogues. This adds to previous research which has only proved the consistency and asymptotic normality of the posterior mode, and developed separate theories for Direct-Likelihood, Bayesian, and Frequentist inference. Practical implications of this result are discussed, and the theory is verified using a previous study of International Human Rights Law.

</details>

<details>

<summary>2022-02-01 18:16:12 - Black-box Bayesian inference for economic agent-based models</summary>

- *Joel Dyer, Patrick Cannon, J. Doyne Farmer, Sebastian Schmon*

- `2202.00625v1` - [abs](http://arxiv.org/abs/2202.00625v1) - [pdf](http://arxiv.org/pdf/2202.00625v1)

> Simulation models, in particular agent-based models, are gaining popularity in economics. The considerable flexibility they offer, as well as their capacity to reproduce a variety of empirically observed behaviours of complex systems, give them broad appeal, and the increasing availability of cheap computing power has made their use feasible. Yet a widespread adoption in real-world modelling and decision-making scenarios has been hindered by the difficulty of performing parameter estimation for such models. In general, simulation models lack a tractable likelihood function, which precludes a straightforward application of standard statistical inference techniques. Several recent works have sought to address this problem through the application of likelihood-free inference techniques, in which parameter estimates are determined by performing some form of comparison between the observed data and simulation output. However, these approaches are (a) founded on restrictive assumptions, and/or (b) typically require many hundreds of thousands of simulations. These qualities make them unsuitable for large-scale simulations in economics and can cast doubt on the validity of these inference methods in such scenarios. In this paper, we investigate the efficacy of two classes of black-box approximate Bayesian inference methods that have recently drawn significant attention within the probabilistic machine learning community: neural posterior estimation and neural density ratio estimation. We present benchmarking experiments in which we demonstrate that neural network based black-box methods provide state of the art parameter inference for economic simulation models, and crucially are compatible with generic multivariate time-series data. In addition, we suggest appropriate assessment criteria for future benchmarking of approximate Bayesian inference procedures for economic simulation models.

</details>

<details>

<summary>2022-02-01 18:22:37 - Bayesian inference and prediction for mean-mixtures of normal distributions</summary>

- *Pankaj Bhagwat, Eric Marchand*

- `2202.00629v1` - [abs](http://arxiv.org/abs/2202.00629v1) - [pdf](http://arxiv.org/pdf/2202.00629v1)

> We study frequentist risk properties of predictive density estimators for mean mixtures of multivariate normal distributions, involving an unknown location parameter $\theta \in \mathbb{R}^d$, and which include multivariate skew normal distributions. We provide explicit representations for Bayesian posterior and predictive densities, including the benchmark minimum risk equivariant (MRE) density, which is minimax and generalized Bayes with respect to an improper uniform density for $\theta$. For four dimensions or more, we obtain Bayesian densities that improve uniformly on the MRE density under Kullback-Leibler loss. We also provide plug-in type improvements, investigate implications for certain type of parametric restrictions on $\theta$, and illustrate and comment the findings based on numerical evaluations.

</details>

<details>

<summary>2022-02-01 19:04:20 - A fully Bayesian semi-parametric scalar-on-function regression (SoFR) with measurement error using instrumental variables</summary>

- *Roger S. Zoh, Yuanyuan Luan, Carmen Tekwe*

- `2202.00711v1` - [abs](http://arxiv.org/abs/2202.00711v1) - [pdf](http://arxiv.org/pdf/2202.00711v1)

> Wearable devices such as the ActiGraph are now commonly used in health studies to monitor or track physical activity. This trend aligns well with the growing need to accurately assess the effects of physical activity on health outcomes such as obesity. When accessing the association between these device-based physical activity measures with health outcomes such as body mass index, the device-based data is considered functions, while the outcome is a scalar-valued. The regression model applied in these settings is the scalar-on-function regression (SoFR). Most estimation approaches in SoFR assume that the functional covariates are precisely observed, or the measurement errors are considered random errors. Violation of this assumption can lead to both under-estimation of the model parameters and sub-optimal analysis. The literature on a measurement corrected approach in SoFR is sparse in the non-Bayesian literature and virtually non-existent in the Bayesian literature. This paper considers a fully nonparametric Bayesian measurement error corrected SoFR model that relaxes all the constraining assumptions often made in these models. Our estimation relies on an instrumental variable (IV) to identify the measurement error model. Finally, we introduce an IV quality scalar parameter that is jointly estimated along with all model parameters. Our method is easy to implement, and we demonstrate its finite sample properties through an extensive simulation. Finally, the developed methods are applied to the National Health and Examination Survey to assess the relationship between wearable-device-based measures of physical activity and body mass index among adults living in the United States.

</details>

<details>

<summary>2022-02-01 22:35:17 - Hierarchical Entity Alignment for Attribute-Rich Event-Driven Graphs</summary>

- *Elizabeth Hou, Joanna Brown, John Fisher*

- `2202.00798v1` - [abs](http://arxiv.org/abs/2202.00798v1) - [pdf](http://arxiv.org/pdf/2202.00798v1)

> This paper addresses the problem of entity alignment in attribute-rich event-driven graphs. Unlike many other entity alignment problems, we are interested in aligning entities based on the similarity of their actions, i.e., entities that participate in similar events are more likely to be the same. We model the generative process of this problem as a Bayesian model and derive our proposed algorithm from the posterior predictive distribution. We apply our Hierarchical Entity AlignmenT (HEAT) algorithm to two datasets, one on publications and the other on financial transactions, derived from real data and provided to us by an external collaborator.

</details>

<details>

<summary>2022-02-02 01:59:02 - Identification and Estimation of Unconditional Policy Effects of an Endogenous Binary Treatment: an Unconditional MTE Approach</summary>

- *Julin Martnez-Iriarte, Yixiao Sun*

- `2010.15864v3` - [abs](http://arxiv.org/abs/2010.15864v3) - [pdf](http://arxiv.org/pdf/2010.15864v3)

> This paper studies identification and estimation of unconditional policy effects when treatment status is binary and endogenous. We introduce a new class of unconditional marginal treatment effects (MTE) based on the influence function of the functional underlying the policy target. We show that an unconditional policy effect can be represented as a weighted average of the newly defined unconditional MTEs over the individuals who are indifferent about their treatment status. We provide conditions for point identification of the unconditional policy effects. When a quantile is the functional of interest, we characterize the asymptotic bias of the unconditional quantile regression (UQR) estimator that ignores the endogeneity of the treatment and elaborate on the channels that the endogeneity can render the UQR estimator inconsistent. We show that, even if the treatment status is exogenous, the UQR estimator can still be inconsistent when there are common covariates affecting both the treatment status and the outcome variable. To overcome the inconsistency of the UQR estimator, we introduce the UNconditional Instrumental Quantile Estimator (UNIQUE) and establish its consistency and asymptotic distribution. In the empirical application, we estimate the effect of changing college enrollment status, induced by higher tuition subsidy, on the quantiles of the wage distribution.

</details>

<details>

<summary>2022-02-02 04:24:52 - Bayesian Imputation with Optimal Look-Ahead-Bias and Variance Tradeoff</summary>

- *Jose Blanchet, Fernando Hernandez, Viet Anh Nguyen, Markus Pelger, Xuhui Zhang*

- `2202.00871v1` - [abs](http://arxiv.org/abs/2202.00871v1) - [pdf](http://arxiv.org/pdf/2202.00871v1)

> Missing time-series data is a prevalent problem in finance. Imputation methods for time-series data are usually applied to the full panel data with the purpose of training a model for a downstream out-of-sample task. For example, the imputation of missing returns may be applied prior to estimating a portfolio optimization model. However, this practice can result in a look-ahead-bias in the future performance of the downstream task. There is an inherent trade-off between the look-ahead-bias of using the full data set for imputation and the larger variance in the imputation from using only the training data. By connecting layers of information revealed in time, we propose a Bayesian consensus posterior that fuses an arbitrary number of posteriors to optimally control the variance and look-ahead-bias trade-off in the imputation. We derive tractable two-step optimization procedures for finding the optimal consensus posterior, with Kullback-Leibler divergence and Wasserstein distance as the measure of dissimilarity between posterior distributions. We demonstrate in simulations and an empirical study the benefit of our imputation mechanism for portfolio optimization with missing returns.

</details>

<details>

<summary>2022-02-02 10:02:52 - Bayesian Optimization for Distributionally Robust Chance-constrained Problem</summary>

- *Yu Inatsu, Shion Takeno, Masayuki Karasuyama, Ichiro Takeuchi*

- `2201.13112v2` - [abs](http://arxiv.org/abs/2201.13112v2) - [pdf](http://arxiv.org/pdf/2201.13112v2)

> In black-box function optimization, we need to consider not only controllable design variables but also uncontrollable stochastic environment variables. In such cases, it is necessary to solve the optimization problem by taking into account the uncertainty of the environmental variables. Chance-constrained (CC) problem, the problem of maximizing the expected value under a certain level of constraint satisfaction probability, is one of the practically important problems in the presence of environmental variables. In this study, we consider distributionally robust CC (DRCC) problem and propose a novel DRCC Bayesian optimization method for the case where the distribution of the environmental variables cannot be precisely specified. We show that the proposed method can find an arbitrary accurate solution with high probability in a finite number of trials, and confirm the usefulness of the proposed method through numerical experiments.

</details>

<details>

<summary>2022-02-02 15:27:28 - Being a Bit Frequentist Improves Bayesian Neural Networks</summary>

- *Agustinus Kristiadi, Matthias Hein, Philipp Hennig*

- `2106.10065v3` - [abs](http://arxiv.org/abs/2106.10065v3) - [pdf](http://arxiv.org/pdf/2106.10065v3)

> Despite their compelling theoretical properties, Bayesian neural networks (BNNs) tend to perform worse than frequentist methods in classification-based uncertainty quantification (UQ) tasks such as out-of-distribution (OOD) detection. In this paper, based on empirical findings in prior works, we hypothesize that this issue is because even recent Bayesian methods have never considered OOD data in their training processes, even though this "OOD training" technique is an integral part of state-of-the-art frequentist UQ methods. To validate this, we treat OOD data as a first-class citizen in BNN training by exploring four different ways of incorporating OOD data into Bayesian inference. We show in extensive experiments that OOD-trained BNNs are competitive to recent frequentist baselines. This work thus provides strong baselines for future work in Bayesian UQ.

</details>

<details>

<summary>2022-02-02 17:47:03 - A Recommender System Based on a Double Feature Allocation Model</summary>

- *Qiaohui Lin, Peter Mueller*

- `2202.01163v1` - [abs](http://arxiv.org/abs/2202.01163v1) - [pdf](http://arxiv.org/pdf/2202.01163v1)

> A collaborative filtering recommender system predicts user preferences by discovering common features among users and items. We implement such inference using a Bayesian double feature allocation model, that is, a model for random pairs of subsets. We use an Indian buffet process (IBP) to link users and items to features. Here a feature is a subset of users and a matching subset of items. By training feature-specific rating effects, we predict ratings. We use MovieLens Data to demonstrate posterior inference in the model and prediction of user preferences for unseen items compared to items they have previously rated.   Part of the implementation is a novel semi-consensus Monte Carlo method to accomodate large numbers of users and items, as is typical for related applications. The proposed approach implements parallel posterior sampling in multiple shards of users while sharing item-related global parameters across shards.

</details>

<details>

<summary>2022-02-02 19:37:26 - Incorporating baseline covariates to validate surrogate endpoints with a constant biomarker under control arm</summary>

- *Emily Roberts, Michael Elliott, Jeremy M. G. Taylor*

- `2104.12947v2` - [abs](http://arxiv.org/abs/2104.12947v2) - [pdf](http://arxiv.org/pdf/2104.12947v2)

> A surrogate endpoint S in a clinical trial is an outcome that may be measured earlier or more easily than the true outcome of interest T. In this work, we extend causal inference approaches to validate such a surrogate using potential outcomes. The causal association paradigm assesses the relationship of the treatment effect on the surrogate with the treatment effect on the true endpoint. Using the principal surrogacy criteria, we utilize the joint conditional distribution of the potential outcomes T, given the potential outcomes S. In particular, our setting of interest allows us to assume the surrogate under the placebo, S(0), is zero-valued, and we incorporate baseline covariates in the setting of normally-distributed endpoints. We develop Bayesian methods to incorporate conditional independence and other modeling assumptions and explore their impact on the assessment of surrogacy. We demonstrate our approach via simulation and data that mimics an ongoing study of a muscular dystrophy gene therapy.

</details>

<details>

<summary>2022-02-02 21:04:37 - Risk bounds for aggregated shallow neural networks using Gaussian prior</summary>

- *Laura Tinsi, Arnak S. Dalalyan*

- `2112.11086v2` - [abs](http://arxiv.org/abs/2112.11086v2) - [pdf](http://arxiv.org/pdf/2112.11086v2)

> Analysing statistical properties of neural networks is a central topic in statistics and machine learning. However, most results in the literature focus on the properties of the neural network minimizing the training error. The goal of this paper is to consider aggregated neural networks using a Gaussian prior. The departure point of our approach is an arbitrary aggregate satisfying the PAC-Bayesian inequality. The main contribution is a precise nonasymptotic assessment of the estimation error appearing in the PAC-Bayes bound. We also review available bounds on the error of approximating a function by a neural network. Combining bounds on estimation and approximation errors, we establish risk bounds that are sharp enough to lead to minimax rates of estimation over Sobolev smoothness classes.

</details>

<details>

<summary>2022-02-02 21:52:09 - Inverse Bayesian Optimization: Learning Human Acquisition Functions in an Exploration vs Exploitation Search Task</summary>

- *Nathan Sandholtz, Yohsuke Miyamoto, Luke Bornn, Maurice Smith*

- `2104.09237v2` - [abs](http://arxiv.org/abs/2104.09237v2) - [pdf](http://arxiv.org/pdf/2104.09237v2)

> This paper introduces a probabilistic framework to estimate parameters of an acquisition function given observed human behavior that can be modeled as a collection of sample paths from a Bayesian optimization procedure. The methodology involves defining a likelihood on observed human behavior from an optimization task, where the likelihood is parameterized by a Bayesian optimization subroutine governed by an unknown acquisition function. This structure enables us to make inference on a subject's acquisition function while allowing their behavior to deviate around the solution to the Bayesian optimization subroutine. To test our methods, we designed a sequential optimization task which forced subjects to balance exploration and exploitation in search of an invisible target location. Applying our proposed methods to the resulting data, we find that many subjects tend to exhibit exploration preferences beyond that of standard acquisition functions to capture. Guided by the model discrepancies, we augment the candidate acquisition functions to yield a superior fit to the human behavior in this task.

</details>

<details>

<summary>2022-02-02 22:12:04 - intRinsic: an R package for model-based estimation of the intrinsic dimension of a dataset</summary>

- *Francesco Denti*

- `2102.11425v2` - [abs](http://arxiv.org/abs/2102.11425v2) - [pdf](http://arxiv.org/pdf/2102.11425v2)

> The estimation of the intrinsic dimension of a dataset is a fundamental step in most dimensionality reduction techniques. This article illustrates intRinsic, an R package that implements novel state-of-the-art likelihood-based estimators of the intrinsic dimension of a dataset. In order to make these novel estimators easily accessible, the package contains a small number of high-level functions that rely on a broader set of efficient, low-level routines. Generally speaking, intRinsic encompasses models that fall into two categories: homogeneous and heterogeneous intrinsic dimension estimators. The first category contains the TWO-NN model, an estimator derived from the distributional properties of the ratios of the distances between each data point and its first two of nearest neighbors. The functions dedicated to this method carry out inference under both the frequentist and Bayesian frameworks. In the second category, we find Hidalgo, a Bayesian mixture model, for which an efficient Gibbs sampler is implemented. After presenting the theoretical background, we demonstrate the performance of the models on simulated datasets. This way, we can facilitate the exposition by immediately assessing the validity of the results. Then, we employ the package to study the intrinsic dimension of the Alon dataset, obtained from a famous microarray experiment. We show how the estimation of homogeneous and heterogeneous intrinsic dimensions allows us to gain valuable insights into the topological structure of a dataset.

</details>

<details>

<summary>2022-02-03 02:50:38 - Information Disclosure and Promotion Policy Design for Platforms</summary>

- *Yonatan Gur, Gregory Macnamara, Ilan Morgenstern, Daniela Saban*

- `1911.09256v3` - [abs](http://arxiv.org/abs/1911.09256v3) - [pdf](http://arxiv.org/pdf/1911.09256v3)

> We consider a platform facilitating trade between sellers and buyers with the objective of maximizing consumer surplus. Even though in many such marketplaces prices are set by revenue-maximizing sellers, platforms can influence prices through (i) price-dependent promotion policies that can increase demand for a product by featuring it in a prominent position on the webpage and (ii) the information revealed to sellers about the value of being promoted. Identifying effective joint information design and promotion policies is a challenging dynamic problem as sellers can sequentially learn the promotion value from sales observations and update prices accordingly. We introduce the notion of confounding promotion policies, which are designed to prevent a Bayesian seller from learning the promotion value (at the expense of the short-run loss of diverting consumers from the best product offering). Leveraging these policies, we characterize the maximum long-run average consumer surplus that is achievable through joint information design and promotion policies when the seller sets prices myopically. We then construct a Bayesian Nash equilibrium in which the seller's best response to the platform's optimal policy is to price myopically at every period. Moreover, the equilibrium we identify is platform-optimal within the class of horizon-maximin equilibria, in which strategies are not predicated on precise knowledge of the horizon length, and are designed to maximize payoff over the worst-case horizon. Our analysis allows one to identify practical long-run average optimal platform policies in a broad range of demand models.

</details>

<details>

<summary>2022-02-03 08:15:53 - Deep Hierarchy in Bandits</summary>

- *Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil Zaheer, Mohammad Ghavamzadeh*

- `2202.01454v1` - [abs](http://arxiv.org/abs/2202.01454v1) - [pdf](http://arxiv.org/pdf/2202.01454v1)

> Mean rewards of actions are often correlated. The form of these correlations may be complex and unknown a priori, such as the preferences of a user for recommended products and their categories. To maximize statistical efficiency, it is important to leverage these correlations when learning. We formulate a bandit variant of this problem where the correlations of mean action rewards are represented by a hierarchical Bayesian model with latent variables. Since the hierarchy can have multiple layers, we call it deep. We propose a hierarchical Thompson sampling algorithm (HierTS) for this problem, and show how to implement it efficiently for Gaussian hierarchies. The efficient implementation is possible due to a novel exact hierarchical representation of the posterior, which itself is of independent interest. We use this exact posterior to analyze the Bayes regret of HierTS in Gaussian bandits. Our analysis reflects the structure of the problem, that the regret decreases with the prior width, and also shows that hierarchies reduce the regret by non-constant factors in the number of actions. We confirm these theoretical findings empirically, in both synthetic and real-world experiments.

</details>

<details>

<summary>2022-02-03 09:01:38 - A Reputation Game Simulation: Emergent Social Phenomena from Information Theory</summary>

- *Torsten Enlin, Viktoria Kainz, Cline Bhm*

- `2106.05414v3` - [abs](http://arxiv.org/abs/2106.05414v3) - [pdf](http://arxiv.org/pdf/2106.05414v3)

> Reputation is a central element of social communications, be it with human or artificial intelligence (AI), and as such can be the primary target of malicious communication strategies. There is already a vast amount of literature on trust networks addressing this issue and proposing ways to simulate these networks dynamics using Bayesian principles and involving Theory of Mind models. The main issue for these simulations is usually the amount of information that can be stored and is usually solved by discretising variables and using hard thresholds. Here we propose a novel approach to the way information is updated that accounts for knowledge uncertainty and is closer to reality. In our game, agents use information compression techniques to capture their complex environment and store it in their finite memories. The loss of information that results from this leads to emergent phenomena, such as echo chambers, self-deception, deception symbiosis, and freezing of group opinions. Various malicious strategies of agents are studied for their impact on group sociology, like sycophancy, egocentricity, pathological lying, and aggressiveness. Even though our modeling could be made more complex, our set-up can already provide insights into social interactions and can be used to investigate the effects of various communication strategies and find ways to counteract malicious ones. Eventually this work should help to safeguard the design of non-abusive AI systems.

</details>

<details>

<summary>2022-02-03 11:44:13 - Sampling Permutations for Shapley Value Estimation</summary>

- *Rory Mitchell, Joshua Cooper, Eibe Frank, Geoffrey Holmes*

- `2104.12199v2` - [abs](http://arxiv.org/abs/2104.12199v2) - [pdf](http://arxiv.org/pdf/2104.12199v2)

> Game-theoretic attribution techniques based on Shapley values are used to interpret black-box machine learning models, but their exact calculation is generally NP-hard, requiring approximation methods for non-trivial models. As the computation of Shapley values can be expressed as a summation over a set of permutations, a common approach is to sample a subset of these permutations for approximation. Unfortunately, standard Monte Carlo sampling methods can exhibit slow convergence, and more sophisticated quasi-Monte Carlo methods have not yet been applied to the space of permutations. To address this, we investigate new approaches based on two classes of approximation methods and compare them empirically. First, we demonstrate quadrature techniques in a RKHS containing functions of permutations, using the Mallows kernel in combination with kernel herding and sequential Bayesian quadrature. The RKHS perspective also leads to quasi-Monte Carlo type error bounds, with a tractable discrepancy measure defined on permutations. Second, we exploit connections between the hypersphere $\mathbb{S}^{d-2}$ and permutations to create practical algorithms for generating permutation samples with good properties. Experiments show the above techniques provide significant improvements for Shapley value estimates over existing methods, converging to a smaller RMSE in the same number of model evaluations.

</details>

<details>

<summary>2022-02-03 16:47:57 - State-of-the-Art Methods for Exposure-Health Studies: results from the Exposome Data Challenge Event</summary>

- *La Maitre, Jean-Baptiste Guimbaud, Charline Warembourg, Nuria Gil-Oumrait, The Exposome Data Challenge Participant Consortium, Paula Marcela Petrone, Marc Chadeau-Hyam, Martine Vrijheid, Juan R. Gonzalez, Xavier Basagaa*

- `2202.01680v1` - [abs](http://arxiv.org/abs/2202.01680v1) - [pdf](http://arxiv.org/pdf/2202.01680v1)

> The exposome recognizes that individuals are exposed simultaneously to a multitude of different environmental factors and takes a holistic approach to the discovery of etiological factors for disease. However, challenges arise when trying to quantify the health effects of complex exposure mixtures. Analytical challenges include dealing with high dimensionality, studying the combined effects of these exposures and their interactions, integrating causal pathways, and integrating omics layers. To tackle these challenges, ISGlobal Exposome Hub held a data challenge event open to researchers from all over the world and from all expertises. Analysts had a chance to compete and apply state-of-the-art methods on a common partially simulated exposome dataset (based on real case data from the HELIX project) with multiple correlated exposure variables (P>100) arising from general and personal environments at different time points, biological molecular data (multi-omics: DNA methylation, gene expression, proteins, metabolomics) and multiple clinical phenotypes in 1301 mother-child pairs. Most of the methods presented included feature selection or feature reduction to deal with the high dimensionality of the exposome dataset. Several approaches explicitly searched for combined effects of exposures and/or their interactions using linear index models or response surface methods, including Bayesian methods. Other methods dealt with the multi-omics dataset in mediation analyses using multiple-step approaches. Here we discuss the statistical models and provide the data and codes used, so that analysts have examples of implementation and can learn how to use these methods. Overall, the exposome data challenge presented a unique opportunity for researchers from different disciplines to create and share methods, setting a new standard for open science in the exposome and environmental health field.

</details>

<details>

<summary>2022-02-03 21:14:07 - Valid predictions of group-level random effects</summary>

- *Nicholas Syring, Fernando Miguez, Jarad Niemi*

- `2202.01848v1` - [abs](http://arxiv.org/abs/2202.01848v1) - [pdf](http://arxiv.org/pdf/2202.01848v1)

> Gaussian linear models with random group-level effects are the standard for modeling randomized experiments carried out over groups, such as locations, farms, hospitals, or schools. Group-level effects can be summarized by prediction intervals for group-level means or responses, but the quality of such summaries depends on whether the intervals are valid in the sense they attain their nominal coverage probability. Many methods for constructing prediction intervals are available -- such as Student's t, bootstrap, and Bayesian methods -- but none of these are guaranteed to be valid, and indeed are not valid over a range of simulation examples. We propose a new method for constructing valid predictions of group-level effects based on an inferential model (IM). The proposed prediction intervals have guaranteed finite-sample validity and outperform existing methods in simulation examples. In an on-farm agricultural study the new IM-based prediction intervals suggest a higher level of uncertainty in farm-specific effects compared to the standard Student's t-based intervals, which are known to undercover.

</details>

<details>

<summary>2022-02-03 21:38:10 - A framework for quantifying the value of vibration-based structural health monitoring</summary>

- *Antonios Kamariotis, Eleni Chatzi, Daniel Straub*

- `2202.01859v1` - [abs](http://arxiv.org/abs/2202.01859v1) - [pdf](http://arxiv.org/pdf/2202.01859v1)

> The difficulty in quantifying the benefit of Structural Health Monitoring (SHM) for decision support is one of the bottlenecks to an extensive adoption of SHM on real-world structures. In this paper, we present a framework for such a quantification of the value of vibration-based SHM, which can be flexibly applied to different use cases. These cover SHM-based decisions at different time scales, from near-real time diagnostics to the prognosis of slowly evolving deterioration processes over the lifetime of a structure. The framework includes an advanced model of the SHM system. It employs a Bayesian filter for the tasks of sequential joint deterioration state-parameter estimation and structural reliability updating, using continuously identified modal and intermittent visual inspection data. It also includes a realistic model of the inspection and maintenance decisions throughout the structural life-cycle. On this basis, the Value of SHM is quantified by the difference in expected total life-cycle costs with and without the SHM. We investigate the framework through application on a numerical model of a two-span bridge system, subjected to gradual and shock deterioration, as well as to changing environmental conditions, over its lifetime. The results show that this framework can be used as an a-priori decision support tool to inform the decision on whether or not to install a vibration-based SHM system on a structure, for a wide range of SHM use cases.

</details>

<details>

<summary>2022-02-03 22:20:35 - EggCounts: a Bayesian hierarchical toolkit to model faecal egg count reductions</summary>

- *Craig Wang, Reinhard Furrer*

- `1804.11224v2` - [abs](http://arxiv.org/abs/1804.11224v2) - [pdf](http://arxiv.org/pdf/1804.11224v2)

> This is a vignette for the R package eggCounts version 2.0. The package implements a suite of Bayesian hierarchical models dealing with faecal egg count reductions. The models are designed for a variety of practical situations, including individual treatment efficacy, zero inflation, small sample size (less than 10) and potential outliers. The functions are intuitive to use and their output are easy to interpret, such that users are protected from being exposed to complex Bayesian hierarchical modelling tasks. In addition, the package includes plotting functions to display data and results in a visually appealing manner. The models are implemented in Stan modelling language, which provides efficient sampling technique to obtain posterior samples. This vignette briefly introduces different models, and provides a short walk-through analysis with example data.

</details>

<details>

<summary>2022-02-04 04:12:42 - Population Calibration using Likelihood-Free Bayesian Inference</summary>

- *Christopher Drovandi, Brodie Lawson, Adrianne L Jenner, Alexander P Browning*

- `2202.01962v1` - [abs](http://arxiv.org/abs/2202.01962v1) - [pdf](http://arxiv.org/pdf/2202.01962v1)

> In this paper we develop a likelihood-free approach for population calibration, which involves finding distributions of model parameters when fed through the model produces a set of outputs that matches available population data. Unlike most other approaches to population calibration, our method produces uncertainty quantification on the estimated distribution. Furthermore, the method can be applied to any population calibration problem, regardless of whether the model of interest is deterministic or stochastic, or whether the population data is observed with or without measurement error. We demonstrate the method on several examples, including one with real data. We also discuss the computational limitations of the approach. Immediate applications for the methodology developed here exist in many areas of medical research including cancer, COVID-19, drug development and cardiology.

</details>

<details>

<summary>2022-02-04 09:10:39 - First-order integer-valued autoregressive processes with Generalized Katz innovations</summary>

- *Federico Bassetti, Giulia Carallo, Roberto Casarin*

- `2202.02029v1` - [abs](http://arxiv.org/abs/2202.02029v1) - [pdf](http://arxiv.org/pdf/2202.02029v1)

> A new integer-valued autoregressive process (INAR) with Generalised Lagrangian Katz (GLK) innovations is defined. We show that our GLK-INAR process is stationary, discrete semi-self-decomposable, infinite divisible, and provides a flexible modelling framework for count data allowing for under- and over-dispersion, asymmetry, and excess of kurtosis. A Bayesian inference framework and an efficient posterior approximation procedure based on Markov Chain Monte Carlo are provided. The proposed model family is applied to a Google Trend dataset which proxies the public concern about climate change around the world. The empirical results provide new evidence of heterogeneity across countries and keywords in the persistence, uncertainty, and long-run public awareness level.

</details>

<details>

<summary>2022-02-04 09:10:54 - A straightforward meta-analysis approach for oncology phase I dose-finding studies</summary>

- *Christian Rver, Moreno Ursino, Tim Friede, Sarah Zohar*

- `2110.06132v2` - [abs](http://arxiv.org/abs/2110.06132v2) - [pdf](http://arxiv.org/pdf/2110.06132v2)

> Phase I early-phase clinical studies aim at investigating the safety and the underlying dose-toxicity relationship of a drug or combination. While little may still be known about the compound's properties, it is crucial to consider quantitative information available from any studies that may have been conducted previously on the same drug. A meta-analytic approach has the advantages of being able to properly account for between-study heterogeneity, and it may be readily extended to prediction or shrinkage applications. Here we propose a simple and robust two-stage approach for the estimation of maximum tolerated dose(s) (MTDs) utilizing penalized logistic regression and Bayesian random-effects meta-analysis methodology. Implementation is facilitated using standard R packages. The properties of the proposed methods are investigated in Monte-Carlo simulations. The investigations are motivated and illustrated by two examples from oncology.

</details>

<details>

<summary>2022-02-04 12:49:59 - The FEDHC Bayesian network learning algorithm</summary>

- *Michail Tsagris*

- `2012.00113v5` - [abs](http://arxiv.org/abs/2012.00113v5) - [pdf](http://arxiv.org/pdf/2012.00113v5)

> The paper proposes a new hybrid Bayesian network learning algorithm, termed Forward Early Dropping Hill Climbing (FEDHC), devised to work with either continuous or categorical variables. Specifically for the case of continuous data, a robust to outliers version of FEDHC, that can be adopted by other BN learning algorithms, is proposed. Further, the paper manifests that the only implementation of MMHC in the statistical software \textit{R}, is prohibitively expensive and a new implementation is offered. The FEDHC is tested via Monte Carlo simulations that distinctly show it is computationally efficient, and produces Bayesian networks of similar to, or of higher accuracy than MMHC and PCHC. Finally, an application of FEDHC, PCHC and MMHC algorithms to real data, from the field of economics, is demonstrated using the statistical software \textit{R}.

</details>

<details>

<summary>2022-02-04 16:12:44 - Affine-Invariant Integrated Rank-Weighted Depth: Definition, Properties and Finite Sample Analysis</summary>

- *Guillaume Staerman, Pavlo Mozharovskyi, Stphan Clmenon*

- `2106.11068v3` - [abs](http://arxiv.org/abs/2106.11068v3) - [pdf](http://arxiv.org/pdf/2106.11068v3)

> Because it determines a center-outward ordering of observations in $\mathbb{R}^d$ with $d\geq 2$, the concept of statistical depth permits to define quantiles and ranks for multivariate data and use them for various statistical tasks (e.g. inference, hypothesis testing). Whereas many depth functions have been proposed \textit{ad-hoc} in the literature since the seminal contribution of \cite{Tukey75}, not all of them possess the properties desirable to emulate the notion of quantile function for univariate probability distributions. In this paper, we propose an extension of the \textit{integrated rank-weighted} statistical depth (IRW depth in abbreviated form) originally introduced in \cite{IRW}, modified in order to satisfy the property of \textit{affine-invariance}, fulfilling thus all the four key axioms listed in the nomenclature elaborated by \cite{ZuoS00a}. The variant we propose, referred to as the Affine-Invariant IRW depth (AI-IRW in short), involves the covariance/precision matrices of the (supposedly square integrable) $d$-dimensional random vector $X$ under study, in order to take into account the directions along which $X$ is most variable to assign a depth value to any point $x\in \mathbb{R}^d$. The accuracy of the sampling version of the AI-IRW depth is investigated from a nonasymptotic perspective. Namely, a concentration result for the statistical counterpart of the AI-IRW depth is proved. Beyond the theoretical analysis carried out, applications to anomaly detection are considered and numerical results are displayed, providing strong empirical evidence of the relevance of the depth function we propose here.

</details>

<details>

<summary>2022-02-04 17:57:18 - A multi-model ensemble Kalman filter for data assimilation and forecasting</summary>

- *Eviatar Bach, Michael Ghil*

- `2202.02272v1` - [abs](http://arxiv.org/abs/2202.02272v1) - [pdf](http://arxiv.org/pdf/2202.02272v1)

> Data assimilation (DA) aims to optimally combine model forecasts and noisy observations. Multi-model DA generalizes the variational or Bayesian formulation of the Kalman filter, and we prove here that it is also the minimum variance linear unbiased estimator. However, previous implementations of this approach have not estimated the model error, and have therewith not been able to correctly weight the separate models and the observations. Here, we show how multiple models can be combined for both forecasting and DA by using an ensemble Kalman filter with adaptive model error estimation. This methodology is applied to the Lorenz96 model, and it results in significant error reductions compared to the best model and to an unweighted multi-model ensemble.

</details>

<details>

<summary>2022-02-06 03:55:57 - On Function-on-Scalar Quantile Regression</summary>

- *Yusha Liu, Meng Li, Jeffrey S. Morris*

- `2002.03355v2` - [abs](http://arxiv.org/abs/2002.03355v2) - [pdf](http://arxiv.org/pdf/2002.03355v2)

> Functional quantile regression (FQR) is a useful alternative to mean regression for functional data as it provides a comprehensive understanding of how scalar predictors influence the conditional distribution of functional responses. In this article, we study the FQR model for densely sampled, high-dimensional functional data without relying on parametric error or independent stochastic process assumptions, with the focus being on statistical inference under this challenging regime along with scalable implementation. This is achieved by a simple but powerful distributed strategy, in which we first perform separate quantile regression to compute $M$-estimators at each sampling location, and then carry out estimation and inference for the entire coefficient functions by properly exploiting the uncertainty quantification and dependence structures of $M$-estimators. We derive a uniform Bahadur representation and a strong Gaussian approximation result for the $M$-estimators on the discrete sampling grid, leading to dimension reduction and serving as the basis for inference. An interpolation-based estimator with minimax optimality and a Bayesian alternative to improve upon finite sample performance are discussed. Large sample properties for point and simultaneous interval estimators are established. The obtained minimax optimal rate under the FQR model shows an interesting phase transition phenomenon that has been previously observed in functional mean regression. The proposed methods are illustrated via simulations and an application to a mass spectrometry proteomics dataset.

</details>

<details>

<summary>2022-02-06 19:20:42 - Bayesian Context Trees: Modelling and exact inference for discrete time series</summary>

- *Ioannis Kontoyiannis, Lambros Mertzanis, Athina Panotopoulou, Ioannis Papageorgiou, Maria Skoularidou*

- `2007.14900v3` - [abs](http://arxiv.org/abs/2007.14900v3) - [pdf](http://arxiv.org/pdf/2007.14900v3)

> We develop a new Bayesian modelling framework for the class of higher-order, variable-memory Markov chains, and introduce an associated collection of methodological tools for exact inference with discrete time series. We show that a version of the context tree weighting algorithm can compute the prior predictive likelihood exactly (averaged over both models and parameters), and two related algorithms are introduced, which identify the a posteriori most likely models and compute their exact posterior probabilities. All three algorithms are deterministic and have linear-time complexity. A family of variable-dimension Markov chain Monte Carlo samplers is also provided, facilitating further exploration of the posterior. The performance of the proposed methods in model selection, Markov order estimation and prediction is illustrated through simulation experiments and real-world applications with data from finance, genetics, neuroscience, and animal communication. The associated algorithms are implemented in the R package BCT.

</details>

<details>

<summary>2022-02-07 01:41:30 - Kanerva++: extending The Kanerva Machine with differentiable, locally block allocated latent memory</summary>

- *Jason Ramapuram, Yan Wu, Alexandros Kalousis*

- `2103.03905v3` - [abs](http://arxiv.org/abs/2103.03905v3) - [pdf](http://arxiv.org/pdf/2103.03905v3)

> Episodic and semantic memory are critical components of the human memory model. The theory of complementary learning systems (McClelland et al., 1995) suggests that the compressed representation produced by a serial event (episodic memory) is later restructured to build a more generalized form of reusable knowledge (semantic memory). In this work we develop a new principled Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. We take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. In contrast to the Kanerva Machine, we simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. We demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values on binarized MNIST (<=41.58 nats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting competitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.

</details>

<details>

<summary>2022-02-07 03:26:40 - Combining Evidence</summary>

- *Michael Evans, Yang Jian Guo*

- `2202.02922v1` - [abs](http://arxiv.org/abs/2202.02922v1) - [pdf](http://arxiv.org/pdf/2202.02922v1)

> The problem of combining the evidence concerning an unknown, contained in each of $k$ Bayesian inference bases, is discussed. This can be considered as a generalization of the problem of pooling $k$ priors to determine a consensus prior. The linear opinion pool of Stone (1961) is seen to have the most appropriate properties for this role. In particular, linear pooling preserves a consensus with respect to the evidence and other rules do not. While linear pooling does not preserve prior independence, it is shown that it still behaves appropriately with respect to the expression of evidence in such a context. For the general problem of combining evidence, Jeffrey conditionalization plays a key role.

</details>

<details>

<summary>2022-02-07 07:25:28 - Bayesian doubly robust causal inference via loss functions</summary>

- *Yu Luo, David A. Stephens, Daniel J. Graham, Emma J. McCoy*

- `2103.04086v2` - [abs](http://arxiv.org/abs/2103.04086v2) - [pdf](http://arxiv.org/pdf/2103.04086v2)

> Doubly robust causal inference has a well-established basis in frequentist semi-parametric theory, with estimation of causal parameters typically conducted via outcome regression and propensity score adjustment. A Bayesian counterpart, however, is not obvious as doubly robust estimation involves a semi-parametric formulation in the absence of a fully specified likelihood function. In this paper, we propose a Bayesian approach for doubly robust causal inference via two general Bayesian updating approaches based on loss functions. First, we specify a loss function for a doubly robust propensity score augmented outcome regression model and apply the traditional Bayesian updating mechanism which uses a prior belief distribution to calculate the posterior. Secondly, we draw inference for the posterior from a Bayesian predictive distribution via a Dirichlet process model, extending the Bayesian bootstrap. We show that these updating procedures yield valid posterior distributions of parameters which exhibit double robustness. Simulation studies show that the proposed methods can recover the true causal effect efficiently and achieve frequentist coverage even when the sample size is small or if the propensity score distribution is highly skewed. Finally, we apply our methods to evaluate the causal impact of speed cameras on traffic collisions in England.

</details>

<details>

<summary>2022-02-07 12:45:11 - A Comparison Between Quantile Regression and Linear Regression on Empirical Quantiles for Phenological Analysis in Migratory Response to Climate Change</summary>

- *Mns Karlsson, Ola Hssjer*

- `2202.02206v2` - [abs](http://arxiv.org/abs/2202.02206v2) - [pdf](http://arxiv.org/pdf/2202.02206v2)

> It is well established that migratory birds in general have advanced their arrival times in spring, and in this paper we investigate potential ways of enhancing the level of detail in future phenological analyses. We perform single as well as multiple species analyses, using linear models on empirical quantiles, non-parametric quantile regression and likelihood-based parametric quantile regression with asymmetric Laplace distributed error terms. We conclude that non-parametric quantile regression appears most suited for single as well as multiple species analyses.

</details>

<details>

<summary>2022-02-07 13:51:19 - Bayesian Linear Bandits for Large-Scale Recommender Systems</summary>

- *Saeed Ghoorchian, Setareh Maghsudi*

- `2202.03167v1` - [abs](http://arxiv.org/abs/2202.03167v1) - [pdf](http://arxiv.org/pdf/2202.03167v1)

> Potentially, taking advantage of available side information boosts the performance of recommender systems; nevertheless, with the rise of big data, the side information has often several dimensions. Hence, it is imperative to develop decision-making algorithms that can cope with such a high-dimensional context in real-time. That is especially challenging when the decision-maker has a variety of items to recommend. In this paper, we build upon the linear contextual multi-armed bandit framework to address this problem. We develop a decision-making policy for a linear bandit problem with high-dimensional context vectors and several arms. Our policy employs Thompson sampling and feeds it with reduced context vectors, where the dimensionality reduction follows by random projection. Our proposed recommender system follows this policy to learn online the item preferences of users while keeping its runtime as low as possible. We prove a regret bound that scales as a factor of the reduced dimension instead of the original one. For numerical evaluation, we use our algorithm to build a recommender system and apply it to real-world datasets. The theoretical and numerical results demonstrate the effectiveness of our proposed algorithm compared to the state-of-the-art in terms of computational complexity and regret performance.

</details>

<details>

<summary>2022-02-07 15:32:07 - Theoretical characterization of uncertainty in high-dimensional linear classification</summary>

- *Lucas Clart, Bruno Loureiro, Florent Krzakala, Lenka Zdeborov*

- `2202.03295v1` - [abs](http://arxiv.org/abs/2202.03295v1) - [pdf](http://arxiv.org/pdf/2202.03295v1)

> Being able to reliably assess not only the accuracy but also the uncertainty of models' predictions is an important endeavour in modern machine learning. Even if the model generating the data and labels is known, computing the intrinsic uncertainty after learning the model from a limited number of samples amounts to sampling the corresponding posterior probability measure. Such sampling is computationally challenging in high-dimensional problems and theoretical results on heuristic uncertainty estimators in high-dimensions are thus scarce. In this manuscript, we characterise uncertainty for learning from limited number of samples of high-dimensional Gaussian input data and labels generated by the probit model. We prove that the Bayesian uncertainty (i.e. the posterior marginals) can be asymptotically obtained by the approximate message passing algorithm, bypassing the canonical but costly Monte Carlo sampling of the posterior. We then provide a closed-form formula for the joint statistics between the logistic classifier, the uncertainty of the statistically optimal Bayesian classifier and the ground-truth probit uncertainty. The formula allows us to investigate calibration of the logistic classifier learning from limited amount of samples. We discuss how over-confidence can be mitigated by appropriately regularising, and show that cross-validating with respect to the loss leads to better calibration than with the 0/1 error.

</details>

<details>

<summary>2022-02-07 17:53:14 - Uniformly Ergodic Data-Augmented MCMC for Fitting the General Stochastic Epidemic Model to Incidence Data</summary>

- *Raphael Morsomme, Jason Xu*

- `2201.09722v2` - [abs](http://arxiv.org/abs/2201.09722v2) - [pdf](http://arxiv.org/pdf/2201.09722v2)

> Stochastic epidemic models provide an interpretable probabilistic description of the spread of a disease through a population. Yet, fitting these models when the epidemic process is only partially observed is a notoriously difficult task due to the intractability of the likelihood for many classical models. To remedy this issue, this article introduces a novel data-augmented MCMC algorithm for fast and exact Bayesian inference for the stochastic SIR model given discretely observed infection incidence counts. In a Metropolis-Hastings step, new event times of the latent data are jointly proposed from a surrogate process that closely resembles the SIR, and from which we can efficiently generate epidemics compatible with the observed data.   The proposed DA-MCMC algorithm is fast and, since the latent data are generated from a faithful approximation of the target model, a large portion thereof can be updated per iteration without prohibitively lowering the acceptance rate. We find that the method explores the high-dimensional latent space efficiently and scales to outbreaks with hundreds of thousands of individuals, and we show that the Markov chain underlying the algorithm is uniformly ergodic. We validate its performance via thorough simulation experiments and a case study on the 2013-2015 Ebola outbreak in Western Africa.

</details>

<details>

<summary>2022-02-07 20:25:49 - Optimising experimental design in neutron reflectometry</summary>

- *James H. Durant, Lucas Wilkins, Joshaniel F. K. Cooper*

- `2108.05605v2` - [abs](http://arxiv.org/abs/2108.05605v2) - [pdf](http://arxiv.org/pdf/2108.05605v2)

> Using the Fisher information (FI), the design of neutron reflectometry experiments can be optimised, leading to greater confidence in parameters of interest and better use of experimental time [Durant, Wilkins, Butler, & Cooper (2021). J. Appl. Cryst. 54, 1100-1110]. In this work, the FI is utilised in optimising the design of a wide range of reflectometry experiments. Two lipid bilayer systems are investigated to determine the optimal choice of measurement angles and liquid contrasts, in addition to the ratio of the total counting time that should be spent measuring each condition. The reduction in parameter uncertainties with the addition of underlayers to these systems is then quantified, using the FI, and validated through the use of experiment simulation and Bayesian sampling methods. For a "one-shot" measurement of a degrading lipid monolayer, it is shown that the common practice of measuring null-reflecting water is indeed optimal, but that the optimal measurement angle is dependent on the deuteration state of the monolayer. Finally, the framework is used to demonstrate the feasibility of measuring magnetic signals as small as $0.01\mu_{B}/\text{atom}$ in layers only $20\r{A}$ thick, given the appropriate experimental design, and that time to reach a given level of confidence in the small magnetic moment is quantifiable.

</details>

<details>

<summary>2022-02-08 11:17:52 - Diversified Sampling for Batched Bayesian Optimization with Determinantal Point Processes</summary>

- *Elvis Nava, Mojmr Mutn, Andreas Krause*

- `2110.11665v2` - [abs](http://arxiv.org/abs/2110.11665v2) - [pdf](http://arxiv.org/pdf/2110.11665v2)

> In Bayesian Optimization (BO) we study black-box function optimization with noisy point evaluations and Bayesian priors. Convergence of BO can be greatly sped up by batching, where multiple evaluations of the black-box function are performed in a single round. The main difficulty in this setting is to propose at the same time diverse and informative batches of evaluation points. In this work, we introduce DPP-Batch Bayesian Optimization (DPP-BBO), a universal framework for inducing batch diversity in sampling based BO by leveraging the repulsive properties of Determinantal Point Processes (DPP) to naturally diversify the batch sampling procedure. We illustrate this framework by formulating DPP-Thompson Sampling (DPP-TS) as a variant of the popular Thompson Sampling (TS) algorithm and introducing a Markov Chain Monte Carlo procedure to sample from it. We then prove novel Bayesian simple regret bounds for both classical batched TS as well as our counterpart DPP-TS, with the latter bound being tighter. Our real-world, as well as synthetic, experiments demonstrate improved performance of DPP-BBO over classical batching methods with Gaussian process and Cox process models.

</details>

<details>

<summary>2022-02-08 13:05:26 - SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization</summary>

- *Marius Lindauer, Katharina Eggensperger, Matthias Feurer, Andr Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhopf, Ren Sass, Frank Hutter*

- `2109.09831v2` - [abs](http://arxiv.org/abs/2109.09831v2) - [pdf](http://arxiv.org/pdf/2109.09831v2)

> Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license at https://github.com/automl/SMAC3.

</details>

<details>

<summary>2022-02-08 14:03:50 - Multilevel Delayed Acceptance MCMC</summary>

- *Mikkel B. Lykkegaard, Tim J. Dodwell, Colin Fox, Grigorios Mingas, Robert Scheichl*

- `2202.03876v1` - [abs](http://arxiv.org/abs/2202.03876v1) - [pdf](http://arxiv.org/pdf/2202.03876v1)

> We develop a novel Markov chain Monte Carlo (MCMC) method that exploits a hierarchy of models of increasing complexity to efficiently generate samples from an unnormalized target distribution. Broadly, the method rewrites the Multilevel MCMC approach of Dodwell et al. (2015) in terms of the Delayed Acceptance (DA) MCMC of Christen & Fox (2005). In particular, DA is extended to use a hierarchy of models of arbitrary depth, and allow subchains of arbitrary length. We show that the algorithm satisfies detailed balance, hence is ergodic for the target distribution. Furthermore, multilevel variance reduction is derived that exploits the multiple levels and subchains, and an adaptive multilevel correction to coarse-level biases is developed. Three numerical examples of Bayesian inverse problems are presented that demonstrate the advantages of these novel methods. The software and examples are available in PyMC3.

</details>

<details>

<summary>2022-02-08 14:31:48 - Asymptotics of representation learning in finite Bayesian neural networks</summary>

- *Jacob A. Zavatone-Veth, Abdulkadir Canatar, Benjamin S. Ruben, Cengiz Pehlevan*

- `2106.00651v5` - [abs](http://arxiv.org/abs/2106.00651v5) - [pdf](http://arxiv.org/pdf/2106.00651v5)

> Recent works have suggested that finite Bayesian neural networks may sometimes outperform their infinite cousins because finite networks can flexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of finite networks differ from the fixed representations of infinite networks remains incomplete. Perturbative finite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and Gaussian likelihood have a largely universal form. We illustrate this explicitly for three tractable network architectures: deep linear fully-connected and convolutional networks, and networks with a single nonlinear hidden layer. Our results begin to elucidate how task-relevant learning signals shape the hidden layer representations of wide Bayesian neural networks.

</details>

<details>

<summary>2022-02-08 15:53:16 - Variance matrix priors for Dirichlet process mixture models with Gaussian kernels</summary>

- *Wei Jing, Michail Papathomas, Silvia Liverani*

- `2202.03946v1` - [abs](http://arxiv.org/abs/2202.03946v1) - [pdf](http://arxiv.org/pdf/2202.03946v1)

> The Dirichlet Process Mixture Model (DPMM) is a Bayesian non-parametric approach widely used for density estimation and clustering. In this manuscript, we study the choice of prior for the variance or precision matrix when Gaussian kernels are adopted. Typically, in the relevant literature, the assessment of mixture models is done by considering observations in a space of only a handful of dimensions. Instead, we are concerned with more realistic problems of higher dimensionality, in a space of up to 20 dimensions. We observe that the choice of prior is increasingly important as the dimensionality of the problem increases. After identifying certain undesirable properties of standard priors in problems of higher dimensionality, we review and implement possible alternative priors. The most promising priors are identified, as well as other factors that affect the convergence of MCMC samplers. Our results show that the choice of prior is critical for deriving reliable posterior inferences. This manuscript offers a thorough overview and comparative investigation into possible priors, with detailed guidelines for their implementation. Although our work focuses on the use of the DPMM in clustering, it is also applicable to density estimation.

</details>

<details>

<summary>2022-02-08 16:01:38 - On Sibson's $$-Mutual Information</summary>

- *Amedeo Roberto Esposito, Adrien Vandenbroucque, Michael Gastpar*

- `2202.03951v1` - [abs](http://arxiv.org/abs/2202.03951v1) - [pdf](http://arxiv.org/pdf/2202.03951v1)

> We explore a family of information measures that stems from R\'enyi's $\alpha$-Divergences with $\alpha<0$. In particular, we extend the definition of Sibson's $\alpha$-Mutual Information to negative values of $\alpha$ and show several properties of these objects. Moreover, we highlight how this family of information measures is related to functional inequalities that can be employed in a variety of fields, including lower-bounds on the Risk in Bayesian Estimation Procedures.

</details>

<details>

<summary>2022-02-08 16:11:04 - Bayesian Inverse Uncertainty Quantification of the Physical Model Parameters for the Spallation Neutron Source First Target Station</summary>

- *Majdi I. Radaideh, Lianshan Lin, Hao Jiang, Sarah Cousineau*

- `2202.03959v1` - [abs](http://arxiv.org/abs/2202.03959v1) - [pdf](http://arxiv.org/pdf/2202.03959v1)

> The reliability of the mercury spallation target is mission-critical for the neutron science program of the spallation neutron source at the Oak Ridge National Laboratory. We present an inverse uncertainty quantification (UQ) study using the Bayesian framework for the mercury equation of state model parameters, with the assistance of polynomial chaos expansion surrogate models. By leveraging high-fidelity structural mechanics simulations and real measured strain data, the inverse UQ results reveal a maximum-a-posteriori estimate, mean, and standard deviation of $6.5\times 10^4$ ($6.49\times 10^4 \pm 2.39\times 10^3$) Pa for the tensile cutoff threshold, $12112.1$ ($12111.8 \pm 14.9$) kg/m$^3$ for the mercury density, and $1850.4$ ($1849.7 \pm 5.3$) m/s for the mercury speed of sound. These values do not necessarily represent the nominal mercury physical properties, but the ones that fit the strain data and the solid mechanics model we have used, and can be explained by three reasons: The limitations of the computer model or what is known as the model-form uncertainty, the biases and errors in the experimental data, and the mercury cavitation damage that also contributes to the change in mercury behavior. Consequently, the equation of state model parameters try to compensate for these effects to improve fitness to the data. The mercury target simulations using the updated parametric values result in an excellent agreement with 88% average accuracy compared to experimental data, 6% average increase compared to reference parameters, with some sensors experiencing an increase of more than 25%. With a more accurate simulated strain response, the component fatigue analysis can utilize the comprehensive strain history data to evaluate the target vessel's lifetime closer to its real limit, saving tremendous target costs.

</details>

<details>

<summary>2022-02-08 16:34:32 - Adaptive Bayesian Variable Clustering via Structural Learning of Breast Cancer Data</summary>

- *Riddhi Pratim Ghosh, Arnab Kumar Maity, Mohsen Pourahmadi, Bani K. Mallick*

- `2202.03979v1` - [abs](http://arxiv.org/abs/2202.03979v1) - [pdf](http://arxiv.org/pdf/2202.03979v1)

> Clustering of proteins is of interest in cancer cell biology. This article proposes a hierarchical Bayesian model for protein (variable) clustering hinging on correlation structure. Starting from a multivariate normal likelihood, we enforce the clustering through prior modeling using angle based unconstrained reparameterization of correlations and assume a truncated Poisson distribution (to penalize the large number of clusters) as prior on the number of clusters. The posterior distributions of the parameters are not in explicit form and we use a reversible jump Markov chain Monte Carlo (RJMCMC) based technique is used to simulate the parameters from the posteriors. The end products of the proposed method are estimated cluster configuration of the proteins (variables) along with the number of clusters. The Bayesian method is flexible enough to cluster the proteins as well as the estimate the number of clusters. The performance of the proposed method has been substantiated with extensive simulation studies and one protein expression data with a hereditary disposition in breast cancer where the proteins are coming from different pathways.

</details>

<details>

<summary>2022-02-08 17:25:58 - BAM: Bayes with Adaptive Memory</summary>

- *Josue Nassar, Jennifer Brennan, Ben Evans, Kendall Lowrey*

- `2202.02405v2` - [abs](http://arxiv.org/abs/2202.02405v2) - [pdf](http://arxiv.org/pdf/2202.02405v2)

> Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to discard/downweight past data; however, this simple mechanism of "forgetting" fails to account for the fact that many real-world environments involve revisiting similar states. We propose a new framework, Bayes with Adaptive Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world.

</details>

<details>

<summary>2022-02-08 18:07:07 - Unsupervised Bayesian classification for models with scalar and functional covariates</summary>

- *Nancy L. Garcia, Mariana Rodrigues-Motta, Helio S. Migon, Eva Petkova, Thaddeus Tarpey, R. Todd Ogden, Julio O. Giodano, Martin Matias Perez*

- `2202.04037v1` - [abs](http://arxiv.org/abs/2202.04037v1) - [pdf](http://arxiv.org/pdf/2202.04037v1)

> We consider unsupervised classification by means of a latent multinomial variable which categorizes a scalar response into one of L components of a mixture model. This process can be thought as a hierarchical model with first level modelling a scalar response according to a mixture of parametric distributions, the second level models the mixture probabilities by means of a generalised linear model with functional and scalar covariates. The traditional approach of treating functional covariates as vectors not only suffers from the curse of dimensionality since functional covariates can be measured at very small intervals leading to a highly parametrised model but also does not take into account the nature of the data. We use basis expansion to reduce the dimensionality and a Bayesian approach to estimate the parameters while providing predictions of the latent classification vector. By means of a simulation study we investigate the behaviour of our approach considering normal mixture model and zero inflated mixture of Poisson distributions. We also compare the performance of the classical Gibbs sampling approach with Variational Bayes Inference.

</details>

<details>

<summary>2022-02-08 22:01:17 - A Practical Approach to Proper Inference with Linked Data</summary>

- *Andee Kaplan, Brenda Betancourt, Rebecca C. Steorts*

- `1810.01538v2` - [abs](http://arxiv.org/abs/1810.01538v2) - [pdf](http://arxiv.org/pdf/1810.01538v2)

> Entity resolution (ER), comprising record linkage and de-duplication, is the process of merging noisy databases in the absence of unique identifiers to remove duplicate entities. One major challenge of analysis with linked data is identifying a representative record among determined matches to pass to an inferential or predictive task, referred to as the \emph{downstream task}. Additionally, incorporating uncertainty from ER in the downstream task is critical to ensure proper inference. To bridge the gap between ER and the downstream task in an analysis pipeline, we propose five methods to choose a representative (or canonical) record from linked data, referred to as canonicalization. Our methods are scalable in the number of records, appropriate in general data scenarios, and provide natural error propagation via a Bayesian canonicalization stage. The proposed methodology is evaluated on three simulated data sets and one application -- determining the relationship between demographic information and party affiliation in voter registration data from the North Carolina State Board of Elections. We first perform Bayesian ER and evaluate our proposed methods for canonicalization before considering the downstream tasks of linear and logistic regression. Bayesian canonicalization methods are empirically shown to improve downstream inference in both settings through prediction and coverage.

</details>

<details>

<summary>2022-02-08 23:31:01 - Multilayer Adjusted Cluster Point Process Model: Application to Microbial Biofilm Image Data Analysis</summary>

- *Suman Majumder, Brent A. Coull, Jessica L. Mark Welch, Patrick J. La Riviere, Floyd E. Dewhirst, Jacqueline R. Starr, Kyu Ha Lee*

- `2202.04198v1` - [abs](http://arxiv.org/abs/2202.04198v1) - [pdf](http://arxiv.org/pdf/2202.04198v1)

> A common problem in spatial statistics tackles spatial distributions of clusters of objects. Such clusters of similar or dissimilar objects are encountered in many fields, including field ecology, astronomy, and biomedical imaging. Still challenging is to quantify spatial clustering when one or more entities clusters around a different entity in multiple layers. Such multi-entity and multi-layered structures are observed, for example, in human dental plaque biofilm images, which exhibit multi-species structures in corncob-like arrangements. We propose a novel, fully Bayesian, multivariate spatial point process model to quantify corncob-like arrangements with "parent-offspring" statistical approaches. The proposed multilayer adjusted cluster point process (MACPP) model departs from commonly used approaches in that it exploits the locations of the central "parent" object in clusters and accounts for multilayered multivariate parent-offspring clustering. In simulated datasets, the MACPP outperforms the classical Neyman-Scott process model, a univariate model for modeling spatially clustered processes, by producing decisively more accurate and precise parameter estimates. We analyzed data from a human dental plaque biofilm image in which \emph{Streptococcus} and \emph{Porphyromonas} simultaneously cluster around \emph{Corynebacterium} and \emph{Pasteurellaceae} clusters around \emph{Streptococcus}. The proposed MACPP model successfully captured the parent-offspring structure for all the taxa involved.

</details>

<details>

<summary>2022-02-09 08:51:37 - Semiparametric Bayesian Estimation of Dynamic Discrete Choice Models</summary>

- *Andriy Norets, Kenichi Shimizu*

- `2202.04339v1` - [abs](http://arxiv.org/abs/2202.04339v1) - [pdf](http://arxiv.org/pdf/2202.04339v1)

> We propose a tractable semiparametric estimation method for dynamic discrete choice models. The distribution of additive utility shocks is modeled by location-scale mixtures of extreme value distributions with varying numbers of mixture components. Our approach exploits the analytical tractability of extreme value distributions and the flexibility of the location-scale mixtures. We implement the Bayesian approach to inference using Hamiltonian Monte Carlo and an approximately optimal reversible jump algorithm from Norets (2021). For binary dynamic choice model, our approach delivers estimation results that are consistent with the previous literature. We also apply the proposed method to multinomial choice models, for which previous literature does not provide tractable estimation methods in general settings without distributional assumptions on the utility shocks. We develop theoretical results on approximations by location-scale mixtures in an appropriate distance and posterior concentration of the set identified utility parameters and the distribution of shocks in the model.

</details>

<details>

<summary>2022-02-09 12:49:39 - Approximate Bayesian inference and forecasting in huge-dimensional multi-country VARs</summary>

- *Martin Feldkircher, Florian Huber, Gary Koop, Michael Pfarrhofer*

- `2103.04944v2` - [abs](http://arxiv.org/abs/2103.04944v2) - [pdf](http://arxiv.org/pdf/2103.04944v2)

> Panel Vector Autoregressions (PVARs) are a popular tool for analyzing multi-country datasets. However, the number of estimated parameters can be enormous, leading to computational and statistical issues. In this paper, we develop fast Bayesian methods for estimating PVARs using integrated rotated Gaussian approximations. We exploit the fact that domestic information is often more important than international information and group the coefficients accordingly. Fast approximations are used to estimate the latter while the former are estimated with precision using Markov chain Monte Carlo techniques. We illustrate, using a huge model of the world economy, that it produces competitive forecasts quickly.

</details>

<details>

<summary>2022-02-09 13:32:24 - Core-periphery structure in networks: a statistical exposition</summary>

- *Eric Yanchenko, Srijan Sengupta*

- `2202.04455v1` - [abs](http://arxiv.org/abs/2202.04455v1) - [pdf](http://arxiv.org/pdf/2202.04455v1)

> Many real-world networks are theorized to have core-periphery structure consisting of a densely-connected core and a loosely-connected periphery. While this network feature has been extensively studied in various scientific disciplines, it has not received sufficient attention in the statistics community. In this expository article, our goal is to raise awareness about this topic and encourage statisticians to address the many interesting open problems in this area. We present the current research landscape by reviewing the most popular metrics and models that have been used for quantitative studies on core-periphery structure. Next, we formulate and explore various inferential problems in this context, such as estimation, hypothesis testing, and Bayesian inference, and discuss related computational techniques. We also outline the multidisciplinary scientific impact of core-periphery structure in a number of real-world networks. Throughout the article, we provide our own interpretation of the literature from a statistical perspective, with the goal of prioritizing open problems where contribution from the statistics community will be effective and important.

</details>

<details>

<summary>2022-02-09 22:05:37 - Personalized pathology test for Cardio-vascular disease: Approximate Bayesian computation with discriminative summary statistics learning</summary>

- *Ritabrata Dutta, Karim Zouaoui-Boudjeltia, Christos Kotsalos, Alexandre Rousseau, Daniel Ribeiro de Sousa, Jean-Marc Desmet, Alain Van Meerhaeghe, Antonietta Mira, Bastien Chopard*

- `2010.06465v2` - [abs](http://arxiv.org/abs/2010.06465v2) - [pdf](http://arxiv.org/pdf/2010.06465v2)

> Cardio/cerebrovascular diseases (CVD) have become one of the major health issue in our societies. But recent studies show that the present pathology tests to detect CVD are ineffectual as they do not consider different stages of platelet activation or the molecular dynamics involved in platelet interactions and are incapable to consider inter-individual variability. Here we propose a stochastic platelet deposition model and an inferential scheme to estimate the biologically meaningful model parameters using approximate Bayesian computation with a summary statistic that maximally discriminates between different types of patients. Inferred parameters from data collected on healthy volunteers and different patient types help us to identify specific biological parameters and hence biological reasoning behind the dysfunction for each type of patients. This work opens up an unprecedented opportunity of personalized pathology test for CVD detection and medical treatment.

</details>

<details>

<summary>2022-02-10 02:24:58 - A Clustering Approach to Integrative Analysis of Multiomic Cancer Data</summary>

- *Dongyan Yan, Subharup Guha*

- `2202.04799v1` - [abs](http://arxiv.org/abs/2202.04799v1) - [pdf](http://arxiv.org/pdf/2202.04799v1)

> Rapid technological advances have allowed for molecular profiling across multiple omics domains from a single sample for clinical decision making in many diseases, especially cancer. As tumor development and progression are dynamic biological processes involving composite genomic aberrations, key challenges are to effectively assimilate information from these domains to identify genomic signatures and biological entities that are druggable, develop accurate risk prediction profiles for future patients, and identify novel patient subgroups for tailored therapy and monitoring.   We propose integrative probabilistic frameworks for high-dimensional multiple-domain cancer data that coherently incorporate dependence within and between domains to accurately detect tumor subtypes, thus providing a catalogue of genomic aberrations associated with cancer taxonomy. We propose an innovative, flexible and scalable Bayesian nonparametric framework for simultaneous clustering of both tumor samples and genomic probes. We describe an efficient variable selection procedure to identify relevant genomic aberrations that can potentially reveal underlying drivers of a disease. Although the work is motivated by several investigations related to lung cancer, the proposed methods are broadly applicable in a variety of contexts involving high-dimensional data. The success of the methodology is demonstrated using artificial data and lung cancer omics profiles publicly available from The Cancer Genome Atlas.

</details>

<details>

<summary>2022-02-10 07:14:21 - Latent Space Network Modelling with Hyperbolic and Spherical Geometries</summary>

- *Marios Papamichalis, Kathryn Turnbull, Simon Lunagomez, Edoardo Airoldi*

- `2109.03343v2` - [abs](http://arxiv.org/abs/2109.03343v2) - [pdf](http://arxiv.org/pdf/2109.03343v2)

> A rich class of network models associate each node with a low-dimensional latent coordinate that controls the propensity for connections to form. Models of this type are well established in the network analysis literature, where it is typical to assume that the underlying geometry is Euclidean. Recent work has explored the consequences of this choice and has motivated the study of models which rely on non-Euclidean latent geometries, with a primary focus on spherical and hyperbolic geometry. In this paper, we examine to what extent latent features can be inferred from the observable links in the network, considering network models which rely on spherical and hyperbolic geometries. For each geometry, we describe a latent space network model, detail constraints on the latent coordinates which remove the well-known identifiability issues, and present Bayesian estimation schemes. Thus, we develop computational procedures to perform inference for network models in which the properties of the underlying geometry play a vital role. Finally, we assess the validity of these models on real data.

</details>

<details>

<summary>2022-02-10 10:30:27 - Flexible Bayesian Nowcasting with application to COVID-19 fatalities in Sweden</summary>

- *Fanny Bergstrm, Felix Gnther, Michael Hhle, Tom Britton*

- `2202.04569v2` - [abs](http://arxiv.org/abs/2202.04569v2) - [pdf](http://arxiv.org/pdf/2202.04569v2)

> The real-time analysis of infectious disease surveillance data, e.g. time-series of reported cases or fatalities, can help to provide situational awareness about the current state of a pandemic. This task is challenged by reporting delays that give rise to occurred-but-not-yet-reported events. If these events are not taken into consideration, this can lead to an under-estimation of the counts-to-be-reported and, hence, introduces misconceptions by the interpreter, the media or the general public -- as has been seen for example for reported fatalities during the COVID-19 pandemic. Nowcasting methods provide close to real-time estimates of the complete number of events using the incomplete time-series of currently reported events by using information about the reporting delays from the past. In this report, we consider nowcasting the number of COVID-19 related fatalities in Sweden. We propose a flexible Bayesian approach that considers temporal changes in the reporting delay distribution and, as an extension to existing nowcasting methods, incorporates a regression component for the (lagged) time-series of the number of ICU admissions. This results in a model considering both the past behavior of the time-series of fatalities as well as additional data streams that are in a time-lagged association with the number of fatalities.

</details>

<details>

<summary>2022-02-10 15:41:47 - Stochastic Processes Under Linear Differential Constraints : Application to Gaussian Process Regression for the 3 Dimensional Free Space Wave Equation</summary>

- *Iain Henderson, Pascal Noble, Olivier Roustant*

- `2111.12035v3` - [abs](http://arxiv.org/abs/2111.12035v3) - [pdf](http://arxiv.org/pdf/2111.12035v3)

> Let $P$ be a linear differential operator over $\mathcal{D} \subset \mathbb{R}^d$ and $U = (U_x)_{x \in \mathcal{D}}$ a second order stochastic process. In the first part of this article, we prove a new necessary and sufficient condition for all the trajectories of $U$ to verify the partial differential equation (PDE) $T(U) = 0$. This condition is formulated in terms of the covariance kernel of $U$. When compared to previous similar results, the novelty lies in that the equality $T(U) = 0$ is understood in the \textit{sense of distributions}, which is a relevant framework for PDEs. This theorem provides precious insights during the second part of this article, devoted to performing "physically informed" machine learning for the homogeneous 3 dimensional free space wave equation. We perform Gaussian process regression (GPR) on pointwise observations of a solution of this PDE. To do so, we propagate Gaussian processes (GP) priors over its initial conditions through the wave equation. We obtain explicit formulas for the covariance kernel of the propagated GP, which can then be used for GPR. We then explore the particular cases of radial symmetry and point source. For the former, we derive convolution-free GPR formulas; for the latter, we show a direct link between GPR and the classical triangulation method for point source localization used in GPS systems. Additionally, this Bayesian framework provides a new answer for the ill-posed inverse problem of reconstructing initial conditions for the wave equation with a limited number of sensors, and simultaneously enables the inference of physical parameters from these data. Finally, we illustrate this physically informed GPR on a number of practical examples.

</details>

<details>

<summary>2022-02-10 17:50:21 - von Mises-Fisher distributions and their statistical divergence</summary>

- *Toru Kitagawa, Jeff Rowley*

- `2202.05192v1` - [abs](http://arxiv.org/abs/2202.05192v1) - [pdf](http://arxiv.org/pdf/2202.05192v1)

> The von Mises-Fisher family is a parametric family of distributions on the surface of the unit ball, summarised by a concentration parameter and a mean direction. As a quasi-Bayesian prior, the von Mises-Fisher distribution is a convenient and parsimonious choice when parameter spaces are isomorphic to the hypersphere (e.g., maximum score estimation in semi-parametric discrete choice, estimation of single-index treatment assignment rules via empirical welfare maximisation, under-identifying linear simultaneous equation models). Despite a long history of application, measures of statistical divergence have not been analytically characterised for von Mises-Fisher distributions. This paper provides analytical expressions for the $f$-divergence of a von Mises-Fisher distribution from another, distinct, von Mises-Fisher distribution in $\mathbb{R}^p$ and the uniform distribution over the hypersphere. This paper also collect several other results pertaining to the von Mises-Fisher family of distributions, and characterises the limiting behaviour of the measures of divergence that we consider.

</details>

<details>

<summary>2022-02-10 17:50:26 - Bayes Optimal Algorithm is Suboptimal in Frequentist Best Arm Identification</summary>

- *Junpei Komiyama*

- `2202.05193v1` - [abs](http://arxiv.org/abs/2202.05193v1) - [pdf](http://arxiv.org/pdf/2202.05193v1)

> We consider the fixed-budget best arm identification problem with Normal rewards. In this problem, the forecaster is given $K$ arms (treatments) and $T$ time steps. The forecaster attempts to find the best arm in terms of the largest mean via an adaptive experiment conducted with an algorithm. The performance of the algorithm is measured by the simple regret, or the quality of the estimated best arm. It is known that the frequentist simple regret can be exponentially small to $T$ for any fixed parameters, whereas the Bayesian simple regret is $\Theta(T^{-1})$ over a continuous prior distribution. This paper shows that Bayes optimal algorithm, which minimizes the Bayesian simple regret, does not have an exponential simple regret for some parameters. This finding contrasts with the many results indicating the asymptotic equivalence of Bayesian and frequentist algorithms in fixed sampling regimes. While the Bayes optimal algorithm is described in terms of a recursive equation that is virtually impossible to compute exactly, we pave the way to an analysis by introducing a key quantity that we call the expected Bellman improvement.

</details>

<details>

<summary>2022-02-10 17:57:44 - Spatial epidemiology and adaptive targeted sampling to manage the Chagas disease vector Triatoma dimidiata</summary>

- *B. K. M. Case, Jean-Gabriel Young, Daniel Penados, Carlota Monroy, Laurent Hbert-Dufresne, Lori Stevens*

- `2111.05964v2` - [abs](http://arxiv.org/abs/2111.05964v2) - [pdf](http://arxiv.org/pdf/2111.05964v2)

> Widespread application of insecticide remains the primary form of control for Chagas disease in Central America, despite only temporarily reducing domestic levels of the endemic vector Triatoma dimidiata and having little long-term impact. Recently, an approach emphasizing community feedback and housing improvements has been shown to yield lasting results. However, the additional resources and personnel required by such an intervention likely hinders its widespread adoption. One solution to this problem would be to target only a subset of houses in a community while still eliminating enough infestations to interrupt disease transfer. Here we develop a sequential sampling framework that adapts to information specific to a community as more houses are visited, thereby allowing us to efficiently find homes with domiciliary vectors while minimizing sampling bias. The method fits Bayesian geostatistical models to make spatially informed predictions, while gradually transitioning from prioritizing houses based on prediction uncertainty to targeting houses with a high risk of infestation. A key feature of the method is the use of a single exploration parameter, $\alpha$, to control the rate of transition between these two design targets. In a simulation study using empirical data from five villages in southeastern Guatemala, we test our method using a range of values for $\alpha$, and find it can consistently select fewer homes than random sampling, while still bringing the village infestation rate below a given threshold. We further find that when additional socioeconomic information is available, much larger savings are possible, but that meeting the target infestation rate is less consistent, particularly among the less exploratory strategies. Our results suggest new options for implementing long-term T. dimidiata control.

</details>

<details>

<summary>2022-02-10 21:11:31 - CIMTx: An R package for causal inference with multiple treatments using observational data</summary>

- *Liangyuan Hu, Jiayi Ji*

- `2110.10276v2` - [abs](http://arxiv.org/abs/2110.10276v2) - [pdf](http://arxiv.org/pdf/2110.10276v2)

> CIMTx provides efficient and unified functions to implement modern methods for causal inferences with multiple treatments using observational data with a focus on binary outcomes. The methods include regression adjustment, inverse probability of treatment weighting, Bayesian additive regression trees, regression adjustment with multivariate spline of the generalized propensity score, vector matching and targeted maximum likelihood estimation. In addition, CIMTx illustrates ways in which users can simulate data adhering to the complex data structures in the multiple treatment setting. Furthermore, the CIMTx package offers a unique set of features to address the key causal assumptions: positivity and ignorability. For the positivity assumption, CIMTx demonstrates techniques to identify the common support region for retaining inferential units. To handle the ignorability assumption, CIMTx provides a flexible Monte Carlo sensitivity analysis approach to evaluate how causal conclusions would be altered in response to different magnitude of departure from ignorable treatment assignment.

</details>

<details>

<summary>2022-02-10 21:23:23 - Predicting Phenotypes from Brain Connection Structure</summary>

- *Subharup Guha, Rex Jung, David Dunson*

- `1910.02506v3` - [abs](http://arxiv.org/abs/1910.02506v3) - [pdf](http://arxiv.org/pdf/1910.02506v3)

> This article focuses on the problem of predicting a response variable based on a network-valued predictor. Our motivation is the development of interpretable and accurate predictive models for cognitive traits and neuro-psychiatric disorders based on an individual's brain connection network (connectome). Current methods reduce the complex, high dimensional brain network into low-dimensional pre-specified features prior to applying standard predictive algorithms. These methods are sensitive to feature choice and inevitably discard important information. Instead, we propose a nonparametric Bayes class of models that utilize the entire adjacency matrix defining brain region connections to adaptively detect predictive algorithms, while maintaining interpretability. The Bayesian Connectomics (BaCon) model class utilizes Poisson-Dirichlet processes to find a lower-dimensional, bidirectional (covariate, subject) pattern in the adjacency matrix. The small n, large p problem is transformed into a "small n, small q" problem, facilitating an effective stochastic search of the predictors. A spike-and-slab prior for the cluster predictors strikes a balance between regression model parsimony and flexibility, resulting in improved inferences and test case predictions. We describe basic properties of the BaCon model and develop efficient algorithms for posterior computation. The resulting methods are found to outperform existing approaches and applied to a creative reasoning data set.

</details>

<details>

<summary>2022-02-10 23:43:17 - Bayesian learning of COVID-19 Vaccine safety while incorporating Adverse Events ontology</summary>

- *Bangyao Zhao, Yuan Zhong, Jian Kang, Lili Zhao*

- `2202.05370v1` - [abs](http://arxiv.org/abs/2202.05370v1) - [pdf](http://arxiv.org/pdf/2202.05370v1)

> While vaccines are crucial to end the COVID-19 pandemic, public confidence in vaccine safety has always been vulnerable. Many statistical methods have been applied to VAERS (Vaccine Adverse Event Reporting System) database to study the safety of COVID-19 vaccines. However, all these methods ignored the adverse event (AE) ontology. AEs are naturally related; for example, events of retching, dysphagia, and reflux are all related to an abnormal digestive system. Explicitly bringing AE relationships into the model can aid in the detection of true AE signals amid the noise while reducing false positives. We propose a Bayesian graphical model to estimate all AEs while incorporating the AE ontology simultaneously. We proposed strategies to construct conjugate forms leading to an efficient Gibbs sampler. Built upon the posterior distributions, we proposed a negative control approach to mitigate reporting bias and an enrichment approach to detect AE groups of concern. The proposed methods were evaluated using simulation studies and were further illustrated on studying the safety of COVID-19 vaccines. The proposed methods were implemented in R package \textit{BGrass} and source code are available at https://github.com/BangyaoZhao/BGrass.

</details>

<details>

<summary>2022-02-11 03:00:52 - High-dimensional properties for empirical priors in linear regression with unknown error variance</summary>

- *Xiao Fang, Malay Ghosh*

- `2202.05419v1` - [abs](http://arxiv.org/abs/2202.05419v1) - [pdf](http://arxiv.org/pdf/2202.05419v1)

> We study full Bayesian procedures for high-dimensional linear regression. We adopt data-dependent empirical priors introduced in [1]. In their paper, these priors have nice posterior contraction properties and are easy to compute. Our paper extend their theoretical results to the case of unknown error variance . Under proper sparsity assumption, we achieve model selection consistency, posterior contraction rates as well as Bernstein von-Mises theorem by analyzing multivariate t-distribution.

</details>

<details>

<summary>2022-02-11 03:14:31 - Posterior Consistency for Bayesian Relevance Vector Machines</summary>

- *Xiao Fang, Malay Ghosh*

- `2202.05422v1` - [abs](http://arxiv.org/abs/2202.05422v1) - [pdf](http://arxiv.org/pdf/2202.05422v1)

> Statistical modeling and inference problems with sample sizes substantially smaller than the number of available covariates are challenging. Chakraborty et al. (2012) did a full hierarchical Bayesian analysis of nonlinear regression in such situations using relevance vector machines based on reproducing kernel Hilbert space (RKHS). But they did not provide any theoretical properties associated with their procedure. The present paper revisits their problem, introduces a new class of global-local priors different from theirs, and provides results on posterior consistency as well as posterior contraction rates

</details>

<details>

<summary>2022-02-11 08:09:39 - Inference for Projection-Based Wasserstein Distances on Finite Spaces</summary>

- *Ryo Okano, Masaaki Imaizumi*

- `2202.05495v1` - [abs](http://arxiv.org/abs/2202.05495v1) - [pdf](http://arxiv.org/pdf/2202.05495v1)

> The Wasserstein distance is a distance between two probability distributions and has recently gained increasing popularity in statistics and machine learning, owing to its attractive properties. One important approach to extending this distance is using low-dimensional projections of distributions to avoid a high computational cost and the curse of dimensionality in empirical estimation, such as the sliced Wasserstein or max-sliced Wasserstein distances. Despite their practical success in machine learning tasks, the availability of statistical inferences for projection-based Wasserstein distances is limited owing to the lack of distributional limit results. In this paper, we consider distances defined by integrating or maximizing Wasserstein distances between low-dimensional projections of two probability distributions. Then we derive limit distributions regarding these distances when the two distributions are supported on finite points. We also propose a bootstrap procedure to estimate quantiles of limit distributions from data. This facilitates asymptotically exact interval estimation and hypothesis testing for these distances. Our theoretical results are based on the arguments of Sommerfeld and Munk (2018) for deriving distributional limits regarding the original Wasserstein distance on finite spaces and the theory of sensitivity analysis in nonlinear programming. Finally, we conduct numerical experiments to illustrate the theoretical results and demonstrate the applicability of our inferential methods to real data analysis.

</details>

<details>

<summary>2022-02-11 08:15:42 - Temporal evolution of the Covid19 pandemic reproduction number: Estimations from proximal optimization to Monte Carlo sampling</summary>

- *Patrice Abry, Gersende Fort, Barbara Pascal, Nelly Pustelnik*

- `2202.05497v1` - [abs](http://arxiv.org/abs/2202.05497v1) - [pdf](http://arxiv.org/pdf/2202.05497v1)

> Monitoring the evolution of the Covid19 pandemic constitutes a critical step in sanitary policy design. Yet, the assessment of the pandemic intensity within the pandemic period remains a challenging task because of the limited quality of data made available by public health authorities (missing data, outliers and pseudoseasonalities, notably), that calls for cumbersome and ad-hoc preprocessing (denoising) prior to estimation. Recently, the estimation of the reproduction number, a measure of the pandemic intensity, was formulated as an inverse problem, combining data-model fidelity and space-time regularity constraints, solved by nonsmooth convex proximal minimizations. Though promising, that formulation lacks robustness against the limited quality of the Covid19 data and confidence assessment. The present work aims to address both limitations: First, it discusses solutions to produce a robust assessment of the pandemic intensity by accounting for the low quality of the data directly within the inverse problem formulation. Second, exploiting a Bayesian interpretation of the inverse problem formulation, it devises a Monte Carlo sampling strategy, tailored to a nonsmooth log-concave a posteriori distribution, to produce relevant credibility intervalbased estimates for the Covid19 reproduction number. Clinical relevance Applied to daily counts of new infections made publicly available by the Health Authorities for around 200 countries, the proposed procedures permit robust assessments of the time evolution of the Covid19 pandemic intensity, updated automatically and on a daily basis.

</details>

<details>

<summary>2022-02-11 08:42:05 - Joint modelling of association networks and longitudinal biomarkers: an application to child obesity</summary>

- *Andrea Cremaschi, Maria De Iorio, Narasimhan Kothandaraman, Fabian Yap, Mya Tway Tint, Johan Eriksson*

- `2111.06212v2` - [abs](http://arxiv.org/abs/2111.06212v2) - [pdf](http://arxiv.org/pdf/2111.06212v2)

> The prevalence of chronic non-communicable diseases such as obesity has noticeably increased in the last decade. The study of these diseases in early life is of paramount importance in determining their course in adult life and in supporting clinical interventions. Recently, attention has been drawn on approaches that study the alteration of metabolic pathways in obese children. In this work, we propose a novel joint modelling approach for the analysis of growth biomarkers and metabolite concentrations, to unveil metabolic pathways related to child obesity. Within a Bayesian framework, we flexibly model the temporal evolution of growth trajectories and metabolic associations through the specification of a joint non-parametric random effect distribution which also allows for clustering of the subjects, thus identifying risk sub-groups. Growth profiles as well as patterns of metabolic associations determine the clustering structure. Inclusion of risk factors is straightforward through the specification of a regression term. We demonstrate the proposed approach on data from the Growing Up in Singapore Towards healthy Outcomes (GUSTO) cohort study, based in Singapore. Posterior inference is obtained via a tailored MCMC algorithm, accommodating a nonparametric prior with mixed support. Our analysis has identified potential key pathways in obese children that allows for exploration of possible molecular mechanisms associated with child obesity.

</details>

<details>

<summary>2022-02-11 11:53:28 - On change of measure inequalities for $f$-divergences</summary>

- *Antoine Picard-Weibel, Benjamin Guedj*

- `2202.05568v1` - [abs](http://arxiv.org/abs/2202.05568v1) - [pdf](http://arxiv.org/pdf/2202.05568v1)

> We propose new change of measure inequalities based on $f$-divergences (of which the Kullback-Leibler divergence is a particular case). Our strategy relies on combining the Legendre transform of $f$-divergences and the Young-Fenchel inequality. By exploiting these new change of measure inequalities, we derive new PAC-Bayesian generalisation bounds with a complexity involving $f$-divergences, and holding in mostly unchartered settings (such as heavy-tailed losses). We instantiate our results for the most popular $f$-divergences.

</details>

<details>

<summary>2022-02-11 14:01:53 - Long-Time Convergence and Propagation of Chaos for Nonlinear MCMC</summary>

- *James Vuckovic*

- `2202.05621v1` - [abs](http://arxiv.org/abs/2202.05621v1) - [pdf](http://arxiv.org/pdf/2202.05621v1)

> In this paper, we study the long-time convergence and uniform strong propagation of chaos for a class of nonlinear Markov chains for Markov chain Monte Carlo (MCMC). Our technique is quite simple, making use of recent contraction estimates for linear Markov kernels and basic techniques from Markov theory and analysis. Moreover, the same proof strategy applies to both the long-time convergence and propagation of chaos. We also show, via some experiments, that these nonlinear MCMC techniques are viable for use in real-world high-dimensional inference such as Bayesian neural networks.

</details>

<details>

<summary>2022-02-11 14:45:52 - Bernstein Flows for Flexible Posteriors in Variational Bayes</summary>

- *Oliver Drr, Stephan Hrling, Daniel Dold, Ivonne Kovylov, Beate Sick*

- `2202.05650v1` - [abs](http://arxiv.org/abs/2202.05650v1) - [pdf](http://arxiv.org/pdf/2202.05650v1)

> Variational inference (VI) is a technique to approximate difficult to compute posteriors by optimization. In contrast to MCMC, VI scales to many observations. In the case of complex posteriors, however, state-of-the-art VI approaches often yield unsatisfactory posterior approximations. This paper presents Bernstein flow variational inference (BF-VI), a robust and easy-to-use method, flexible enough to approximate complex multivariate posteriors. BF-VI combines ideas from normalizing flows and Bernstein polynomial-based transformation models. In benchmark experiments, we compare BF-VI solutions with exact posteriors, MCMC solutions, and state-of-the-art VI methods including normalizing flow based VI. We show for low-dimensional models that BF-VI accurately approximates the true posterior; in higher-dimensional models, BF-VI outperforms other VI methods. Further, we develop with BF-VI a Bayesian model for the semi-structured Melanoma challenge data, combining a CNN model part for image data with an interpretable model part for tabular data, and demonstrate for the first time how the use of VI in semi-structured models.

</details>

<details>

<summary>2022-02-11 16:46:23 - Parameter uncertainty estimation for exponential semi-variogram models: Two generalized bootstrap methods with check- and quantile-based filtering</summary>

- *Julia Dyck, Odile Sauzet*

- `2202.05752v1` - [abs](http://arxiv.org/abs/2202.05752v1) - [pdf](http://arxiv.org/pdf/2202.05752v1)

> The estimation of parameter standard errors for semi-variogram models is challenging, given the two-step process required to fit a parametric model to spatially correlated data. Motivated by an application in the social-epidemiology, we focus on exponential semi-variogram models fitted to data between 500 to 2000 observations and little control over the sampling design. Previously proposed methods for the estimation of standard errors cannot be applied in this context. Approximate closed form solutions are too costly using generalized least squares in terms of memory capacities. The generalized bootstrap proposed by Olea and Pardo-Ig\'uzquiza is nonetheless applicable with weighted instead of generalized least squares. However, the standard error estimates are hugely biased and imprecise. Therefore, we propose a filtering method added to the generalized bootstrap. The new development is presented and evaluated with a simulation study which shows that the generalized bootstrap with check-based filtering leads to massively improved results compared to the quantile-based filter method and previously developed approaches. We provide a case study using birthweight data.

</details>

<details>

<summary>2022-02-11 18:35:58 - Consistent Bayesian community recovery in multilayer networks</summary>

- *Kalle Alaluusua, Lasse Leskel*

- `2202.05823v1` - [abs](http://arxiv.org/abs/2202.05823v1) - [pdf](http://arxiv.org/pdf/2202.05823v1)

> Revealing underlying relations between nodes in a network is one of the most important tasks in network analysis. Using tools and techniques from a variety of disciplines, many community recovery methods have been developed for different scenarios. Despite the recent interest on community recovery in multilayer networks, theoretical results on the accuracy of the estimates are few and far between. Given a multilayer, e.g. temporal, network and a multilayer stochastic block model, we derive bounds for sufficient separation between intra- and inter-block connectivity parameters to achieve posterior exact and almost exact community recovery. These conditions are comparable to a well known threshold for community detection by a single-layer stochastic block model. A simulation study shows that the derived bounds translate to classification accuracy that improves as the number of observed layers increases.

</details>

<details>

<summary>2022-02-11 20:41:56 - Bayesian Elastic Net based on Empirical Likelihood</summary>

- *Chul Moon, Adel Bedoui*

- `2006.10258v2` - [abs](http://arxiv.org/abs/2006.10258v2) - [pdf](http://arxiv.org/pdf/2006.10258v2)

> We propose a Bayesian elastic net that uses empirical likelihood and develop an efficient tuning of Hamiltonian Monte Carlo for posterior sampling. The proposed model relaxes the assumptions on the identity of the error distribution, performs well when the variables are highly correlated, and enables more straightforward inference by providing posterior distributions of the regression coefficients. The Hamiltonian Monte Carlo method implemented in Bayesian empirical likelihood overcomes the challenges that the posterior distribution lacks a closed analytic form and its domain is nonconvex. We develop the leapfrog parameter tuning algorithm for Bayesian empirical likelihood. We also show that the posterior distributions of the regression coefficients are asymptotically normal. Simulation studies and real data analysis demonstrate the advantages of the proposed method in prediction accuracy.

</details>

<details>

<summary>2022-02-11 21:16:47 - Bayesian non-parametric ordinal regression under a monotonicity constraint</summary>

- *Olli Saarela, Christian Rohrbeck, Elja Arjas*

- `2007.01390v5` - [abs](http://arxiv.org/abs/2007.01390v5) - [pdf](http://arxiv.org/pdf/2007.01390v5)

> Compared to the nominal scale, the ordinal scale for a categorical outcome variable has the property of making a monotonicity assumption for the covariate effects meaningful. This assumption is encoded in the commonly used proportional odds model, but there it is combined with other parametric assumptions such as linearity and additivity. Herein, the considered models are non-parametric and the only condition imposed is that the effects of the covariates on the outcome categories are stochastically monotone according to the ordinal scale. We are not aware of the existence of other comparable multivariable models that would be suitable for inference purposes. We generalize our previously proposed Bayesian monotonic multivariable regression model to ordinal outcomes, and propose an estimation procedure based on reversible jump Markov chain Monte Carlo. The model is based on a marked point process construction, which allows it to approximate arbitrary monotonic regression function shapes, and has a built-in covariate selection property. We study the performance of the proposed approach through extensive simulation studies, and demonstrate its practical application in two real data examples.

</details>

<details>

<summary>2022-02-12 11:43:51 - Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks</summary>

- *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, Sung Ju Hwang*

- `1905.12917v3` - [abs](http://arxiv.org/abs/1905.12917v3) - [pdf](http://arxiv.org/pdf/1905.12917v3)

> While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that the number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on multiple realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework.

</details>

<details>

<summary>2022-02-12 11:59:58 - Which Type of Statistical Uncertainty Helps Evidence-Based Policymaking? An Insight from a Survey Experiment in Ireland</summary>

- *Akisato Suzuki*

- `2108.05100v3` - [abs](http://arxiv.org/abs/2108.05100v3) - [pdf](http://arxiv.org/pdf/2108.05100v3)

> Which type of statistical uncertainty -- statistical (in)significance with a p-value, or a Bayesian probability -- enables people to see the continuous nature of uncertainty more clearly in a policymaking context? An original survey experiment asked participants from Ireland to read a hypothetical policymaking scenario and decide to or not to introduce a new bus line as a policy to reduce traffic jams, given a research report estimating its effectiveness. The four types of information were given as the treatments: statistical significance with a p-value of 2%, statistical insignificance with a p-value of 25%, a 95% probability that the estimate is correct, and a 68% probability that the estimate is correct. The effect size and cost of the policy were fixed across all treatment groups. In the case of lower uncertainty, both significance and Bayesian frameworks resulted in a large proportion of participants adopting the policy (.82 and .91 respectively), while in the case of higher uncertainty, the significance framework led a much smaller proportion of participants to adopt the policy than the Bayesian framework (.39 against .83). The findings suggest participants were able to see the continuous nature of uncertainty more clearly in the Bayesian framework than in the significance framework.

</details>

<details>

<summary>2022-02-12 18:32:14 - Adaptive truncation of infinite sums: applications to Statistics</summary>

- *Luiz Max Carvalho, Guido A. Moreira*

- `2202.06121v1` - [abs](http://arxiv.org/abs/2202.06121v1) - [pdf](http://arxiv.org/pdf/2202.06121v1)

> It is often the case in Statistics that one needs to compute sums of infinite series, especially in marginalising over discrete latent variables. This has become more relevant with the popularization of gradient-based techniques (e.g. Hamiltonian Monte Carlo) in the Bayesian inference context, for which discrete latent variables are hard or impossible to deal with. For many commonly used infinite series, custom algorithms have been developed which exploit specific features of each problem. General techniques, suitable for a large class of problems with limited input from the user are less established. We employ basic results from the theory of infinite series to investigate general, problem-agnostic algorithms to truncate infinite sums within an arbitrary tolerance $\varepsilon > 0$ and provide robust computational implementations with provable guarantees. We compare three tentative solutions to estimating the infinite sum of interest: (i) a "naive" approach that sums terms until the terms are below the threshold $\varepsilon$; (ii) a `bounding pair' strategy based on trapping the true value between two partial sums; and (iii) a `batch' strategy that computes the partial sums in regular intervals and stops when their difference is less than $\varepsilon$. We show under which conditions each strategy guarantees the truncated sum is within the required tolerance and compare the error achieved by each approach, as well as the number of function evaluations necessary for each one. A detailed discussion of numerical issues in practical implementations is also provided. The paper provides some theoretical discussion of a variety of statistical applications, including raw and factorial moments and count models with observation error. Finally, detailed illustrations in the form noisy MCMC for Bayesian inference and maximum marginal likelihood estimation are presented.

</details>

<details>

<summary>2022-02-13 07:15:40 - Cauchy Markov Random Field Priors for Bayesian Inversion</summary>

- *Neil K. Chada, Lassi Roininen, Jarkko Suuronen*

- `2105.12488v2` - [abs](http://arxiv.org/abs/2105.12488v2) - [pdf](http://arxiv.org/pdf/2105.12488v2)

> The use of Cauchy Markov random field priors in statistical inverse problems can potentially lead to posterior distributions which are non-Gaussian, high-dimensional, multimodal and heavy-tailed. In order to use such priors successfully, sophisticated optimization and Markov chain Monte Carlo (MCMC) methods are usually required. In this paper, our focus is largely on reviewing recently developed Cauchy difference priors, while introducing interesting new variants, whilst providing a comparison. We firstly propose a one-dimensional second order Cauchy difference prior, and construct new first and second order two-dimensional isotropic Cauchy difference priors. Another new Cauchy prior is based on the stochastic partial differential equation approach, derived from Mat\'{e}rn type Gaussian presentation. The comparison also includes Cauchy sheets. Our numerical computations are based on both maximum a posteriori and conditional mean estimation.We exploit state-of-the-art MCMC methodologies such as Metropolis-within-Gibbs, Repelling-Attracting Metropolis, and No-U-Turn sampler variant of Hamiltonian Monte Carlo. We demonstrate the models and methods constructed for one-dimensional and two-dimensional deconvolution problems. Thorough MCMC statistics are provided for all test cases, including potential scale reduction factors.

</details>

<details>

<summary>2022-02-14 04:11:27 - Approximate Inference via Clustering</summary>

- *Qianqian Song*

- `2111.14219v2` - [abs](http://arxiv.org/abs/2111.14219v2) - [pdf](http://arxiv.org/pdf/2111.14219v2)

> In recent years, large-scale Bayesian learning draws a great deal of attention. However, in big-data era, the amount of data we face is growing much faster than our ability to deal with it. Fortunately, it is observed that large-scale datasets usually own rich internal structure and is somewhat redundant. In this paper, we attempt to simplify the Bayesian posterior via exploiting this structure. Specifically, we restrict our interest to the so-called well-clustered datasets and construct an \emph{approximate posterior} according to the clustering information. Fortunately, the clustering structure can be efficiently obtained via a particular clustering algorithm. When constructing the approximate posterior, the data points in the same cluster are all replaced by the centroid of the cluster. As a result, the posterior can be significantly simplified. Theoretically, we show that under certain conditions the approximate posterior we construct is close (measured by KL divergence) to the exact posterior. Furthermore, thorough experiments are conducted to validate the fact that the constructed posterior is a good approximation to the true posterior and much easier to sample from.

</details>

<details>

<summary>2022-02-14 04:25:28 - Order of Commitments in Bayesian Persuasion with Partial-informed Senders</summary>

- *Shih-Tang Su, Vijay G. Subramanian*

- `2202.06479v1` - [abs](http://arxiv.org/abs/2202.06479v1) - [pdf](http://arxiv.org/pdf/2202.06479v1)

> The commitment power of senders distinguishes Bayesian persuasion problems from other games with (strategic) communication. Persuasion games with multiple senders have largely studied simultaneous commitment and signalling settings. However, many real-world instances with multiple senders have sequential signalling. In such contexts, commitments can also be made sequentially, and then the order of commitment by the senders -- the sender signalling last committing first or last -- could significantly impact the equilibrium payoffs and strategies. For a two-sender persuasion game where the senders are partially aware of the state of the world, we find necessary and sufficient conditions to determine when different commitment orders yield different payoff profiles. In particular, for the two-sender setting, we show that different payoff profiles arise if two properties hold: 1) the two senders are willing to collaborate in persuading the receiver in some state(s); and 2) the sender signalling second can carry out a credible threat when committing first such that the other sender's room to design signals gets constrained.

</details>

<details>

<summary>2022-02-14 04:29:04 - Sequential Bayesian experimental designs via reinforcement learning</summary>

- *Hikaru Asano*

- `2202.07472v1` - [abs](http://arxiv.org/abs/2202.07472v1) - [pdf](http://arxiv.org/pdf/2202.07472v1)

> Bayesian experimental design (BED) has been used as a method for conducting efficient experiments based on Bayesian inference. The existing methods, however, mostly focus on maximizing the expected information gain (EIG); the cost of experiments and sample efficiency are often not taken into account. In order to address this issue and enhance practical applicability of BED, we provide a new approach Sequential Experimental Design via Reinforcement Learning to construct BED in a sequential manner by applying reinforcement learning in this paper. Here, reinforcement learning is a branch of machine learning in which an agent learns a policy to maximize its reward by interacting with the environment. The characteristics of interacting with the environment are similar to the sequential experiment, and reinforcement learning is indeed a method that excels at sequential decision making.   By proposing a new real-world-oriented experimental environment, our approach aims to maximize the EIG while keeping the cost of experiments and sample efficiency in mind simultaneously. We conduct numerical experiments for three different examples. It is confirmed that our method outperforms the existing methods in various indices such as the EIG and sampling efficiency, indicating that our proposed method and experimental environment can make a significant contribution to application of BED to the real world.

</details>

<details>

<summary>2022-02-14 10:28:44 - On the cost of Bayesian posterior mean strategy for log-concave models</summary>

- *Sbastien Gadat, Fabien Panloup, Clment Pellegrini*

- `2010.06420v2` - [abs](http://arxiv.org/abs/2010.06420v2) - [pdf](http://arxiv.org/pdf/2010.06420v2)

> In this paper, we investigate the problem of computing Bayesian estimators using Langevin Monte-Carlo type approximation. The novelty of this paper is to consider together the statistical and numerical counterparts (in a general log-concave setting). More precisely, we address the following question: given $n$ observations in $\mathbb{R}^q$ distributed under an unknown probability $\mathbb{P}_{\theta^\star}$ with $\theta^\star \in \mathbb{R}^d$ , what is the optimal numerical strategy and its cost for the approximation of $\theta^\star$ with the Bayesian posterior mean? To answer this question, we establish some quantitative statistical bounds related to the underlying Poincar\'e constant of the model and establish new results about the numerical approximation of Gibbs measures by Cesaro averages of Euler schemes of (over-damped) Langevin diffusions. These last results include in particular some quantitative controls in the weakly convex case based on new bounds on the solution of the related Poisson equation of the diffusion.

</details>

<details>

<summary>2022-02-14 11:44:11 - Bayesian semi-parametric inference for clustered recurrent events with zero-inflation and a terminal event/4163305</summary>

- *Yize Zhao, Xinyuan Tian, Jiachen Cai, Erich Greene, Denise Esserman, Fan Li, Maria Ciarleglio*

- `2202.06636v1` - [abs](http://arxiv.org/abs/2202.06636v1) - [pdf](http://arxiv.org/pdf/2202.06636v1)

> Recurrent event data are common in clinical studies when participants are followed longitudinally, and are often subject to a terminal event. With the increasing popularity of large pragmatic trials and a heterogeneous source population, participants are often nested in clinics and can be either susceptible or structurally unsusceptible to the recurrent process. These complications require new modeling strategies to accommodate potential zero-event inflation as well as hierarchical data structures in both the terminal and non-terminal event processes. In this paper, we develop a Bayesian semiparametric model to jointly characterize the zero-inflated recurrent event process and the terminal event process. We use a point mass mixture of non-homogeneous Poisson processes to describe the recurrent intensity and introduce shared random effects from different sources to bridge the non-terminal and terminal event processes. To achieve robustness, we consider nonparametric Dirichlet processes to model the residual of the accelerated failure time model for the survival process as well as the cluster-specific frailty distribution, and develop a Markov Chain Monte Carlo algorithm for posterior inference. We demonstrate the superiority of our proposed model compared with competing models via simulations and apply our method to a pragmatic cluster-randomized trial for fall injury prevention among the elderly.

</details>

<details>

<summary>2022-02-14 15:26:53 - Meta-analysis of Censored Adverse Events</summary>

- *Xinyue Qi, Shouhao Zhou, Christine Peterson, Yucai Wang, Xinying Fang, Michael L. Wang, Chan Shen*

- `2101.07934v2` - [abs](http://arxiv.org/abs/2101.07934v2) - [pdf](http://arxiv.org/pdf/2101.07934v2)

> Meta-analysis is a powerful tool for drug safety assessment by synthesizing treatment-related toxicological findings from independent clinical trials. However, published clinical studies may or may not report all adverse events (AEs) if the observed number of AEs were fewer than a pre-specified study-dependent cutoff. Subsequently, with censored information ignored, the estimated incidence rate of AEs could be significantly biased. To address this non-ignorable missing data problem in meta-analysis, we propose a Bayesian multilevel regression model to accommodate the censored rare event data. The performance of the proposed Bayesian model of censored data compared to other existing methods is demonstrated through simulation studies under various censoring scenarios. Finally, the proposed approach is illustrated using data from a recent meta-analysis of 125 clinical trials involving PD-1/PD-L1 inhibitors with respect to their toxicity profiles.

</details>

<details>

<summary>2022-02-14 17:33:26 - Persuading a Wishful Thinker</summary>

- *Victor Augias, Daniel M. A. Barreto*

- `2011.13846v4` - [abs](http://arxiv.org/abs/2011.13846v4) - [pdf](http://arxiv.org/pdf/2011.13846v4)

> We analyze a model of persuasion in which Receiver forms wishful non-Bayesian beliefs. The effectiveness of persuasion depends on Receiver's material stakes: it is more effective when intended to encourage risky behavior that potentially lead to a high payoff and less effective when intended to encourage more cautious behavior. We illustrate this insight with applications showing why informational interventions are often ineffective in inducing greater investment in preventive health treatments, how financial advisors might take advantage of their clients overoptimistic beliefs and why strategic information disclosure to voters with different partisan preferences can lead to belief polarization in an electorate.

</details>

<details>

<summary>2022-02-14 22:28:51 - Sequential Monte Carlo With Model Tempering</summary>

- *Marko Mlikota, Frank Schorfheide*

- `2202.07070v1` - [abs](http://arxiv.org/abs/2202.07070v1) - [pdf](http://arxiv.org/pdf/2202.07070v1)

> Modern macroeconometrics often relies on time series models for which it is time-consuming to evaluate the likelihood function. We demonstrate how Bayesian computations for such models can be drastically accelerated by reweighting and mutating posterior draws from an approximating model that allows for fast likelihood evaluations, into posterior draws from the model of interest, using a sequential Monte Carlo (SMC) algorithm. We apply the technique to the estimation of a vector autoregression with stochastic volatility and a nonlinear dynamic stochastic general equilibrium model. The runtime reductions we obtain range from 27% to 88%.

</details>

<details>

<summary>2022-02-15 00:34:34 - Optimal scaling of random-walk Metropolis algorithms using Bayesian large-sample asymptotics</summary>

- *Sebastian M Schmon, Philippe Gagnon*

- `2104.06384v3` - [abs](http://arxiv.org/abs/2104.06384v3) - [pdf](http://arxiv.org/pdf/2104.06384v3)

> High-dimensional limit theorems have been shown useful to derive tuning rules for finding the optimal scaling in random-walk Metropolis algorithms. The assumptions under which weak convergence results are proved are however restrictive: the target density is typically assumed to be of a product form. Users may thus doubt the validity of such tuning rules in practical applications. In this paper, we shed some light on optimal-scaling problems from a different perspective, namely a large-sample one. This allows to prove weak convergence results under realistic assumptions and to propose novel parameter-dimension-dependent tuning guidelines. The proposed guidelines are consistent with previous ones when the target density is close to having a product form, and the results highlight that the correlation structure has to be accounted for to avoid performance deterioration if that is not the case, while justifying the use of a natural (asymptotically exact) approximation to the correlation matrix that can be employed for the very first algorithm run.

</details>

<details>

<summary>2022-02-15 02:52:05 - Bayesian spatio-temporal models for stream networks</summary>

- *Edgar Santos-Fernandez, Jay M. Ver Hoef, Erin E. Peterson, James McGree, Daniel Isaak, Kerrie Mengersen*

- `2103.03538v2` - [abs](http://arxiv.org/abs/2103.03538v2) - [pdf](http://arxiv.org/pdf/2103.03538v2)

> Spatio-temporal models are widely used in many research areas including ecology. The recent proliferation of the use of in-situ sensors in streams and rivers supports space-time water quality modelling and monitoring in near real-time. A new family of spatio-temporal models is introduced. These models incorporate spatial dependence using stream distance while temporal autocorrelation is captured using vector autoregression approaches. Several variations of these novel models are proposed using a Bayesian framework. The results show that our proposed models perform well using spatio-temporal data collected from real stream networks, particularly in terms of out-of-sample RMSPE. This is illustrated considering a case study of water temperature data in the northwestern United States.

</details>

<details>

<summary>2022-02-15 03:24:01 - SSNbayes: An R package for Bayesian spatio-temporal modelling on stream networks</summary>

- *Edgar Santos-Fernandez, Jay M. Ver Hoef, James M. McGree, Daniel J. Isaak, Kerrie Mengersen, Erin E. Peterson*

- `2202.07166v1` - [abs](http://arxiv.org/abs/2202.07166v1) - [pdf](http://arxiv.org/pdf/2202.07166v1)

> Spatio-temporal models are widely used in many research areas from ecology to epidemiology. However, most covariance functions describe spatial relationships based on Euclidean distance only. In this paper, we introduce the R package SSNbayes for fitting Bayesian spatio-temporal models and making predictions on branching stream networks. SSNbayes provides a linear regression framework with multiple options for incorporating spatial and temporal autocorrelation. Spatial dependence is captured using stream distance and flow connectivity while temporal autocorrelation is modelled using vector autoregression approaches. SSNbayes provides the functionality to make predictions across the whole network, compute exceedance probabilities and other probabilistic estimates such as the proportion of suitable habitat. We illustrate the functionality of the package using a stream temperature dataset collected in Idaho, USA.

</details>

<details>

<summary>2022-02-15 09:44:14 - Private Quantiles Estimation in the Presence of Atoms</summary>

- *Clment Lalanne, Clment Gastaud, Nicolas Grislain, Aurlien Garivier, Rmi Gribonval*

- `2202.08969v1` - [abs](http://arxiv.org/abs/2202.08969v1) - [pdf](http://arxiv.org/pdf/2202.08969v1)

> We address the differentially private estimation of multiple quantiles (MQ) of a dataset, a key building block in modern data analysis. We apply the recent non-smoothed Inverse Sensitivity (IS) mechanism to this specific problem and establish that the resulting method is closely related to the current state-of-the-art, the JointExp algorithm, sharing in particular the same computational complexity and a similar efficiency. However, we demonstrate both theoretically and empirically that (non-smoothed) JointExp suffers from an important lack of performance in the case of peaked distributions, with a potentially catastrophic impact in the presence of atoms. While its smoothed version would allow to leverage the performance guarantees of IS, it remains an open challenge to implement. As a proxy to fix the problem we propose a simple and numerically efficient method called Heuristically Smoothed JointExp (HSJointExp), which is endowed with performance guarantees for a broad class of distributions and achieves results that are orders of magnitude better on problematic datasets.

</details>

<details>

<summary>2022-02-15 11:39:52 - Unbiased estimator for the variance of the leave-one-out cross-validation estimator for a Bayesian normal model with fixed variance</summary>

- *Tuomas Sivula, Mns Magnusson, Aki Vehtari*

- `2008.10859v2` - [abs](http://arxiv.org/abs/2008.10859v2) - [pdf](http://arxiv.org/pdf/2008.10859v2)

> When evaluating and comparing models using leave-one-out cross-validation (LOO-CV), the uncertainty of the estimate is typically assessed using the variance of the sampling distribution. Considering the uncertainty is important, as the variability of the estimate can be high in some cases. An important result by Bengio and Grandvalet (2004) states that no general unbiased variance estimator can be constructed, that would apply for any utility or loss measure and any model. We show that it is possible to construct an unbiased estimator considering a specific predictive performance measure and model. We demonstrate an unbiased sampling distribution variance estimator for the Bayesian normal model with fixed model variance using the expected log pointwise predictive density (elpd) utility score. This example demonstrates that it is possible to obtain improved, problem-specific, unbiased estimators for assessing the uncertainty in LOO-CV estimation.

</details>

<details>

<summary>2022-02-15 16:13:40 - Information-Theoretic Analysis of Minimax Excess Risk</summary>

- *Hassan Hafez-Kolahi, Behrad Moniri, Shohreh Kasaei*

- `2202.07537v1` - [abs](http://arxiv.org/abs/2202.07537v1) - [pdf](http://arxiv.org/pdf/2202.07537v1)

> Two main concepts studied in machine learning theory are generalization gap (difference between train and test error) and excess risk (difference between test error and the minimum possible error). While information-theoretic tools have been used extensively to study the generalization gap of learning algorithms, the information-theoretic nature of excess risk has not yet been fully investigated. In this paper, some steps are taken toward this goal. We consider the frequentist problem of minimax excess risk as a zero-sum game between algorithm designer and the world. Then, we argue that it is desirable to modify this game in a way that the order of play can be swapped. We prove that, under some regularity conditions, if the world and designer can play randomly the duality gap is zero and the order of play can be changed. In this case, a Bayesian problem surfaces in the dual representation. This makes it possible to utilize recent information-theoretic results on minimum excess risk in Bayesian learning to provide bounds on the minimax excess risk. We demonstrate the applicability of the results by providing information theoretic insight on two important classes of problems: classification when the hypothesis space has finite VC-dimension, and regularized least squares.

</details>

<details>

<summary>2022-02-15 17:40:51 - Scalable Spatiotemporally Varying Coefficient Modelling with Bayesian Kernelized Tensor Regression</summary>

- *Mengying Lei, Aurelie Labbe, Lijun Sun*

- `2109.00046v2` - [abs](http://arxiv.org/abs/2109.00046v2) - [pdf](http://arxiv.org/pdf/2109.00046v2)

> As a regression technique in spatial statistics, the spatiotemporally varying coefficient model (STVC) is an important tool for discovering nonstationary and interpretable response-covariate associations over both space and time. However, it is difficult to apply STVC for large-scale spatiotemporal analyses due to the high computational cost. To address this challenge, we summarize the spatiotemporally varying coefficients using a third-order tensor structure and propose to reformulate the spatiotemporally varying coefficient model as a special low-rank tensor regression problem. The low-rank decomposition can effectively model the global patterns of the large data sets with a substantially reduced number of parameters. To further incorporate the local spatiotemporal dependencies, we use Gaussian process (GP) priors on the spatial and temporal factor matrices. We refer to the overall framework as Bayesian Kernelized Tensor Regression (BKTR). For model inference, we develop an efficient Markov chain Monte Carlo (MCMC) algorithm, which uses Gibbs sampling to update factor matrices and slice sampling to update kernel hyperparameters. We conduct extensive experiments on both synthetic and real-world data sets, and our results confirm the superior performance and efficiency of BKTR for model estimation and parameter inference.

</details>

<details>

<summary>2022-02-15 19:00:00 - GIGA-Lens: Fast Bayesian Inference for Strong Gravitational Lens Modeling</summary>

- *A. Gu, X. Huang, W. Sheu, G. Aldering, A. S. Bolton, K. Boone, A. Dey, A. Filipp, E. Jullo, S. Perlmutter, D. Rubin, E. F. Schlafly, D. J. Schlegel, Y. Shu, S. H. Suyu*

- `2202.07663v1` - [abs](http://arxiv.org/abs/2202.07663v1) - [pdf](http://arxiv.org/pdf/2202.07663v1)

> We present GIGA-Lens: a gradient-informed, GPU-accelerated Bayesian framework for modeling strong gravitational lensing systems, implemented in TensorFlow and JAX. The three components, optimization using multi-start gradient descent, posterior covariance estimation with variational inference, and sampling via Hamiltonian Monte Carlo, all take advantage of gradient information through automatic differentiation and massive parallelization on graphics processing units (GPUs). We test our pipeline on a large set of simulated systems and demonstrate in detail its high level of performance. The average time to model a single system on four Nvidia A100 GPUs is 105 seconds. The robustness, speed, and scalability offered by this framework make it possible to model the large number of strong lenses found in current surveys and present a very promising prospect for the modeling of $\mathcal{O}(10^5)$ lensing systems expected to be discovered in the era of the Vera C. Rubin Observatory, Euclid, and the Nancy Grace Roman Space Telescope.

</details>

<details>

<summary>2022-02-15 22:57:05 - The efficacy and generalizability of conditional GANs for posterior inference in physics-based inverse problems</summary>

- *Deep Ray, Harisankar Ramaswamy, Dhruv V. Patel, Assad A. Oberai*

- `2202.07773v1` - [abs](http://arxiv.org/abs/2202.07773v1) - [pdf](http://arxiv.org/pdf/2202.07773v1)

> In this work, we train conditional Wasserstein generative adversarial networks to effectively sample from the posterior of physics-based Bayesian inference problems. The generator is constructed using a U-Net architecture, with the latent information injected using conditional instance normalization. The former facilitates a multiscale inverse map, while the latter enables the decoupling of the latent space dimension from the dimension of the measurement, and introduces stochasticity at all scales of the U-Net. We solve PDE-based inverse problems to demonstrate the performance of our approach in quantifying the uncertainty in the inferred field. Further, we show the generator can learn inverse maps which are local in nature, which in turn promotes generalizability when testing with out-of-distribution samples.

</details>

<details>

<summary>2022-02-16 00:48:53 - Point forecasting and forecast evaluation with generalized Huber loss</summary>

- *Robert J. Taggart*

- `2108.12426v2` - [abs](http://arxiv.org/abs/2108.12426v2) - [pdf](http://arxiv.org/pdf/2108.12426v2)

> Huber loss, its asymmetric variants and their associated functionals (here named Huber functionals) are studied in the context of point forecasting and forecast evaluation. The Huber functional of a distribution is the set of minimizers of the expected (asymmetric) Huber loss, is an intermediary between a quantile and corresponding expectile, and also arises in M-estimation. Each Huber functional is elicitable, generating the precise set of minimizers of an expected score, subject to weak regularity conditions on the class of probability distributions, and has a complete characterization of its consistent scoring functions. Such scoring functions admit a mixture representation as a weighted average of elementary scoring functions. Each elementary score can be interpreted as the relative economic loss of using a particular forecast for a class of investment decisions where profits and losses are capped. The relevance of this theory for comparative assessment of weather forecasts is also discussed.

</details>

<details>

<summary>2022-02-16 10:59:21 - Bayesian Learning with Wasserstein Barycenters</summary>

- *Julio Backhoff-Veraguas, Joaquin Fontbona, Gonzalo Rios, Felipe Tobar*

- `1805.10833v4` - [abs](http://arxiv.org/abs/1805.10833v4) - [pdf](http://arxiv.org/pdf/1805.10833v4)

> Based on recent developments in optimal transport theory, we propose a novel model-selection strategy for Bayesian learning. More precisely, the goal of this paper is to introduce the Wasserstein barycenter of the posterior law on models, as a Bayesian predictive posterior, alternative to classical choices such as the maximum a posteriori and the model average Bayesian estimators. After formulating the general problem of Bayesian model selection in a common, parameter-free framework, we exhibit conditions granting the existence and statistical consistency of this estimator, discuss some of its general and specific properties, and provide insight into its theoretical advantages. Furthermore, we illustrate how it can be computed using the theoretical stochastic gradient descent (SGD) algorithm in Wasserstein space introduced in a companion paper arXiv:2201.04232v2 [math.OC] , and provide a numerical example for experimental validation of the proposed method.

</details>

<details>

<summary>2022-02-16 16:22:47 - Marginal and Conditional Multiple Inference for Linear Mixed Model Predictors</summary>

- *Peter Kramlinger, Tatyana Krivobokova, Stefan Sperlich*

- `1812.09250v6` - [abs](http://arxiv.org/abs/1812.09250v6) - [pdf](http://arxiv.org/pdf/1812.09250v6)

> In spite of its high practical relevance, cluster specific multiple inference for linear mixed model predictors has hardly been addressed so far. While marginal inference for population parameters is well understood, conditional inference for the cluster specific predictors is more intricate. This work introduces a general framework for multiple inference in linear mixed models for cluster specific predictors. Consistent confidence sets for multiple inference are constructed under both, the marginal and the conditional law. Furthermore, it is shown that, remarkably, corresponding multiple marginal confidence sets are also asymptotically valid for conditional inference. Those lend themselves for testing linear hypotheses using standard quantiles without the need of re-sampling techniques. All findings are validated in simulations and illustrated along a study on Covid-19 mortality in US state prisons.

</details>

<details>

<summary>2022-02-16 17:48:59 - Bayesian subset selection and variable importance for interpretable prediction and classification</summary>

- *Daniel R. Kowal*

- `2104.10150v2` - [abs](http://arxiv.org/abs/2104.10150v2) - [pdf](http://arxiv.org/pdf/2104.10150v2)

> Subset selection is a valuable tool for interpretable learning, scientific discovery, and data compression. However, classical subset selection is often avoided due to selection instability, lack of regularization, and difficulties with post-selection inference. We address these challenges from a Bayesian perspective. Given any Bayesian predictive model $\mathcal{M}$, we extract a family of near-optimal subsets of variables for linear prediction or classification. This strategy deemphasizes the role of a single "best" subset and instead advances the broader perspective that often many subsets are highly competitive. The acceptable family of subsets offers a new pathway for model interpretation and is neatly summarized by key members such as the smallest acceptable subset, along with new (co-) variable importance metrics based on whether variables (co-) appear in all, some, or no acceptable subsets. More broadly, we apply Bayesian decision analysis to derive the optimal linear coefficients for any subset of variables. These coefficients inherit both regularization and predictive uncertainty quantification via $\mathcal{M}$. For both simulated and real data, the proposed approach exhibits better prediction, interval estimation, and variable selection than competing Bayesian and frequentist selection methods. These tools are applied to a large education dataset with highly correlated covariates. Our analysis provides unique insights into the combination of environmental, socioeconomic, and demographic factors that predict educational outcomes, and identifies over 200 distinct subsets of variables that offer near-optimal out-of-sample predictive accuracy.

</details>

<details>

<summary>2022-02-16 18:56:25 - The Taxicab Sampler: MCMC for Discrete Spaces with Application to Tree Models</summary>

- *Vincent Geels, Matthew Pratola, Radu Herbei*

- `2107.07313v2` - [abs](http://arxiv.org/abs/2107.07313v2) - [pdf](http://arxiv.org/pdf/2107.07313v2)

> Motivated by the problem of exploring discrete but very complex state spaces in Bayesian models, we propose a novel Markov Chain Monte Carlo search algorithm: the taxicab sampler. We describe the construction of this sampler and discuss how its interpretation and usage differs from that of standard Metropolis-Hastings as well as the related Hamming ball sampler. The proposed sampling algorithm is then shown to demonstrate substantial improvement in computation time without any loss of efficiency relative to a na\"ive Metropolis-Hastings search in a motivating Bayesian regression tree count model, in which we leverage the discrete state space assumption to construct a novel likelihood function that allows for flexibly describing different mean-variance relationships while preserving parameter interpretability compared to existing likelihood functions for count data.

</details>

<details>

<summary>2022-02-16 20:19:26 - A flexible approach for causal inference with multiple treatments and clustered survival outcomes</summary>

- *Liangyuan Hu, Jiayi Ji, Ronald D. Ennis, Joseph W. Hogan*

- `2202.08318v1` - [abs](http://arxiv.org/abs/2202.08318v1) - [pdf](http://arxiv.org/pdf/2202.08318v1)

> When drawing causal inferences about the effects of multiple treatments on clustered survival outcomes using observational data, we need to address implications of the multilevel data structure, multiple treatments, censoring and unmeasured confounding for causal analyses. Few off-the-shelf causal inference tools are available to simultaneously tackle these issues. We develop a flexible random-intercept accelerated failure time model, in which we use Bayesian additive regression trees to capture arbitrarily complex relationships between censored survival times and pre-treatment covariates and use the random intercepts to capture cluster-specific main effects. We develop an efficient Markov chain Monte Carlo algorithm to draw posterior inferences about the population survival effects of multiple treatments and examine the variability in cluster-level effects. We further propose an interpretable sensitivity analysis approach to evaluate the sensitivity of drawn causal inferences about treatment effect to the potential magnitude of departure from the causal assumption of no unmeasured confounding. Expansive simulations empirically validate and demonstrate good practical operating characteristics of our proposed methods. Applying the proposed methods to a dataset on older high-risk localized prostate cancer patients drawn from the National Cancer Database, we evaluate the comparative effects of three treatment approaches on patient survival, and assess the ramifications of potential unmeasured confounding. The methods developed in this work are readily available in the $\textsf{R}$ package $\textsf{riAFTBART}$.

</details>

<details>

<summary>2022-02-16 21:23:53 - Simplified algorithms for adaptive experiment design in parameter estimation</summary>

- *Robert D. McMichael, Sean M. Blakley*

- `2202.08344v1` - [abs](http://arxiv.org/abs/2202.08344v1) - [pdf](http://arxiv.org/pdf/2202.08344v1)

> In experiments to estimate parameters of a parametric model, Bayesian experiment design allows measurement settings to be chosen based on utility, which is the predicted improvement of parameter distributions due to modeled measurement results. In this paper we compare information-theory-based utility with three alternative utility algorithms. Tests of these utility alternatives in simulated adaptive measurements demonstrate large improvements in computational speed with slight impacts on measurement efficiency.

</details>

<details>

<summary>2022-02-16 22:35:39 - Convex Loss Functions for Contextual Pricing with Observational Posted-Price Data</summary>

- *Max Biggs*

- `2202.10944v1` - [abs](http://arxiv.org/abs/2202.10944v1) - [pdf](http://arxiv.org/pdf/2202.10944v1)

> We study an off-policy contextual pricing problem where the seller has access to samples of prices which customers were previously offered, whether they purchased at that price, and auxiliary features describing the customer and/or item being sold. This is in contrast to the well-studied setting in which samples of the customer's valuation (willingness to pay) are observed. In our setting, the observed data is influenced by the historic pricing policy, and we do not know how customers would have responded to alternative prices. We introduce suitable loss functions for this pricing setting which can be directly optimized to find an effective pricing policy with expected revenue guarantees without the need for estimation of an intermediate demand function. We focus on convex loss functions. This is particularly relevant when linear pricing policies are desired for interpretability reasons, resulting in a tractable convex revenue optimization problem. We further propose generalized hinge and quantile pricing loss functions, which price at a multiplicative factor of the conditional expected value or a particular quantile of the valuation distribution when optimized, despite the valuation data not being observed. We prove expected revenue bounds for these pricing policies respectively when the valuation distribution is log-concave, and provide generalization bounds for the finite sample case. Finally, we conduct simulations on both synthetic and real-world data to demonstrate that this approach is competitive with, and in some settings outperforms, state-of-the-art methods in contextual pricing.

</details>

<details>

<summary>2022-02-17 00:11:15 - Nearest Neighbor Dirichlet Mixtures</summary>

- *Shounak Chattopadhyay, Antik Chakraborty, David B. Dunson*

- `2003.07953v3` - [abs](http://arxiv.org/abs/2003.07953v3) - [pdf](http://arxiv.org/pdf/2003.07953v3)

> There is a rich literature on Bayesian methods for density estimation, which characterize the unknown density as a mixture of kernels. Such methods have advantages in terms of providing uncertainty quantification in estimation, while being adaptive to a rich variety of densities. However, relative to frequentist locally adaptive kernel methods, Bayesian approaches can be slow and unstable to implement in relying on Markov chain Monte Carlo algorithms. To maintain most of the strengths of Bayesian approaches without the computational disadvantages, we propose a class of nearest neighbor-Dirichlet mixtures. The approach starts by grouping the data into neighborhoods based on standard algorithms. Within each neighborhood, the density is characterized via a Bayesian parametric model, such as a Gaussian with unknown parameters. Assigning a Dirichlet prior to the weights on these local kernels, we obtain a pseudo-posterior for the weights and kernel parameters. A simple and embarrassingly parallel Monte Carlo algorithm is proposed to sample from the resulting pseudo-posterior for the unknown density. Desirable asymptotic properties are shown, and the methods are evaluated in simulation studies and applied to a motivating data set in the context of classification.

</details>

<details>

<summary>2022-02-17 01:45:01 - Bayesian Optimisation for Mixed-Variable Inputs using Value Proposals</summary>

- *Yan Zuo, Amir Dezfouli, Iadine Chades, David Alexander, Benjamin Ward Muir*

- `2202.04832v2` - [abs](http://arxiv.org/abs/2202.04832v2) - [pdf](http://arxiv.org/pdf/2202.04832v2)

> Many real-world optimisation problems are defined over both categorical and continuous variables, yet efficient optimisation methods such asBayesian Optimisation (BO) are not designed tohandle such mixed-variable search spaces. Recent approaches to this problem cast the selection of the categorical variables as a bandit problem, operating independently alongside a BO component which optimises the continuous variables. In this paper, we adopt a holistic view and aim to consolidate optimisation of the categorical and continuous sub-spaces under a single acquisition metric. We derive candidates from the ExpectedImprovement criterion, which we call value proposals, and use these proposals to make selections on both the categorical and continuous components of the input. We show that this unified approach significantly outperforms existing mixed-variable optimisation approaches across several mixed-variable black-box optimisation tasks.

</details>

<details>

<summary>2022-02-17 16:15:57 - Conjugate priors and bias reduction for logistic regression models</summary>

- *Tommaso Rigon, Emanuele Aliverti*

- `2202.08734v1` - [abs](http://arxiv.org/abs/2202.08734v1) - [pdf](http://arxiv.org/pdf/2202.08734v1)

> Logistic regression models for binomial responses are routinely used in statistical practice. However, the maximum likelihood estimate may not exist due to data separability. We address this issue by considering a conjugate prior penalty which always produces finite estimates. Such a specification has a clear Bayesian interpretation and enjoys several invariance properties, making it an appealing prior choice. We show that the proposed method leads to an accurate approximation of the reduced-bias approach of Firth (1993), resulting in estimators with smaller asymptotic bias than the maximum-likelihood and whose existence is always guaranteed. Moreover, the considered penalized likelihood can be expressed as a genuine likelihood, in which the original data are replaced with a collection of pseudo-counts. Hence, our approach may leverage well established and scalable algorithms for logistic regression. We compare our estimator with alternative reduced-bias methods, vastly improving their computational performance and achieving appealing inferential results.

</details>

<details>

<summary>2022-02-17 18:15:03 - Hybridizing Physical and Data-driven Prediction Methods for Physicochemical Properties</summary>

- *Fabian Jirasek, Robert Bamler, Stephan Mandt*

- `2202.08804v1` - [abs](http://arxiv.org/abs/2202.08804v1) - [pdf](http://arxiv.org/pdf/2202.08804v1)

> We present a generic way to hybridize physical and data-driven methods for predicting physicochemical properties. The approach `distills' the physical method's predictions into a prior model and combines it with sparse experimental data using Bayesian inference. We apply the new approach to predict activity coefficients at infinite dilution and obtain significant improvements compared to the data-driven and physical baselines and established ensemble methods from the machine learning literature.

</details>

<details>

<summary>2022-02-17 21:01:54 - Inference for stochastic kinetic models from multiple data sources for joint estimation of infection dynamics from aggregate reports and virological data</summary>

- *Oksana A. Chkrebtii, Yury E. Garca, Marcos A. Capistrn, Daniel E. Noyola*

- `1710.10346v4` - [abs](http://arxiv.org/abs/1710.10346v4) - [pdf](http://arxiv.org/pdf/1710.10346v4)

> Before the current pandemic, influenza and respiratory syncytial virus (RSV) were the leading etiological agents of seasonal acute respiratory infections (ARI) around the world. In this setting, medical doctors typically based the diagnosis of ARI on patients' symptoms alone and did not routinely conduct virological tests necessary to identify individual viruses, limiting the ability to study the interaction between multiple pathogens and to make public health recommendations. We consider a stochastic kinetic model (SKM) for two interacting ARI pathogens circulating in a large population and an empirically-motivated background process for infections with other pathogens causing similar symptoms. An extended marginal sampling approach, based on the linear noise approximation to the SKM, integrates multiple data sources and additional model components. We infer the parameters defining the pathogens' dynamics and interaction within a Bayesian model and explore the posterior trajectories of infections for each illness based on aggregate infection reports from six epidemic seasons collected by the state health department and a subset of virological tests from a sentinel program at a general hospital in San Luis Potos\'{i}, M\'{e}xico. We interpret the results and make recommendations for future data collection strategies.

</details>

<details>

<summary>2022-02-17 21:43:50 - Sampling Approximately Low-Rank Ising Models: MCMC meets Variational Methods</summary>

- *Frederic Koehler, Holden Lee, Andrej Risteski*

- `2202.08907v1` - [abs](http://arxiv.org/abs/2202.08907v1) - [pdf](http://arxiv.org/pdf/2202.08907v1)

> We consider Ising models on the hypercube with a general interaction matrix $J$, and give a polynomial time sampling algorithm when all but $O(1)$ eigenvalues of $J$ lie in an interval of length one, a situation which occurs in many models of interest. This was previously known for the Glauber dynamics when *all* eigenvalues fit in an interval of length one; however, a single outlier can force the Glauber dynamics to mix torpidly. Our general result implies the first polynomial time sampling algorithms for low-rank Ising models such as Hopfield networks with a fixed number of patterns and Bayesian clustering models with low-dimensional contexts, and greatly improves the polynomial time sampling regime for the antiferromagnetic/ferromagnetic Ising model with inconsistent field on expander graphs. It also improves on previous approximation algorithm results based on the naive mean-field approximation in variational methods and statistical physics.   Our approach is based on a new fusion of ideas from the MCMC and variational inference worlds. As part of our algorithm, we define a new nonconvex variational problem which allows us to sample from an exponential reweighting of a distribution by a negative definite quadratic form, and show how to make this procedure provably efficient using stochastic gradient descent. On top of this, we construct a new simulated tempering chain (on an extended state space arising from the Hubbard-Stratonovich transform) which overcomes the obstacle posed by large positive eigenvalues, and combine it with the SGD-based sampler to solve the full problem.

</details>

<details>

<summary>2022-02-17 22:53:02 - From Pareto to Weibull -- a constructive review of distributions on $\mathbb{R}^+$</summary>

- *Corinne Sinner, Yves Dominicy, Julien Trufin, Wout Waterschoot, Patrick Weber, Christophe Ley*

- `2012.13348v2` - [abs](http://arxiv.org/abs/2012.13348v2) - [pdf](http://arxiv.org/pdf/2012.13348v2)

> Power laws and power laws with exponential cut-off are two distinct families of distributions on the positive real half-line. In the present paper, we propose a unified treatment of both families by building a family of distributions that interpolates between them, which we call Interpolating Family (IF) of distributions. Our original construction, which relies on techniques from statistical physics, provides a connection for hitherto unrelated distributions like the Pareto and Weibull distributions, and sheds new light on them. The IF also contains several distributions that are neither of power law nor of power law with exponential cut-off type. We calculate quantile-based properties, moments and modes for the IF. This allows us to review known properties of famous distributions on $\mathbb{R}^+$ and to provide in a single sweep these characteristics for various less known (and new) special cases of our Interpolating Family.

</details>

<details>

<summary>2022-02-18 16:05:29 - Surf or sleep? Understanding the influence of bedtime patterns on campus</summary>

- *Teng Guo, Linhong Li, Dongyu Zhang, Feng Xia*

- `2202.09283v1` - [abs](http://arxiv.org/abs/2202.09283v1) - [pdf](http://arxiv.org/pdf/2202.09283v1)

> Poor sleep habits may cause serious problems of mind and body, and it is a commonly observed issue for college students due to study workload as well as peer and social influence. Understanding its impact and identifying students with poor sleep habits matters a lot in educational management. Most of the current research is either based on self-reports and questionnaires, suffering from a small sample size and social desirability bias, or the methods used are not suitable for the education system. In this paper, we develop a general data-driven method for identifying students' sleep patterns according to their Internet access pattern stored in the education management system and explore its influence from various aspects. First, we design a Possion-based probabilistic mixture model to cluster students according to the distribution of bedtime and identify students who are used to staying up late. Second, we profile students from five aspects (including eight dimensions) based on campus-behavior data and build Bayesian networks to explore the relationship between behavioral characteristics and sleeping habits. Finally, we test the predictability of sleeping habits. This paper not only contributes to the understanding of student sleep from a cognitive and behavioral perspective but also presents a new approach that provides an effective framework for various educational institutions to detect the sleeping patterns of students.

</details>

<details>

<summary>2022-02-18 16:05:39 - Sequentially guided MCMC proposals for synthetic likelihoods and correlated synthetic likelihoods</summary>

- *Umberto Picchini, Umberto Simola, Jukka Corander*

- `2004.04558v5` - [abs](http://arxiv.org/abs/2004.04558v5) - [pdf](http://arxiv.org/pdf/2004.04558v5)

> Synthetic likelihood (SL) is a strategy for parameter inference when the likelihood function is analytically or computationally intractable. In SL, the likelihood function of the data is replaced by a multivariate Gaussian density over summary statistics of the data. SL requires simulation of many replicate datasets at every parameter value considered by a sampling algorithm, such as Markov chain Monte Carlo (MCMC), making the method computationally-intensive. We propose two strategies to alleviate the computational burden. First, we introduce an algorithm producing a proposal distribution that is sequentially tuned and made conditional to data, thus it rapidly \textit{guides} the proposed parameters towards high posterior density regions. In our experiments, a small number of iterations of our algorithm is enough to rapidly locate high density regions, which we use to initialize one or several chains that make use of off-the-shelf adaptive MCMC methods. Our "guided" approach can also be potentially used with MCMC samplers for approximate Bayesian computation (ABC). Second, we exploit strategies borrowed from the correlated pseudo-marginal MCMC literature, to improve the chains mixing in a SL framework. Moreover, our methods enable inference for challenging case studies, when the posterior is multimodal and when the chain is initialised in low posterior probability regions of the parameter space, where standard samplers failed. To illustrate the advantages stemming from our framework we consider five benchmark examples, including estimation of parameters for a cosmological model and a stochastic model with highly non-Gaussian summary statistics.

</details>

<details>

<summary>2022-02-18 17:26:37 - Multiple combined gamma kernel estimations for nonnegative data with Bayesian adaptive bandwidths</summary>

- *Sobom M. Som, Clestin C. Kokonendji, Smail Adjabi, Naushad A. Mamode Khan, Said Beddek*

- `2202.09314v1` - [abs](http://arxiv.org/abs/2202.09314v1) - [pdf](http://arxiv.org/pdf/2202.09314v1)

> A modified gamma kernel should not be automatically preferred to the standard gamma kernel, especially for univariate convex densities with a pole at the origin. In the multivariate case, multiple combined gamma kernels, defined as a product of univariate standard and modified ones, are here introduced for nonparametric and semiparametric smoothing of unknown orthant densities with support $[0,\infty)^d$. Asymptotical properties of these multivariate associated kernel estimators are established. Bayesian estimation of adaptive bandwidth vectors using multiple pure combined gamma smoothers, and in semiparametric setup, are exactly derived under the usual quadratic function. The simulation results and four illustrations on real datasets reveal very interesting advantages of the proposed combined approach for nonparametric smoothing, compare to both pure standard and pure modified gamma kernel versions, and under integrated squared error and average log-likelihood criteria.

</details>

<details>

<summary>2022-02-19 00:04:56 - Graph Reparameterizations for Enabling 1000+ Monte Carlo Iterations in Bayesian Deep Neural Networks</summary>

- *Jurijs Nazarovs, Ronak R. Mehta, Vishnu Suresh Lokhande, Vikas Singh*

- `2202.09478v1` - [abs](http://arxiv.org/abs/2202.09478v1) - [pdf](http://arxiv.org/pdf/2202.09478v1)

> Uncertainty estimation in deep models is essential in many real-world applications and has benefited from developments over the last several years. Recent evidence suggests that existing solutions dependent on simple Gaussian formulations may not be sufficient. However, moving to other distributions necessitates Monte Carlo (MC) sampling to estimate quantities such as the KL divergence: it could be expensive and scales poorly as the dimensions of both the input data and the model grow. This is directly related to the structure of the computation graph, which can grow linearly as a function of the number of MC samples needed. Here, we construct a framework to describe these computation graphs, and identify probability families where the graph size can be independent or only weakly dependent on the number of MC samples. These families correspond directly to large classes of distributions. Empirically, we can run a much larger number of iterations for MC approximations for larger architectures used in computer vision with gains in performance measured in confident accuracy, stability of training, memory and training time.

</details>

<details>

<summary>2022-02-19 01:00:25 - Bayesian inference for link travel time correlation of a bus route</summary>

- *Xiaoxu Chen, Zhanhong Cheng, Lijun Sun*

- `2202.09485v1` - [abs](http://arxiv.org/abs/2202.09485v1) - [pdf](http://arxiv.org/pdf/2202.09485v1)

> Estimation of link travel time correlation of a bus route is essential to many bus operation applications, such as timetable scheduling, travel time forecasting and transit service assessment/improvement. Most previous studies rely on either independent assumptions or simplified local spatial correlation structures. In the real world, however, link travel time on a bus route could exhibit complex correlation structures, such as long-range correlations, negative correlations, and time-varying correlations. Therefore, before introducing strong assumptions, it is essential to empirically quantify and examine the correlation structure of link travel time from real-world bus operation data. To this end, this paper develops a Bayesian Gaussian model to estimate the link travel time correlation matrix of a bus route using smart-card-like data. Our method overcomes the small-sample-size problem in correlation matrix estimation by borrowing/integrating those incomplete observations (i.e., with missing/ragged values and overlapped link segments) from other bus routes. Next, we propose an efficient Gibbs sampling framework to marginalize over the missing and ragged values and obtain the posterior distribution of the correlation matrix. Three numerical experiments are conducted to evaluate model performance. We first conduct a synthetic experiment and our results show that the proposed method produces an accurate estimation for travel time correlations with credible intervals. Next, we perform experiments on a real-world bus route with smart card data; our results show that both local and long-range correlations exist on this bus route. Finally, we demonstrate an application of using the estimated covariance matrix to make probabilistic forecasting of link and trip travel time.

</details>

<details>

<summary>2022-02-19 06:03:23 - Bayesian Quantile Trend Filtering on Graphs using Shrinkage Priors</summary>

- *Takahiro Onizuka, Shintaro Hashimoto, Shonosuke Sugasawa*

- `2202.09534v1` - [abs](http://arxiv.org/abs/2202.09534v1) - [pdf](http://arxiv.org/pdf/2202.09534v1)

> Quantiles are useful characteristics of random variables that can provide substantial information of distributions compared with commonly used summary statistics such as means. In this paper, we propose a Bayesian quantile trend filtering method to estimate non-stationary trend of quantiles on graphs. We introduce general shrinkage priors for graph differences to induce locally adaptive Bayesian inference on trends. Introducing so-called shadow priors with multivariate truncated distribution for local scale parameters and mixture representation of the asymmetric Laplace distribution, we provide a simple Gibbs sampling algorithm to generate posterior samples. We also develop variational Bayes approximation to quickly compute point estimates (e.g. posterior means). The numerical performance of the proposed method is demonstrated through simulation study with time series data, application of quantile regression and robust spatial quantile smoothing.

</details>

<details>

<summary>2022-02-19 09:58:28 - Deep kernel machines: exact inference with representation learning in infinite Bayesian neural networks</summary>

- *Adam Yang, Maxime Robeyns, Nandi Schoots, Laurence Aitchison*

- `2108.13097v3` - [abs](http://arxiv.org/abs/2108.13097v3) - [pdf](http://arxiv.org/pdf/2108.13097v3)

> Deep neural networks (DNNs) with the flexibility to learn good top-layer representations have eclipsed shallow kernel methods without that flexibility. Here, we take inspiration from DNNs to develop the deep kernel machine. Optimizing the deep kernel machine objective is equivalent to exact Bayesian inference (or noisy gradient descent) in an infinitely wide Bayesian neural network or deep Gaussian process, which has been scaled carefully to retain representation learning. Our work thus has important implications for theoretical understanding of neural networks. In addition, we show that the deep kernel machine objective has more desirable properties and better performance than other choices of objective. Finally, we conjecture that the deep kernel machine objective is unimodal. We give a proof of unimodality for linear kernels, and a number of experiments in the nonlinear case in which all deep kernel machines initializations we tried converged to the same solution.

</details>

<details>

<summary>2022-02-19 10:33:04 - Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems</summary>

- *Ingvar Ziemann, Henrik Sandberg*

- `2201.01680v2` - [abs](http://arxiv.org/abs/2201.01680v2) - [pdf](http://arxiv.org/pdf/2201.01680v2)

> This paper presents local minimax regret lower bounds for adaptively controlling linear-quadratic-Gaussian (LQG) systems. We consider smoothly parametrized instances and provide an understanding of when logarithmic regret is impossible which is both instance specific and flexible enough to take problem structure into account. This understanding relies on two key notions: That of local-uninformativeness; when the optimal policy does not provide sufficient excitation for identification of the optimal policy, and yields a degenerate Fisher information matrix; and that of information-regret-boundedness, when the small eigenvalues of a policy-dependent information matrix are boundable in terms of the regret of that policy. Combined with a reduction to Bayesian estimation and application of Van Trees' inequality, these two conditions are sufficient for proving regret bounds on order of magnitude $\sqrt{T}$ in the time horizon, $T$. This method yields lower bounds that exhibit tight dimensional dependencies and scale naturally with control-theoretic problem constants. For instance, we are able to prove that systems operating near marginal stability are fundamentally hard to learn to control. We further show that large classes of systems satisfy these conditions, among them any state-feedback system with both $A$- and $B$-matrices unknown. Most importantly, we also establish that a nontrivial class of partially observable systems, essentially those that are over-actuated, satisfy these conditions, thus providing a $\sqrt{T}$ lower bound also valid for partially observable systems. Finally, we turn to two simple examples which demonstrate that our lower bound captures classical control-theoretic intuition: our lower bounds diverge for systems operating near marginal stability or with large filter gain -- these can be arbitrarily hard to (learn to) control.

</details>

<details>

<summary>2022-02-19 11:03:34 - Scaled process priors for Bayesian nonparametric estimation of the unseen genetic variation</summary>

- *Federico Camerlenghi, Stefano Favaro, Lorenzo Masoero, Tamara Broderick*

- `2106.15480v2` - [abs](http://arxiv.org/abs/2106.15480v2) - [pdf](http://arxiv.org/pdf/2106.15480v2)

> There is a growing interest in the estimation of the number of unseen features, mostly driven by biological applications. A recent work brought out a peculiar property of the popular completely random measures (CRMs) as prior models in Bayesian nonparametric (BNP) inference for the unseen-features problem: for fixed prior's parameters, they all lead to a Poisson posterior distribution for the number of unseen features, which depends on the sampling information only through the sample size. CRMs are thus not a flexible prior model for the unseen-features problem and, while the Poisson posterior distribution may be appealing for analytical tractability and ease of interpretability, its independence from the sampling information makes the BNP approach a questionable oversimplification, with posterior inferences being completely determined by the estimation of unknown prior's parameters. In this paper, we introduce the stable-Beta scaled process (SB-SP) prior, and we show that it allows to enrich the posterior distribution of the number of unseen features arising under CRM priors, while maintaining its analytical tractability and interpretability. That is, the SB-SP prior leads to a negative Binomial posterior distribution, which depends on the sampling information through the sample size and the number of distinct features, with corresponding estimates being simple, linear in the sampling information and computationally efficient. We apply our BNP approach to synthetic data and to real cancer genomic data, showing that: i) it outperforms the most popular parametric and nonparametric competitors in terms of estimation accuracy; ii) it provides improved coverage for the estimation with respect to a BNP approach under CRM priors.

</details>

<details>

<summary>2022-02-20 00:24:22 - Generalized Bayesian Upper Confidence Bound with Approximate Inference for Bandit Problems</summary>

- *Ziyi Huang, Henry Lam, Amirhossein Meisami, Haofeng Zhang*

- `2201.12955v2` - [abs](http://arxiv.org/abs/2201.12955v2) - [pdf](http://arxiv.org/pdf/2201.12955v2)

> Bayesian bandit algorithms with approximate inference have been widely used in practice with superior performance. Yet, few studies regarding the fundamental understanding of their performances are available. In this paper, we propose a Bayesian bandit algorithm, which we call Generalized Bayesian Upper Confidence Bound (GBUCB), for bandit problems in the presence of approximate inference. Our theoretical analysis demonstrates that in Bernoulli multi-armed bandit, GBUCB can achieve $O(\sqrt{T}(\log T)^c)$ frequentist regret if the inference error measured by symmetrized Kullback-Leibler divergence is controllable. This analysis relies on a novel sensitivity analysis for quantile shifts with respect to inference errors. To our best knowledge, our work provides the first theoretical regret bound that is better than $o(T)$ in the setting of approximate inference. Our experimental evaluations on multiple approximate inference settings corroborate our theory, showing that our GBUCB is consistently superior to BUCB and Thompson sampling.

</details>

<details>

<summary>2022-02-20 03:34:27 - Smooth multi-period forecasting with application to prediction of COVID-19 cases</summary>

- *Elena Tuzhilina, Trevor J. Hastie, Daniel J. McDonald, J. Kenneth Tay, Robert Tibshirani*

- `2202.09723v1` - [abs](http://arxiv.org/abs/2202.09723v1) - [pdf](http://arxiv.org/pdf/2202.09723v1)

> Forecasting methodologies have always attracted a lot of attention and have become an especially hot topic since the beginning of the COVID-19 pandemic. In this paper we consider the problem of multi-period forecasting that aims to predict several horizons at once. We propose a novel approach that forces the prediction to be "smooth" across horizons and apply it to two tasks: point estimation via regression and interval prediction via quantile regression. This methodology was developed for real-time distributed COVID-19 forecasting. We illustrate the proposed technique with the CovidCast dataset as well as a small simulation example.

</details>

<details>

<summary>2022-02-20 14:54:13 - Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems</summary>

- *Kunal Pattanayak, Vikram Krishnamurthy*

- `2007.03481v4` - [abs](http://arxiv.org/abs/2007.03481v4) - [pdf](http://arxiv.org/pdf/2007.03481v4)

> This paper presents an inverse reinforcement learning~(IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function. In a Bayesian (partially observed) setting, the inverse learner can at best identify optimality wrt the observed actions. Our IRL algorithm identifies optimality and then constructs set valued estimates of the cost function. To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. We illustrate the proposed IRL scheme using two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. Finally, for finite datasets, we propose an IRL detection algorithm and give finite sample bounds on its error probabilities.

</details>

<details>

<summary>2022-02-20 21:09:44 - Thou Shalt Not Reject the P-value</summary>

- *Oliver Y. Chn, Ral G. Saraiva, Guy Nagels, Huy Phan, Tom Schwantje, Hengyi Cao, Jiangtao Gou, Jenna M. Reinen, Bin Xiong, Maarten de Vos*

- `2002.07270v6` - [abs](http://arxiv.org/abs/2002.07270v6) - [pdf](http://arxiv.org/pdf/2002.07270v6)

> Since its debut in the 18th century, the P-value has been an important part of hypothesis testing-based scientific discoveries. As the statistical engine accelerates, questions are beginning to be raised, asking to what extent scientific discoveries based on P-values are reliable and reproducible, and the voice calling for adjusting the significance level or banning the P-value has been increasingly heard. Inspired by these questions and discussions, here we enquire into the useful roles and misuses of the P-value in scientific studies. For common misuses and misinterpretations, we provide modest recommendations for practitioners. Additionally, we compare statistical significance with clinical relevance. In parallel, we review the Bayesian alternatives for seeking evidence. Finally, we discuss the promises and risks of using meta-analysis to pool P-values from multiple studies to aggregate evidence. Taken together, the P-value underpins a useful probabilistic decision-making system and provides evidence at a continuous scale. But its interpretation must be contextual, considering the scientific question, experimental design (including the model specification, sample size, and significance level), statistical power, effect size, and reproducibility.

</details>

<details>

<summary>2022-02-20 22:52:07 - Generalized Bayesian Additive Regression Trees Models: Beyond Conditional Conjugacy</summary>

- *Antonio R. Linero*

- `2202.09924v1` - [abs](http://arxiv.org/abs/2202.09924v1) - [pdf](http://arxiv.org/pdf/2202.09924v1)

> Bayesian additive regression trees have seen increased interest in recent years due to their ability to combine machine learning techniques with principled uncertainty quantification. The Bayesian backfitting algorithm used to fit BART models, however, limits their application to a small class of models for which conditional conjugacy exists. In this article, we greatly expand the domain of applicability of BART to arbitrary \emph{generalized BART} models by introducing a very simple, tuning-parameter-free, reversible jump Markov chain Monte Carlo algorithm. Our algorithm requires only that the user be able to compute the likelihood and (optionally) its gradient and Fisher information. The potential applications are very broad; we consider examples in survival analysis, structured heteroskedastic regression, and gamma shape regression.

</details>

<details>

<summary>2022-02-21 05:13:38 - Weakly informative priors and prior-data conflict checking for likelihood-free inference</summary>

- *Atlanta Chakraborty, David J. Nott, Michael Evans*

- `2202.09993v1` - [abs](http://arxiv.org/abs/2202.09993v1) - [pdf](http://arxiv.org/pdf/2202.09993v1)

> Bayesian likelihood-free inference, which is used to perform Bayesian inference when the likelihood is intractable, enjoys an increasing number of important scientific applications. However, many aspects of a Bayesian analysis become more challenging in the likelihood-free setting. One example of this is prior-data conflict checking, where the goal is to assess whether the information in the data and the prior are inconsistent. Conflicts of this kind are important to detect, since they may reveal problems in an investigator's understanding of what are relevant values of the parameters, and can result in sensitivity of Bayesian inferences to the prior. Here we consider methods for prior-data conflict checking which are applicable regardless of whether the likelihood is tractable or not. In constructing our checks, we consider checking statistics based on prior-to-posterior Kullback-Leibler divergences. The checks are implemented using mixture approximations to the posterior distribution and closed-form approximations to Kullback-Leibler divergences for mixtures, which make Monte Carlo approximation of reference distributions for calibration computationally feasible. When prior-data conflicts occur, it is useful to consider weakly informative prior specifications in alternative analyses as part of a sensitivity analysis. As a main application of our methodology, we develop a technique for searching for weakly informative priors in likelihood-free inference, where the notion of a weakly informative prior is formalized using prior-data conflict checks. The methods are demonstrated in three examples.

</details>

<details>

<summary>2022-02-21 06:48:18 - Augmenting Neural Networks with Priors on Function Values</summary>

- *Hunter Nisonoff, Yixin Wang, Jennifer Listgarten*

- `2202.04798v3` - [abs](http://arxiv.org/abs/2202.04798v3) - [pdf](http://arxiv.org/pdf/2202.04798v3)

> The need for function estimation in label-limited settings is common in the natural sciences. At the same time, prior knowledge of function values is often available in these domains. For example, data-free biophysics-based models can be informative on protein properties, while quantum-based computations can be informative on small molecule properties. How can we coherently leverage such prior knowledge to help improve a neural network model that is quite accurate in some regions of input space -- typically near the training data -- but wildly wrong in other regions? Bayesian neural networks (BNN) enable the user to specify prior information only on the neural network weights, not directly on the function values. Moreover, there is in general no clear mapping between these. Herein, we tackle this problem by developing an approach to augment BNNs with prior information on the function values themselves. Our probabilistic approach yields predictions that rely more heavily on the prior information when the epistemic uncertainty is large, and more heavily on the neural network when the epistemic uncertainty is small.

</details>

<details>

<summary>2022-02-21 07:51:33 - Honour Thesis: A Joint Value at Risk and Expected Shortfall Combination Framework and its Applications in the Cryptocurrency Market</summary>

- *Zhengkun Li*

- `2202.10918v1` - [abs](http://arxiv.org/abs/2202.10918v1) - [pdf](http://arxiv.org/pdf/2202.10918v1)

> Value at risk and expected shortfall are increasingly popular tail risk measures in the financial risk management field. Both academia and financial institutions are working to improve tail risk forecasts in order to meet the requirements of the Basel Capital Accord; it states that one purpose of risk management and measuring risk accuracy is, since extreme movements cannot always be avoided, financial institutions can prepare for these extreme returns by capital allocation, and putting aside the appropriate amount of capital so as to avoid default in times of extreme price or index movements. Forecast combination has drawn much attention, as a combined forecast can outperform the individual forecasts under certain conditions. We propose two methodology, one is a semiparametric combination framework that can jointly produce combined value at risk and expected shortfall forecasts, another one is a parametric regression framework named as Quantile-ES regression that can produce combined expected shortfall forecasts. The favourability of the semiparametric combination framework has been presented via an empirical study - application in cryptocurrency markets with high-frequency data where the necessity of risk management application increases as the cryptocurrency market becomes more popular and mature. Additionally, the general framework of the parametric Quantile-ES regression has been presented via a simulation study, whereas it still need to be improved in the future. The contributions of this work include but are not limited to the enabling of the combination of expected shortfall forecasts and the application of risk management procedures in the cryptocurrency market with high-frequency data.

</details>

<details>

<summary>2022-02-21 13:53:37 - Stochastic Modeling of Inhomogeneities in the Aortic Wall and Uncertainty Quantification using a Bayesian Encoder-Decoder Surrogate</summary>

- *Sascha Ranftl, Malte Rolf-Pissarczyk, Gloria Wolkerstorfer, Antonio Pepe, Jan Egger, Wolfgang von der Linden, Gerhard A. Holzapfel*

- `2202.10244v1` - [abs](http://arxiv.org/abs/2202.10244v1) - [pdf](http://arxiv.org/pdf/2202.10244v1)

> Inhomogeneities in the aortic wall can lead to localized stress accumulations, possibly initiating dissection. In many cases, a dissection results from pathological changes such as fragmentation or loss of elastic fibers. But it has been shown that even the healthy aortic wall has an inherent heterogeneous microstructure. Some parts of the aorta are particularly susceptible to the development of inhomogeneities due to pathological changes, however, the distribution in the aortic wall and the spatial extent, such as size, shape, and type, are difficult to predict. Motivated by this observation, we describe the heterogeneous distribution of elastic fiber degradation in the dissected aortic wall using a stochastic constitutive model. For this purpose, random field realizations, which model the stochastic distribution of degraded elastic fibers, are generated over a non-equidistant grid. The random field then serves as input for a uni-axial extension test of the pathological aortic wall, solved with the finite-element (FE) method. To include the microstructure of the dissected aortic wall, a constitutive model developed in a previous study is applied, which also includes an approach to model the degradation of inter-lamellar elastic fibers. Then to assess the uncertainty in the output stress distribution due to this stochastic constitutive model, a convolutional neural network, specifically a Bayesian encoder-decoder, was used as a surrogate model that maps the random input fields to the output stress distribution obtained from the FE analysis. The results show that the neural network is able to predict the stress distribution of the FE analysis while significantly reducing the computational time. In addition, it provides the probability for exceeding critical stresses within the aortic wall, which could allow for the prediction of delamination or fatal rupture.

</details>

<details>

<summary>2022-02-21 15:48:11 - Generalized Geographically Weighted Regression Model within a Modularized Bayesian Framework</summary>

- *Yang Liu, Robert J. B. Goudie*

- `2106.00996v2` - [abs](http://arxiv.org/abs/2106.00996v2) - [pdf](http://arxiv.org/pdf/2106.00996v2)

> Geographically weighted regression (GWR) models handle geographical dependence through a spatially varying coefficient model and have been widely used in applied science, but its general Bayesian extension is unclear because it involves a weighted log-likelihood which does not imply a probability distribution on data. We present a Bayesian GWR model and show that its essence is dealing with partial misspecification of the model. Current modularized Bayesian inference models accommodate partial misspecification from a single component of the model. We extend these models to handle partial misspecification in more than one component of the model, as required for our Bayesian GWR model. Information from the various spatial locations is manipulated via a geographically weighted kernel and the optimal manipulation is chosen according to a Kullback-Leibler (KL) divergence. We justify the model via an information risk minimization approach and show the consistency of the proposed estimator in terms of a geographically weighted KL divergence.

</details>

<details>

<summary>2022-02-21 15:49:04 - On out-of-distribution detection with Bayesian neural networks</summary>

- *Francesco D'Angelo, Christian Henning*

- `2110.06020v2` - [abs](http://arxiv.org/abs/2110.06020v2) - [pdf](http://arxiv.org/pdf/2110.06020v2)

> The question whether inputs are valid for the problem a neural network is trying to solve has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that Bayesian neural networks (BNNs) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers. In this paper, we question this assumption and show that proper Bayesian inference with function space priors induced by neural networks does not necessarily lead to good OOD detection. To circumvent the use of approximate inference, we start by studying the infinite-width case, where Bayesian inference can be exact due to the correspondence with Gaussian processes. Strikingly, the kernels derived from common architectural choices lead to function space priors which induce predictive uncertainties that do not reflect the underlying input data distribution and are therefore unsuited for OOD detection. Importantly, we find the OOD behavior in this limiting case to be consistent with the corresponding finite-width case. To overcome this limitation, useful function space properties can also be encoded in the prior in weight space, however, this can currently only be applied to a specified subset of the domain and thus does not inherently extend to OOD data. Finally, we argue that a trade-off between generalization and OOD capabilities might render the application of BNNs for OOD detection undesirable in practice. Overall, our study discloses fundamental problems when naively using BNNs for OOD detection and opens interesting avenues for future research.

</details>

<details>

<summary>2022-02-21 16:31:51 - A first-stage representation for instrumental variables quantile regression</summary>

- *Javier Alejo, Antonio F. Galvao, Gabriel Montes-Rojas*

- `2102.01212v4` - [abs](http://arxiv.org/abs/2102.01212v4) - [pdf](http://arxiv.org/pdf/2102.01212v4)

> This paper develops a first-stage linear regression representation for the instrumental variables (IV) quantile regression (QR) model. The quantile first-stage is analogous to the least squares case, i.e., a linear projection of the endogenous variables on the instruments and other exogenous covariates, with the difference that the QR case is a weighted projection. The weights are given by the conditional density function of the innovation term in the QR structural model, conditional on the endogeneous and exogenous covariates, and the instruments as well, at a given quantile. We also show that the required Jacobian identification conditions for IVQR models are embedded in the quantile first-stage. We then suggest inference procedures to evaluate the adequacy of instruments by evaluating their statistical significance using the first-stage result. The test is developed in an over-identification context, since consistent estimation of the weights for implementation of the first-stage requires at least one valid instrument to be available. Monte Carlo experiments provide numerical evidence that the proposed tests work as expected in terms of empirical size and power in finite samples. An empirical application illustrates that checking for the statistical significance of the instruments at different quantiles is important. The proposed procedures may be specially useful in QR since the instruments may be relevant at some quantiles but not at others.

</details>

<details>

<summary>2022-02-21 16:59:31 - A Predictive Approach to Bayesian Nonparametric Survival Analysis</summary>

- *Edwin Fong, Brieuc Lehmann*

- `2202.10361v1` - [abs](http://arxiv.org/abs/2202.10361v1) - [pdf](http://arxiv.org/pdf/2202.10361v1)

> Bayesian nonparametric methods are a popular choice for analysing survival data due to their ability to flexibly model the distribution of survival times. These methods typically employ a nonparametric prior on the survival function that is conjugate with respect to right-censored data. Eliciting these priors, particularly in the presence of covariates, can be challenging and inference typically relies on computationally intensive Markov chain Monte Carlo schemes. In this paper, we build on recent work that recasts Bayesian inference as assigning a predictive distribution on the unseen values of a population conditional on the observed samples, thus avoiding the need to specify a complex prior. We describe a copula-based predictive update which admits a scalable sequential importance sampling algorithm to perform inference that properly accounts for right-censoring. We provide theoretical justification through an extension of Doob's consistency theorem and illustrate the method on a number of simulated and real data sets, including an example with covariates. Our approach enables analysts to perform Bayesian nonparametric inference through only the specification of a predictive distribution.

</details>

<details>

<summary>2022-02-21 17:43:49 - Assessing epidemic curves for evidence of superspreading</summary>

- *Joe Meagher, Nial Friel*

- `2106.12064v2` - [abs](http://arxiv.org/abs/2106.12064v2) - [pdf](http://arxiv.org/pdf/2106.12064v2)

> The expected number of secondary infections arising from each index case, referred to as the reproduction or $R$ number, is a vital summary statistic for understanding and managing epidemic diseases. There are many methods for estimating $R$; however, few explicitly model heterogeneous disease reproduction, which gives rise to superspreading within the population. We propose a parsimonious discrete-time branching process model for epidemic curves that incorporates heterogeneous individual reproduction numbers. Our Bayesian approach to inference illustrates that this heterogeneity results in less certainty on estimates of the time-varying cohort reproduction number $R_t$. We apply these methods to a COVID-19 epidemic curve for the Republic of Ireland and find support for heterogeneous disease reproduction. Our analysis allows us to estimate the expected proportion of secondary infections attributable to the most infectious proportion of the population. For example, we estimate that the 20% most infectious index cases account for approximately 75-98% of the expected secondary infections with 95% posterior probability. In addition, we highlight that heterogeneity is a vital consideration when estimating $R_t$.

</details>

<details>

<summary>2022-02-21 18:20:15 - Bayesian Space-time SIR modeling of Covid-19 in two US states during the 2020-2021 pandemic</summary>

- *Andrew B Lawson, Joanne Kim*

- `2202.06879v2` - [abs](http://arxiv.org/abs/2202.06879v2) - [pdf](http://arxiv.org/pdf/2202.06879v2)

> This paper describes the Bayesian SIR modeling of the 3 waves of Covid-19 in two contrasting US states during 2020-2021. A variety of models are evaluated at the county level for goodness-of-fit and an assessment of confounding predictors is also made. It is found that models with three deprivation predictors and neighborhood effects are important. In addition the work index from Google mobility was also found to provide increased explanation of the transmission dynamic.

</details>

<details>

<summary>2022-02-21 20:09:43 - Accelerating Stochastic Simulation with Spatiotemporal Neural Processes</summary>

- *Dongxia Wu, Matteo Chinazzi, Alessandro Vespignani, Yi-An Ma, Rose Yu*

- `2106.02770v5` - [abs](http://arxiv.org/abs/2106.02770v5) - [pdf](http://arxiv.org/pdf/2106.02770v5)

> Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. We propose Spatiotemporal Neural Processes (STNP), a neural latent variable model to mimic the spatiotemporal dynamics of stochastic simulators. To further speed up training, we use a Bayesian active learning strategy to proactively query the simulator, gather more data, and continuously improve the model. Our model can automatically infer the latent processes which describe the intrinsic uncertainty of the simulator. This also gives rise to a new acquisition function based on latent information gain. Theoretical analysis demonstrates that our approach reduces sample complexity compared with random sampling in high dimension. Empirically, we demonstrate that our framework can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples, enabling rapid simulation and scenario exploration.

</details>

<details>

<summary>2022-02-21 20:30:33 - Gaussian Imagination in Bandit Learning</summary>

- *Yueyang Liu, Adithya M. Devraj, Benjamin Van Roy, Kuang Xu*

- `2201.01902v3` - [abs](http://arxiv.org/abs/2201.01902v3) - [pdf](http://arxiv.org/pdf/2201.01902v3)

> Assuming distributions are Gaussian often facilitates computations that are otherwise intractable. We study the performance of an agent that attains a bounded information ratio with respect to a bandit environment with a Gaussian prior distribution and a Gaussian likelihood function when applied instead to a Bernoulli bandit. Relative to an information-theoretic bound on the Bayesian regret the agent would incur when interacting with the Gaussian bandit, we bound the increase in regret when the agent interacts with the Bernoulli bandit. If the Gaussian prior distribution and likelihood function are sufficiently diffuse, this increase grows at a rate which is at most linear in the square-root of the time horizon, and thus the per-timestep increase vanishes. Our results formalize the folklore that so-called Bayesian agents remain effective when instantiated with diffuse misspecified distributions.

</details>

<details>

<summary>2022-02-22 01:49:41 - Order-Optimal Error Bounds for Noisy Kernel-Based Bayesian Quadrature</summary>

- *Xu Cai, Chi Thanh Lam, Jonathan Scarlett*

- `2202.10615v1` - [abs](http://arxiv.org/abs/2202.10615v1) - [pdf](http://arxiv.org/pdf/2202.10615v1)

> In this paper, we study the sample complexity of {\em noisy Bayesian quadrature} (BQ), in which we seek to approximate an integral based on noisy black-box queries to the underlying function. We consider functions in a {\em Reproducing Kernel Hilbert Space} (RKHS) with the Mat\'ern-$\nu$ kernel, focusing on combinations of the parameter $\nu$ and dimension $d$ such that the RKHS is equivalent to a Sobolev class. In this setting, we provide near-matching upper and lower bounds on the best possible average error. Specifically, we find that when the black-box queries are subject to Gaussian noise having variance $\sigma^2$, any algorithm making at most $T$ queries (even with adaptive sampling) must incur a mean absolute error of $\Omega(T^{-\frac{\nu}{d}-1} + \sigma T^{-\frac{1}{2}})$, and there exists a non-adaptive algorithm attaining an error of at most $O(T^{-\frac{\nu}{d}-1} + \sigma T^{-\frac{1}{2}})$. Hence, the bounds are order-optimal, and establish that there is no adaptivity gap in terms of scaling laws.

</details>

<details>

<summary>2022-02-22 02:51:11 - Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations</summary>

- *Alexander Immer, Tycho F. A. van der Ouderaa, Vincent Fortuin, Gunnar Rtsch, Mark van der Wilk*

- `2202.10638v1` - [abs](http://arxiv.org/abs/2202.10638v1) - [pdf](http://arxiv.org/pdf/2202.10638v1)

> Data augmentation is commonly applied to improve performance of deep learning by enforcing the knowledge that certain transformations on the input preserve the output. Currently, the correct data augmentation is chosen by human effort and costly cross-validation, which makes it cumbersome to apply to new datasets. We develop a convenient gradient-based method for selecting the data augmentation. Our approach relies on phrasing data augmentation as an invariance in the prior distribution and learning it using Bayesian model selection, which has been shown to work in Gaussian processes, but not yet for deep neural networks. We use a differentiable Kronecker-factored Laplace approximation to the marginal likelihood as our objective, which can be optimised without human supervision or validation data. We show that our method can successfully recover invariances present in the data, and that this improves generalisation on image datasets.

</details>

<details>

<summary>2022-02-22 03:45:13 - Reconstruction on Trees and Low-Degree Polynomials</summary>

- *Frederic Koehler, Elchanan Mossel*

- `2109.06915v2` - [abs](http://arxiv.org/abs/2109.06915v2) - [pdf](http://arxiv.org/pdf/2109.06915v2)

> The study of Markov processes and broadcasting on trees has deep connections to a variety of areas including statistical physics, graphical models, phylogenetic reconstruction, MCMC algorithms, and community detection in random graphs. Notably, the celebrated Belief Propagation (BP) algorithm achieves optimal performance for the reconstruction problem of predicting the value of the Markov process at the root of the tree from its values at the leaves.   Recently, the analysis of low-degree polynomials has emerged as a valuable tool for predicting computational-to-statistical gaps. In this work, we investigate the performance of low-degree polynomials for the reconstruction problem. Perhaps surprisingly, we show that there are simple tree models of fixed arity $d$ and growing depth $\ell$ (so $N = 2^{\ell \log_2(d)}$ leaves) where (1) nontrivial reconstruction of the root value is possible with a simple polynomial time algorithm and with robustness to noise, but not with any polynomial of degree $2^{c \ell} = N^{c/\log_2(d)}$ for $c > 0$ a constant, and (2) when the tree is unknown and given multiple samples with correlated root assignments, nontrivial reconstruction of the root value is possible with a simple, noise-robust, and computationally efficient SQ algorithm but not with any polynomial of degree $2^{c \ell}$. These results clarify limitations of low-degree polynomials vs. polynomial time algorithms for Bayesian estimation problems. They also complement recent work of Moitra, Mossel, and Sandon who studied the circuit complexity of Belief Propagation. As a consequence of our main result, we show that for some $c' > 0$, $\exp(2^{c'\ell}) = \exp(N^{c'/\log_2(d)})$ many samples are needed for RBF kernel regression to obtain nontrivial correlation with the true regression function (BP). We pose related open questions about low-degree polynomials and the Kesten-Stigum threshold.

</details>

<details>

<summary>2022-02-22 09:59:20 - A Cramr Distance perspective on Quantile Regression based Distributional Reinforcement Learning</summary>

- *Alix Lhritier, Nicolas Bondoux*

- `2110.00535v2` - [abs](http://arxiv.org/abs/2110.00535v2) - [pdf](http://arxiv.org/pdf/2110.00535v2)

> Distributional reinforcement learning (DRL) extends the value-based approach by approximating the full distribution over future returns instead of the mean only, providing a richer signal that leads to improved performances. Quantile Regression (QR) based methods like QR-DQN project arbitrary distributions into a parametric subset of staircase distributions by minimizing the 1-Wasserstein distance. However, due to biases in the gradients, the quantile regression loss is used instead for training, guaranteeing the same minimizer and enjoying unbiased gradients. Non-crossing constraints on the quantiles have been shown to improve the performance of QR-DQN for uncertainty-based exploration strategies. The contribution of this work is in the setting of fixed quantile levels and is twofold. First, we prove that the Cram\'er distance yields a projection that coincides with the 1-Wasserstein one and that, under non-crossing constraints, the squared Cram\'er and the quantile regression losses yield collinear gradients, shedding light on the connection between these important elements of DRL. Second, we propose a low complexity algorithm to compute the Cram\'er distance.

</details>

<details>

<summary>2022-02-22 10:14:51 - Linear Regression, Covariate Selection and the Failure of Modelling</summary>

- *Laurie Davies*

- `2112.08738v4` - [abs](http://arxiv.org/abs/2112.08738v4) - [pdf](http://arxiv.org/pdf/2112.08738v4)

> It is argued that all model based approaches to the selection of covariates in linear regression have failed. This applies to frequentist approaches based on P-values and to Bayesian approaches although for different reasons. In the first part of the paper 13 model based procedures are compared to the model-free Gaussian covariate procedure in terms of the covariates selected and the time required. The comparison is based on seven data sets and three simulations. There is nothing special about these data sets which are often used as examples in the literature. All the model based procedures failed.   In the second part of the paper it is argued that the cause of this failure is the very use of a model. If the model involves all the available covariates standard P-values can be used. The use of P-values in this situation is quite straightforward. As soon as the model specifies only some unknown subset of the covariates the problem being to identify this subset the situation changes radically. There are many P-values, they are dependent and most of them are invalid. The P-value based approach collapses. The Bayesian paradigm also assumes a correct model but although there are no conceptual problems with a large number of covariates there is a considerable overhead causing computational and allocation problems even for moderately sized data sets.   The Gaussian covariate procedure is based on P-values which are defined as the probability that a random Gaussian covariate is better than the covariate being considered. These P-values are exact and valid whatever the situation. The allocation requirements and the algorithmic complexity are both linear in the size of the data making the procedure capable of handling large data sets. It outperforms all the other procedures in every respect.

</details>

<details>

<summary>2022-02-22 10:38:25 - A Bayesian Hierarchical Time Series Model for Reconstructing Hydroclimate from Multiple Proxies</summary>

- *Niamh Cahill, Jacky Croke, Micheline Campbell, Kate Hughes, John Vitkovsky, Jack Eaton Kilgallen, Andrew Parnell*

- `2202.09383v2` - [abs](http://arxiv.org/abs/2202.09383v2) - [pdf](http://arxiv.org/pdf/2202.09383v2)

> We propose a Bayesian model which produces probabilistic reconstructions of hydroclimatic variability in Queensland Australia. The approach uses instrumental records of hydroclimate indices such as rain and evaporation, as well as palaeoclimate proxy records derived from natural archives such as sediment cores, speleothems, ice cores and tree rings. The method provides a standardised approach to using multiple palaeoclimate proxy records for hydroclimate reconstruction. Our approach combines time series modelling with an inverse prediction approach to quantify the relationships between the hydroclimate and proxies over the instrumental period and subsequently reconstruct the hydroclimate back through time. Further analysis of the model output allows us to estimate the probability that a hydroclimate index in any reconstruction year was lower (higher) than the minimum (maximum) hydroclimate value observed over the instrumental period. We present model-based reconstructions of the Rainfall Index (RFI) and Standardised Precipitation-Evapotranspiration Index (SPEI) for two case study catchment areas, namely Brisbane and Fitzroy. In Brisbane, we found that the RFI is unlikely (probability between 0 and 20%) to have exhibited extremes beyond the minimum/maximum of what has been observed between 1889 and 2017. However, in Fitzroy there are several years during the reconstruction period where the RFI is likely (> 50% probability) to have exhibited behaviour beyond the minimum/maximum of what has been observed. For SPEI, the probability of observing such extremes since the end of the instrumental period in 1889 doesn't exceed 50% in any reconstruction year in the Brisbane or Fitzroy catchments.

</details>

<details>

<summary>2022-02-22 12:24:32 - VCBART: Bayesian trees for varying coefficients</summary>

- *Sameer K. Deshpande, Ray Bai, Cecilia Balocchi, Jennifer E. Starling, Jordan Weiss*

- `2003.06416v5` - [abs](http://arxiv.org/abs/2003.06416v5) - [pdf](http://arxiv.org/pdf/2003.06416v5)

> Many studies have reported associations between later-life cognition and socioeconomic position in childhood, young adulthood, and mid-life. However, the vast majority of these studies are unable to quantify how these associations vary over time and with respect to several demographic factors. Varying coefficient (VC) models, which treat the covariate effects in a linear model as nonparametric functions of additional effect modifiers, offer an appealing way to overcome these limitations. Unfortunately, state-of-the-art VC modeling methods require computationally prohibitive parameter tuning or make restrictive assumptions about the functional form of the covariate effects.   In response, we propose VCBART, which estimates the covariate effects in a VC model using Bayesian Additive Regression Trees. With simple default hyperparameter settings, VCBART outperforms existing methods in terms of covariate effect estimation and prediction. Using VCBART, we predict the cognitive trajectories of 4,167 subjects from the Health and Retirement Study using multiple measures of socioeconomic position and physical health. We find that socioeconomic position in childhood and young adulthood have small effects that do not vary with age. In contrast, the effects of measures of mid-life physical health tend to vary with respect to age, race, and marital status. An R package implementing VCBART is available at https://github.com/skdeshpande91/VCBART

</details>

<details>

<summary>2022-02-22 13:32:47 - Information Design in Concave Games</summary>

- *Alex Smolin, Takuro Yamashita*

- `2202.10883v1` - [abs](http://arxiv.org/abs/2202.10883v1) - [pdf](http://arxiv.org/pdf/2202.10883v1)

> We study information design in games with a continuum of actions such that the players' payoffs are concave in their own actions. A designer chooses an information structure--a joint distribution of a state and a private signal of each player. The information structure induces a Bayesian game and is evaluated according to the expected designer's payoff under the equilibrium play.   We develop a method that facilitates the search for an optimal information structure, i.e., one that cannot be outperformed by any other information structure, however complex. We show an information structure is optimal whenever it induces the strategies that can be implemented by an incentive contract in a dual, principal-agent problem which aggregates marginal payoffs of the players in the original game. We use this result to establish the optimality of Gaussian information structures in settings with quadratic payoffs and a multivariate normally distributed state. We analyze the details of optimal structures in a differentiated Bertrand competition and in a prediction game.

</details>

<details>

<summary>2022-02-22 13:46:57 - Trimmed Harrell-Davis quantile estimator based on the highest density interval of the given width</summary>

- *Andrey Akinshin*

- `2111.11776v3` - [abs](http://arxiv.org/abs/2111.11776v3) - [pdf](http://arxiv.org/pdf/2111.11776v3)

> Traditional quantile estimators that are based on one or two order statistics are a common way to estimate distribution quantiles based on the given samples. These estimators are robust, but their statistical efficiency is not always good enough. A more efficient alternative is the Harrell-Davis quantile estimator which uses a weighted sum of all order statistics. Whereas this approach provides more accurate estimations for the light-tailed distributions, it's not robust. To be able to customize the trade-off between statistical efficiency and robustness, we could consider a trimmed modification of the Harrell-Davis quantile estimator. In this approach, we discard order statistics with low weights according to the highest density interval of the beta distribution.

</details>

<details>

<summary>2022-02-22 14:55:00 - Bag of DAGs: Flexible Nonstationary Modeling of Spatiotemporal Dependence</summary>

- *Bora Jin, Michele Peruzzi, David B. Dunson*

- `2112.11870v2` - [abs](http://arxiv.org/abs/2112.11870v2) - [pdf](http://arxiv.org/pdf/2112.11870v2)

> We propose a class of nonstationary processes that characterize varying directional associations in space and time for point-referenced data. Our construction is based on local mixtures of sparse directed acyclic graphs (DAGs). In stochastically choosing DAG edges from a "bag," we account for uncertainty in directional correlation patterns across a domain. The resulting "bag of DAGs" processes (BAGs) lead to interpretable nonstationarity and scalability for large data due to sparsity of all DAGs in the bag. We are motivated by spatiotemporal modeling of air pollutants in which a directed edge in a DAG represents a prevailing wind direction causing some associated covariance in the pollutants. We outline Bayesian hierarchical models embedding the resulting nonstationary BAGs and illustrate inferential and performance gains of our methods compared to existing alternatives. We analyze fine particulate matter (PM2.5) in California with high-resolution data from low-cost air quality sensors. The code for all analyses is publicly available at https://github.com/jinbora0720/BAG.

</details>

<details>

<summary>2022-02-22 17:53:01 - Generalised bayesian sample copula of order $m$</summary>

- *Luis E. Nieto-Barajas, Ricardo Hoyos-Argelles*

- `2202.11069v1` - [abs](http://arxiv.org/abs/2202.11069v1) - [pdf](http://arxiv.org/pdf/2202.11069v1)

> In this work we propose a semiparametric bivariate copula whose density is defined by a picewise constant function on disjoint squares. We obtain the maximum likelihood estimators which reduce to the sample copula under specific conditions. We carry out a full Bayesian analysis of the model and propose a spatial dependent prior distribution for the model parameters. This prior allows the parameters to borrow strength across neighbouring regions to produce smooth posterior estimates. We implement a simulation study and illustrate the performance of our model with a real dataset.

</details>

<details>

<summary>2022-02-22 18:30:21 - A gradient-free subspace-adjusting ensemble sampler for infinite-dimensional Bayesian inverse problems</summary>

- *Matthew M. Dunlop, Georg Stadler*

- `2202.11088v1` - [abs](http://arxiv.org/abs/2202.11088v1) - [pdf](http://arxiv.org/pdf/2202.11088v1)

> Sampling of sharp posteriors in high dimensions is a challenging problem, especially when gradients of the likelihood are unavailable. In low to moderate dimensions, affine-invariant methods, a class of ensemble-based gradient-free methods, have found success in sampling concentrated posteriors. However, the number of ensemble members must exceed the dimension of the unknown state in order for the correct distribution to be targeted. Conversely, the preconditioned Crank-Nicolson (pCN) algorithm succeeds at sampling in high dimensions, but samples become highly correlated when the posterior differs significantly from the prior. In this article we combine the above methods in two different ways as an attempt to find a compromise. The first method involves inflating the proposal covariance in pCN with that of the current ensemble, whilst the second performs approximately affine-invariant steps on a continually adapting low-dimensional subspace, while using pCN on its orthogonal complement.

</details>

<details>

<summary>2022-02-22 19:02:11 - Testing with p*-values: Between p-values, mid p-values, and e-values</summary>

- *Ruodu Wang*

- `2010.14010v4` - [abs](http://arxiv.org/abs/2010.14010v4) - [pdf](http://arxiv.org/pdf/2010.14010v4)

> We introduce the notion of p*-values (p*-variables), which generalizes p-values (p-variables) in several senses. The new notion has four natural interpretations: operational, probabilistic, Bayesian, and frequentist. A main example of a p*-value is a mid p-value, which arises in the presence of discrete test statistics. A unified stochastic representation for p-values, mid p-values, and p*-values is obtained to illustrate the relationship between the three objects. We study several ways of merging arbitrarily dependent or independent p*-values into one p-value or p*-value. Admissible calibrators of p*-values to and from p-values and e-values are obtained with nice mathematical forms, revealing the role of p*-values as a bridge between p-values and e-values. The notion of p*-values becomes useful in many situations even if one is only interested in p-values, mid p-values, or e-values. In particular, deterministic tests based on p*-values can be applied to improve some classic methods for p-values and e-values.

</details>

<details>

<summary>2022-02-22 22:04:53 - An open-source integrated framework for the automation of citation collection and screening in systematic reviews</summary>

- *Angelo D'Ambrosio, Hajo Grundmann, Tjibbe Donker*

- `2202.10033v2` - [abs](http://arxiv.org/abs/2202.10033v2) - [pdf](http://arxiv.org/pdf/2202.10033v2)

> The exponential growth of scientific production makes secondary literature abridgements increasingly demanding. We introduce a new open-source framework for systematic reviews that significantly reduces time and workload for collecting and screening scientific literature. The framework provides three main tools: 1) an automatic citation search engine and manager that collects records from multiple online sources with a unified query syntax, 2) a Bayesian, active machine learning, citation screening tool based on iterative human-machine interaction to increase predictive accuracy and, 3) a semi-automatic, data-driven query generator to create new search queries from existing citation data sets. To evaluate the automatic screener's performance, we estimated the median posterior sensitivity and efficiency [90% Credible Intervals] using Bayesian simulation to predict the distribution of undetected potentially relevant records. Tested on an example topic, the framework collected 17,755 unique records through the citation manager; 766 records required human evaluation while the rest were excluded by the automatic classifier; the theoretical efficiency was 95.6% [95.3%, 95.7%] with a sensitivity of 100% [93.5%, 100%]. A new search query was generated from the labelled dataset, and 82,579 additional records were collected; only 567 records required human review after automatic screening, and six additional positive matches were found. The overall expected sensitivity decreased to 97.3% [73.8%, 100%] while the efficiency increased to 98.6% [98.2%, 98.7%]. The framework can significantly reduce the workload required to conduct large literature reviews by simplifying citation collection and screening while demonstrating exceptional sensitivity. Such a tool can improve the standardization and repeatability of systematic reviews.

</details>

<details>

<summary>2022-02-23 03:49:53 - Random weighting in LASSO regression</summary>

- *Tun Lee Ng, Michael A. Newton*

- `2002.02629v4` - [abs](http://arxiv.org/abs/2002.02629v4) - [pdf](http://arxiv.org/pdf/2002.02629v4)

> We establish statistical properties of random-weighting methods in LASSO regression under different regularization parameters $\lambda_n$ and suitable regularity conditions. The random-weighting methods in view concern repeated optimization of a randomized objective function, motivated by the need for computational approximations to Bayesian posterior sampling. In the context of LASSO regression, we repeatedly assign analyst-drawn random weights to terms in the objective function (including the penalty terms), and optimize to obtain a sample of random-weighting estimators. We show that existing approaches have conditional model selection consistency and conditional asymptotic normality at different growth rates of $\lambda_n$ as $n \to \infty$. We propose an extension to the available random-weighting methods and establish that the resulting samples attain conditional sparse normality and conditional consistency in a growing-dimension setting. We find that random-weighting has both approximate-Bayesian and sampling-theory interpretations. Finally, we illustrate the proposed methodology via extensive simulation studies and a benchmark data example.

</details>

<details>

<summary>2022-02-23 05:17:24 - Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting</summary>

- *Youngsuk Park, Danielle Maddix, Franois-Xavier Aubet, Kelvin Kan, Jan Gasthaus, Yuyang Wang*

- `2111.06581v2` - [abs](http://arxiv.org/abs/2111.06581v2) - [pdf](http://arxiv.org/pdf/2111.06581v2)

> Quantile regression is an effective technique to quantify uncertainty, fit challenging underlying distributions, and often provide full probabilistic predictions through joint learnings over multiple quantile levels. A common drawback of these joint quantile regressions, however, is \textit{quantile crossing}, which violates the desirable monotone property of the conditional quantile function. In this work, we propose the Incremental (Spline) Quantile Functions I(S)QF, a flexible and efficient distribution-free quantile estimation framework that resolves quantile crossing with a simple neural network layer. Moreover, I(S)QF inter/extrapolate to predict arbitrary quantile levels that differ from the underlying training ones. Equipped with the analytical evaluation of the continuous ranked probability score of I(S)QF representations, we apply our methods to NN-based times series forecasting cases, where the savings of the expensive re-training costs for non-trained quantile levels is particularly significant. We also provide a generalization error analysis of our proposed approaches under the sequence-to-sequence setting. Lastly, extensive experiments demonstrate the improvement of consistency and accuracy errors over other baselines.

</details>

<details>

<summary>2022-02-23 05:22:03 - Multivariate Quantile Function Forecaster</summary>

- *Kelvin Kan, Franois-Xavier Aubet, Tim Januschowski, Youngsuk Park, Konstantinos Benidis, Lars Ruthotto, Jan Gasthaus*

- `2202.11316v1` - [abs](http://arxiv.org/abs/2202.11316v1) - [pdf](http://arxiv.org/pdf/2202.11316v1)

> We propose Multivariate Quantile Function Forecaster (MQF$^2$), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF$^2$ combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF$^2$: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.

</details>

<details>

<summary>2022-02-23 06:43:56 - Empirical distributions of the robustified $t$-test statistics</summary>

- *Chanseok Park, Min Wang*

- `1807.02215v2` - [abs](http://arxiv.org/abs/1807.02215v2) - [pdf](http://arxiv.org/pdf/1807.02215v2)

> Based on the median and the median absolute deviation estimators, and the Hodges-Lehmann and Shamos estimators, robustified analogues of the conventional $t$-test statistic are proposed. The asymptotic distributions of these statistics are recently provided. However, when the sample size is small, it is not appropriate to use the asymptotic distribution of the robustified $t$-test statistics for making a statistical inference including hypothesis testing, confidence interval, p-value, etc.   In this article, through extensive Monte Carlo simulations, we obtain the empirical distributions of the robustified $t$-test statistics and their quantile values. Then these quantile values can be used for making a statistical inference.

</details>

<details>

<summary>2022-02-23 10:23:43 - Transformers Can Do Bayesian Inference</summary>

- *Samuel Mller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, Frank Hutter*

- `2112.10510v4` - [abs](http://arxiv.org/abs/2112.10510v4) - [pdf](http://arxiv.org/pdf/2112.10510v4)

> Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github.com/automl/TransformersCanDoBayesianInference.

</details>

<details>

<summary>2022-02-23 10:58:39 - Social Learning in Non-Stationary Environments</summary>

- *Etienne Boursier, Vianney Perchet, Marco Scarsini*

- `2007.09996v3` - [abs](http://arxiv.org/abs/2007.09996v3) - [pdf](http://arxiv.org/pdf/2007.09996v3)

> Potential buyers of a product or service, before making their decisions, tend to read reviews written by previous consumers. We consider Bayesian consumers with heterogeneous preferences, who sequentially decide whether to buy an item of unknown quality, based on previous buyers' reviews. The quality is multi-dimensional and may occasionally vary over time; the reviews are also multi-dimensional. In the simple uni-dimensional and static setting, beliefs about the quality are known to converge to its true value. Our paper extends this result in several ways. First, a multi-dimensional quality is considered, second, rates of convergence are provided, third, a dynamical Markovian model with varying quality is studied. In this dynamical setting the cost of learning is shown to be small.

</details>

<details>

<summary>2022-02-23 11:50:05 - On Margins and Derandomisation in PAC-Bayes</summary>

- *Felix Biggs, Benjamin Guedj*

- `2107.03955v3` - [abs](http://arxiv.org/abs/2107.03955v3) - [pdf](http://arxiv.org/pdf/2107.03955v3)

> We give a general recipe for derandomising PAC-Bayesian bounds using margins, with the critical ingredient being that our randomised predictions concentrate around some value. The tools we develop straightforwardly lead to margin bounds for various classifiers, including linear prediction -- a class that includes boosting and the support vector machine -- single-hidden-layer neural networks with an unusual \(\erf\) activation function, and deep ReLU networks. Further, we extend to partially-derandomised predictors where only some of the randomness is removed, letting us extend bounds to cases where the concentration properties of our predictors are otherwise poor.

</details>

<details>

<summary>2022-02-23 12:11:05 - On PAC-Bayesian reconstruction guarantees for VAEs</summary>

- *Badr-Eddine Chrief-Abdellatif, Yuyang Shi, Arnaud Doucet, Benjamin Guedj*

- `2202.11455v1` - [abs](http://arxiv.org/abs/2202.11455v1) - [pdf](http://arxiv.org/pdf/2202.11455v1)

> Despite its wide use and empirical successes, the theoretical understanding and study of the behaviour and performance of the variational autoencoder (VAE) have only emerged in the past few years. We contribute to this recent line of work by analysing the VAE's reconstruction ability for unseen test data, leveraging arguments from the PAC-Bayes theory. We provide generalisation bounds on the theoretical reconstruction error, and provide insights on the regularisation effect of VAE objectives. We illustrate our theoretical results with supporting experiments on classical benchmark datasets.

</details>

<details>

<summary>2022-02-23 13:37:46 - A bias-adjusted estimator in quantile regression for clustered data</summary>

- *Maria Laura Battagliola, Helle Srensen, Anders Tolver, Ana-Maria Staicu*

- `2202.11501v1` - [abs](http://arxiv.org/abs/2202.11501v1) - [pdf](http://arxiv.org/pdf/2202.11501v1)

> The manuscript discusses how to incorporate random effects for quantile regression models for clustered data with focus on settings with many but small clusters. The paper has three contributions: (i) documenting that existing methods may lead to severely biased estimators for fixed effects parameters; (ii) proposing a new two-step estimation methodology where predictions of the random effects are first computed {by a pseudo likelihood approach (the LQMM method)} and then used as offsets in standard quantile regression; (iii) proposing a novel bootstrap sampling procedure in order to reduce bias of the two-step estimator and compute confidence intervals. The proposed estimation and associated inference is assessed numerically through rigorous simulation studies and applied to an AIDS Clinical Trial Group (ACTG) study.

</details>

<details>

<summary>2022-02-23 14:00:08 - Curiosity-Driven Exploration via Latent Bayesian Surprise</summary>

- *Pietro Mazzaglia, Ozan Catal, Tim Verbelen, Bart Dhoedt*

- `2104.07495v2` - [abs](http://arxiv.org/abs/2104.07495v2) - [pdf](http://arxiv.org/pdf/2104.07495v2)

> The human intrinsic desire to pursue knowledge, also known as curiosity, is considered essential in the process of skill acquisition. With the aid of artificial curiosity, we could equip current techniques for control, such as Reinforcement Learning, with more natural exploration capabilities. A promising approach in this respect has consisted of using Bayesian surprise on model parameters, i.e. a metric for the difference between prior and posterior beliefs, to favour exploration. In this contribution, we propose to apply Bayesian surprise in a latent space representing the agent's current understanding of the dynamics of the system, drastically reducing the computational costs. We extensively evaluate our method by measuring the agent's performance in terms of environment exploration, for continuous tasks, and looking at the game scores achieved, for video games. Our model is computationally cheap and compares positively with current state-of-the-art methods on several problems. We also investigate the effects caused by stochasticity in the environment, which is often a failure case for curiosity-driven agents. In this regime, the results suggest that our approach is resilient to stochastic transitions.

</details>

<details>

<summary>2022-02-23 15:12:15 - Single Gaussian Process Method for Arbitrary Tokamak Regimes with a Statistical Analysis</summary>

- *Jarrod Leddy, Sandeep Madireddy, Eric Howell, Scott Kruger*

- `2202.11557v1` - [abs](http://arxiv.org/abs/2202.11557v1) - [pdf](http://arxiv.org/pdf/2202.11557v1)

> Gaussian Process Regression (GPR) is a Bayesian method for inferring profiles based on input data. The technique is increasing in popularity in the fusion community due to its many advantages over traditional fitting techniques including intrinsic uncertainty quantification and robustness to over-fitting. This work investigates the use of a new method, the change-point method, for handling the varying length scales found in different tokamak regimes. The use of the Student's t-distribution for the Bayesian likelihood probability is also investigated and shown to be advantageous in providing good fits in profiles with many outliers. To compare different methods, synthetic data generated from analytic profiles is used to create a database enabling a quantitative statistical comparison of which methods perform the best. Using a full Bayesian approach with the change-point method, Mat\'ern kernel for the prior probability, and Student's t-distribution for the likelihood is shown to give the best results.

</details>

<details>

<summary>2022-02-23 16:22:28 - A Dimensionality Reduction Method for Finding Least Favorable Priors with a Focus on Bregman Divergence</summary>

- *Alex Dytso, Mario Goldenbaum, H. Vincent Poor, Shlomo Shamai*

- `2202.11598v1` - [abs](http://arxiv.org/abs/2202.11598v1) - [pdf](http://arxiv.org/pdf/2202.11598v1)

> A common way of characterizing minimax estimators in point estimation is by moving the problem into the Bayesian estimation domain and finding a least favorable prior distribution. The Bayesian estimator induced by a least favorable prior, under mild conditions, is then known to be minimax. However, finding least favorable distributions can be challenging due to inherent optimization over the space of probability distributions, which is infinite-dimensional. This paper develops a dimensionality reduction method that allows us to move the optimization to a finite-dimensional setting with an explicit bound on the dimension. The benefit of this dimensionality reduction is that it permits the use of popular algorithms such as projected gradient ascent to find least favorable priors. Throughout the paper, in order to make progress on the problem, we restrict ourselves to Bayesian risks induced by a relatively large class of loss functions, namely Bregman divergences.

</details>

<details>

<summary>2022-02-23 16:59:53 - Functional delta residuals and applications to simultaneous confidence bands of moment based statistics</summary>

- *Fabian J. E. Telschow, Samuel Davenport, Armin Schwartzman*

- `2005.10041v3` - [abs](http://arxiv.org/abs/2005.10041v3) - [pdf](http://arxiv.org/pdf/2005.10041v3)

> Given a functional central limit (fCLT) for an estimator and a parameter transformation, we construct random processes, called functional delta residuals, which asymptotically have the same covariance structure as the limit process of the functional delta method. An explicit construction of these residuals for transformations of moment-based estimators and a multiplier bootstrap fCLT for the resulting functional delta residuals are proven. The latter is used to consistently estimate the quantiles of the maximum of the limit process of the functional delta method in order to construct asymptotically valid simultaneous confidence bands for the transformed functional parameters. Performance of the coverage rate of the developed construction, applied to functional versions of Cohen's d, skewness and kurtosis, is illustrated in simulations and their application to test Gaussianity is discussed.

</details>

<details>

<summary>2022-02-23 17:31:42 - Cyclical Variational Bayes Monte Carlo for Efficient Multi-Modal Posterior Distributions Evaluation</summary>

- *Felipe Igea, Alice Cicirello*

- `2202.11645v1` - [abs](http://arxiv.org/abs/2202.11645v1) - [pdf](http://arxiv.org/pdf/2202.11645v1)

> Statistical model updating is frequently used in engineering to calculate the uncertainty of some unknown latent parameters when a set of measurements on observable quantities is given. Variational inference is an alternative approach to sampling methods that has been developed by the machine learning community to estimate posterior approximations through an optimization approach. In this paper, the Variational Bayesian Monte Carlo (VBMC) method is investigated with the purpose of dealing with statistical model updating problems in engineering involving expensive-to-run models. This method combines the active-sampling Bayesian quadrature with a Gaussian-process based variational inference to yield a non-parametric estimation of the posterior distribution of the identified parameters involving few runs of the expensive-to-run model. VBMC can also be used for model selection as it produces an estimation of the model's evidence lower bound. In this paper, a variant of the VBMC algorithm is developed through the introduction of a cyclical annealing schedule into the algorithm. The proposed cyclical VBMC algorithm allows to deal effectively with multi-modal posteriors by having multiple cycles of exploration and exploitation phases. Four numerical examples are used to compare the standard VBMC algorithm, the monotonic VBMC, the cyclical VBMC and the Transitional Ensemble Markov Chain Monte Carlo (TEMCMC). Overall, it is found that the proposed cyclical VBMC approach yields accurate results with a very reduced number of model runs compared to the state of the art sampling technique TEMCMC. In the presence of potential multi-modal problems, the proposed cyclical VBMC algorithm outperforms all the other approaches in terms of accuracy of the resulting posterior.

</details>

<details>

<summary>2022-02-23 18:21:50 - Wide Mean-Field Bayesian Neural Networks Ignore the Data</summary>

- *Beau Coker, Wessel P. Bruinsma, David R. Burt, Weiwei Pan, Finale Doshi-Velez*

- `2202.11670v1` - [abs](http://arxiv.org/abs/2202.11670v1) - [pdf](http://arxiv.org/pdf/2202.11670v1)

> Bayesian neural networks (BNNs) combine the expressive power of deep learning with the advantages of Bayesian formalism. In recent years, the analysis of wide, deep BNNs has provided theoretical insight into their priors and posteriors. However, we have no analogous insight into their posteriors under approximate inference. In this work, we show that mean-field variational inference entirely fails to model the data when the network width is large and the activation function is odd. Specifically, for fully-connected BNNs with odd activation functions and a homoscedastic Gaussian likelihood, we show that the optimal mean-field variational posterior predictive (i.e., function space) distribution converges to the prior predictive distribution as the width tends to infinity. We generalize aspects of this result to other likelihoods. Our theoretical results are suggestive of underfitting behavior previously observered in BNNs. While our convergence bounds are non-asymptotic and constants in our analysis can be computed, they are currently too loose to be applicable in standard training regimes. Finally, we show that the optimal approximate posterior need not tend to the prior if the activation function is not odd, showing that our statements cannot be generalized arbitrarily.

</details>

<details>

<summary>2022-02-23 18:22:56 - Distributional Counterfactual Analysis in High-Dimensional Setup</summary>

- *Ricardo Masini*

- `2202.11671v1` - [abs](http://arxiv.org/abs/2202.11671v1) - [pdf](http://arxiv.org/pdf/2202.11671v1)

> In the context of treatment effect estimation, this paper proposes a new methodology to recover the counterfactual distribution when there is a single (or a few) treated unit and possibly a high-dimensional number of potential controls observed in a panel structure. The methodology accommodates, albeit does not require, the number of units to be larger than the number of time periods (high-dimensional setup). As opposed to model only the conditional mean, we propose to model the entire conditional quantile function (CQF) in the absence of intervention and estimate it using the pre-intervention period using a penalized regression. We derive non-asymptotic bounds for the estimated CQF valid uniformly over the quantiles, allowing the practitioner to re-construct the entire contractual distribution. Moreover, we bound the probability coverage of this estimated CQF which can be used to construct valid confidence intervals for the (possibly random) treatment effect for every post-intervention period or simultaneously. We also propose a new hypothesis test for the sharp null of no-effect based on the $\mathcal{L}^p$ norm of deviation of the estimated CQF to the population one. Interestingly, the null distribution is quasi-pivotal in the sense that it only depends on the estimated CQF, $\mathcal{L}^p$ norm, and the number of post-intervention periods, but not on the size of the post-intervention period. For that reason, critical values can then be easily simulated. We illustrate the methodology is by revisiting the empirical study in Acemoglu et al (2016).

</details>

<details>

<summary>2022-02-23 19:17:06 - Statistical Inference for Functional Linear Quantile Regression</summary>

- *Peijun Sang, Zuofeng Shang, Pang Du*

- `2202.11747v1` - [abs](http://arxiv.org/abs/2202.11747v1) - [pdf](http://arxiv.org/pdf/2202.11747v1)

> We propose inferential tools for functional linear quantile regression where the conditional quantile of a scalar response is assumed to be a linear functional of a functional covariate. In contrast to conventional approaches, we employ kernel convolution to smooth the original loss function. The coefficient function is estimated under a reproducing kernel Hilbert space framework. A gradient descent algorithm is designed to minimize the smoothed loss function with a roughness penalty. With the aid of the Banach fixed-point theorem, we show the existence and uniqueness of our proposed estimator as the minimizer of the regularized loss function in an appropriate Hilbert space. Furthermore, we establish the convergence rate as well as the weak convergence of our estimator. As far as we know, this is the first weak convergence result for a functional quantile regression model. Pointwise confidence intervals and a simultaneous confidence band for the true coefficient function are then developed based on these theoretical properties. Numerical studies including both simulations and a data application are conducted to investigate the performance of our estimator and inference tools in finite sample.

</details>

<details>

<summary>2022-02-23 19:51:47 - Robust Bayesian Inference for Simulator-based Models via the MMD Posterior Bootstrap</summary>

- *Charita Dellaporta, Jeremias Knoblauch, Theodoros Damoulas, Franois-Xavier Briol*

- `2202.04744v2` - [abs](http://arxiv.org/abs/2202.04744v2) - [pdf](http://arxiv.org/pdf/2202.04744v2)

> Simulator-based models are models for which the likelihood is intractable but simulation of synthetic data is possible. They are often used to describe complex real-world phenomena, and as such can often be misspecified in practice. Unfortunately, existing Bayesian approaches for simulators are known to perform poorly in those cases. In this paper, we propose a novel algorithm based on the posterior bootstrap and maximum mean discrepancy estimators. This leads to a highly-parallelisable Bayesian inference algorithm with strong robustness properties. This is demonstrated through an in-depth theoretical study which includes generalisation bounds and proofs of frequentist consistency and robustness of our posterior. The approach is then assessed on a range of examples including a g-and-k distribution and a toggle-switch model.

</details>

<details>

<summary>2022-02-23 20:47:22 - baker: An R package for Nested Partially-Latent Class Models</summary>

- *Irena B Chen, Qiyuan Shi, Scott L Zeger, Zhenke Wu*

- `2202.11778v1` - [abs](http://arxiv.org/abs/2202.11778v1) - [pdf](http://arxiv.org/pdf/2202.11778v1)

> This paper describes and illustrates the functionality of the baker R package. The package estimates a suite of nested partially-latent class models (NPLCM) for multivariate binary responses that are observed under a case-control design. The baker package allows researchers to flexibly estimate population-level class prevalences and posterior probabilities of class membership for individual cases. Estimation is accomplished by calling a cross-platform automatic Bayesian inference software JAGS through a wrapper R function that parses model specifications and data inputs. The baker package provides many useful features, including data ingestion, exploratory data analyses, model diagnostics, extensive plotting and visualization options, catalyzing communications between practitioners and domain scientists. Package features and workflows are illustrated using simulated and real data sets. Package URL: https://github.com/zhenkewu/baker

</details>

<details>

<summary>2022-02-23 20:56:00 - Fast Non-Bayesian Poisson Factorization for Implicit-Feedback Recommendations</summary>

- *David Cortes*

- `1811.01908v5` - [abs](http://arxiv.org/abs/1811.01908v5) - [pdf](http://arxiv.org/pdf/1811.01908v5)

> This work explores non-negative low-rank matrix factorization based on regularized Poisson models (PF or "Poisson factorization" for short) for recommender systems with implicit-feedback data. The properties of Poisson likelihood allow a shortcut for very fast computations over zero-valued inputs, and oftentimes results in very sparse factors for both users and items. Compared to HPF (a popular Bayesian formulation of the problem with hierarchical priors), the frequentist optimization-based approach presented here tends to produce better top-N recommendations with significantly shorter fitting times, on top of having sparse solutions.

</details>

<details>

<summary>2022-02-23 22:48:50 - Cutting feedback and modularized analyses in generalized Bayesian inference</summary>

- *David T. Frazier, David J. Nott*

- `2202.09968v2` - [abs](http://arxiv.org/abs/2202.09968v2) - [pdf](http://arxiv.org/pdf/2202.09968v2)

> Even in relatively simple settings, model misspecification can make the application and interpretation of Bayesian inference difficult. One approach to make Bayesian analyses fit-for-purpose in the presence of model misspecification is the use of cutting feedback methods. These methods modify conventional Bayesian inference by limiting the influence of one part of the model by "cutting" the link between certain components. We examine cutting feedback methods in the context of generalized posterior distributions, i.e., posteriors built from arbitrary loss functions, and provide novel results on their behaviour. A direct product of our results are diagnostic tools that allow for the quick, and easy, analysis of two key features of cut posterior distributions: one, how uncertainty about the model unknowns in one component impacts inferences about unknowns in other components; two, how the incorporation of additional information impacts the cut posterior distribution.

</details>

<details>

<summary>2022-02-24 07:40:57 - Robust Deep Learning from Crowds with Belief Propagation</summary>

- *Hoyoung Kim, Seunghyuk Cho, Dongwoo Kim, Jungseul Ok*

- `2111.00734v2` - [abs](http://arxiv.org/abs/2111.00734v2) - [pdf](http://arxiv.org/pdf/2111.00734v2)

> Crowdsourcing systems enable us to collect large-scale dataset, but inherently suffer from noisy labels of low-paid workers. We address the inference and learning problems using such a crowdsourced dataset with noise. Due to the nature of sparsity in crowdsourcing, it is critical to exploit both probabilistic model to capture worker prior and neural network to extract task feature despite risks from wrong prior and overfitted feature in practice. We hence establish a neural-powered Bayesian framework, from which we devise deepMF and deepBP with different choice of variational approximation methods, mean field (MF) and belief propagation (BP), respectively. This provides a unified view of existing methods, which are special cases of deepMF with different priors. In addition, our empirical study suggests that deepBP is a new approach, which is more robust against wrong prior, feature overfitting and extreme workers thanks to the more sophisticated BP than MF.

</details>

<details>

<summary>2022-02-24 08:40:31 - Robust Real-Time Delay Predictions in a Network of High-Frequency Urban Buses</summary>

- *Hector Rodriguez-Deniz, Mattias Villani*

- `2106.13576v2` - [abs](http://arxiv.org/abs/2106.13576v2) - [pdf](http://arxiv.org/pdf/2106.13576v2)

> Providing transport users and operators with accurate forecasts on travel times is challenging due to a highly stochastic traffic environment. Public transport users are particularly sensitive to unexpected waiting times, which negatively affect their perception on the system's reliability. In this paper we develop a robust model for real-time bus travel time prediction that depart from Gaussian assumptions by using Student-$t$ errors. The proposed approach uses spatiotemporal characteristics from the route and previous bus trips to model short-term effects, and date/time variables and Gaussian processes for long-run forecasts. The model allows for flexible modeling of mean, variance and kurtosis spaces. We propose algorithms for Bayesian inference and for computing probabilistic forecast distributions. Experiments are performed using data from high-frequency buses in Stockholm, Sweden. Results show that Student-$t$ models outperform Gaussian ones in terms of log-posterior predictive power to forecast bus delays at specific stops, which reveals the importance of accounting for predictive uncertainty in model selection. Estimated Student-$t$ regressions capture typical temporal variability between within-day hours and different weekdays. Strong spatiotemporal effects are detected for incoming buses from immediately previous stops, which is in line with many recently developed models. We finally show how Bayesian inference naturally allows for predictive uncertainty quantification, e.g. by returning the predictive probability that the delay of an incoming bus exceeds a given threshold.

</details>

<details>

<summary>2022-02-24 08:49:05 - A Multilayered Block Network Model to Forecast Large Dynamic Transportation Graphs: an Application to US Air Transport</summary>

- *Hector Rodriguez-Deniz, Mattias Villani, Augusto Voltes-Dorta*

- `1911.13136v4` - [abs](http://arxiv.org/abs/1911.13136v4) - [pdf](http://arxiv.org/pdf/1911.13136v4)

> Dynamic transportation networks have been analyzed for years by means of static graph-based indicators in order to study the temporal evolution of relevant network components, and to reveal complex dependencies that would not be easily detected by a direct inspection of the data. This paper presents a state-of-the-art probabilistic latent network model to forecast multilayer dynamic graphs that are increasingly common in transportation and proposes a community-based extension to reduce the computational burden. Flexible time series analysis is obtained by modeling the probability of edges between vertices through latent Gaussian processes. The models and Bayesian inference are illustrated on a sample of 10-year data from four major airlines within the US air transportation system. Results show how the estimated latent parameters from the models are related to the airline's connectivity dynamics, and their ability to project the multilayer graph into the future for out-of-sample full network forecasts, while stochastic blockmodeling allows for the identification of relevant communities. Reliable network predictions would allow policy-makers to better understand the dynamics of the transport system, and help in their planning on e.g. route development, or the deployment of new regulations.

</details>

<details>

<summary>2022-02-24 12:17:50 - Conditionally Gaussian PAC-Bayes</summary>

- *Eugenio Clerico, George Deligiannidis, Arnaud Doucet*

- `2110.11886v2` - [abs](http://arxiv.org/abs/2110.11886v2) - [pdf](http://arxiv.org/pdf/2110.11886v2)

> Recent studies have empirically investigated different methods to train stochastic neural networks on a classification task by optimising a PAC-Bayesian bound via stochastic gradient descent. Most of these procedures need to replace the misclassification error with a surrogate loss, leading to a mismatch between the optimisation objective and the actual generalisation bound. The present paper proposes a novel training algorithm that optimises the PAC-Bayesian bound, without relying on any surrogate loss. Empirical results show that this approach outperforms currently available PAC-Bayesian training methods.

</details>

<details>

<summary>2022-02-24 13:24:50 - Where to Drill Next? A Dual-Weighted Approach to Adaptive Optimal Design of Groundwater Surveys</summary>

- *Mikkel B Lykkegaard, Tim J Dodwell*

- `2111.07670v2` - [abs](http://arxiv.org/abs/2111.07670v2) - [pdf](http://arxiv.org/pdf/2111.07670v2)

> We present a novel approach to adaptive optimal design of groundwater surveys - a methodology for choosing the location of the next monitoring well. Our dual-weighted approach borrows ideas from Bayesian Optimisation and goal-oriented error estimation to propose the next monitoring well, given that some data is already available from existing wells. Our method is distinct from other optimal design strategies in that it does not rely on Fisher Information and it instead directly exploits the posterior uncertainty and the expected solution to a dual (or adjoint) problem to construct an acquisition function that optimally reduces the uncertainty in the model as a whole and some engineering quantity of interest in particular. We demonstrate our approach in the context of 2D groundwater flow example and show that the dual-weighted approach outperforms the baseline approach with respect to reducing the error in the posterior estimate of the quantity of interest.

</details>

<details>

<summary>2022-02-24 13:35:47 - Multivariate Deep Evidential Regression</summary>

- *Nis Meinert, Alexander Lavin*

- `2104.06135v4` - [abs](http://arxiv.org/abs/2104.06135v4) - [pdf](http://arxiv.org/pdf/2104.06135v4)

> There is significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, yet several important gaps in the theory and implementation of these networks remain. We discuss three issues with a proposed solution to extract aleatoric and epistemic uncertainties from regression-based neural networks. The approach derives a technique by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. Doing so allows for the simultaneous extraction of both uncertainties without sampling or utilization of out-of-distribution data for univariate regression tasks. We describe the outstanding issues in detail, provide a possible solution, and generalize the deep evidential regression technique for multivariate cases.

</details>

<details>

<summary>2022-02-24 14:38:27 - Robust random walk-like Metropolis-Hastings algorithms for concentrating posteriors</summary>

- *Daniel Rudolf, Bjrn Sprungk*

- `2202.12127v1` - [abs](http://arxiv.org/abs/2202.12127v1) - [pdf](http://arxiv.org/pdf/2202.12127v1)

> Motivated by Bayesian inference with highly informative data we analyze the performance of random walk-like Metropolis-Hastings algorithms for approximate sampling of increasingly concentrating target distributions. We focus on Gaussian proposals which use a Hessian-based approximation of the target covariance. By means of pushforward transition kernels we show that for Gaussian target measures the spectral gap of the corresponding Metropolis-Hastings algorithm is independent of the concentration of the posterior, i.e., the noise level in the observational data that is used for Bayesian inference. Moreover, by exploiting the convergence of the concentrating posteriors to their Laplace approximation we extend the analysis to non-Gaussian target measures which either concentrate around a single point or along a linear manifold. In particular, in that setting we show that the average acceptance rate as well as the expected squared jump distance of suitable Metropolis-Hastings Markov chains do not deteriorate as the target concentrates.

</details>

<details>

<summary>2022-02-24 15:10:08 - Unconditional Quantile Regression with High Dimensional Data</summary>

- *Yuya Sasaki, Takuya Ura, Yichong Zhang*

- `2007.13659v4` - [abs](http://arxiv.org/abs/2007.13659v4) - [pdf](http://arxiv.org/pdf/2007.13659v4)

> This paper considers estimation and inference for heterogeneous counterfactual effects with high-dimensional data. We propose a novel robust score for debiased estimation of the unconditional quantile regression (Firpo, Fortin, and Lemieux, 2009) as a measure of heterogeneous counterfactual marginal effects. We propose a multiplier bootstrap inference and develop asymptotic theories to guarantee the size control in large sample. Simulation studies support our theories. Applying the proposed method to Job Corps survey data, we find that a policy which counterfactually extends the duration of exposures to the Job Corps training program will be effective especially for the targeted subpopulations of lower potential wage earners.

</details>

<details>

<summary>2022-02-24 18:33:58 - Fast and Scalable Spike and Slab Variable Selection in High-Dimensional Gaussian Processes</summary>

- *Hugh Dance, Brooks Paige*

- `2111.04558v2` - [abs](http://arxiv.org/abs/2111.04558v2) - [pdf](http://arxiv.org/pdf/2111.04558v2)

> Variable selection in Gaussian processes (GPs) is typically undertaken by thresholding the inverse lengthscales of automatic relevance determination kernels, but in high-dimensional datasets this approach can be unreliable. A more probabilistically principled alternative is to use spike and slab priors and infer a posterior probability of variable inclusion. However, existing implementations in GPs are very costly to run in both high-dimensional and large-$n$ datasets, or are only suitable for unsupervised settings with specific kernels. As such, we develop a fast and scalable variational inference algorithm for the spike and slab GP that is tractable with arbitrary differentiable kernels. We improve our algorithm's ability to adapt to the sparsity of relevant variables by Bayesian model averaging over hyperparameters, and achieve substantial speed ups using zero temperature posterior restrictions, dropout pruning and nearest neighbour minibatching. In experiments our method consistently outperforms vanilla and sparse variational GPs whilst retaining similar runtimes (even when $n=10^6$) and performs competitively with a spike and slab GP using MCMC but runs up to $1000$ times faster.

</details>

<details>

<summary>2022-02-24 20:00:27 - Bayesian information criteria for clustering normally distributed data</summary>

- *Anthony J. Webster*

- `2008.03974v6` - [abs](http://arxiv.org/abs/2008.03974v6) - [pdf](http://arxiv.org/pdf/2008.03974v6)

> Maximum likelihood estimates (MLEs) are asymptotically normally distributed, and this property is used in meta-analyses to test the heterogeneity of estimates, either for a single cluster or for several sub-groups. More recently, MLEs for associations between risk factors and diseases have been hierarchically clustered to search for diseases with shared underlying causes, but an objective statistical criterion is needed to determine the number and composition of clusters. To tackle this problem, conventional statistical tests are briefly reviewed, before considering the posterior distribution for a partition of data into clusters. The posterior distribution is calculated by marginalising out the unknown cluster centres, and is different to the likelihood associated with mixture models. The calculation is equivalent to that used to obtain the Bayesian Information Criterion (BIC), but is exact, without a Laplace approximation. The result includes a sum of squares term, and terms that depend on the number and composition of clusters, that penalise the number of free parameters in the model. The usual BIC is shown to be unsuitable for clustering applications unless the number of items in each individual cluster is sufficiently large.

</details>

<details>

<summary>2022-02-24 20:18:41 - Bayesian Deep Learning for Graphs</summary>

- *Federico Errica*

- `2202.12348v1` - [abs](http://arxiv.org/abs/2202.12348v1) - [pdf](http://arxiv.org/pdf/2202.12348v1)

> The adaptive processing of structured data is a long-standing research topic in machine learning that investigates how to automatically learn a mapping from a structured input to outputs of various nature. Recently, there has been an increasing interest in the adaptive processing of graphs, which led to the development of different neural network-based methodologies. In this thesis, we take a different route and develop a Bayesian Deep Learning framework for graph learning. The dissertation begins with a review of the principles over which most of the methods in the field are built, followed by a study on graph classification reproducibility issues. We then proceed to bridge the basic ideas of deep learning for graphs with the Bayesian world, by building our deep architectures in an incremental fashion. This framework allows us to consider graphs with discrete and continuous edge features, producing unsupervised embeddings rich enough to reach the state of the art on several classification tasks. Our approach is also amenable to a Bayesian nonparametric extension that automatizes the choice of almost all model's hyper-parameters. Two real-world applications demonstrate the efficacy of deep learning for graphs. The first concerns the prediction of information-theoretic quantities for molecular simulations with supervised neural models. After that, we exploit our Bayesian models to solve a malware-classification task while being robust to intra-procedural code obfuscation techniques. We conclude the dissertation with an attempt to blend the best of the neural and Bayesian worlds together. The resulting hybrid model is able to predict multimodal distributions conditioned on input graphs, with the consequent ability to model stochasticity and uncertainty better than most works. Overall, we aim to provide a Bayesian perspective into the articulated research field of deep learning for graphs.

</details>

<details>

<summary>2022-02-24 22:13:00 - Median Optimal Treatment Regimes</summary>

- *Liu Leqi, Edward H. Kennedy*

- `2103.01802v2` - [abs](http://arxiv.org/abs/2103.01802v2) - [pdf](http://arxiv.org/pdf/2103.01802v2)

> Optimal treatment regimes are personalized policies for making a treatment decision based on subject characteristics, with the policy chosen to maximize some value. It is common to aim to maximize the mean outcome in the population, via a regime assigning treatment only to those whose mean outcome is higher under treatment versus control. However, the mean can be an unstable measure of centrality, resulting in imprecise statistical procedures, as well as unrobust decisions that can be overly influenced by a small fraction of subjects. In this work, we propose a new median optimal treatment regime that instead treats individuals whose conditional median is higher under treatment. This ensures that optimal decisions for individuals from the same group are not overly influenced either by (i) a small fraction of the group (unlike the mean criterion), or (ii) unrelated subjects from different groups (unlike marginal median/quantile criteria). We introduce a new measure of value, the Average Conditional Median Effect (ACME), which summarizes across-group median treatment outcomes of a policy, and which the median optimal treatment regime maximizes. After developing key motivating examples that distinguish median optimal treatment regimes from mean and marginal median optimal treatment regimes, we give a nonparametric efficiency bound for estimating the ACME of a policy, and propose a new doubly robust-style estimator that achieves the efficiency bound under weak conditions. To construct the median optimal treatment regime, we introduce a new doubly robust-style estimator for the conditional median treatment effect. Finite-sample properties are explored via numerical simulations and the proposed algorithm is illustrated using data from a randomized clinical trial in patients with HIV.

</details>

<details>

<summary>2022-02-25 00:51:17 - Bayesian inference for discretely observed continuous time multi-state models</summary>

- *Rosario Barone, Andrea Tancredi*

- `2202.12447v1` - [abs](http://arxiv.org/abs/2202.12447v1) - [pdf](http://arxiv.org/pdf/2202.12447v1)

> Multi-state models are frequently applied for representing processes evolving through a discrete set of state. Important classes of multi-state models arise when transitions between states may depend on the time since entry into the current state or on the time elapsed from the starting of the process. The former models are called semi-Markov while the latter are known as inhomogeneous Markov models. Inference for both the models presents computational difficulties when the process is only observed at discrete time points with no additional information about the state transitions. Indeed, in both the cases, the likelihood function is not available in closed form. In order to obtain Bayesian inference under these two classes of models we reconstruct the whole unobserved trajectories conditioned on the observed points via a Metropolis-Hastings algorithm. As proposal density we use that given by the nested Markov models whose conditioned trajectories can be easily drawn by the uniformization technique. The resulting inference is illustrated via simulation studies and the analysis of two benchmark data sets for multi state models.

</details>

<details>

<summary>2022-02-25 07:49:46 - Bridging Level-K to Nash Equilibrium</summary>

- *Dan Levin, Luyao Zhang*

- `2202.12292v2` - [abs](http://arxiv.org/abs/2202.12292v2) - [pdf](http://arxiv.org/pdf/2202.12292v2)

> We introduce NLK, a model that connects the Nash equilibrium (NE) and Level-K. It allows a player in a game to believe that her opponent may be either less or as sophisticated as, she is, a view supported in psychology. We apply NLK to data from five published papers on static, dynamic, and auction games. NLK provides different predictions than those of the NE and Level-K; moreover, a simple version of NLK explains the experimental data better in many cases, with the same or lower number of parameters. We discuss extensions to games with more than two players and heterogeneous beliefs.

</details>

<details>

<summary>2022-02-25 07:58:41 - Summarizing empirical information on between-study heterogeneity for Bayesian random-effects meta-analysis</summary>

- *Christian Rver, Sibylle Sturtz, Jona Lilienthal, Ralf Bender, Tim Friede*

- `2202.12538v1` - [abs](http://arxiv.org/abs/2202.12538v1) - [pdf](http://arxiv.org/pdf/2202.12538v1)

> In Bayesian meta-analysis, the specification of prior probabilities for the between-study heterogeneity is commonly required, and is of particular benefit in situations where only few studies are included. Among the considerations in the set-up of such prior distributions, the consultation of available empirical data on a set of relevant past analyses sometimes plays a role. How exactly to summarize historical data sensibly is not immediately obvious; in particular, the investigation of an empirical collection of heterogeneity estimates will not target the actual problem and will usually only be of limited use. The commonly used normal-normal hierarchical model for random-effects meta-analysis is extended to infer a heterogeneity prior. Using an example data set, we demonstrate how to fit a distribution to empirically observed heterogeneity data from a set of meta-analyses. Considerations also include the choice of a parametric distribution family. Here, we focus on simple and readily applicable approaches to then translate these into (prior) probability distributions.

</details>

<details>

<summary>2022-02-25 08:03:15 - Familial Inference</summary>

- *Ryan Thompson, Catherine S. Forbes, Steven N. MacEachern, Mario Peruggia*

- `2202.12540v1` - [abs](http://arxiv.org/abs/2202.12540v1) - [pdf](http://arxiv.org/pdf/2202.12540v1)

> Statistical hypotheses are translations of scientific hypotheses into statements about one or more distributions, often concerning their center. Tests that assess statistical hypotheses of center implicitly assume a specific center, e.g., the mean or median. Yet, scientific hypotheses do not always specify a particular center. This ambiguity leaves the possibility for a gap between scientific theory and statistical practice that can lead to rejection of a true null. In the face of replicability crises in many scientific disciplines, "significant results" of this kind are concerning. Rather than testing a single center, this paper proposes testing a family of plausible centers, such as that induced by the Huber loss function (the "Huber family"). Each center in the family generates a testing problem, and the resulting family of hypotheses constitutes a familial hypothesis. A Bayesian nonparametric procedure is devised to test familial hypotheses, enabled by a pathwise optimization routine to fit the Huber family. The favorable properties of the new test are verified through numerical simulation in one- and two-sample settings. Two experiments from psychology serve as real-world case studies.

</details>

<details>

<summary>2022-02-25 09:34:04 - Dimension-independent Markov chain Monte Carlo on the sphere</summary>

- *H. C. Lie, D. Rudolf, B. Sprungk, T. J. Sullivan*

- `2112.12185v2` - [abs](http://arxiv.org/abs/2112.12185v2) - [pdf](http://arxiv.org/pdf/2112.12185v2)

> We consider Bayesian analysis on high-dimensional spheres with angular central Gaussian priors. These priors model antipodally-symmetric directional data, are easily defined in Hilbert spaces and occur, for instance, in Bayesian binary classification and level set inversion. In this paper we derive efficient Markov chain Monte Carlo methods for approximate sampling of posteriors with respect to these priors. Our approaches rely on lifting the sampling problem to the ambient Hilbert space and exploit existing dimension-independent samplers in linear spaces. By a push-forward Markov kernel construction we then obtain Markov chains on the sphere, which inherit reversibility and spectral gap properties from samplers in linear spaces. Moreover, our proposed algorithms show dimension-independent efficiency in numerical experiments.

</details>

<details>

<summary>2022-02-25 12:09:43 - Sparse multivariate modeling for stock returns predictability</summary>

- *Mauro Bernardi, Daniele Bianchi, Nicolas Bianco*

- `2202.12644v1` - [abs](http://arxiv.org/abs/2202.12644v1) - [pdf](http://arxiv.org/pdf/2202.12644v1)

> We develop a new variational Bayes estimation method for large-dimensional sparse multivariate predictive regression models. Our approach allows to elicit ordering-invariant shrinkage priors directly on the regression coefficient matrix rather than a Cholesky-based linear transformation, as typically implemented in existing MCMC and variational Bayes approaches. Both a simulation and an empirical study on the cross-industry predictability of equity risk premiums in the US, show that by directly shrinking weak industry inter-dependencies one can substantially improve both the statistical and economic out-of-sample performance of multivariate regression models for return predictability. This holds across alternative continuous shrinkage priors, such as the adaptive Bayesian lasso, adaptive normal-gamma and the horseshoe.

</details>

<details>

<summary>2022-02-25 12:20:04 - Bayesian autoencoders with uncertainty quantification: Towards trustworthy anomaly detection</summary>

- *Bang Xiang Yong, Alexandra Brintrup*

- `2202.12653v1` - [abs](http://arxiv.org/abs/2202.12653v1) - [pdf](http://arxiv.org/pdf/2202.12653v1)

> Despite numerous studies of deep autoencoders (AEs) for unsupervised anomaly detection, AEs still lack a way to express uncertainty in their predictions, crucial for ensuring safe and trustworthy machine learning systems in high-stake applications. Therefore, in this work, the formulation of Bayesian autoencoders (BAEs) is adopted to quantify the total anomaly uncertainty, comprising epistemic and aleatoric uncertainties. To evaluate the quality of uncertainty, we consider the task of classifying anomalies with the additional option of rejecting predictions of high uncertainty. In addition, we use the accuracy-rejection curve and propose the weighted average accuracy as a performance metric. Our experiments demonstrate the effectiveness of the BAE and total anomaly uncertainty on a set of benchmark datasets and two real datasets for manufacturing: one for condition monitoring, the other for quality inspection.

</details>

<details>

<summary>2022-02-25 16:35:26 - High-Dimensional Sparse Bayesian Learning without Covariance Matrices</summary>

- *Alexander Lin, Andrew H. Song, Berkin Bilgic, Demba Ba*

- `2202.12808v1` - [abs](http://arxiv.org/abs/2202.12808v1) - [pdf](http://arxiv.org/pdf/2202.12808v1)

> Sparse Bayesian learning (SBL) is a powerful framework for tackling the sparse coding problem. However, the most popular inference algorithms for SBL become too expensive for high-dimensional settings, due to the need to store and compute a large covariance matrix. We introduce a new inference scheme that avoids explicit construction of the covariance matrix by solving multiple linear systems in parallel to obtain the posterior moments for SBL. Our approach couples a little-known diagonal estimation result from numerical linear algebra with the conjugate gradient algorithm. On several simulations, our method scales better than existing approaches in computation time and memory, especially for structured dictionaries capable of fast matrix-vector multiplication.

</details>

<details>

<summary>2022-02-25 17:16:09 - Enriched Pitman-Yor processes</summary>

- *Tommaso Rigon, Bruno Scarpa, Sonia Petrone*

- `2003.12200v2` - [abs](http://arxiv.org/abs/2003.12200v2) - [pdf](http://arxiv.org/pdf/2003.12200v2)

> In Bayesian nonparametrics there exists a rich variety of discrete priors, including the Dirichlet process and its generalizations, which are nowadays well-established tools. Despite the remarkable advances, few proposals are tailored for modeling observations lying on product spaces, such as $\mathbb{R}^p$. Indeed, for multivariate random measures, most available priors lack flexibility and do not allow for separate partition structures among the spaces. We introduce a discrete nonparametric prior, termed enriched Pitman-Yor process (EPY), aimed at addressing these issues. Theoretical properties of this novel prior are extensively investigated. We discuss its formal link with the enriched Dirichlet process and normalized random measures, we describe a square-breaking representation and we obtain closed-form expressions for the posterior law and the involved urn schemes. In second place, we show that several existing approaches, including Dirichlet processes with a spike and slab base measure and mixture of mixtures models, implicitly rely on special cases of the EPY, which therefore constitutes a unified probabilistic framework for many Bayesian nonparametric priors. Interestingly, our unifying formulation will allow us to naturally extend these models while preserving their analytical tractability. As an illustration, we employ the EPY for a species sampling problem in ecology and for functional clustering in an e-commerce application.

</details>

<details>

<summary>2022-02-25 17:45:26 - A Robust Multi-Objective Bayesian Optimization Framework Considering Input Uncertainty</summary>

- *J. Qing, I. Couckuyt, T. Dhaene*

- `2202.12848v1` - [abs](http://arxiv.org/abs/2202.12848v1) - [pdf](http://arxiv.org/pdf/2202.12848v1)

> Bayesian optimization is a popular tool for data-efficient optimization of expensive objective functions. In real-life applications like engineering design, the designer often wants to take multiple objectives as well as input uncertainty into account to find a set of robust solutions. While this is an active topic in single-objective Bayesian optimization, it is less investigated in the multi-objective case. We introduce a novel Bayesian optimization framework to efficiently perform multi-objective optimization considering input uncertainty. We propose a robust Gaussian Process model to infer the Bayes risk criterion to quantify robustness, and we develop a two-stage Bayesian optimization process to search for a robust Pareto frontier. The complete framework supports various distributions of the input uncertainty and takes full advantage of parallel computing. We demonstrate the effectiveness of the framework through numerical benchmarks.

</details>

<details>

<summary>2022-02-25 17:56:30 - Boosting Distributional Copula Regression</summary>

- *Nicolai Hans, Nadja Klein, Florian Faschingbauer, Michael Schneider, Andreas Mayr*

- `2202.12851v1` - [abs](http://arxiv.org/abs/2202.12851v1) - [pdf](http://arxiv.org/pdf/2202.12851v1)

> Capturing complex dependence structures between outcome variables (e.g., study endpoints) is of high relevance in contemporary biomedical data problems and medical research. Distributional copula regression provides a flexible tool to model the joint distribution of multiple outcome variables by disentangling the marginal response distributions and their dependence structure. In a regression setup each parameter of the copula model, i.e. the marginal distribution parameters and the copula dependence parameters, can be related to covariates via structured additive predictors. We propose a framework to fit distributional copula regression models via a model-based boosting algorithm. Model-based boosting is a modern estimation technique that incorporates useful features like an intrinsic variable selection mechanism, parameter shrinkage and the capability to fit regression models in high dimensional data setting, i.e. situations with more covariates than observations. Thus, model-based boosting does not only complement existing Bayesian and maximum-likelihood based estimation frameworks for this model class but rather enables unique intrinsic mechanisms that can be helpful in many applied problems. The performance of our boosting algorithm in the context of copula regression models with continuous margins is evaluated in simulation studies that cover low- and high-dimensional data settings and situations with and without dependence between the responses. Moreover, distributional copula boosting is used to jointly analyze and predict the length and the weight of newborns conditional on sonographic measurements of the fetus before delivery together with other clinical variables.

</details>

<details>

<summary>2022-02-25 18:42:37 - Bayesian Inference of Fiber Orientation and Polymer Properties in Short Fiber-Reinforced Polymer Composites</summary>

- *Akshay J. Thomas, Eduardo Barocio, Ilias Bilionis, R. Byron Pipes*

- `2202.12881v1` - [abs](http://arxiv.org/abs/2202.12881v1) - [pdf](http://arxiv.org/pdf/2202.12881v1)

> We present a Bayesian methodology to infer the elastic modulus of the constituent polymer and the fiber orientation state in a short-fiber reinforced polymer composite (SFRP). The properties are inversely determined using only a few experimental tests. Developing composite manufacturing digital twins for SFRP composite processes, including injection molding and extrusion deposition additive manufacturing (EDAM) requires extensive experimental material characterization. In particular, characterizing the composite mechanical properties is time consuming and therefore, micromechanics models are used to fully identify the elasticity tensor. Hence, the objective of this paper is to infer the fiber orientation and the effective polymer modulus and therefore, identify the elasticity tensor of the composite with minimal experimental tests. To that end, we develop a hierarchical Bayesian model coupled with a micromechanics model to infer the fiber orientation and the polymer elastic modulus simultaneously which we then use to estimate the composite elasticity tensor. We motivate and demonstrate the methodology for the EDAM process but the development is such that it is applicable to other SFRP composites processed via other methods. Our results demonstrate that the approach provides a reliable framework for the inference, with as few as three tensile tests, while accounting for epistemic and aleatory uncertainty. Posterior predictive checks show that the model is able to recreate the experimental data well. The ability of the Bayesian approach to calibrate the material properties and its associated uncertainties, make it a promising tool for enabling a probabilistic predictive framework for composites manufacturing digital twins.

</details>

<details>

<summary>2022-02-25 18:56:54 - Meta-Learning for Simple Regret Minimization</summary>

- *Mohammadjavad Azizi, Branislav Kveton, Mohammad Ghavamzadeh, Sumeet Katariya*

- `2202.12888v1` - [abs](http://arxiv.org/abs/2202.12888v1) - [pdf](http://arxiv.org/pdf/2202.12888v1)

> We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist algorithms for this meta-learning problem. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\tilde{O}(m / \sqrt{n})$. This is while we show that the meta simple regret of the frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$, and thus, worse. However, the algorithm is more general, because it does not need a prior distribution over the meta-parameters, and is easier to implement for various distributions. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them empirically in several environments.

</details>

<details>

<summary>2022-02-26 01:43:59 - Distributed Bayesian Varying Coefficient Modeling Using a Gaussian Process Prior</summary>

- *Rajarshi Guhaniyogi, Cheng Li, Terrance D. Savitsky, Sanvesh Srivastava*

- `2006.00783v2` - [abs](http://arxiv.org/abs/2006.00783v2) - [pdf](http://arxiv.org/pdf/2006.00783v2)

> Varying coefficient models (VCMs) are widely used for estimating nonlinear regression functions for functional data. Their Bayesian variants using Gaussian process priors on the functional coefficients, however, have received limited attention in massive data applications, mainly due to the prohibitively slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms. We address this problem using a divide-and-conquer Bayesian approach. We first create a large number of data subsamples with much smaller sizes. Then, we formulate the VCM as a linear mixed-effects model and develop a data augmentation algorithm for obtaining MCMC draws on all the subsets in parallel. Finally, we aggregate the MCMC-based estimates of subset posteriors into a single Aggregated Monte Carlo (AMC) posterior, which is used as a computationally efficient alternative to the true posterior distribution. Theoretically, we derive minimax optimal posterior convergence rates for the AMC posteriors of both the varying coefficients and the mean regression function. We provide quantification on the orders of subset sample sizes and the number of subsets. The empirical results show that the combination schemes that satisfy our theoretical assumptions, including the AMC posterior, have better estimation performance than their main competitors across diverse simulations and in a real data analysis.

</details>

<details>

<summary>2022-02-26 11:55:56 - A Log-Gaussian Cox Process with Sequential Monte Carlo for Line Narrowing in Spectroscopy</summary>

- *Teemu Hrknen, Emma Hannula, Matthew T. Moores, Erik M. Vartiainen, Lassi Roininen*

- `2202.13120v1` - [abs](http://arxiv.org/abs/2202.13120v1) - [pdf](http://arxiv.org/pdf/2202.13120v1)

> We propose a statistical model for narrowing line shapes in spectroscopy that are well approximated as linear combinations of Lorentzian or Voigt functions. We introduce a log-Gaussian Cox process to represent the peak locations thereby providing uncertainty quantification for the line narrowing. Bayesian formulation of the method allows for robust and explicit inclusion of prior information as probability distributions for parameters of the model. Estimation of the signal and its parameters is performed using a sequential Monte Carlo algorithm allowing for parallelization of model likelihood computation. The method is validated using simulated spectra and applied to an experimental Raman spectrum obtained from a protein droplet measurement.

</details>

<details>

<summary>2022-02-26 15:48:07 - Hydrological post-processing for predicting extreme quantiles</summary>

- *Hristos Tyralis, Georgia Papacharalampous*

- `2202.13166v1` - [abs](http://arxiv.org/abs/2202.13166v1) - [pdf](http://arxiv.org/pdf/2202.13166v1)

> Hydrological post-processing using quantile regression algorithms constitutes a prime means of estimating the uncertainty of hydrological predictions. Nonetheless, conventional large-sample theory for quantile regression does not apply sufficiently far in the tails of the probability distribution of the dependent variable. To overcome this limitation that could be crucial when the interest lies on flood events, we here introduce hydrological post-processing through extremal quantile regression for estimating the extreme quantiles of hydrological responses. In summary, the new hydrological post-processing method exploits properties of the Hill's estimator from the extreme value theory to extrapolate quantile regression's predictions to high quantiles. As a proof of concept, the new method is here tested in post-processing daily streamflow simulations provided by three process-based hydrological models for 180 basins in the contiguous United States (CONUS) and is further compared to conventional quantile regression. With this large-scale comparison, it is demonstrated that hydrological post-processing using conventional quantile regression severely underestimates high quantiles (at the quantile level 0.9999) compared to hydrological post-processing using extremal quantile regression, although both methods are equivalent at lower quantiles (at the quantile level 0.9700). Moreover, it is shown that, in the same context, extremal quantile regression estimates the high predictive quantiles with efficiency that is, on average, equivalent in the large-sample study for the three process-based hydrological models.

</details>

<details>

<summary>2022-02-26 21:00:20 - Multi-Objective Model-based Reinforcement Learning for Infectious Disease Control</summary>

- *Runzhe Wan, Xinyu Zhang, Rui Song*

- `2009.04607v3` - [abs](http://arxiv.org/abs/2009.04607v3) - [pdf](http://arxiv.org/pdf/2009.04607v3)

> Severe infectious diseases such as the novel coronavirus (COVID-19) pose a huge threat to public health. Stringent control measures, such as school closures and stay-at-home orders, while having significant effects, also bring huge economic losses. In the face of an emerging infectious disease, a crucial question for policymakers is how to make the trade-off and implement the appropriate interventions timely given the huge uncertainty. In this work, we propose a Multi-Objective Model-based Reinforcement Learning framework to facilitate data-driven decision-making and minimize the overall long-term cost. Specifically, at each decision point, a Bayesian epidemiological model is first learned as the environment model, and then the proposed model-based multi-objective planning algorithm is applied to find a set of Pareto-optimal policies. This framework, combined with the prediction bands for each policy, provides a real-time decision support tool for policymakers. The application is demonstrated with the spread of COVID-19 in China.

</details>

<details>

<summary>2022-02-27 00:38:32 - The art of BART: Minimax optimality over nonhomogeneous smoothness in high dimension</summary>

- *Seonghyun Jeong, Veronika Rockova*

- `2008.06620v3` - [abs](http://arxiv.org/abs/2008.06620v3) - [pdf](http://arxiv.org/pdf/2008.06620v3)

> Considerable effort has been directed to developing asymptotically minimax procedures in problems of recovering functions and densities. These methods often rely on somewhat arbitrary and restrictive assumptions such as isotropy or spatial homogeneity. This work enhances theoretical understanding of Bayesian forests (including BART) under substantially relaxed smoothness assumptions. In particular, we provide a comprehensive study of asymptotic optimality and posterior contraction of Bayesian forests when the regression function has anisotropic smoothness that possibly varies over the function domain. The regression function can also be possibly discontinuous. We introduce a new class of sparse {\em piecewise heterogeneous anisotropic} H\"{o}lder functions and derive their minimax lower bound of estimation in high-dimensional scenarios under the $L_2$-loss. Next, we find that the default Bayesian tree priors, coupled with a Dirichlet subset selection prior for sparse estimation in high-dimensional scenarios, adapt to unknown heterogeneous smoothness, discontinuity, and sparsity. These results show that Bayesian forests are uniquely suited for more general estimation problems which would render other default machine learning tools, such as Gaussian processes, suboptimal. Our numerical study shows that Bayesian forests outperform other competitors such as random forests and deep neural networks, which are believed to work well for discontinuous or complicated smooth functions. Beyond nonparametric regression, we also show that Bayesian forests can be successfully applied to many other problems including density estimation and binary classification.

</details>

<details>

<summary>2022-02-27 08:51:50 - Sampling in Dirichlet Process Mixture Models for Clustering Streaming Data</summary>

- *Or Dinari, Oren Freifeld*

- `2202.13312v1` - [abs](http://arxiv.org/abs/2202.13312v1) - [pdf](http://arxiv.org/pdf/2202.13312v1)

> Practical tools for clustering streaming data must be fast enough to handle the arrival rate of the observations. Typically, they also must adapt on the fly to possible lack of stationarity; i.e., the data statistics may be time-dependent due to various forms of drifts, changes in the number of clusters, etc. The Dirichlet Process Mixture Model (DPMM), whose Bayesian nonparametric nature allows it to adapt its complexity to the data, seems a natural choice for the streaming-data case. In its classical formulation, however, the DPMM cannot capture common types of drifts in the data statistics. Moreover, and regardless of that limitation, existing methods for online DPMM inference are too slow to handle rapid data streams. In this work we propose adapting both the DPMM and a known DPMM sampling-based non-streaming inference method for streaming-data clustering. We demonstrate the utility of the proposed method on several challenging settings, where it obtains state-of-the-art results while being on par with other methods in terms of speed.

</details>

<details>

<summary>2022-02-27 09:25:24 - Bayesian Robust Tensor Ring Model for Incomplete Multiway Data</summary>

- *Zhenhao Huang, Guoxu Zhou, Yuning Qiu*

- `2202.13321v1` - [abs](http://arxiv.org/abs/2202.13321v1) - [pdf](http://arxiv.org/pdf/2202.13321v1)

> Low-rank tensor completion aims to recover missing entries from the observed data. However, the observed data may be disturbed by noise and outliers. Therefore, robust tensor completion (RTC) is proposed to solve this problem. The recently proposed tensor ring (TR) structure is applied to RTC due to its superior abilities in dealing with high-dimensional data with predesigned TR rank. To avoid manual rank selection and achieve a balance between low-rank component and sparse component, in this paper, we propose a Bayesian robust tensor ring (BRTR) decomposition method for RTC problem. Furthermore, we develop a variational Bayesian (VB) algorithm to infer the probability distribution of posteriors. During the learning process, the frontal slices of previous tensor and horizontal slices of latter tensor shared with the same TR rank with zero components are pruned, resulting in automatic rank determination. Compared with existing methods, BRTR can automatically learn TR rank without manual fine-tuning of parameters. Extensive experiments indicate that BRTR has better recovery performance and ability to remove noise than other state-of-the-art methods.

</details>

<details>

<summary>2022-02-27 11:29:53 - Surrogate Model For Field Optimization Using Beta-VAE Based Regression</summary>

- *Ajitabh Kumar*

- `2008.11433v2` - [abs](http://arxiv.org/abs/2008.11433v2) - [pdf](http://arxiv.org/pdf/2008.11433v2)

> Oilfield development related decisions are made using reservoir simulation-based optimization study in which different production scenarios and well controls are compared. Such simulations are computationally expensive and so surrogate models are used to accelerate studies. Deep learning has been used in past to generate surrogates, but such models often fail to quantify prediction uncertainty and are not interpretable. In this work, beta-VAE based regression is proposed to generate simulation surrogates for use in optimization workflow. beta-VAE enables interpretable, factorized representation of decision variables in latent space, which is then further used for regression. Probabilistic dense layers are used to quantify prediction uncertainty and enable approximate Bayesian inference. Surrogate model developed using beta-VAE based regression finds interpretable and relevant latent representation. A reasonable value of beta ensures a good balance between factor disentanglement and reconstruction. Probabilistic dense layer helps in quantifying predicted uncertainty for objective function, which is then used to decide whether full-physics simulation is required for a case.

</details>

<details>

<summary>2022-02-27 19:07:04 - Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness</summary>

- *Namuk Park, Songkuk Kim*

- `2105.12639v3` - [abs](http://arxiv.org/abs/2105.12639v3) - [pdf](http://arxiv.org/pdf/2105.12639v3)

> Neural network ensembles, such as Bayesian neural networks (BNNs), have shown success in the areas of uncertainty estimation and robustness. However, a crucial challenge prohibits their use in practice. BNNs require a large number of predictions to produce reliable results, leading to a significant increase in computational cost. To alleviate this issue, we propose spatial smoothing, a method that spatially ensembles neighboring feature map points of convolutional neural networks. By simply adding a few blur layers to the models, we empirically show that spatial smoothing improves accuracy, uncertainty estimation, and robustness of BNNs across a whole range of ensemble sizes. In particular, BNNs incorporating spatial smoothing achieve high predictive performance merely with a handful of ensembles. Moreover, this method also can be applied to canonical deterministic neural networks to improve the performances. A number of evidences suggest that the improvements can be attributed to the stabilized feature maps and the smoothing of the loss landscape. In addition, we provide a fundamental explanation for prior works - namely, global average pooling, pre-activation, and ReLU6 - by addressing them as special cases of spatial smoothing. These not only enhance accuracy, but also improve uncertainty estimation and robustness by making the loss landscape smoother in the same manner as spatial smoothing. The code is available at https://github.com/xxxnell/spatial-smoothing.

</details>

<details>

<summary>2022-02-27 19:07:12 - Bayesian Active Learning for Discrete Latent Variable Models</summary>

- *Aditi Jha, Zoe C. Ashwood, Jonathan W. Pillow*

- `2202.13426v1` - [abs](http://arxiv.org/abs/2202.13426v1) - [pdf](http://arxiv.org/pdf/2202.13426v1)

> Active learning seeks to reduce the number of samples required to estimate the parameters of a model, thus forming an important class of techniques in modern machine learning. However, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. Here we address this gap in the literature and propose a novel framework for maximum-mutual-information input selection for learning discrete latent variable regression models. We first examine a class of models known as "mixtures of linear regressions" (MLR). This example is striking because it is well known that active learning confers no advantage for standard least-squares regression. However, we show -- both in simulations and analytically using Fisher information -- that optimal input selection can nevertheless provide dramatic gains for mixtures of regression models; we also validate this on a real-world application of MLRs. We then consider a powerful class of temporally structured latent variable models known as Input-Output Hidden Markov Models (IO-HMMs), which have recently gained prominence in neuroscience. We show that our method substantially speeds up learning, and outperforms a variety of approximate methods based on variational and amortized inference.

</details>

<details>

<summary>2022-02-27 19:11:04 - Simplifying small area estimation with rFIA: a demonstration of tools and techniques</summary>

- *Hunter Stanke, Andrew O. Finley, Grant M. Domke*

- `2109.03863v2` - [abs](http://arxiv.org/abs/2109.03863v2) - [pdf](http://arxiv.org/pdf/2109.03863v2)

> The United States (US) Forest Service Forest Inventory and Analysis (FIA) program operates the national forest inventory of the US. Traditionally, the FIA program has relied on sample-based approaches -- permanent plot networks and associated design-based estimators -- to estimate forest variables across large geographic areas and long periods of time. These approaches generally offer unbiased inference on large domains but fail to provide reliable estimates for small domains due to low sample sizes. Rising demand for small domain estimates will thus require the FIA program to adopt non-traditional estimation approaches that are capable of delivering defensible estimates of forest variables at increased spatial and temporal resolution, without the expense of collecting additional field data. In light of this challenge, the development of small area estimation (SAE) methods for FIA data has become an active and highly productive area of research. Yet, SAE methods remain difficult to apply to FIA data, due in part to the complex data structures and inventory design used by the FIA program. Thus, we argue that a new suite of estimation tools (i.e., software) will be required to accommodate shifts in demand for inference on large geographic areas and long time periods to inference on small spatial and/or temporal domains. Herein, we present rFIA, an open-source R package designed to increase the accessibility of FIA data, as one such tool. Specifically, we present two case studies chosen to demonstrate rFIA's potential to simplify the application of a broad suite of SAE methods to FIA data: (1) estimation of contemporary county-level forest carbon stocks across the conterminous US using a spatial Fay-Herriot model; and (2) temporally-explicit estimation of multi-decadal trends in merchantable wood volume in Washington County, Maine using a Bayesian mixed-effects model.

</details>

<details>

<summary>2022-02-28 02:51:29 - The Computational Asymptotics of Gaussian Variational Inference and the Laplace Aprroximation</summary>

- *Zuheng Xu, Trevor Campbell*

- `2104.05886v2` - [abs](http://arxiv.org/abs/2104.05886v2) - [pdf](http://arxiv.org/pdf/2104.05886v2)

> Gaussian variational inference and the Laplace approximation are popular alternatives to Markov chain Monte Carlo that formulate Bayesian posterior inference as an optimization problem, enabling the use of simple and scalable stochastic optimization algorithms. However, a key limitation of both methods is that the solution to the optimization problem is typically not tractable to compute; even in simple settings the problem is nonconvex. Thus, recently developed statistical guarantees -- which all involve the (data) asymptotic properties of the global optimum -- are not reliably obtained in practice. In this work, we provide two major contributions: a theoretical analysis of the asymptotic convexity properties of variational inference with a Gaussian family and the maximum a posteriori (MAP) problem required by the Laplace approximation; and two algorithms -- consistent Laplace approximation (CLA) and consistent stochastic variational inference (CSVI) -- that exploit these properties to find the optimal approximation in the asymptotic regime. Both CLA and CSVI involve a tractable initialization procedure that finds the local basin of the optimum, and CSVI further includes a scaled gradient descent algorithm that provably stays locally confined to that basin. Experiments on nonconvex synthetic and real-data examples show that compared with standard variational and Laplace approximations, both CSVI and CLA improve the likelihood of obtaining the global optimum of their respective optimization problems.

</details>

<details>

<summary>2022-02-28 08:11:02 - Rectified Max-Value Entropy Search for Bayesian Optimization</summary>

- *Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet*

- `2202.13597v1` - [abs](http://arxiv.org/abs/2202.13597v1) - [pdf](http://arxiv.org/pdf/2202.13597v1)

> Although the existing max-value entropy search (MES) is based on the widely celebrated notion of mutual information, its empirical performance can suffer due to two misconceptions whose implications on the exploration-exploitation trade-off are investigated in this paper. These issues are essential in the development of future acquisition functions and the improvement of the existing ones as they encourage an accurate measure of the mutual information such as the rectified MES (RMES) acquisition function we develop in this work. Unlike the evaluation of MES, we derive a closed-form probability density for the observation conditioned on the max-value and employ stochastic gradient ascent with reparameterization to efficiently optimize RMES. As a result of a more principled acquisition function, RMES shows a consistent improvement over MES in several synthetic function benchmarks and real-world optimization problems.

</details>

<details>

<summary>2022-02-28 11:17:49 - Bayesian Hierarchical Copula Model for Financial Time series</summary>

- *Paolo Onorati, Brunero Liseo*

- `2202.13689v1` - [abs](http://arxiv.org/abs/2202.13689v1) - [pdf](http://arxiv.org/pdf/2202.13689v1)

> We discuss a Bayesian hierarchical copula model for clusters of financial time series. A similar approach has been developed in Zhuang et al. (2020). However, the prior distributions proposed there do not always provide a proper posterior. In order to circumvent the problem, we adopt a proper global-local shrinkage prior, which is also able to account for potential dependence structure among different clusters. The performance of the proposed model is presented through simulations and a real data analysis.

</details>

<details>

<summary>2022-02-28 13:24:00 - Rule-based Evolutionary Bayesian Learning</summary>

- *Themistoklis Botsas, Lachlan R. Mason, Omar K. Matar, Indranil Pan*

- `2202.13778v1` - [abs](http://arxiv.org/abs/2202.13778v1) - [pdf](http://arxiv.org/pdf/2202.13778v1)

> In our previous work, we introduced the rule-based Bayesian Regression, a methodology that leverages two concepts: (i) Bayesian inference, for the general framework and uncertainty quantification and (ii) rule-based systems for the incorporation of expert knowledge and intuition. The resulting method creates a penalty equivalent to a common Bayesian prior, but it also includes information that typically would not be available within a standard Bayesian context. In this work, we extend the aforementioned methodology with grammatical evolution, a symbolic genetic programming technique that we utilise for automating the rules' derivation. Our motivation is that grammatical evolution can potentially detect patterns from the data with valuable information, equivalent to that of expert knowledge. We illustrate the use of the rule-based Evolutionary Bayesian learning technique by applying it to synthetic as well as real data, and examine the results in terms of point predictions and associated uncertainty.

</details>

<details>

<summary>2022-02-28 13:39:04 - Forecasting US Inflation Using Bayesian Nonparametric Models</summary>

- *Todd E. Clark, Florian Huber, Gary Koop, Massimiliano Marcellino*

- `2202.13793v1` - [abs](http://arxiv.org/abs/2202.13793v1) - [pdf](http://arxiv.org/pdf/2202.13793v1)

> The relationship between inflation and predictors such as unemployment is potentially nonlinear with a strength that varies over time, and prediction errors error may be subject to large, asymmetric shocks. Inspired by these concerns, we develop a model for inflation forecasting that is nonparametric both in the conditional mean and in the error using Gaussian and Dirichlet processes, respectively. We discuss how both these features may be important in producing accurate forecasts of inflation. In a forecasting exercise involving CPI inflation, we find that our approach has substantial benefits, both overall and in the left tail, with nonparametric modeling of the conditional mean being of particular importance.

</details>

<details>

<summary>2022-02-28 14:47:14 - A Probabilistic Deep Image Prior for Computational Tomography</summary>

- *Javier Antorn, Riccardo Barbano, Johannes Leuschner, Jos Miguel Hernndez-Lobato, Bangti Jin*

- `2203.00479v1` - [abs](http://arxiv.org/abs/2203.00479v1) - [pdf](http://arxiv.org/pdf/2203.00479v1)

> Existing deep-learning based tomographic image reconstruction methods do not provide accurate estimates of reconstruction uncertainty, hindering their real-world deployment. To address this limitation, we construct a Bayesian prior for tomographic reconstruction, which combines the classical total variation (TV) regulariser with the modern deep image prior (DIP). Specifically, we use a change of variables to connect our prior beliefs on the image TV semi-norm with the hyper-parameters of the DIP network. For the inference, we develop an approach based on the linearised Laplace method, which is scalable to high-dimensional settings. The resulting framework provides pixel-wise uncertainty estimates and a marginal likelihood objective for hyperparameter optimisation. We demonstrate the method on synthetic and real-measured high-resolution $\mu$CT data, and show that it provides superior calibration of uncertainty estimates relative to previous probabilistic formulations of the DIP.

</details>

<details>

<summary>2022-02-28 15:38:02 - Several Remarks on the Numerical Integrator in Lagrangian Monte Carlo</summary>

- *James A. Brofos, Roy R. Lederman*

- `2202.13888v1` - [abs](http://arxiv.org/abs/2202.13888v1) - [pdf](http://arxiv.org/pdf/2202.13888v1)

> Riemannian manifold Hamiltonian Monte Carlo (RMHMC) is a powerful method of Bayesian inference that exploits underlying geometric information of the posterior distribution in order to efficiently traverse the parameter space. However, the form of the Hamiltonian necessitates complicated numerical integrators, such as the generalized leapfrog method, that preserve the detailed balance condition. The distinguishing feature of these numerical integrators is that they involve solutions to implicitly defined equations. Lagrangian Monte Carlo (LMC) proposes to eliminate the fixed point iterations by transitioning from the Hamiltonian formalism to Lagrangian dynamics, wherein a fully explicit integrator is available. This work makes several contributions regarding the numerical integrator used in LMC. First, it has been claimed in the literature that the integrator is only first-order accurate for the Lagrangian equations of motion; to the contrary, we show that the LMC integrator enjoys second order accuracy. Second, the current conception of LMC requires four determinant computations in every step in order to maintain detailed balance; we propose a simple modification to the integration procedure in LMC in order to reduce the number of determinant computations from four to two while still retaining a fully explicit numerical integration scheme. Third, we demonstrate that the LMC integrator enjoys a certain robustness to human error that is not shared with the generalized leapfrog integrator, which can invalidate detailed balance in the latter case. We discuss these contributions within the context of several benchmark Bayesian inference tasks.

</details>

<details>

<summary>2022-02-28 18:29:34 - Classification Under Partial Reject Options</summary>

- *Mns Karlsson, Ola Hssjer*

- `2202.14011v1` - [abs](http://arxiv.org/abs/2202.14011v1) - [pdf](http://arxiv.org/pdf/2202.14011v1)

> We study set-valued classification for a Bayesian model where data originates from one of a finite number $N$ of possible hypotheses. Thus we consider the scenario where the size of the classified set of categories ranges from 0 to $N$. Empty sets corresponds to an outlier, size 1 represents a firm decision that singles out one hypotheses, size $N$ corresponds to a rejection to classify, whereas sizes $2\ldots,N-1$ represent a partial rejection, where some hypotheses are excluded from further analysis. We introduce a general framework of reward functions with a set-valued argument and derive the corresponding optimal Bayes classifiers, for a homogeneous block of hypotheses and for when hypotheses are partitioned into blocks, where ambiguity within and between blocks are of different severity. We illustrate classification using an ornithological dataset, with taxa partitioned into blocks and parameters estimated using MCMC. The associated reward function's tuning parameters are chosen through cross-validation.

</details>

<details>

<summary>2022-02-28 18:54:19 - Hierarchical Gaussian Process Models for Regression Discontinuity/Kink under Sharp and Fuzzy Designs</summary>

- *Ximing Wu*

- `2110.00921v2` - [abs](http://arxiv.org/abs/2110.00921v2) - [pdf](http://arxiv.org/pdf/2110.00921v2)

> We propose nonparametric Bayesian estimators for causal inference exploiting Regression Discontinuity/Kink (RD/RK) under sharp and fuzzy designs. Our estimators are based on Gaussian Process (GP) regression and classification. The GP methods are powerful probabilistic machine learning approaches that are advantageous in terms of derivative estimation and uncertainty quantification, facilitating RK estimation and inference of RD/RK models. These estimators are extended to hierarchical GP models with an intermediate Bayesian neural network layer and can be characterized as hybrid deep learning models. Monte Carlo simulations show that our estimators perform comparably to and sometimes better than competing estimators in terms of precision, coverage and interval length. The hierarchical GP models considerably improve upon one-layer GP models. We apply the proposed methods to estimate the incumbency advantage of US house elections. Our estimations suggest a significant incumbency advantage in terms of both vote share and probability of winning in the next elections. Lastly we present an extension to accommodate covariate adjustment.

</details>

<details>

<summary>2022-02-28 19:12:26 - Mixed Variational Inference</summary>

- *Nikolaos Gianniotis*

- `1901.04791v4` - [abs](http://arxiv.org/abs/1901.04791v4) - [pdf](http://arxiv.org/pdf/1901.04791v4)

> The Laplace approximation has been one of the workhorses of Bayesian inference. It often delivers good approximations in practice despite the fact that it does not strictly take into account where the volume of posterior density lies. Variational approaches avoid this issue by explicitly minimising the Kullback-Leibler divergence DKL between a postulated posterior and the true (unnormalised) logarithmic posterior. However, they rely on a closed form DKL in order to update the variational parameters. To address this, stochastic versions of variational inference have been devised that approximate the intractable DKL with a Monte Carlo average. This approximation allows calculating gradients with respect to the variational parameters. However, variational methods often postulate a factorised Gaussian approximating posterior. In doing so, they sacrifice a-posteriori correlations. In this work, we propose a method that combines the Laplace approximation with the variational approach. The advantages are that we maintain: applicability on non-conjugate models, posterior correlations and a reduced number of free variational parameters. Numerical experiments demonstrate improvement over the Laplace approximation and variational inference with factorised Gaussian posteriors.

</details>

<details>

<summary>2022-02-28 19:42:51 - Fast Bayesian estimation of brain activation with cortical surface and subcortical fMRI data using EM</summary>

- *Daniel Spencer, David Bolin, Mary Beth Nebel, Amanda Mejia*

- `2203.00053v1` - [abs](http://arxiv.org/abs/2203.00053v1) - [pdf](http://arxiv.org/pdf/2203.00053v1)

> Analysis of brain imaging scans is critical to understanding the way the human brain functions, which can be leveraged to treat injuries and conditions that affect the quality of life for a significant portion of the human population. In particular, functional magnetic resonance imaging (fMRI) scans give detailed data on a living subject at high spatial and temporal resolutions. Due to the high cost involved in the collection of these scans, robust methods of analysis are of critical importance in order to produce meaningful inference. Bayesian methods in particular allow for the inclusion of expected behavior from prior study into an analysis, increasing the power of the results while circumventing problems that arise in classical analyses, including the effects of smoothing results and sensitivity to multiple comparison testing corrections. Recent development of a surface-based spatial Bayesian general linear model for cortical surface fMRI (cs-fMRI) data provides the desired power increase in task fMRI data using stochastic partial differential equation (SPDE) priors. This model relies on the computational efficiencies of the integrated nested Laplace approximation (INLA) to perform powerful analyses that have been validated to outperform classical analyses. In this article, we develop an exact Bayesian analysis method for the GLM, employing an expectation-maximization (EM) algorithm to find maximum a posteriori (MAP) estimates of task-based regressors on cs-fMRI and subcortical fMRI data while using minimal computational resources. Our proposed method is compared to the INLA implementation of the Bayesian GLM, as well as a classical GLM on simulated data. A validation of the method on data from the Human Connectome Project is also provided.

</details>

<details>

<summary>2022-02-28 20:21:06 - Manifold-constrained Gaussian process inference for time-varying parameters in dynamic systems</summary>

- *Yan Sun, Shihao Yang*

- `2105.13407v2` - [abs](http://arxiv.org/abs/2105.13407v2) - [pdf](http://arxiv.org/pdf/2105.13407v2)

> Identification of parameters in ordinary differential equations (ODEs) is an important and challenging task when modeling dynamic systems in biomedical research and other scientific areas, especially with the presence of time-varying parameters. This article proposes a fast and accurate method, TVMAGI (Time-Varying MAnifold-constrained Gaussian process Inference), to estimate both time-constant and time-varying parameters in the ODE using noisy and sparse observation data. TVMAGI imposes a Gaussian process model over the time series of system components as well as time-varying parameters, and restricts the derivative process to satisfy ODE conditions. Consequently, TVMAGI completely bypasses numerical integration and achieves substantial savings in computation time. By incorporating the ODE structures through manifold constraints, TVMAGI enjoys a principled statistical construct under the Bayesian paradigm, which further enables it to handle systems with missing data or unobserved components. The Gaussian process prior also alleviates the identifiability issue often associated with the time-varying parameters in ODE. Unlike existing approaches, TVMAGI can be applied to general nonlinear systems without specific structural assumptions. Three simulation examples, including an infectious disease compartmental model, are provided to illustrate the robustness and efficiency of our method compared with numerical integration and Bayesian filtering methods.

</details>


## 2022-03

<details>

<summary>2022-03-01 01:06:07 - Application of Equal Local Levels to Improve Q-Q Plot Testing Bands with R Package qqconf</summary>

- *Eric Weine, Mary Sara McPeek, Mark Abney*

- `2111.15082v2` - [abs](http://arxiv.org/abs/2111.15082v2) - [pdf](http://arxiv.org/pdf/2111.15082v2)

> Quantile-Quantile (Q-Q) plots are often difficult to interpret because it is unclear how large the deviation from the theoretical distribution must be to indicate a lack of fit. Most Q-Q plots could benefit from the addition of meaningful global testing bands, but the use of such bands unfortunately remains rare because of the drawbacks of current approaches and packages. These drawbacks include incorrect global Type I error rate, lack of power to detect deviations in the tails of the distribution, relatively slow computation for large data sets, and limited applicability. To solve these problems, we apply the equal local levels global testing method, which we have implemented in the R Package qqconf, a versatile tool to create Q-Q plots and probability-probability (P-P) plots in a wide variety of settings, with simultaneous testing bands rapidly created using recently-developed algorithms. In addition to being quick to compute, these bands have a variety of desirable properties, including accurate global levels, equal sensitivity to deviations in all parts of the null distribution (including the tails), and applicability to a range of null distributions. We illustrate the use of qqconf in several applications: assessing normality of residuals from regression, assessing accuracy of p values, and use of Q-Q plots in genome-wide association studies.

</details>

<details>

<summary>2022-03-01 01:48:11 - Oncology Dose Finding Using Approximate Bayesian Computation Design</summary>

- *Huaqing Jin, Wenbin Du, Guosheng Yin*

- `2203.00173v1` - [abs](http://arxiv.org/abs/2203.00173v1) - [pdf](http://arxiv.org/pdf/2203.00173v1)

> In the development of new cancer treatment, an essential step is to determine the maximum tolerated dose (MTD) via phase I clinical trials. Generally speaking, phase I trial designs can be classified as either model-based or algorithm-based approaches. Model-based phase I designs are typically more efficient by using all observed data, while there is a potential risk of model misspecification that may lead to unreliable dose assignment and incorrect MTD identification. In contrast, most of the algorithm-based designs are less efficient in using cumulative information, because they tend to focus on the observed data in the neighborhood of the current dose level for dose movement. To use the data more efficiently yet without any model assumption, we propose a novel approximate Bayesian computation (ABC) approach for phase I trial design. Not only is the ABC design free of any dose--toxicity curve assumption, but it can also aggregate all the available information accrued in the trial for dose assignment. Extensive simulation studies demonstrate its robustness and efficiency compared with other phase I designs. We apply the ABC design to the MEK inhibitor selumetinib trial to demonstrate its satisfactory performance. The proposed design can be a useful addition to the family of phase I clinical trial designs due to its simplicity, efficiency and robustness.

</details>

<details>

<summary>2022-03-01 02:23:01 - On the impact of outliers in loss reserving</summary>

- *Benjamin Avanzi, Mark Lavender, Greg Taylor, Bernard Wong*

- `2203.00184v1` - [abs](http://arxiv.org/abs/2203.00184v1) - [pdf](http://arxiv.org/pdf/2203.00184v1)

> The sensitivity of loss reserving techniques to outliers in the data or deviations from model assumptions is a well known challenge. It has been shown that the popular chain-ladder reserving approach is at significant risk to such aberrant observations in that reserve estimates can be significantly shifted in the presence of even one outlier. As a consequence the chain-ladder reserving technique is non-robust. In this paper we investigate the sensitivity of reserves and mean squared errors of prediction under Mack's Model (Mack, 1993). This is done through the derivation of impact functions which are calculated by taking the first derivative of the relevant statistic of interest with respect to an observation. We also provide and discuss the impact functions for quantiles when total reserves are assumed to be lognormally distributed. Additionally, comparisons are made between the impact functions for individual accident year reserves under Mack's Model and the Bornhuetter-Ferguson methodology. It is shown that the impact of incremental claims on these statistics of interest varies widely throughout a loss triangle and is heavily dependent on other cells in the triangle.   Results are illustrated using data from a Belgian non-life insurer.

</details>

<details>

<summary>2022-03-01 07:09:47 - Mixture-of-Variational-Experts for Continual Learning</summary>

- *Heinke Hihn, Daniel A. Braun*

- `2110.12667v4` - [abs](http://arxiv.org/abs/2110.12667v4) - [pdf](http://arxiv.org/pdf/2110.12667v4)

> One weakness of machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning (CL) paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We discuss this principle from a Bayesian perspective and show its connections to previous approaches to CL. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method in continual supervised learning and in continual reinforcement learning.

</details>

<details>

<summary>2022-03-01 10:57:23 - Minimum Message Length Autoregressive Moving Average Model Order Selection</summary>

- *Zheng Fang, David L. Dowe, Shelton Peiris, Dedi Rosadi*

- `2110.03250v2` - [abs](http://arxiv.org/abs/2110.03250v2) - [pdf](http://arxiv.org/pdf/2110.03250v2)

> This paper derives a Minimum Message Length (MML) criterion for the model selection of the Autoregressive Moving Average (ARMA) time series model. The MML87 performances on the ARMA model compared with other well known model selection criteria, Akaike Information Criterion (AIC), Corrected AIC (AICc), Bayesian Information Criterion (BIC), and Hannan Quinn (HQ). The experimental results show that the MML87 is outperformed the other model selection criteria as it select most of the models with lower prediction errors and the models selected by MML87 to have a lower mean squared error in different in-sample and out-sample sizes.

</details>

<details>

<summary>2022-03-01 13:27:06 - Last Layer Marginal Likelihood for Invariance Learning</summary>

- *Pola Schwbel, Martin Jrgensen, Sebastian W. Ober, Mark van der Wilk*

- `2106.07512v2` - [abs](http://arxiv.org/abs/2106.07512v2) - [pdf](http://arxiv.org/pdf/2106.07512v2)

> Data augmentation is often used to incorporate inductive biases into models. Traditionally, these are hand-crafted and tuned with cross validation. The Bayesian paradigm for model selection provides a path towards end-to-end learning of invariances using only the training data, by optimising the marginal likelihood. Computing the marginal likelihood is hard for neural networks, but success with tractable approaches that compute the marginal likelihood for the last layer only raises the question of whether this convenient approach might be employed for learning invariances. We show partial success on standard benchmarks, in the low-data regime and on a medical imaging dataset by designing a custom optimisation routine. Introducing a new lower bound to the marginal likelihood allows us to perform inference for a larger class of likelihood functions than before. On the other hand, we demonstrate failure modes on the CIFAR10 dataset, where the last layer approximation is not sufficient due to the increased complexity of our neural network. Our results indicate that once more sophisticated approximations become available the marginal likelihood is a promising approach for invariance learning in neural networks.

</details>

<details>

<summary>2022-03-01 17:42:47 - Measuring diachronic sense change: new models and Monte Carlo methods for Bayesian inference</summary>

- *Schyan Zafar, Geoff Nicholls*

- `2105.00819v2` - [abs](http://arxiv.org/abs/2105.00819v2) - [pdf](http://arxiv.org/pdf/2105.00819v2)

> In a bag-of-words model, the senses of a word with multiple meanings, e.g. "bank" (used either in a river-bank or an institution sense), are represented as probability distributions over context words, and sense prevalence is represented as a probability distribution over senses. Both of these may change with time. Modelling and measuring this kind of sense change is challenging due to the typically high-dimensional parameter space and sparse datasets. A recently published corpus of ancient Greek texts contains expert-annotated sense labels for selected target words. Automatic sense-annotation for the word "kosmos" (meaning decoration, order or world) has been used as a test case in recent work with related generative models and Monte Carlo methods. We adapt an existing generative sense change model to develop a simpler model for the main effects of sense and time, and give MCMC methods for Bayesian inference on all these models that are more efficient than existing methods. We carry out automatic sense-annotation of snippets containing "kosmos" using our model, and measure the time-evolution of its three senses and their prevalence. As far as we are aware, ours is the first analysis of this data, within the class of generative models we consider, that quantifies uncertainty and returns credible sets for evolving sense prevalence in good agreement with those given by expert annotation.

</details>

<details>

<summary>2022-03-01 19:23:56 - An extreme value approach to CoVaR estimation</summary>

- *Natalia Nolde, Chen Zhou, Menglin Zhou*

- `2201.00892v2` - [abs](http://arxiv.org/abs/2201.00892v2) - [pdf](http://arxiv.org/pdf/2201.00892v2)

> The global financial crisis of 2007-2009 highlighted the crucial role systemic risk plays in ensuring stability of financial markets. Accurate assessment of systemic risk would enable regulators to introduce suitable policies to mitigate the risk as well as allow individual institutions to monitor their vulnerability to market movements. One popular measure of systemic risk is the conditional value-at-risk (CoVaR), proposed in Adrian and Brunnermeier (2011). We develop a methodology to estimate CoVaR semi-parametrically within the framework of multivariate extreme value theory. According to its definition, CoVaR can be viewed as a high quantile of the conditional distribution of one institution's (or the financial system) potential loss, where the conditioning event corresponds to having large losses in the financial system (or the given financial institution). We relate this conditional distribution to the tail dependence function between the system and the institution, then use parametric modelling of the tail dependence function to address data sparsity in the joint tail regions. We prove consistency of the proposed estimator, and illustrate its performance via simulation studies and a real data example.

</details>

<details>

<summary>2022-03-01 22:52:50 - Bayesian adaptive and interpretable functional regression for exposure profiles</summary>

- *Yunan Gao, Daniel R. Kowal*

- `2203.00784v1` - [abs](http://arxiv.org/abs/2203.00784v1) - [pdf](http://arxiv.org/pdf/2203.00784v1)

> Pollutant exposures during gestation are a known and adverse factor for birth and health outcomes. However, the links between prenatal air pollution exposures and educational outcomes are less clear, in particular the critical windows of susceptibility during pregnancy. Using a large cohort of students in North Carolina, we study prenatal $\mbox{PM}_{2.5}$ exposures recorded at near-continuous resolutions and linked to 4th end-of-grade reading scores. We develop a locally-adaptive Bayesian regression model for scalar responses with functional and scalar predictors. The proposed model pairs a B-spline basis expansion with dynamic shrinkage priors to capture both smooth and rapidly-changing features in the regression surface. The local adaptivity is manifested in more accurate point estimates and more precise uncertainty quantification than existing methods on simulated data. The model is accompanied by a highly scalable Gibbs sampler for fully Bayesian inference on large datasets. In addition, we describe broad limitations with the interpretability of scalar-on-function regression models, and introduce new decision analysis tools to guide the model interpretation. Using these methods, we identify a period within the third trimester as the critical window of susceptibility to $\mbox{PM}_{2.5}$ exposure.

</details>

<details>

<summary>2022-03-02 04:24:13 - A Unifying Framework for Some Directed Distances in Statistics</summary>

- *Michel Broniatowski, Wolfgang Stummer*

- `2203.00863v1` - [abs](http://arxiv.org/abs/2203.00863v1) - [pdf](http://arxiv.org/pdf/2203.00863v1)

> Density-based directed distances -- particularly known as divergences -- between probability distributions are widely used in statistics as well as in the adjacent research fields of information theory, artificial intelligence and machine learning. Prominent examples are the Kullback-Leibler information distance (relative entropy) which e.g. is closely connected to the omnipresent maximum likelihood estimation method, and Pearson's chisquare-distance which e.g. is used for the celebrated chisquare goodness-of-fit test. Another line of statistical inference is built upon distribution-function-based divergences such as e.g. the prominent (weighted versions of) Cramer-von Mises test statistics respectively Anderson-Darling test statistics which are frequently applied for goodness-of-fit investigations; some more recent methods deal with (other kinds of) cumulative paired divergences and closely related concepts. In this paper, we provide a general framework which covers in particular both the above-mentioned density-based and distribution-function-based divergence approaches; the dissimilarity of quantiles respectively of other statistical functionals will be included as well. From this framework, we structurally extract numerous classical and also state-of-the-art (including new) procedures. Furthermore, we deduce new concepts of dependence between random variables, as alternatives to the celebrated mutual information. Some variational representations are discussed, too.

</details>

<details>

<summary>2022-03-02 06:37:24 - Online Competitive Influence Maximization</summary>

- *Jinhang Zuo, Xutong Liu, Carlee Joe-Wong, John C. S. Lui, Wei Chen*

- `2006.13411v4` - [abs](http://arxiv.org/abs/2006.13411v4) - [pdf](http://arxiv.org/pdf/2006.13411v4)

> Online influence maximization has attracted much attention as a way to maximize influence spread through a social network while learning the values of unknown network parameters. Most previous works focus on single-item diffusion. In this paper, we introduce a new Online Competitive Influence Maximization (OCIM) problem, where two competing items (e.g., products, news stories) propagate in the same network and influence probabilities on edges are unknown. We adopt a combinatorial multi-armed bandit (CMAB) framework for OCIM, but unlike the non-competitive setting, the important monotonicity property (influence spread increases when influence probabilities on edges increase) no longer holds due to the competitive nature of propagation, which brings a significant new challenge to the problem. We provide a nontrivial proof showing that the Triggering Probability Modulated (TPM) condition for CMAB still holds in OCIM, which is instrumental for our proposed algorithms OCIM-TS and OCIM-OFU to achieve sublinear Bayesian and frequentist regret, respectively. We also design an OCIM-ETC algorithm that requires less feedback and easier offline computation, at the expense of a worse frequentist regret bound. Experimental evaluations demonstrate the effectiveness of our algorithms.

</details>

<details>

<summary>2022-03-02 07:45:07 - Metalearning Linear Bandits by Prior Update</summary>

- *Amit Peleg, Naama Pearl, Ron Meir*

- `2107.05320v2` - [abs](http://arxiv.org/abs/2107.05320v2) - [pdf](http://arxiv.org/pdf/2107.05320v2)

> Fully Bayesian approaches to sequential decision-making assume that problem parameters are generated from a known prior. In practice, such information is often lacking. This problem is exacerbated in setups with partial information, where a misspecified prior may lead to poor exploration and performance. In this work we prove, in the context of stochastic linear bandits and Gaussian priors, that as long as the prior is sufficiently close to the true prior, the performance of the applied algorithm is close to that of the algorithm that uses the true prior. Furthermore, we address the task of learning the prior through metalearning, where a learner updates her estimate of the prior across multiple task instances in order to improve performance on future tasks. We provide an algorithm and regret bounds, demonstrate its effectiveness in comparison to an algorithm that knows the correct prior, and support our theoretical results empirically. Our theoretical results hold for a broad class of algorithms, including Thompson Sampling and Information Directed Sampling.

</details>

<details>

<summary>2022-03-02 11:07:42 - Structural Gaussian Priors for Bayesian CT reconstruction of Subsea Pipes</summary>

- *Silja W. Christensen, Nicolai A. B. Riis, Felipe Uribe, Jakob S. Jrgensen*

- `2203.01030v1` - [abs](http://arxiv.org/abs/2203.01030v1) - [pdf](http://arxiv.org/pdf/2203.01030v1)

> A non-destructive testing (NDT) application of X-ray computed tomography (CT) is inspection of subsea pipes in operation via 2D cross-sectional scans. Data acquisition is time-consuming and costly due to the challenging subsea environment. Reducing the number of projections in a scan can yield time and cost savings, but compromises the reconstruction quality, if conventional reconstruction methods are used. In this work we take a Bayesian approach to CT reconstruction and focus on designing an effective prior to make use of available structural information about the pipe geometry. We propose a new class of structural Gaussian priors to enforce expected material properties in different regions of the reconstructed image based on independent Gaussian priors in combination with global regularity through a Gaussian Markov Random Field (GMRF) prior. Numerical experiments with synthetic and real data show that the proposed structural Gaussian prior can reduce artifacts and enhance contrast in the reconstruction compared to using only a global GMRF prior or no prior at all. We show how the resulting posterior distribution can be efficiently sampled even for large-scale images, which is essential for practical NDT applications.

</details>

<details>

<summary>2022-03-02 11:44:06 - A Bayesian Approach to CT Reconstruction with Uncertain Geometry</summary>

- *Frederik H. Pedersen, Jakob S. Jrgensen, Martin S. Andersen*

- `2203.01045v1` - [abs](http://arxiv.org/abs/2203.01045v1) - [pdf](http://arxiv.org/pdf/2203.01045v1)

> Computed tomography is a method for synthesizing volumetric or cross-sectional images of an object from a collection of projections. Popular reconstruction methods for computed tomography are based on idealized models and assumptions that may not be valid in practice. One such assumption is that the exact projection geometry is known. The projection geometry describes the relative location of the radiation source, object, and detector for each projection. However, in practice, the geometric parameters used to describe the position and orientation of the radiation source, object, and detector are estimated quantities with uncertainty. A failure to accurately estimate the geometry may lead to reconstructions with severe misalignment artifacts that significantly decrease their scientific or diagnostic value. We propose a novel reconstruction method that jointly estimates the reconstruction and the projection geometry. The reconstruction method is based on a Bayesian approach that yields a point estimate for the reconstruction and geometric parameters and, in addition, provides valuable information regarding their uncertainty. This is achieved by approximately sampling from the joint posterior distribution of the reconstruction and projection geometry using a hierarchical Gibbs sampler. Using real tomographic data, we demonstrate that the proposed reconstruction method significantly reduces misalignment artifacts. Compared with two commonly used alignment methods, our proposed method achieves comparable or better results under challenging conditions.

</details>

<details>

<summary>2022-03-02 17:24:36 - Variational Bayesian Approximation of Inverse Problems using Sparse Precision Matrices</summary>

- *Jan Povala, Ieva Kazlauskaite, Eky Febrianto, Fehmi Cirak, Mark Girolami*

- `2110.11840v2` - [abs](http://arxiv.org/abs/2110.11840v2) - [pdf](http://arxiv.org/pdf/2110.11840v2)

> Inverse problems involving partial differential equations (PDEs) are widely used in science and engineering. Although such problems are generally ill-posed, different regularisation approaches have been developed to ameliorate this problem. Among them is the Bayesian formulation, where a prior probability measure is placed on the quantity of interest. The resulting posterior probability measure is usually analytically intractable. The Markov Chain Monte Carlo (MCMC) method has been the go-to method for sampling from those posterior measures. MCMC is computationally infeasible for large-scale problems that arise in engineering practice. Lately, Variational Bayes (VB) has been recognised as a more computationally tractable method for Bayesian inference, approximating a Bayesian posterior distribution with a simpler trial distribution by solving an optimisation problem. In this work, we argue, through an empirical assessment, that VB methods are a flexible and efficient alternative to MCMC for this class of problems. We propose a natural choice of a family of Gaussian trial distributions parametrised by precision matrices, thus taking advantage of the inherent sparsity of the inverse problem encoded in its finite element discretisation. We utilise stochastic optimisation to efficiently estimate the variational objective and assess not only the error in the solution mean but also the ability to quantify the uncertainty of the estimate. We test this on PDEs based on the Poisson equation in 1D and 2D. A Tensorflow implementation is made publicly available on GitHub.

</details>

<details>

<summary>2022-03-02 18:41:22 - An Analysis of Ensemble Sampling</summary>

- *Chao Qin, Zheng Wen, Xiuyuan Lu, Benjamin Van Roy*

- `2203.01303v1` - [abs](http://arxiv.org/abs/2203.01303v1) - [pdf](http://arxiv.org/pdf/2203.01303v1)

> Ensemble sampling serves as a practical approximation to Thompson sampling when maintaining an exact posterior distribution over model parameters is computationally intractable. In this paper, we establish a Bayesian regret bound that ensures desirable behavior when ensemble sampling is applied to the linear bandit problem. This represents the first rigorous regret analysis of ensemble sampling and is made possible by leveraging information-theoretic concepts and novel analytic techniques that may prove useful beyond the scope of this paper.

</details>

<details>

<summary>2022-03-02 19:40:13 - Bayesian Nonlinear Models for Repeated Measurement Data: An Overview, Implementation, and Applications</summary>

- *Se Yoon Lee*

- `2201.12430v3` - [abs](http://arxiv.org/abs/2201.12430v3) - [pdf](http://arxiv.org/pdf/2201.12430v3)

> Nonlinear mixed effects models have become a standard platform for analysis when data is in the form of continuous and repeated measurements of subjects from a population of interest, while temporal profiles of subjects commonly follow a nonlinear tendency. While frequentist analysis of nonlinear mixed effects models has a long history, Bayesian analysis of the models has received comparatively little attention until the late 1980s due primarily to the time-consuming nature of Bayesian computation. Since the early 1990s Bayesian approaches for the models began to emerge to leverage rapid developments in computing power, and recently, have received significant attention due to (1) superiority to quantify the uncertainty of parameter estimation; (2) utility to incorporate prior knowledge into the models; and (3) flexibility to match exactly the increasing complexity of scientific research arising from diverse industrial and academic fields. This review article presents an overview of modeling strategies to implement Bayesian approaches for the nonlinear mixed effects models, ranging from designing a scientific question out of real-life problems to practical computations.

</details>

<details>

<summary>2022-03-02 20:27:09 - Dynamic Bayesian Predictive Synthesis in Time Series Forecasting</summary>

- *Kenichiro McAlinn, Mike West*

- `1601.07463v6` - [abs](http://arxiv.org/abs/1601.07463v6) - [pdf](http://arxiv.org/pdf/1601.07463v6)

> We discuss model and forecast combination in time series forecasting. A foundational Bayesian perspective based on agent opinion analysis theory defines a new framework for density forecast combination, and encompasses several existing forecast pooling methods. We develop a novel class of dynamic latent factor models for time series forecast synthesis; simulation-based computation enables implementation. These models can dynamically adapt to time-varying biases, miscalibration and inter-dependencies among multiple models or forecasters. A macroeconomic forecasting study highlights the dynamic relationships among synthesized forecast densities, as well as the potential for improved forecast accuracy at multiple horizons.

</details>

<details>

<summary>2022-03-02 22:01:40 - Fast and accurate approximation to informed Bayes factors for focal parameters</summary>

- *Frantiek Barto, Eric-Jan Wagenmakers*

- `2203.01435v1` - [abs](http://arxiv.org/abs/2203.01435v1) - [pdf](http://arxiv.org/pdf/2203.01435v1)

> We outline an approximation to informed Bayes factors for a focal parameter $\theta$ that requires only the maximum likelihood estimate $\hat\theta$ and its standard error. The approximation uses an estimated likelihood of $\theta$ and assumes that the posterior distribution for $\theta$ is unaffected by the choice of prior distribution for the nuisance parameters. The resulting Bayes factor for the null hypothesis $\mathcal{H}_0: \theta = \theta_0$ versus the alternative hypothesis $\mathcal{H}_1: \theta \sim g(\theta)$ is then easily obtained using the Savage--Dickey density ratio. Three real-data examples highlight the speed and closeness of the approximation compared to bridge sampling and Laplace's method. The proposed approximation facilitates Bayesian reanalyses of standard frequentist results, encourages application of Bayesian tests with informed priors, and alleviates the computational challenges that often frustrate both Bayesian sensitivity analyses and Bayes factor design analyses. The approximation is shown to fail under small sample sizes and when the posterior distribution of the focal parameter is substantially influenced by the prior distributions on the nuisance parameters. The methodology may also be used to approximate the posterior distribution for $\theta$ under $\mathcal{H}_1$.

</details>

<details>

<summary>2022-03-02 23:55:14 - Scalable Bayesian Optimization Using Vecchia Approximations of Gaussian Processes</summary>

- *Felix Jimenez, Matthias Katzfuss*

- `2203.01459v1` - [abs](http://arxiv.org/abs/2203.01459v1) - [pdf](http://arxiv.org/pdf/2203.01459v1)

> Bayesian optimization is a technique for optimizing black-box target functions. At the core of Bayesian optimization is a surrogate model that predicts the output of the target function at previously unseen inputs to facilitate the selection of promising input values. Gaussian processes (GPs) are commonly used as surrogate models but are known to scale poorly with the number of observations. We adapt the Vecchia approximation, a popular GP approximation from spatial statistics, to enable scalable high-dimensional Bayesian optimization. We develop several improvements and extensions, including training warped GPs using mini-batch gradient descent, approximate neighbor search, and selecting multiple input values in parallel. We focus on the use of our warped Vecchia GP in trust-region Bayesian optimization via Thompson sampling. On several test functions and on two reinforcement-learning problems, our methods compared favorably to the state of the art.

</details>

<details>

<summary>2022-03-03 00:23:18 - Bayesian Geostatistical Modeling for Discrete-Valued Processes</summary>

- *Xiaotian Zheng, Athanasios Kottas, Bruno Sans*

- `2111.01840v2` - [abs](http://arxiv.org/abs/2111.01840v2) - [pdf](http://arxiv.org/pdf/2111.01840v2)

> We introduce a flexible and scalable class of Bayesian geostatistical models for discrete data, based on the class of nearest neighbor mixture transition distribution processes (NNMP), referred to as discrete NNMP. The proposed class characterizes spatial variability by a weighted combination of first-order conditional probability mass functions (pmfs) for each one of a given number of neighbors. The approach supports flexible modeling for multivariate dependence through specification of general bivariate discrete distributions that define the conditional pmfs. Moreover, the discrete NNMP allows for construction of models given a pre-specified family of marginal distributions that can vary in space, facilitating covariate inclusion. In particular, we develop a modeling and inferential framework for copula-based NNMPs that can attain flexible dependence structures, motivating the use of bivariate copula families for spatial processes. Compared to the traditional class of spatial generalized linear mixed models, where spatial dependence is introduced through a transformation of response means, our process-based modeling approach provides both computational and inferential advantages. We illustrate the benefits with synthetic data examples and an analysis of North American Breeding Bird Survey data.

</details>

<details>

<summary>2022-03-03 03:35:53 - Flood hazard model calibration using multiresolution model output</summary>

- *Samantha Roth, Ben Seiyon Lee, Sanjib Sharma, Iman Hosseini-Shakib, Klaus Keller, Murali Haran*

- `2203.00840v2` - [abs](http://arxiv.org/abs/2203.00840v2) - [pdf](http://arxiv.org/pdf/2203.00840v2)

> Riverine floods pose a considerable risk to many communities. Improving the projections of flood hazard has the potential to inform the design and implementation of flood risk management strategies. Current flood hazard projections are uncertain. One uncertainty that is often overlooked is uncertainty about model parameters. Calibration methods use observations to quantify model parameter uncertainty. With limited computational resources, researchers typically calibrate models using either relatively few expensive model runs at a high spatial resolution or many cheaper runs at a lower spatial resolution. This leads to an open question: Is it possible to effectively combine information from the high and low resolution model runs? We propose a Gaussian process-based Bayesian emulation-calibration approach that assimilates model outputs and observations at multiple resolutions. We demonstrate our approach using the LISFLOOD-FP flood hazard model as a case study for a riverine community in Pennsylvania in the Eastern United States. Compared to considered existing single-resolution approaches, our method yields more accurate flood predictions. Our method is rather general and can be applied to calibrate other high dimensional computer models to help improve future projections.

</details>

<details>

<summary>2022-03-03 12:49:25 - Approximate Bayesian Computation Based on Maxima Weighted Isolation Kernel Mapping</summary>

- *Iurii S. Nagornov*

- `2201.12745v3` - [abs](http://arxiv.org/abs/2201.12745v3) - [pdf](http://arxiv.org/pdf/2201.12745v3)

> Motivation: A branching processes model yields an unevenly stochastically distributed dataset that consists of sparse and dense regions. This work addresses the problem of precisely evaluating parameters for such a model. Applying a branching processes model to an area such as cancer cell evolution faces a number of obstacles, including high dimensionality and the rare appearance of a result of interest. We take on the ambitious task of obtaining the coefficients of a model that reflects the relationship of driver gene mutations and cancer hallmarks on the basis of personal data regarding variant allele frequencies. Results: An approximate Bayesian computation method based on Isolation Kernel is developed. The method involves the transformation of row data to a Hilbert space (mapping) and the measurement of the similarity between simulated points and maxima weighted Isolation Kernel mapping related to the observation point. We also design a heuristic algorithm for parameter estimation that requires no calculation and is dimension independent. The advantages of the proposed machine learning method are illustrated using multidimensional test data as well as a specific example focused on cancer cell evolution.

</details>

<details>

<summary>2022-03-03 17:11:07 - Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers</summary>

- *Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Marios Kountouris, David Gesbert*

- `2203.01859v1` - [abs](http://arxiv.org/abs/2203.01859v1) - [pdf](http://arxiv.org/pdf/2203.01859v1)

> Standard Bayesian learning is known to have suboptimal generalization capabilities under model misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training criterion produces predictive distributions that are able to concurrently counteract the detrimental effects of model misspecification and outliers.

</details>

<details>

<summary>2022-03-03 18:25:33 - Sparse Bayesian Optimization</summary>

- *Sulin Liu, Qing Feng, David Eriksson, Benjamin Letham, Eytan Bakshy*

- `2203.01900v1` - [abs](http://arxiv.org/abs/2203.01900v1) - [pdf](http://arxiv.org/pdf/2203.01900v1)

> Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box objective functions. However, the application of BO to areas such as recommendation systems often requires taking the interpretability and simplicity of the configurations into consideration, a setting that has not been previously studied in the BO literature. To make BO applicable in this setting, we present several regularization-based approaches that allow us to discover sparse and more interpretable configurations. We propose a novel differentiable relaxation based on homotopy continuation that makes it possible to target sparsity by working directly with $L_0$ regularization. We identify failure modes for regularized BO and develop a hyperparameter-free method, sparsity exploring Bayesian optimization (SEBO) that seeks to simultaneously maximize a target objective and sparsity. SEBO and methods based on fixed regularization are evaluated on synthetic and real-world problems, and we show that we are able to efficiently optimize for sparsity.

</details>

<details>

<summary>2022-03-03 19:05:48 - Prior-preconditioned conjugate gradient method for accelerated Gibbs sampling in "large $n$ & large $p$" Bayesian sparse regression</summary>

- *Akihiko Nishimura, Marc A. Suchard*

- `1810.12437v6` - [abs](http://arxiv.org/abs/1810.12437v6) - [pdf](http://arxiv.org/pdf/1810.12437v6)

> In a modern observational study based on healthcare databases, the number of observations and of predictors typically range in the order of $10^5$ ~ $10^6$ and of $10^4$ ~ $10^5$. Despite the large sample size, data rarely provide sufficient information to reliably estimate such a large number of parameters. Sparse regression techniques provide potential solutions, one notable approach being the Bayesian methods based on shrinkage priors. In the "large n & large p" setting, however, posterior computation encounters a major bottleneck at repeated sampling from a high-dimensional Gaussian distribution, whose precision matrix $\Phi$ is expensive to compute and factorize. In this article, we present a novel algorithm to speed up this bottleneck based on the following observation: we can cheaply generate a random vector $b$ such that the solution to the linear system $\Phi \beta = b$ has the desired Gaussian distribution. We can then solve the linear system by the conjugate gradient (CG) algorithm through matrix-vector multiplications by $\Phi$; this involves no explicit factorization or calculation of $\Phi$ itself. Rapid convergence of CG in this context is guaranteed by the theory of prior-preconditioning we develop. We apply our algorithm to a clinically relevant large-scale observational study with n = 72,489 patients and p = 22,175 clinical covariates, designed to assess the relative risk of adverse events from two alternative blood anti-coagulants. Our algorithm demonstrates an order of magnitude speed-up in posterior inference, in our case cutting the computation time from two weeks to less than a day.

</details>

<details>

<summary>2022-03-03 20:10:56 - Optimality in Multivariate Tie-breaker Designs</summary>

- *Tim P. Morrison, Art B. Owen*

- `2202.10030v3` - [abs](http://arxiv.org/abs/2202.10030v3) - [pdf](http://arxiv.org/pdf/2202.10030v3)

> Tie-breaker designs (TBDs), in which subjects with extreme values are assigned treatment deterministically and those in the middle are randomized, are intermediate between regression discontinuity designs (RDDs) and randomized controlled trials (RCTs). TBDs thus provide a convenient mechanism by which to trade off between the treatment benefit of an RDD and the statistical efficiency gains of an RCT. We study a model where the expected response is one multivariate regression for treated subjects and another one for control subjects. For a given set of subject data we show how to use convex optimization to choose treatment probabilities that optimize a prospective $D$-optimality condition (expected information gain) adapted from Bayesian optimal design. We can incorporate economically motivated linear constraints on those treatment probabilities as well as monotonicity constraints that have a strong ethical motivation. Our condition can be used in two scenarios: known covariates with random treatments, and random covariates with random treatments. We find that optimality for the treatment effect coincides with optimality for the whole regression, and that the RCT satisfies moment conditions for optimality. For Gaussian data we can find optimal linear scorings of subjects, one for statistical efficiency and another for short term treatment benefit.   We apply the convex optimization solution to some real emergency triage data from MIMIC.

</details>

<details>

<summary>2022-03-03 20:23:18 - Multi-objective robust optimization using adaptive surrogate models for problems with mixed continuous-categorical parameters</summary>

- *M. Moustapha, A. Galimshina, G. Habert, B. Sudret*

- `2203.01996v1` - [abs](http://arxiv.org/abs/2203.01996v1) - [pdf](http://arxiv.org/pdf/2203.01996v1)

> Explicitly accounting for uncertainties is paramount to the safety of engineering structures. Optimization which is often carried out at the early stage of the structural design offers an ideal framework for this task. When the uncertainties are mainly affecting the objective function, robust design optimization is traditionally considered. This work further assumes the existence of multiple and competing objective functions that need to be dealt with simultaneously. The optimization problem is formulated by considering quantiles of the objective functions which allows for the combination of both optimality and robustness in a single metric. By introducing the concept of common random numbers, the resulting nested optimization problem may be solved using a general-purpose solver, herein the non-dominated sorting genetic algorithm (NSGA-II). The computational cost of such an approach is however a serious hurdle to its application in real-world problems. We therefore propose a surrogate-assisted approach using Kriging as an inexpensive approximation of the associated computational model. The proposed approach consists of sequentially carrying out NSGA-II while using an adaptively built Kriging model to estimate of the quantiles. Finally, the methodology is adapted to account for mixed categorical-continuous parameters as the applications involve the selection of qualitative design parameters as well. The methodology is first applied to two analytical examples showing its efficiency. The third application relates to the selection of optimal renovation scenarios of a building considering both its life cycle cost and environmental impact. It shows that when it comes to renovation, the heating system replacement should be the priority.

</details>

<details>

<summary>2022-03-04 01:02:01 - Hierarchical Bayesian Modeling of Ocean Heat Content and its Uncertainty</summary>

- *Samuel Baugh, Karen McKinnon*

- `2110.09717v2` - [abs](http://arxiv.org/abs/2110.09717v2) - [pdf](http://arxiv.org/pdf/2110.09717v2)

> The accurate quantification of changes in the heat content of the world's oceans is crucial for our understanding of the effects of increasing greenhouse gas concentrations. The Argo program, consisting of Lagrangian floats that measure vertical temperature profiles throughout the global ocean, has provided a wealth of data from which to estimate ocean heat content. However, creating a globally consistent statistical model for ocean heat content remains challenging due to the need for a globally valid covariance model that can capture complex nonstationarity. In this paper, we develop a hierarchical Bayesian Gaussian process model that uses kernel convolutions with cylindrical distances to allow for spatial non-stationarity in all model parameters while using a Vecchia process to remain computationally feasible for large spatial datasets. Our approach can produce valid credible intervals for globally integrated quantities that would not be possible using previous approaches. These advantages are demonstrated through the application of the model to Argo data, yielding credible intervals for the spatially varying trend in ocean heat content that accounts for both the uncertainty induced from interpolation and from estimating the mean field and other parameters. Through cross-validation, we show that our model out-performs an out-of-the-box approach as well as other simpler models. The code for performing this analysis is provided as the R package BayesianOHC.

</details>

<details>

<summary>2022-03-04 01:58:35 - Bayesian community detection for networks with covariates</summary>

- *Luyi Shen, Arash Amini, Nathaniel Josephs, Lizhen Lin*

- `2203.02090v1` - [abs](http://arxiv.org/abs/2203.02090v1) - [pdf](http://arxiv.org/pdf/2203.02090v1)

> The increasing prevalence of network data in a vast variety of fields and the need to extract useful information out of them have spurred fast developments in related models and algorithms. Among the various learning tasks with network data, community detection, the discovery of node clusters or "communities," has arguably received the most attention in the scientific community. In many real-world applications, the network data often come with additional information in the form of node or edge covariates that should ideally be leveraged for inference. In this paper, we add to a limited literature on community detection for networks with covariates by proposing a Bayesian stochastic block model with a covariate-dependent random partition prior. Under our prior, the covariates are explicitly expressed in specifying the prior distribution on the cluster membership. Our model has the flexibility of modeling uncertainties of all the parameter estimates including the community membership. Importantly, and unlike the majority of existing methods, our model has the ability to learn the number of the communities via posterior inference without having to assume it to be known. Our model can be applied to community detection in both dense and sparse networks, with both categorical and continuous covariates, and our MCMC algorithm is very efficient with good mixing properties. We demonstrate the superior performance of our model over existing models in a comprehensive simulation study and an application to two real datasets.

</details>

<details>

<summary>2022-03-04 11:03:40 - Elements of Sequential Monte Carlo</summary>

- *Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schn*

- `1903.04797v2` - [abs](http://arxiv.org/abs/1903.04797v2) - [pdf](http://arxiv.org/pdf/1903.04797v2)

> A core problem in statistics and probabilistic machine learning is to compute probability distributions and expectations. This is the fundamental problem of Bayesian statistics and machine learning, which frames all inference as expectations with respect to the posterior distribution. The key challenge is to approximate these intractable expectations. In this tutorial, we review sequential Monte Carlo (SMC), a random-sampling-based class of methods for approximate inference. First, we explain the basics of SMC, discuss practical issues, and review theoretical results. We then examine two of the main user design choices: the proposal distributions and the so called intermediate target distributions. We review recent results on how variational inference and amortization can be used to learn efficient proposals and target distributions. Next, we discuss the SMC estimate of the normalizing constant, how this can be used for pseudo-marginal inference and inference evaluation. Throughout the tutorial we illustrate the use of SMC on various models commonly used in machine learning, such as stochastic recurrent neural networks, probabilistic graphical models, and probabilistic programs.

</details>

<details>

<summary>2022-03-04 12:36:01 - Variance Reduction for Metropolis-Hastings Samplers</summary>

- *Angelos Alexopoulos, Petros Dellaportas, Michalis K. Titsias*

- `2203.02268v1` - [abs](http://arxiv.org/abs/2203.02268v1) - [pdf](http://arxiv.org/pdf/2203.02268v1)

> We introduce a general framework that constructs estimators with reduced variance for random walk Metropolis and Metropolis-adjusted Langevin algorithms. The resulting estimators require negligible computational cost and are derived in a post-process manner utilising all proposal values of the Metropolis algorithms. Variance reduction is achieved by producing control variates through the approximate solution of the Poisson equation associated with the target density of the Markov chain. The proposed method is based on approximating the target density with a Gaussian and then utilising accurate solutions of the Poisson equation for the Gaussian case. This leads to an estimator that uses two key elements: (i) a control variate from the Poisson equation that contains an intractable expectation under the proposal distribution, (ii) a second control variate to reduce the variance of a Monte Carlo estimate of this latter intractable expectation. Simulated data examples are used to illustrate the impressive variance reduction achieved in the Gaussian target case and the corresponding effect when target Gaussianity assumption is violated. Real data examples on Bayesian logistic regression and stochastic volatility models verify that considerable variance reduction is achieved with negligible extra computational cost.

</details>

<details>

<summary>2022-03-04 20:26:48 - Equivalence testing for standardized effect sizes in linear regression</summary>

- *Harlan Campbell*

- `2004.01757v6` - [abs](http://arxiv.org/abs/2004.01757v6) - [pdf](http://arxiv.org/pdf/2004.01757v6)

> We introduce equivalence testing procedures for standardized effect sizes in a linear regression analysis. Such tests are useful for confirming the lack of a meaningful association between a continuous outcome and continuous/binary covariates and may be particularly valuable if the covariates of interest are measured on different and somewhat arbitrary scales. We consider how to define valid hypotheses and how to calculate p-values for these equivalence tests. Via simulation study we examine type I error rates and statistical power; and we compare the proposed frequentist equivalence testing to an alternative Bayesian testing approach.

</details>

<details>

<summary>2022-03-05 01:51:25 - Theoretical results and modeling under the discrete Birnbaum-Saunders distribution</summary>

- *Filidor Vilca, Roberto Vila, Helton Saulo, Luis Snchez, Jeremias Leo*

- `2203.02639v1` - [abs](http://arxiv.org/abs/2203.02639v1) - [pdf](http://arxiv.org/pdf/2203.02639v1)

> In this paper, we discuss some theoretical results and properties of a discrete version of the Birnbaum-Saunders distribution. We present a proof of the unimodality of this model. Moreover, results on moments, quantile function, reliability and order statistics are also presented. In addition, we propose a regression model based on the discrete Birnbaum-Saunders distribution. The model parameters are estimated by the maximum likelihood method and a Monte Carlo study is performed to evaluate the performance of the estimators. Finally, we illustrate the proposed methodology with the use of real data sets.

</details>

<details>

<summary>2022-03-05 07:44:22 - A Bayesian Graphical Approach for Large-Scale Portfolio Management with Fewer Historical Data</summary>

- *Sakae Oya*

- `2103.05880v4` - [abs](http://arxiv.org/abs/2103.05880v4) - [pdf](http://arxiv.org/pdf/2103.05880v4)

> Managing a large-scale portfolio with many assets is one of the most challenging tasks in the field of finance. It is partly because estimation of either covariance or precision matrix of asset returns tends to be unstable or even infeasible when the number of assets $p$ exceeds the number of observations $n$. For this reason, most of the previous studies on portfolio management have focused on the case of$ p < n$. To deal with the case of $p > n$, we propose to use a new Bayesian framework based on adaptive graphical LASSO for estimating the precision matrix of asset returns in a large-scale portfolio. Unlike the previous studies on graphical LASSO in the literature, our approach utilizes a Bayesian estimation method for the precision matrix proposed by Oya and Nakatsuma (2022) so that the positive definiteness of the precision matrix should be always guaranteed. As an empirical application, we construct the global minimum variance portfolio of $p = 100$ for various values of n with the proposed approach as well as the non-Bayesian graphical LASSO approach, and compare their out-of-sample performance with the equal weight portfolio as the benchmark. In this comparison, the proposed approach produces more stable results than the non-Bayesian approach in terms of Sharpe ratio, portfolio composition and turnover. Furthermore, the proposed approach succeeds in estimating the precision matrix even if $n$ is much smaller than $p$ and the non-Bayesian approach fails to do so.

</details>

<details>

<summary>2022-03-05 15:24:39 - Remiod: Reference-based Controlled Multiple Imputation of Longitudinal Binary and Ordinal Outcomes with non-ignorable missingness</summary>

- *Tony Wang, Ying Liu*

- `2203.02771v1` - [abs](http://arxiv.org/abs/2203.02771v1) - [pdf](http://arxiv.org/pdf/2203.02771v1)

> Missing data on response variables are common in clinical studies. Corresponding to the uncertainty of missing mechanism, theoretical frameworks on controlled imputation have been developed. In practice, it is recommended to conduct a statistically valid analysis under the primary assumptions on missing data, followed by sensitivity analysis under alternative assumptions to assess the robustness of results. Due to the availability of software, controlled multiple imputation (MI) procedures, including delta-based and reference-based approaches, have become popular for analyzing continuous variables under missing-not-at-random assumptions. Similar tools, however, still limit application of these methods to categorical data. In this paper, we introduce the R package \textbf{remiod}, which utilizes the Bayesian framework to perform imputation in regression models on binary and ordinal outcomes. Following outlining theoretical backgrounds, usage and features of \textbf{remiod} are described and illustrated using examples.

</details>

<details>

<summary>2022-03-05 18:52:28 - Estimating Design Operating Characteristics in Bayesian Adaptive Clinical Trials</summary>

- *Shirin Golchi*

- `2105.03022v2` - [abs](http://arxiv.org/abs/2105.03022v2) - [pdf](http://arxiv.org/pdf/2105.03022v2)

> Bayesian adaptive designs have gained popularity in all phases of clinical trials with numerous new developments in the past few decades. During the COVID-19 pandemic, the need to establish evidence for the effectiveness of vaccines, therapeutic treatments and policies that could resolve or control the crisis emphasized the advantages offered by efficient and flexible clinical trial designs. In many COVID-19 clinical trials, due to the high level of uncertainty, Bayesian adaptive designs were considered advantageous. Designing Bayesian adaptive trials, however, requires extensive simulation studies that are generally considered challenging, particularly in time-sensitive settings such as a pandemic. In this article, we propose a set of methods for efficient estimation and uncertainty quantification for design operating characteristics of Bayesian adaptive trials. Specifically, we model the sampling distribution of Bayesian probability statements that are commonly used as the basis of decision making. To showcase the implementation and performance of the proposed approach, we use a clinical trial design with an ordinal disease-progression scale endpoint that was popular among COVID-19 trial. However, the proposed methodology may be applied generally in clinical trial context where design operating characteristics cannot be obtained analytically.

</details>

<details>

<summary>2022-03-05 19:06:38 - Posterior Representations for Bayesian Context Trees: Sampling, Estimation and Convergence</summary>

- *Ioannis Papageorgiou, Ioannis Kontoyiannis*

- `2202.02239v2` - [abs](http://arxiv.org/abs/2202.02239v2) - [pdf](http://arxiv.org/pdf/2202.02239v2)

> We revisit the Bayesian Context Trees (BCT) modelling framework for discrete time series, which was recently found to be very effective in numerous tasks including model selection, estimation and prediction. A novel representation of the induced posterior distribution on model space is derived in terms of a simple branching process, and several consequences of this are explored in theory and in practice. First, it is shown that the branching process representation leads to a simple variable-dimensional Monte Carlo sampler for the joint posterior distribution on models and parameters, which can efficiently produce independent samples. This sampler is found to be more efficient than earlier MCMC samplers for the same tasks. Then, the branching process representation is used to establish the asymptotic consistency of the BCT posterior, including the derivation of an almost-sure convergence rate. Finally, an extensive study is carried out on the performance of the induced Bayesian entropy estimator. Its utility is illustrated through both simulation experiments and real-world applications, where it is found to outperform several state-of-the-art methods.

</details>

<details>

<summary>2022-03-06 13:08:33 - Sparse Bayesian Learning with Diagonal Quasi-Newton Method for Large Scale Classification</summary>

- *Jiahua Luo, Chi-Man Vong, Jie Du*

- `2107.08195v3` - [abs](http://arxiv.org/abs/2107.08195v3) - [pdf](http://arxiv.org/pdf/2107.08195v3)

> Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity O(M^3 ) (M: feature size) for updating the regularization priors, making it difficult for practical use. There are three issues in SBL: 1) Inverting the covariance matrix may obtain singular solutions in some cases, which hinders SBL from convergence; 2) Poor scalability to problems with high dimensional feature space or large data size; 3) SBL easily suffers from memory overflow for large-scale data. This paper addresses these issues with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity and memory storage are reduced to O(M). The DQN-SBL is thoroughly evaluated on non-linear classifiers and linear feature selection using various benchmark datasets of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.

</details>

<details>

<summary>2022-03-06 17:30:55 - Unbiased Estimation using a Class of Diffusion Processes</summary>

- *Hamza Ruzayqat, Alexandros Beskos, Dan Crisan, Ajay Jasra, Nikolas Kantas*

- `2203.03013v1` - [abs](http://arxiv.org/abs/2203.03013v1) - [pdf](http://arxiv.org/pdf/2203.03013v1)

> We study the problem of unbiased estimation of expectations with respect to (w.r.t.) $\pi$ a given, general probability measure on $(\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$ that is absolutely continuous with respect to a standard Gaussian measure. We focus on simulation associated to a particular class of diffusion processes, sometimes termed the Schr\"odinger-F\"ollmer Sampler, which is a simulation technique that approximates the law of a particular diffusion bridge process $\{X_t\}_{t\in [0,1]}$ on $\mathbb{R}^d$, $d\in \mathbb{N}_0$. This latter process is constructed such that, starting at $X_0=0$, one has $X_1\sim \pi$. Typically, the drift of the diffusion is intractable and, even if it were not, exact sampling of the associated diffusion is not possible. As a result, \cite{sf_orig,jiao} consider a stochastic Euler-Maruyama scheme that allows the development of biased estimators for expectations w.r.t.~$\pi$. We show that for this methodology to achieve a mean square error of $\mathcal{O}(\epsilon^2)$, for arbitrary $\epsilon>0$, the associated cost is $\mathcal{O}(\epsilon^{-5})$. We then introduce an alternative approach that provides unbiased estimates of expectations w.r.t.~$\pi$, that is, it does not suffer from the time discretization bias or the bias related with the approximation of the drift function. We prove that to achieve a mean square error of $\mathcal{O}(\epsilon^2)$, the associated cost is, with high probability, $\mathcal{O}(\epsilon^{-2}|\log(\epsilon)|^{2+\delta})$, for any $\delta>0$. We implement our method on several examples including Bayesian inverse problems.

</details>

<details>

<summary>2022-03-06 19:06:53 - Weighted-average quantile regression</summary>

- *Denis Chetverikov, Yukun Liu, Aleh Tsyvinski*

- `2203.03032v1` - [abs](http://arxiv.org/abs/2203.03032v1) - [pdf](http://arxiv.org/pdf/2203.03032v1)

> In this paper, we introduce the weighted-average quantile regression framework, $\int_0^1 q_{Y|X}(u)\psi(u)du = X'\beta$, where $Y$ is a dependent variable, $X$ is a vector of covariates, $q_{Y|X}$ is the quantile function of the conditional distribution of $Y$ given $X$, $\psi$ is a weighting function, and $\beta$ is a vector of parameters. We argue that this framework is of interest in many applied settings and develop an estimator of the vector of parameters $\beta$. We show that our estimator is $\sqrt T$-consistent and asymptotically normal with mean zero and easily estimable covariance matrix, where $T$ is the size of available sample. We demonstrate the usefulness of our estimator by applying it in two empirical settings. In the first setting, we focus on financial data and study the factor structures of the expected shortfalls of the industry portfolios. In the second setting, we focus on wage data and study inequality and social welfare dependence on commonly used individual characteristics.

</details>

<details>

<summary>2022-03-06 21:38:18 - Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?</summary>

- *Lisha Chen, Tianyi Chen*

- `2203.03059v1` - [abs](http://arxiv.org/abs/2203.03059v1) - [pdf](http://arxiv.org/pdf/2203.03059v1)

> Meta learning aims at learning a model that can quickly adapt to unseen tasks. Widely used meta learning methods include model agnostic meta learning (MAML), implicit MAML, Bayesian MAML. Thanks to its ability of modeling uncertainty, Bayesian MAML often has advantageous empirical performance. However, the theoretical understanding of Bayesian MAML is still limited, especially on questions such as if and when Bayesian MAML has provably better performance than MAML. In this paper, we aim to provide theoretical justifications for Bayesian MAML's advantageous performance by comparing the meta test risks of MAML and Bayesian MAML. In the meta linear regression, under both the distribution agnostic and linear centroid cases, we have established that Bayesian MAML indeed has provably lower meta test risks than MAML. We verify our theoretical results through experiments.

</details>

<details>

<summary>2022-03-07 06:24:32 - Bayesian calibration of simulation models: A tutorial and an Australian smoking behaviour model</summary>

- *Stephen Wade, Marianne F Weber, Peter Sarich, Pavla Vaneckova, Silvia Behar-Harpaz, Preston J Ngo, Sonya Cressman, Coral E Gartner, John M Murray, Tony A Blakely, Emily Banks, Martin C Tammemagi, Karen Canfell, Michael Caruana*

- `2202.02923v2` - [abs](http://arxiv.org/abs/2202.02923v2) - [pdf](http://arxiv.org/pdf/2202.02923v2)

> Simulation models of epidemiological, biological, ecological, and environmental processes are increasingly being calibrated using Bayesian statistics. The Bayesian approach provides simple rules to synthesise multiple data sources and to calculate uncertainty in model output due to uncertainty in the calibration data. As the number of tutorials and studies published grow, the solutions to common difficulties in Bayesian calibration across these fields have become more apparent, and a step-by-step process for successful calibration across all these fields is emerging. We provide a statement of the key steps in a Bayesian calibration, and we outline analyses and approaches to each step that have emerged from one or more of these applied sciences. Thus we present a synthesis of Bayesian calibration methodologies that cut across a number of scientific disciplines.   To demonstrate these steps and to provide further detail on the computations involved in Bayesian calibration, we calibrated a compartmental model of tobacco smoking behaviour in Australia. We found that the proportion of a birth cohort estimated to take up smoking before they reach age 20 years in 2016 was at its lowest value since the early 20th century, and that quit rates were at their highest. As a novel outcome, we quantified the rate that ex-smokers switched to reporting as a 'never smoker' when surveyed later in life; a phenomenon that, to our knowledge, has never been quantified using cross-sectional survey data.

</details>

<details>

<summary>2022-03-07 11:23:12 - PAC-Bayesian Lifelong Learning For Multi-Armed Bandits</summary>

- *Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters*

- `2203.03303v1` - [abs](http://arxiv.org/abs/2203.03303v1) - [pdf](http://arxiv.org/pdf/2203.03303v1)

> We present a PAC-Bayesian analysis of lifelong learning. In the lifelong learning problem, a sequence of learning tasks is observed one-at-a-time, and the goal is to transfer information acquired from previous tasks to new learning tasks. We consider the case when each learning task is a multi-armed bandit problem. We derive lower bounds on the expected average reward that would be obtained if a given multi-armed bandit algorithm was run in a new task with a particular prior and for a set number of steps. We propose lifelong learning algorithms that use our new bounds as learning objectives. Our proposed algorithms are evaluated in several lifelong multi-armed bandit problems and are found to perform better than a baseline method that does not use generalisation bounds.

</details>

<details>

<summary>2022-03-07 13:03:16 - Discovering Inductive Bias with Gibbs Priors: A Diagnostic Tool for Approximate Bayesian Inference</summary>

- *Luca Rendsburg, Agustinus Kristiadi, Philipp Hennig, Ulrike von Luxburg*

- `2203.03353v1` - [abs](http://arxiv.org/abs/2203.03353v1) - [pdf](http://arxiv.org/pdf/2203.03353v1)

> Full Bayesian posteriors are rarely analytically tractable, which is why real-world Bayesian inference heavily relies on approximate techniques. Approximations generally differ from the true posterior and require diagnostic tools to assess whether the inference can still be trusted. We investigate a new approach to diagnosing approximate inference: the approximation mismatch is attributed to a change in the inductive bias by treating the approximations as exact and reverse-engineering the corresponding prior. We show that the problem is more complicated than it appears to be at first glance, because the solution generally depends on the observation. By reframing the problem in terms of incompatible conditional distributions we arrive at a natural solution: the Gibbs prior. The resulting diagnostic is based on pseudo-Gibbs sampling, which is widely applicable and easy to implement. We illustrate how the Gibbs prior can be used to discover the inductive bias in a controlled Gaussian setting and for a variety of Bayesian models and approximations.

</details>

<details>

<summary>2022-03-07 14:22:25 - Information Field Theory and Artificial Intelligence</summary>

- *Torsten Enlin*

- `2112.10133v3` - [abs](http://arxiv.org/abs/2112.10133v3) - [pdf](http://arxiv.org/pdf/2112.10133v3)

> Information field theory (IFT), the information theory for fields, is a mathematical framework for signal reconstruction and non-parametric inverse problems. Artificial intelligence (AI) and machine learning (ML) aim at generating intelligent systems including such for perception, cognition, and learning. This overlaps with IFT, which is designed to address perception, reasoning, and inference tasks. Here, the relation between concepts and tools in IFT and those in AI and ML research are discussed. In the context of IFT, fields denote physical quantities that change continuously as a function of space (and time) and information theory refers to Bayesian probabilistic logic equipped with the associated entropic information measures. Reconstructing a signal with IFT is a computational problem similar to training a generative neural network (GNN) in ML. In this paper, the process of inference in IFT is reformulated in terms of GNN training. In contrast to classical neural networks, IFT based GNNs can operate without pre-training thanks to incorporating expert knowledge into their architecture. Furthermore, the cross-fertilization of variational inference methods used in IFT and ML are discussed. These discussions suggests that IFT is well suited to address many problems in AI and ML research and application.

</details>

<details>

<summary>2022-03-07 15:43:41 - Bayesian Mendelian randomization testing of interval causal null hypotheses: ternary decision rules and loss function calibration</summary>

- *Linyi Zou, Teresa Fazia, Hui Guo, Carlo Berzuini*

- `2203.03474v1` - [abs](http://arxiv.org/abs/2203.03474v1) - [pdf](http://arxiv.org/pdf/2203.03474v1)

> Our approach to Mendelian Randomization (MR) analysis is designed to increase reproducibility of causal effect "discoveries" by: (i) using a Bayesian approach to inference; (ii) replacing the point null hypothesis with a region of practical equivalence consisting of values of negligible magnitude for the effect of interest, while exploiting the ability of Bayesian analysis to quantify the evidence of the effect falling inside/outside the region; (iii) rejecting the usual binary decision logic in favour of a ternary logic where the hypothesis test may result in either an acceptance or a rejection of the null, while also accommodating an "uncertain" outcome. We present an approach to calibration of the proposed method via loss function, which we use to compare our approach with a frequentist one. We illustrate the method with the aid of a study of the causal effect of obesity on risk of juvenile myocardial infarction.

</details>

<details>

<summary>2022-03-07 15:55:29 - Sequential Bayesian experimental design for estimation of extreme-event probability in stochastic dynamical systems</summary>

- *Xianliang Gong, Yulin Pan*

- `2102.11108v4` - [abs](http://arxiv.org/abs/2102.11108v4) - [pdf](http://arxiv.org/pdf/2102.11108v4)

> We consider an input-to-response (ItR) system characterized by (1) parameterized input with a known probability distribution and (2) stochastic ItR function with heteroscedastic randomness. Our purpose is to efficiently quantify the extreme response probability when the ItR function is expensive to evaluate. The problem setup arises often in physics and engineering problems, with randomness in ItR coming from either intrinsic uncertainties (say, as a solution to a stochastic equation) or additional (critical) uncertainties that are not incorporated in a low-dimensional input parameter space (as a result of dimension reduction applied to the original high-dimensional input space). To reduce the required sampling numbers, we develop a sequential Bayesian experimental design method leveraging the variational heteroscedastic Gaussian process regression (VHGPR) to account for the stochastic ItR, along with a new criterion to select the next-best samples sequentially. The validity of our new method is first tested in two synthetic problems with the stochastic ItR functions defined artificially. Finally, we demonstrate the application of our method to an engineering problem of estimating the extreme ship motion probability in irregular waves, where the uncertainty in ItR naturally originates from standard wave group parameterization, which reduces the original high-dimensional wave field into a two-dimensional parameter space.

</details>

<details>

<summary>2022-03-07 18:59:54 - Bayesian Bilinear Neural Network for Predicting the Mid-price Dynamics in Limit-Order Book Markets</summary>

- *Martin Magris, Mostafa Shabani, Alexandros Iosifidis*

- `2203.03613v1` - [abs](http://arxiv.org/abs/2203.03613v1) - [pdf](http://arxiv.org/pdf/2203.03613v1)

> The prediction of financial markets is a challenging yet important task. In modern electronically-driven markets traditional time-series econometric methods often appear incapable of capturing the true complexity of the multi-level interactions driving the price dynamics. While recent research has established the effectiveness of traditional machine learning (ML) models in financial applications, their intrinsic inability in dealing with uncertainties, which is a great concern in econometrics research and real business applications, constitutes a major drawback. Bayesian methods naturally appear as a suitable remedy conveying the predictive ability of ML methods with the probabilistically-oriented practice of econometric research. By adopting a state-of-the-art second-order optimization algorithm, we train a Bayesian bilinear neural network with temporal attention, suitable for the challenging time-series task of predicting mid-price movements in ultra-high-frequency limit-order book markets. By addressing the use of predictive distributions to analyze errors and uncertainties associated with the estimated parameters and model forecasts, we thoroughly compare our Bayesian model with traditional ML alternatives. Our results underline the feasibility of the Bayesian deep learning approach and its predictive and decisional advantages in complex econometric tasks, prompting future research in this direction.

</details>

<details>

<summary>2022-03-08 00:15:10 - Bayesian Analysis of Social Influence</summary>

- *Johan Koskinen, Galina Daraganova*

- `2006.16464v3` - [abs](http://arxiv.org/abs/2006.16464v3) - [pdf](http://arxiv.org/pdf/2006.16464v3)

> The network influence model is a model for binary outcome variables that accounts for dependencies between outcomes for units that are relationally tied. The basic influence model was previously extended to afford a suite of new dependence assumptions and because of its relation to traditional Markov random field models it is often referred to as the auto logistic actor-attribute model (ALAAM). We extend on current approaches for fitting ALAAMs by presenting a comprehensive Bayesian inference scheme that supports testing of dependencies across subsets of data and the presence of missing data. We illustrate different aspects of the procedures through three empirical examples: masculinity attitudes in an all-male Australian school class, educational progression in Swedish schools, and un-employment among adults in a community sample in Australia.

</details>

<details>

<summary>2022-03-08 06:08:59 - The application of accumulation tests in Peaks-Over-Threshold modeling with Norwegian Fire insurance Data</summary>

- *Bowen Liu, Malwane M. A. Ananda*

- `2203.03866v1` - [abs](http://arxiv.org/abs/2203.03866v1) - [pdf](http://arxiv.org/pdf/2203.03866v1)

> Modeling excess remains to be an important topic in insurance data modeling. Among the alternatives of modeling excess, the Peaks Over Threshold (POT) framework with Generalized Pareto distribution (GPD) is regarded as an efficient approach due to its flexibility. However, the selection of an appropriate threshold for such framework is a major difficulty. To address such difficulty, we applied several accumulation tests along with Anderson-Darling test to determine an optimal threshold. Based on the selected thresholds, the fitted GPD with the estimated quantiles can be found. We applied the procedure to the well-known Norwegian Fire Insurance data and constructed the confidence intervals for the Value-at-Risks (VaR). The accumulation test approach provides satisfactory performance in modeling the high quantiles of Norwegian Fire Insurance data compared to the previous graphical methods.

</details>

<details>

<summary>2022-03-08 14:25:30 - ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models</summary>

- *Louis Rouillard, Demian Wassermann*

- `2106.12248v3` - [abs](http://arxiv.org/abs/2106.12248v3) - [pdf](http://arxiv.org/pdf/2106.12248v3)

> Frequently, population studies feature pyramidally-organized data represented using Hierarchical Bayesian Models (HBM) enriched with plates. These models can become prohibitively large in settings such as neuroimaging, where a sample is composed of a functional MRI signal measured on 300 brain locations, across 4 measurement sessions, and 30 subjects, resulting in around 1 million latent parameters.Such high dimensionality hampers the usage of modern, expressive flow-based techniques.To infer parameter posterior distributions in this challenging class of problems, we designed a novel methodology that automatically produces a variational family dual to a target HBM. This variational family, represented as a neural network, consists in the combination of an attention-based hierarchical encoder feeding summary statistics to a set of normalizing flows. Our automatically-derived neural network exploits exchangeability in the plate-enriched HBM and factorizes its parameter space. The resulting architecture reduces by orders of magnitude its parameterization with respect to that of a typical flow-based representation, while maintaining expressivity.Our method performs inference on the specified HBM in an amortized setup: once trained, it can readily be applied to a new data sample to compute the parameters' full posterior.We demonstrate the capability and scalability of our method on simulated data, as well as a challenging high-dimensional brain parcellation experiment. We also open up several questions that lie at the intersection between normalizing flows, SBI, structured Variational Inference, and inference amortization.

</details>

<details>

<summary>2022-03-08 15:37:00 - Introduction to Automatic Backward Filtering Forward Guiding</summary>

- *Frank van der Meulen*

- `2203.04155v1` - [abs](http://arxiv.org/abs/2203.04155v1) - [pdf](http://arxiv.org/pdf/2203.04155v1)

> In this document I aim to give an informal treatment of automatic Backward Filtering Forward Guiding, a general algorithm for conditional sampling from a Markov process on a directed acyclic graph. I'll show that the underlying ideas can be understood with a basic background in probability and statistics. The more technical treatment is the paper Automatic backward filtering forward guiding for Markov processes and graphical models (Van der Meulen and Schauer, 2021). I specifically assume some background knowledge on likelihood based inference and Bayesian statistics. The final sections are more demanding and assume familiarity with continuous-time stochastic processes constructed from their infinitesimal generator.   Clearly, all work discussed here is the result of research carried out over the past decade together with various collaborators, most importantly Moritz Schauer (Chalmers University of Technology and University of Gothenburg, Sweden). Section 8 is based on joint work with Marcin Mider (Trium Analysis Online GmbH, Germany) and Frank Sch\"afer (University of Basel, Switzerland) as well.

</details>

<details>

<summary>2022-03-08 15:52:26 - On the intrinsic dimensionality of Covid-19 data: a global perspective</summary>

- *Abhishek Varghese, Edgar Santos-Fernandez, Francesco Denti, Antonietta Mira, Kerrie Mengersen*

- `2203.04165v1` - [abs](http://arxiv.org/abs/2203.04165v1) - [pdf](http://arxiv.org/pdf/2203.04165v1)

> This paper aims to develop a global perspective of the complexity of the relationship between the standardised per-capita growth rate of Covid-19 cases, deaths, and the OxCGRT Covid-19 Stringency Index, a measure describing a country's stringency of lockdown policies. To achieve our goal, we use a heterogeneous intrinsic dimension estimator implemented as a Bayesian mixture model, called Hidalgo. We identify that the Covid-19 dataset may project onto two low-dimensional manifolds without significant information loss. The low dimensionality suggests strong dependency among the standardised growth rates of cases and deaths per capita and the OxCGRT Covid-19 Stringency Index for a country over 2020-2021. Given the low dimensional structure, it may be feasible to model observable Covid-19 dynamics with few parameters. Importantly, we identify spatial autocorrelation in the intrinsic dimension distribution worldwide. Moreover, we highlight that high-income countries are more likely to lie on low-dimensional manifolds, likely arising from aging populations, comorbidities, and increased per capita mortality burden from Covid-19. Finally, we temporally stratify the dataset to examine the intrinsic dimension at a more granular level throughout the Covid-19 pandemic.

</details>

<details>

<summary>2022-03-08 16:00:34 - Valid prediction intervals for regression problems</summary>

- *Nicolas Dewolf, Bernard De Baets, Willem Waegeman*

- `2107.00363v3` - [abs](http://arxiv.org/abs/2107.00363v3) - [pdf](http://arxiv.org/pdf/2107.00363v3)

> Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. In this work, we review the above four classes of methods from a conceptual and experimental point of view. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step.

</details>

<details>

<summary>2022-03-08 18:47:01 - Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models</summary>

- *Vincent Lim, Ellen Novoseller, Jeffrey Ichnowski, Huang Huang, Ken Goldberg*

- `2203.04272v1` - [abs](http://arxiv.org/abs/2203.04272v1) - [pdf](http://arxiv.org/pdf/2203.04272v1)

> For applications in healthcare, physics, energy, robotics, and many other fields, designing maximally informative experiments is valuable, particularly when experiments are expensive, time-consuming, or pose safety hazards. While existing approaches can sequentially design experiments based on prior observation history, many of these methods do not extend to implicit models, where simulation is possible but computing the likelihood is intractable. Furthermore, they often require either significant online computation during deployment or a differentiable simulation system. We introduce Reinforcement Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based optimal experimental design for non-differentiable implicit models. RL-DAD extends prior work in policy-based Bayesian Optimal Experimental Design (BOED) by reformulating it as a Markov Decision Process with a reward function based on likelihood-free information lower bounds, which is used to learn a policy via deep reinforcement learning. The learned design policy maps prior histories to experiment designs offline and can be quickly deployed during online execution. We evaluate RL-DAD and find that it performs competitively with baselines on three benchmarks.

</details>

<details>

<summary>2022-03-08 18:57:42 - Bayesian Persuasion with Mediators</summary>

- *Itai Arieli, Yakov Babichenko, Fedor Sandomirskiy*

- `2203.04285v1` - [abs](http://arxiv.org/abs/2203.04285v1) - [pdf](http://arxiv.org/pdf/2203.04285v1)

> A sender communicates with a receiver through a sequence of mediators. The sender is the only informed agent and the receiver is the only one taking an action. All the agents have their own utility functions, which depend on the receiver's action and the state.   For any number of mediators, the sender's optimal value is characterized. For one mediator, the characterization has a clear geometric meaning of constrained concavification of the sender's utility, optimal persuasion requires the same number of signals as without mediators, and the presence of the mediator is never profitable for the sender. Surprisingly, the second mediator may improve the sender's utility; however, optimal persuasion with several mediators may require more signals.

</details>

<details>

<summary>2022-03-08 20:50:39 - Structural Learning of Simple Staged Trees</summary>

- *Manuele Leonelli, Gherardo Varando*

- `2203.04390v1` - [abs](http://arxiv.org/abs/2203.04390v1) - [pdf](http://arxiv.org/pdf/2203.04390v1)

> Bayesian networks faithfully represent the symmetric conditional independences existing between the components of a random vector. Staged trees are an extension of Bayesian networks for categorical random vectors whose graph represents non-symmetric conditional independences via vertex coloring. However, since they are based on a tree representation of the sample space, the underlying graph becomes cluttered and difficult to visualize as the number of variables increases. Here we introduce the first structural learning algorithms for the class of simple staged trees, entertaining a compact coalescence of the underlying tree from which non-symmetric independences can be easily read. We show that data-learned simple staged trees often outperform Bayesian networks in model fit and illustrate how the coalesced graph is used to identify non-symmetric conditional independences.

</details>

<details>

<summary>2022-03-08 21:38:07 - Bayesian Calibration for Activity Based Models</summary>

- *Laura Schultz, Joshua Auld, Vadim Sokolov*

- `2203.04414v1` - [abs](http://arxiv.org/abs/2203.04414v1) - [pdf](http://arxiv.org/pdf/2203.04414v1)

> We consider the problem of calibration and uncertainty analysis for activity-based transportation simulators. ABMs rely on statistical models of traveler's behavior to predict travel patterns in a metropolitan area. Input parameters are typically estimated from traveler's surveys using maximum likelihood. We develop an approach that uses Gaussian process emulator to calibrate an activity-based model of a metropolitan transplantation system. Our approach extends traditional emulators to handle high-dimensional and non-stationary nature of the transportation simulator. Our methodology is applied to transportation simulator of Bloomington, Illinois. We calibrate key parameters of the model and compare to the ad-hoc calibration process.

</details>

<details>

<summary>2022-03-09 03:12:34 - Optimal approximation for unconstrained non-submodular minimization</summary>

- *Marwa El Halabi, Stefanie Jegelka*

- `1905.12145v4` - [abs](http://arxiv.org/abs/1905.12145v4) - [pdf](http://arxiv.org/pdf/1905.12145v4)

> Submodular function minimization is well studied, and existing algorithms solve it exactly or up to arbitrary accuracy. However, in many applications, such as structured sparse learning or batch Bayesian optimization, the objective function is not exactly submodular, but close. In this case, no theoretical guarantees exist. Indeed, submodular minimization algorithms rely on intricate connections between submodularity and convexity. We show how these relations can be extended to obtain approximation guarantees for minimizing non-submodular functions, characterized by how close the function is to submodular. We also extend this result to noisy function evaluations. Our approximation results are the first for minimizing non-submodular functions, and are optimal, as established by our matching lower bound.

</details>

<details>

<summary>2022-03-09 13:09:55 - Resampling-free bootstrap inference for quantiles</summary>

- *Mrten Schultzberg, Sebastian Ankargren*

- `2202.10992v2` - [abs](http://arxiv.org/abs/2202.10992v2) - [pdf](http://arxiv.org/pdf/2202.10992v2)

> Bootstrap inference is a powerful tool for obtaining robust inference for quantiles and difference-in-quantiles estimators. The computationally intensive nature of bootstrap inference has made it infeasible in large-scale experiments. In this paper, the theoretical properties of the Poisson bootstrap algorithm and quantile estimators are used to derive alternative resampling-free algorithms for Poisson bootstrap inference that reduce the computational complexity substantially without additional assumptions. These findings are connected to existing literature on analytical confidence intervals for quantiles based on order statistics. The results unlock bootstrap inference for difference-in-quantiles for almost arbitrarily large samples. At Spotify, we can now easily calculate bootstrap confidence intervals for quantiles and difference-in-quantiles in A/B tests with hundreds of millions of observations.

</details>

<details>

<summary>2022-03-09 14:15:02 - Parsimonious Bayesian sparse tensor regression using the Tucker decomposition</summary>

- *Daniel Spencer, Rajarshi Guhaniyogi, Raquel Prado*

- `2203.04733v1` - [abs](http://arxiv.org/abs/2203.04733v1) - [pdf](http://arxiv.org/pdf/2203.04733v1)

> Tensors, or multidimensional data arrays, require dimension reduction in modeling applications due to their large size. In addition, these structures typically exhibit inherent sparsity, requiring the use of regularization methods to properly characterize an association between a covariate and a response. In this paper, we propose a Bayesian method to parsimoniously model a scalar response with a tensor-valued covariate using the Tucker tensor decomposition. This method retains the spatial relationship within a tensor-valued covariate, while reducing the number of parameters varying within the model and applying appropriate regularization methods. Simulated data are analyzed to demonstrate model effectiveness, with comparisons made to both classical and Bayesian methods. A neuroimaging analysis using data from the Alzheimer's Data Neuroimaging Initiative is also included.

</details>

<details>

<summary>2022-03-09 15:00:49 - Probabilistic Numerical Method of Lines for Time-Dependent Partial Differential Equations</summary>

- *Nicholas Krmer, Jonathan Schmidt, Philipp Hennig*

- `2110.11847v2` - [abs](http://arxiv.org/abs/2110.11847v2) - [pdf](http://arxiv.org/pdf/2110.11847v2)

> This work develops a class of probabilistic algorithms for the numerical solution of nonlinear, time-dependent partial differential equations (PDEs). Current state-of-the-art PDE solvers treat the space- and time-dimensions separately, serially, and with black-box algorithms, which obscures the interactions between spatial and temporal approximation errors and misguides the quantification of the overall error. To fix this issue, we introduce a probabilistic version of a technique called method of lines. The proposed algorithm begins with a Gaussian process interpretation of finite difference methods, which then interacts naturally with filtering-based probabilistic ordinary differential equation (ODE) solvers because they share a common language: Bayesian inference. Joint quantification of space- and time-uncertainty becomes possible without losing the performance benefits of well-tuned ODE solvers. Thereby, we extend the toolbox of probabilistic programs for differential equation simulation to PDEs.

</details>

<details>

<summary>2022-03-09 16:01:10 - Bayesian Multi-Arm De-Intensification Designs</summary>

- *Steffen Ventz, Lorenzo Trippa*

- `2203.04829v1` - [abs](http://arxiv.org/abs/2203.04829v1) - [pdf](http://arxiv.org/pdf/2203.04829v1)

> In recent years new cancer treatments improved survival in multiple histologies. Some of these therapeutics, and in particular treatment combinations, are often associated with severe treatment-related adverse events (AEs). It is therefore important to identify alternative de-intensified therapies, for example dose-reduced therapies, with reduced AEs and similar efficacy. We introduce a sequential design for multi-arm de-intensification studies. The design evaluates multiple de-intensified therapies at different dose levels, one at the time, based on modeling of toxicity and efficacy endpoints. We study the utility of the design in oropharynx cancer de-intensification studies. We use a Bayesian nonparametric model for efficacy and toxicity outcomes to define decision rules at interim and final analysis. Interim decisions include early termination of the study due to inferior survival of experimental arms compared to the standard of care (SOC), and transitions from one de-intensified treatment arm to another with a further reduced dose when there is sufficient evidence of non-inferior survival. We evaluate the operating characteristics of the design using simulations and data from recent de-intensification studies in human papillomavirus (HPV)-associated oropharynx cancer.

</details>

<details>

<summary>2022-03-09 18:59:28 - A continuous multiple hypothesis testing framework for optimal exoplanet detection</summary>

- *Nathan C. Hara, Thibault de Poyferr, Jean-Baptiste Delisle, Marc Hoffmann*

- `2203.04957v1` - [abs](http://arxiv.org/abs/2203.04957v1) - [pdf](http://arxiv.org/pdf/2203.04957v1)

> The detection of exoplanets is hindered by the presence of complex astrophysical and instrumental noises. Given the difficulty of the task, it is important to ensure that the data are exploited to their fullest potential. In the present work, we search for an optimal exoplanet detection criterion. We adopt a general Bayesian multiple hypothesis testing framework, where the hypotheses are indexed by continuous variables. This framework is adaptable to the different observational methods used to detect exoplanets as well as other data analysis problems. We describe the data as a combination of several parametrized patterns and nuisance signals. We wish to determine which patterns are present, and for a detection to be valid, the parameters of the claimed pattern have to correspond to a true one with a certain accuracy. We search for a detection criterion minimizing false and missed detections, either as a function of their relative cost, or when the expected number of false detections is bounded. We find that if the patterns can be separated in a technical sense, the two approaches lead to the same optimal procedure. We apply it to the retrieval of periodic signals in unevenly sampled time series, emulating the search for exoplanets in radial velocity data. We show on a simulation that, for a given tolerance to false detections, the new criterion leads to 15 to 30\% more true detections than other criteria, including the Bayes factor.

</details>

<details>

<summary>2022-03-09 19:30:13 - Statistical methods for modeling spatially-referenced paired genetic relatedness data</summary>

- *Joshua L. Warren, Melanie H. Chitwood, Benjamin Sobkowiak, Valeriu Crudu, Caroline Colijn, Ted Cohen*

- `2109.14003v2` - [abs](http://arxiv.org/abs/2109.14003v2) - [pdf](http://arxiv.org/pdf/2109.14003v2)

> Understanding factors that contribute to the increased likelihood of disease transmission between two individuals is important for infection control. Measures of genetic relatedness of bacterial isolates between two individuals are often analyzed to determine their associations with these factors using simple correlation or regression analyses. However, these standard approaches ignore the potential for correlation in paired data of this type, arising from the presence of the same individual across multiple paired outcomes. We develop two novel hierarchical Bayesian methods for properly analyzing paired genetic relatedness data in the form of patristic distances and transmission probabilities. Using individual-level spatially correlated random effect parameters, we account for multiple sources of correlation in the outcomes as well as other important features of their distribution. Through simulation, we show that the standard analyses drastically underestimate uncertainty in the associations when correlation is present in the data, leading to incorrect conclusions regarding the covariates of interest. Conversely, the newly developed methods perform well under various levels of correlated and uncorrelated data. All methods are applied to Mycobacterium tuberculosis data from the Republic of Moldova where we identify factors associated with disease transmission and, through analysis of the random effect parameters, key individuals and areas with increased transmission activity. Model comparisons show the importance of the new methodology in this setting. The methods are implemented in the R package GenePair.

</details>

<details>

<summary>2022-03-09 19:58:07 - Methods for Eliciting Informative Prior Distributions: A Critical Review</summary>

- *Julia R. Falconer, Eibe Frank, Devon L. L. Polaschek, Chaitanya Joshi*

- `2112.07090v2` - [abs](http://arxiv.org/abs/2112.07090v2) - [pdf](http://arxiv.org/pdf/2112.07090v2)

> Eliciting informative prior distributions for Bayesian inference can often be complex and challenging. While popular methods rely on asking experts probability based questions to quantify uncertainty, these methods are not without their drawbacks and many alternative elicitation methods exist. This paper explores methods for eliciting informative priors categorized by type and briefly discusses their strengths and limitations. Most of the review literature in this field focuses on a particular type of elicitation approach. The primary aim of this work, however, is to provide a more complete yet macro view of the state of the art by highlighting new (and old) approaches in one clear, easy to read article. Two representative applications are used throughout to explore the suitability, or lack thereof, of the existing methods; one of which, highlights a challenge that has not been addressed in the literature yet. We identify some of the gaps in the present work and discuss directions for future research.

</details>

<details>

<summary>2022-03-09 20:24:04 - A Multi-state Markov Model to Infer the Latent Deterioration Process From the Maintenance Effect on Reliability Engineering of Ships</summary>

- *Hyunji Moon, Jungin Choi, Seoyeon Cha*

- `2111.14368v2` - [abs](http://arxiv.org/abs/2111.14368v2) - [pdf](http://arxiv.org/pdf/2111.14368v2)

> Maintenance optimization of naval ship equipment is crucial in terms of national defense. However, the mixed effect of the maintenance and the pure deterioration processes in the observed data hinders an exact comparison between candidate maintenance policies. That is, the observed data-annual failure counts of naval ships reflect counteracting actions between the maintenance and deterioration. The inference of the latent deteriorating process is needed in advance for choosing an optimal maintenance policy to be carried out. This study proposes a new framework for the separation of the true deterioration effect by predicting it from the current maintenance effect through the multi-state Markov model. Using an annual engine failure count of 99 ships in the Korean navy, we construct the framework consisting of imputation, transition matrix design, optimization, and validation. The hierarchical Gaussian process model is used for the imputation and the three-state Markov model is applied for the estimation of parameters in the deterioration and maintenance effect. To consider the natural (deterioration) and artificial (maintenance) effect respectively, the Bayesian HMM model with a categorical distribution is employed. Computational experiments under multiple settings showed the robustness of the estimated parameters, as well as an accurate recovery of the observed data, thereby confirming the credibility of our model. The framework could further be employed to establish a reliable maintenance system and to reduce an overall maintenance cost.

</details>

<details>

<summary>2022-03-09 23:41:44 - Bayesian estimation of dynamic weights in Gaussian mixture models</summary>

- *Michel H. Montoril, Leandro T. Correia, Helio S. Migon*

- `2104.03395v5` - [abs](http://arxiv.org/abs/2104.03395v5) - [pdf](http://arxiv.org/pdf/2104.03395v5)

> This paper proposes a generalization of Gaussian mixture models, where the mixture weight is allowed to behave as an unknown function of time. This model is capable of successfully capturing the features of the data, as demonstrated by simulated and real datasets. It can be useful in studies such as clustering, change-point and process control. In order to estimate the mixture weight function, we propose two new Bayesian nonlinear dynamic approaches for polynomial models, that can be extended to other problems involving polynomial nonlinear dynamic models. One of the methods, called here component-wise Metropolis-Hastings, apply the Metropolis-Hastings algorithm to each local level component of the state equation. It is more general and can be used in any situation where the observation and state equations are nonlinearly connected. The other method tends to be faster, but is applied specifically to binary data (using the probit link function). The performance of these methods of estimation, in the context of the proposed dynamic Gaussian mixture model, is evaluated through simulated datasets. Also, an application to an array Comparative Genomic Hybridization (aCGH) dataset from glioblastoma cancer illustrates our proposal, highlighting the ability of the method to detect chromosome aberrations.

</details>

<details>

<summary>2022-03-10 03:22:46 - Bayesian Copula Directional Dependence for causal inference on gene expression data</summary>

- *Vasiliki Vamvaka, Clara Grazian*

- `2203.05133v1` - [abs](http://arxiv.org/abs/2203.05133v1) - [pdf](http://arxiv.org/pdf/2203.05133v1)

> Modelling and understanding directional gene networks is a major challenge in biology as they play an important role in the architecture and function of genetic systems. Copula Directional Dependence (CDD) can measure the directed connectivity among variables without any strict requirements of distributional and linearity assumptions. Furthermore, copulas can achieve that by isolating the dependence structure of a joint distribution. In this work, a novel extension of the frequentist CDD in the Bayesian setting is introduced. The new method is compared against the frequentist CDD and validated on six gene interactions, three coming from a mouse scRNA-seq dataset and three coming from a bulk epigenome dataset. The results illustrate that the novel proposed Bayesian CDD was able to identify four out of six true interactions with increased robustness compared to the frequentist method. Therefore, the Bayesian CDD can be considered as an alternative way for modeling the information flow in gene networks.

</details>

<details>

<summary>2022-03-10 12:56:36 - Multi-index Sequential Monte Carlo ratio estimators for Bayesian Inverse problems</summary>

- *Kody J. H. Law, Neil Walton, Shangda Yang, Ajay Jasra*

- `2203.05351v1` - [abs](http://arxiv.org/abs/2203.05351v1) - [pdf](http://arxiv.org/pdf/2203.05351v1)

> We consider the problem of estimating expectations with respect to a target distribution with an unknown normalizing constant, and where even the unnormalized target needs to be approximated at finite resolution. This setting is ubiquitous across science and engineering applications, for example in the context of Bayesian inference where a physics-based model governed by an intractable partial differential equation (PDE) appears in the likelihood. A multi-index Sequential Monte Carlo (MISMC) method is used to construct ratio estimators which provably enjoy the complexity improvements of multi-index Monte Carlo (MIMC) as well as the efficiency of Sequential Monte Carlo (SMC) for inference. In particular, the proposed method provably achieves the canonical complexity of MSE$^{-1}$, while single level methods require MSE$^{-\xi}$ for $\xi>1$. This is illustrated on examples of Bayesian inverse problems with an elliptic PDE forward model in $1$ and $2$ spatial dimensions, where $\xi=5/4$ and $\xi=3/2$, respectively. It is also illustrated on a more challenging log Gaussian process models, where single level complexity is approximately $\xi=9/4$ and multilevel Monte Carlo (or MIMC with an inappropriate index set) gives $\xi = 5/4 + \omega$, for any $\omega > 0$, whereas our method is again canonical.

</details>

<details>

<summary>2022-03-10 14:02:07 - Static and Dynamic Models for Multivariate Distribution Forecasts: Proper Scoring Rule Tests of Factor-Quantile vs. Multivariate GARCH Models</summary>

- *Carol Alexander, Yang Han*

- `2004.14108v2` - [abs](http://arxiv.org/abs/2004.14108v2) - [pdf](http://arxiv.org/pdf/2004.14108v2)

> A plethora of static and dynamic models exist to forecast Value-at-Risk and other quantile-related metrics used in financial risk management. Industry practice tends to favour simpler, static models such as historical simulation or its variants whereas most academic research centres on dynamic models in the GARCH family. While numerous studies examine the accuracy of multivariate models for forecasting risk metrics, there is little research on accurately predicting the entire multivariate distribution. Yet this is an essential element of asset pricing or portfolio optimization problems having non-analytic solutions. We approach this highly complex problem using a variety of proper multivariate scoring rules to evaluate over 100,000 forecasts of eight-dimensional multivariate distributions: of exchange rates, interest rates and commodity futures. This way we test the performance of static models, viz. empirical distribution functions and a new factor-quantile model, with commonly used dynamic models in the asymmetric multivariate GARCH class.

</details>

<details>

<summary>2022-03-10 20:56:06 - Hamiltonian zigzag sampler got more momentum than its Markovian counterpart: Equivalence of two zigzags under a momentum refreshment limit</summary>

- *Akihiko Nishimura, Zhenyu Zhang, Marc A. Suchard*

- `2104.07694v2` - [abs](http://arxiv.org/abs/2104.07694v2) - [pdf](http://arxiv.org/pdf/2104.07694v2)

> Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. In this article, we uncover a remarkable connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in the position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but re-sample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit model applied to HIV virus data.

</details>

<details>

<summary>2022-03-11 02:36:59 - Bayesian inference via sparse Hamiltonian flows</summary>

- *Naitong Chen, Zuheng Xu, Trevor Campbell*

- `2203.05723v1` - [abs](http://arxiv.org/abs/2203.05723v1) - [pdf](http://arxiv.org/pdf/2203.05723v1)

> A Bayesian coreset is a small, weighted subset of data that replaces the full dataset during Bayesian inference, with the goal of reducing computational cost. Although past work has shown empirically that there often exists a coreset with low inferential error, efficiently constructing such a coreset remains a challenge. Current methods tend to be slow, require a secondary inference step after coreset construction, and do not provide bounds on the data marginal evidence. In this work, we introduce a new method -- sparse Hamiltonian flows -- that addresses all three of these challenges. The method involves first subsampling the data uniformly, and then optimizing a Hamiltonian flow parametrized by coreset weights and including periodic momentum quasi-refreshment steps. Theoretical results show that the method enables an exponential compression of the dataset in a representative model, and that the quasi-refreshment steps reduce the KL divergence to the target. Real and synthetic experiments demonstrate that sparse Hamiltonian flows provide accurate posterior approximations with significantly reduced runtime compared with competing dynamical-system-based inference methods.

</details>

<details>

<summary>2022-03-11 09:40:17 - AntBO: Towards Real-World Automated Antibody Design with Combinatorial Bayesian Optimisation</summary>

- *Asif Khan, Alexander I. Cowen-Rivers, Derrick-Goh-Xin Deik, Antoine Grosnit, Kamil Dreczkowski, Philippe A. Robert, Victor Greiff, Rasul Tutunov, Dany Bou-Ammar, Jun Wang, Haitham Bou-Ammar*

- `2201.12570v3` - [abs](http://arxiv.org/abs/2201.12570v3) - [pdf](http://arxiv.org/pdf/2201.12570v3)

> Antibodies are canonically Y-shaped multimeric proteins capable of highly specific molecular recognition. The CDRH3 region located at the tip of variable chains of an antibody dominates antigen-binding specificity. Therefore, it is a priority to design optimal antigen-specific CDRH3 regions to develop therapeutic antibodies to combat harmful pathogens. However, the combinatorial nature of CDRH3 sequence space makes it impossible to search for an optimal binding sequence exhaustively and efficiently, especially not experimentally. Here, we present AntBO: a Combinatorial Bayesian Optimisation framework enabling efficient in silico design of the CDRH3 region. Ideally, antibodies should bind to their target antigen and be free from any harmful outcomes. Therefore, we introduce the CDRH3 trust region that restricts the search to sequences with feasible developability scores. To benchmark AntBO, we use the Absolut! software suite as a black-box oracle because it can score the target specificity and affinity of designed antibodies in silico in an unconstrained fashion. The results across 188 antigens demonstrate the benefit of AntBO in designing CDRH3 regions with diverse biophysical properties. In under 200 protein designs, AntBO can suggest antibody sequences that outperform the best binding sequence drawn from 6.9 million experimentally obtained CDRH3s and a commonly used genetic algorithm baseline. Additionally, AntBO finds very-high affinity CDRH3 sequences in only 38 protein designs whilst requiring no domain knowledge. We conclude AntBO brings automated antibody design methods closer to what is practically viable for in vitro experimentation.

</details>

<details>

<summary>2022-03-11 14:51:01 - Grassmann Stein Variational Gradient Descent</summary>

- *Xing Liu, Harrison Zhu, Jean-Franois Ton, George Wynne, Andrew Duncan*

- `2202.03297v2` - [abs](http://arxiv.org/abs/2202.03297v2) - [pdf](http://arxiv.org/pdf/2202.03297v2)

> Stein variational gradient descent (SVGD) is a deterministic particle inference algorithm that provides an efficient alternative to Markov chain Monte Carlo. However, SVGD has been found to suffer from variance underestimation when the dimensionality of the target distribution is high. Recent developments have advocated projecting both the score function and the data onto real lines to sidestep this issue, although this can severely overestimate the epistemic (model) uncertainty. In this work, we propose Grassmann Stein variational gradient descent (GSVGD) as an alternative approach, which permits projections onto arbitrary dimensional subspaces. Compared with other variants of SVGD that rely on dimensionality reduction, GSVGD updates the projectors simultaneously for the score function and the data, and the optimal projectors are determined through a coupled Grassmann-valued diffusion process which explores favourable subspaces. Both our theoretical and experimental results suggest that GSVGD enjoys efficient state-space exploration in high-dimensional problems that have an intrinsic low-dimensional structure.

</details>

<details>

<summary>2022-03-11 16:43:26 - MAGI: A Package for Inference of Dynamic Systems from Noisy and Sparse Data via Manifold-constrained Gaussian Processes</summary>

- *Samuel W. K. Wong, Shihao Yang, S. C. Kou*

- `2203.06066v1` - [abs](http://arxiv.org/abs/2203.06066v1) - [pdf](http://arxiv.org/pdf/2203.06066v1)

> This article presents the MAGI software package for inference of dynamic systems. The focus of MAGI is where the dynamics are modeled by nonlinear ordinary differential equations with unknown parameters. While such models are widely used in science and engineering, the available experimental data for parameter estimation may be noisy and sparse. Furthermore, some system components may be entirely unobserved. MAGI solves this inference problem with the help of manifold-constrained Gaussian processes within a Bayesian statistical framework, whereas the unobserved components have posed a significant challenge for existing software. We use three realistic examples to illustrate the functionality of MAGI. The user may choose to use the package in any of the R, MATLAB, and Python environments.

</details>

<details>

<summary>2022-03-11 16:53:59 - Bayesian Nonparametric Inference for "Species-sampling" Problems</summary>

- *Cecilia Balocchi, Stefano Favaro, Zacharie Naulet*

- `2203.06076v1` - [abs](http://arxiv.org/abs/2203.06076v1) - [pdf](http://arxiv.org/pdf/2203.06076v1)

> "Species-sampling" problems (SSPs) refer to a broad class of statistical problems that, given an observable sample from an unknown population of individuals belonging to some species, call for estimating features of the unknown species composition of additional unobservable samples from the same population. Among SSPs, the problems of estimating coverage probabilities, the number of unseen species and coverages of prevalences have emerged over the past three decades for being the objects of numerous studies, both in methods and applications, mostly within the field of biological sciences but also in machine learning, electrical engineering and information theory. In this paper, we present an overview of Bayesian nonparametric (BNP) inference for such three SSPs under the popular Pitman--Yor process (PYP) prior: i) we introduce each SSP in the classical (frequentist) nonparametric framework, and review its posterior analyses in the BNP framework; ii) we improve on computation and interpretability of existing posterior distributions, typically expressed through complicated combinatorial numbers, by establishing novel posterior representations in terms of simple compound Binomial and Hypergeometric distributions. The critical question of estimating the discount and scale parameters of the PYP prior is also considered and investigated, establishing a general property of Bayesian consistency with respect to the hierarchical Bayes and empirical Bayes approaches, that is: the discount parameter can be always estimated consistently, whereas the scale parameter cannot be estimated consistently, thus advising caution in posterior inference. We conclude our work by discussing other SSPs, and presenting some emerging generalizations of SSPs, mostly in biological sciences, which deal with "feature-sampling" problems, multiple populations of individuals sharing species and classes of Markov chains.

</details>

<details>

<summary>2022-03-11 19:47:36 - re:Linde et al. (2021): The Bayes factor, HDI-ROPE and frequentist equivalence tests can all be reverse engineered -- almost exactly -- from one another</summary>

- *Harlan Campbell, Paul Gustafson*

- `2104.07834v3` - [abs](http://arxiv.org/abs/2104.07834v3) - [pdf](http://arxiv.org/pdf/2104.07834v3)

> Following an extensive simulation study comparing the operating characteristics of three different procedures used for establishing equivalence (the frequentist "TOST", the Bayesian "HDI-ROPE", and the Bayes factor interval null procedure), Linde et al. (2021) conclude with the recommendation that "researchers rely more on the Bayes factor interval null approach for quantifying evidence for equivalence." We redo the simulation study of Linde et al. (2021) in its entirety but with the different procedures calibrated to have the same predetermined maximum type 1 error rate. Our results suggest that, when calibrated in this way, the Bayes Factor, HDI-ROPE, and frequentist equivalence tests all have similar -- almost exactly -- type 2 error rates. In general any advocating for frequentist testing as better or worse than Bayesian testing in terms of empirical findings seems dubious at best. If one decides on which underlying principle to subscribe to in tackling a given problem, then the method follows naturally. Bearing in mind that each procedure can be reverse-engineered from the others (at least approximately), trying to use empirical performance to argue for one approach over another seems like tilting at windmills.

</details>

<details>

<summary>2022-03-11 19:55:49 - Learning cardiac activation maps from 12-lead ECG with multi-fidelity Bayesian optimization on manifolds</summary>

- *Simone Pezzuto, Paris Perdikaris, Francisco Sahli Costabal*

- `2203.06222v1` - [abs](http://arxiv.org/abs/2203.06222v1) - [pdf](http://arxiv.org/pdf/2203.06222v1)

> We propose a method for identifying an ectopic activation in the heart non-invasively. Ectopic activity in the heart can trigger deadly arrhythmias. The localization of the ectopic foci or earliest activation sites (EASs) is therefore a critical information for cardiologists in deciding the optimal treatment. In this work, we formulate the identification problem as a global optimization problem, by minimizing the mismatch between the ECG predicted by a cardiac model, when paced at a given EAS, and the observed ECG during the ectopic activity. Our cardiac model amounts at solving an anisotropic eikonal equation for cardiac activation and the forward bidomain model in the torso with the lead field approach for computing the ECG. We build a Gaussian process surrogate model of the loss function on the heart surface to perform Bayesian optimization. In this procedure, we iteratively evaluate the loss function following the lower confidence bound criterion, which combines exploring the surface with exploitation of the minimum region. We also extend this framework to incorporate multiple levels of fidelity of the model. We show that our procedure converges to the minimum only after $11.7\pm10.4$ iterations (20 independent runs) for the single-fidelity case and $3.5\pm1.7$ iterations for the multi-fidelity case. We envision that this tool could be applied in real time in a clinical setting to identify potentially dangerous EASs.

</details>

<details>

<summary>2022-03-11 21:25:24 - Fast and flexible inference approach for joint models of multivariate longitudinal and survival data using Integrated Nested Laplace Approximations</summary>

- *Denis Rustand, Janet van Niekerk, Elias Teixeira Krainski, Hvard Rue, Ccile Proust-Lima*

- `2203.06256v1` - [abs](http://arxiv.org/abs/2203.06256v1) - [pdf](http://arxiv.org/pdf/2203.06256v1)

> Joint modeling longitudinal and survival data offers many advantages such as addressing measurement error and missing data in the longitudinal processes, understanding and quantifying the association between the longitudinal markers and the survival events and predicting the risk of events based on the longitudinal markers. A joint model involves multiple submodels (one for each longitudinal/survival outcome) usually linked together through correlated or shared random effects. Their estimation is computationally expensive (particularly due to a multidimensional integration of the likelihood over the random effects distribution) so that inference methods become rapidly intractable, and restricts applications of joint models to a small number of longitudinal markers and/or random effects. We introduce a Bayesian approximation based on the Integrated Nested Laplace Approximation algorithm implemented in the R package R-INLA to alleviate the computational burden and allow the estimation of multivariate joint models with less restrictions. Our simulation studies show that R-INLA substantially reduces the computation time and the variability of the parameter estimates compared to alternative estimation strategies. We further apply the methodology to analyze 5 longitudinal markers (3 continuous, 1 count, 1 binary, and 16 random effects) and competing risks of death and transplantation in a clinical trial on primary biliary cholangitis. R-INLA provides a fast and reliable inference technique for applying joint models to the complex multivariate data encountered in health research.

</details>

<details>

<summary>2022-03-12 05:12:30 - Synthesizing cross-design evidence and cross-format data using network meta-regression</summary>

- *Tasnim Hamza, Konstantina Chalkou, Fabio Pellegrini, Jens Kuhle, Pascal Benkert, Johannes Lorscheider, Chiara Zecca, Cynthia P Iglesias-Urrutia, Andrea Manca, Toshi A. Furukawa, Andrea Cipriani, Georgia Salanti*

- `2203.06350v1` - [abs](http://arxiv.org/abs/2203.06350v1) - [pdf](http://arxiv.org/pdf/2203.06350v1)

> In network meta-analysis (NMA), we synthesize all relevant evidence about health outcomes with competing treatments. The evidence may come from randomized controlled trials (RCT) or non-randomized studies (NRS) as individual participant data (IPD) or as aggregate data (AD). We present a suite of Bayesian NMA and network meta-regression (NMR) models allowing for cross-design and cross-format synthesis. The models integrate a three-level hierarchical model for synthesizing IPD and AD into four approaches. The four approaches account for differences in the design and risk of bias in the RCT and NRS evidence. These four approaches variously ignoring differences in risk of bias, using NRS to construct penalized treatment effect priors and bias-adjustment models that control the contribution of information from high risk of bias studies in two different ways. We illustrate the methods in a network of three pharmacological interventions and placebo for patients with relapsing-remitting multiple sclerosis. The estimated relative treatment effects do not change much when we accounted for differences in design and risk of bias. Conducting network meta-regression showed that intervention efficacy decreases with increasing participant age. We re-analysed a network of 431 RCT comparing 21 antidepressants, and we did not observe material changes in intervention efficacy when adjusting for studies high risk of bias. In summary, the described suite of NMA/NMR models enables inclusion of all relevant evidence while incorporating information on the within-study bias in both observational and experimental data and enabling estimation of individualized treatment effects through the inclusion of participant characteristics.

</details>

<details>

<summary>2022-03-12 09:43:43 - G-optimal grid designs for kriging models</summary>

- *Subhadra Dasgupta, Siuli Mukhopadhyay, Jonathan Keith*

- `2111.06632v2` - [abs](http://arxiv.org/abs/2111.06632v2) - [pdf](http://arxiv.org/pdf/2111.06632v2)

> This work is focused on finding G-optimal designs theoretically for kriging models with two-dimensional inputs and separable exponential covariance structures. For design comparison, the notion of evenness of two-dimensional grid designs is developed. The mathematical relationship between the design and the supremum of the mean squared prediction error ($SMSPE$) function is studied and then optimal designs are explored for both prospective and retrospective design scenarios. In the case of prospective designs, the new design is developed before the experiment is conducted and the regularly spaced grid is shown to be the G-optimal design. The retrospective designs are constructed by adding or deleting points from an already existing design. Deterministic algorithms are developed to find the best possible retrospective designs (which minimizes the $SMSPE$). It is found that a more evenly spread design under the G-optimality criterion leads to the best possible retrospective design. For all the cases of finding the optimal prospective designs and the best possible retrospective designs, both frequentist and Bayesian frameworks have been considered. The proposed methodology for finding retrospective designs is illustrated with a methane flux monitoring design.

</details>

<details>

<summary>2022-03-12 16:33:27 - GATSBI: Generative Adversarial Training for Simulation-Based Inference</summary>

- *Poornima Ramesh, Jan-Matthis Lueckmann, Jan Boelts, lvaro Tejero-Cantero, David S. Greenberg, Pedro J. Gonalves, Jakob H. Macke*

- `2203.06481v1` - [abs](http://arxiv.org/abs/2203.06481v1) - [pdf](http://arxiv.org/pdf/2203.06481v1)

> Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. On a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a state-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations. Overall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models.

</details>

<details>

<summary>2022-03-12 22:49:22 - Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal Stochastic Linear Mixing Model</summary>

- *Rui Meng, Kristofer Bouchard*

- `2106.13379v2` - [abs](http://arxiv.org/abs/2106.13379v2) - [pdf](http://arxiv.org/pdf/2106.13379v2)

> Many modern time-series datasets contain large numbers of output response variables sampled for prolonged periods of time. For example, in neuroscience, the activities of 100s-1000's of neurons are recorded during behaviors and in response to sensory stimuli. Multi-output Gaussian process models leverage the nonparametric nature of Gaussian processes to capture structure across multiple outputs. However, this class of models typically assumes that the correlations between the output response variables are invariant in the input space. Stochastic linear mixing models (SLMM) assume the mixture coefficients depend on input, making them more flexible and effective to capture complex output dependence. However, currently, the inference for SLMMs is intractable for large datasets, making them inapplicable to several modern time-series problems. In this paper, we propose a new regression framework, the orthogonal stochastic linear mixing model (OSLMM) that introduces an orthogonal constraint amongst the mixing coefficients. This constraint reduces the computational burden of inference while retaining the capability to handle complex output dependence. We provide Markov chain Monte Carlo inference procedures for both SLMM and OSLMM and demonstrate superior model scalability and reduced prediction error of OSLMM compared with state-of-the-art methods on several real-world applications. In neurophysiology recordings, we use the inferred latent functions for compact visualization of population responses to auditory stimuli, and demonstrate superior results compared to a competing method (GPFA). Together, these results demonstrate that OSLMM will be useful for the analysis of diverse, large-scale time-series datasets.

</details>

<details>

<summary>2022-03-12 23:59:32 - Fast Simulation-Based Bayesian Estimation of Heterogeneous and Representative Agent Models using Normalizing Flow Neural Networks</summary>

- *Cameron Fen*

- `2203.06537v1` - [abs](http://arxiv.org/abs/2203.06537v1) - [pdf](http://arxiv.org/pdf/2203.06537v1)

> This paper proposes a simulation-based deep learning Bayesian procedure for the estimation of macroeconomic models. This approach is able to derive posteriors even when the likelihood function is not tractable. Because the likelihood is not needed for Bayesian estimation, filtering is also not needed. This allows Bayesian estimation of HANK models with upwards of 800 latent states as well as estimation of representative agent models that are solved with methods that don't yield a likelihood--for example, projection and value function iteration approaches. I demonstrate the validity of the approach by estimating a 10 parameter HANK model solved via the Reiter method that generates 812 covariates per time step, where 810 are latent variables, showing this can handle a large latent space without model reduction. I also estimate the algorithm with an 11-parameter model solved via value function iteration, which cannot be estimated with Metropolis-Hastings or even conventional maximum likelihood estimators. In addition, I show the posteriors estimated on Smets-Wouters 2007 are higher quality and faster using simulation-based inference compared to Metropolis-Hastings. This approach helps address the computational expense of Metropolis-Hastings and allows solution methods which don't yield a tractable likelihood to be estimated.

</details>

<details>

<summary>2022-03-13 14:20:17 - On Sparse High-Dimensional Graphical Model Learning For Dependent Time Series</summary>

- *Jitendra K. Tugnait*

- `2111.07897v2` - [abs](http://arxiv.org/abs/2111.07897v2) - [pdf](http://arxiv.org/pdf/2111.07897v2)

> We consider the problem of inferring the conditional independence graph (CIG) of a sparse, high-dimensional stationary multivariate Gaussian time series. A sparse-group lasso-based frequency-domain formulation of the problem based on frequency-domain sufficient statistic for the observed time series is presented. We investigate an alternating direction method of multipliers (ADMM) approach for optimization of the sparse-group lasso penalized log-likelihood. We provide sufficient conditions for convergence in the Frobenius norm of the inverse PSD estimators to the true value, jointly across all frequencies, where the number of frequencies are allowed to increase with sample size. This results also yields a rate of convergence. We also empirically investigate selection of the tuning parameters based on Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</details>

<details>

<summary>2022-03-13 15:33:12 - Modelling hetegeneous treatment effects by quantitle local polynomial decision tree and forest</summary>

- *Lai Xinglin*

- `2111.15320v2` - [abs](http://arxiv.org/abs/2111.15320v2) - [pdf](http://arxiv.org/pdf/2111.15320v2)

> To further develop the statistical inference problem for heterogeneous treatment effects, this paper builds on Breiman's (2001) random forest tree (RFT)and Wager et al.'s (2018) causal tree to parameterize the nonparametric problem using the excellent statistical properties of classical OLS and the division of local linear intervals based on covariate quantile points, while preserving the random forest trees with the advantages of constructible confidence intervals and asymptotic normality properties [Athey and Imbens (2016),Efron (2014),Wager et al.(2014)\citep{wager2014asymptotic}], we propose a decision tree using quantile classification according to fixed rules combined with polynomial estimation of local samples, which we call the quantile local linear causal tree (QLPRT) and forest (QLPRF).

</details>

<details>

<summary>2022-03-13 23:41:00 - Learning-augmented count-min sketches via Bayesian nonparametrics</summary>

- *Emanuele Dolera, Stefano Favaro, Stefano Peluchetti*

- `2102.04462v2` - [abs](http://arxiv.org/abs/2102.04462v2) - [pdf](http://arxiv.org/pdf/2102.04462v2)

> The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens' frequencies in a data stream of tokens, i.e. point queries, based on random hashed data. A learning-augmented version of the CMS, referred to as CMS-DP, has been proposed by Cai, Mitzenmacher and Adams (\textit{NeurIPS} 2018), and it relies on Bayesian nonparametric (BNP) modeling of the data stream of tokens via a Dirichlet process (DP) prior, with estimates of a point query being obtained as mean functionals of the posterior distribution of the point query, given the hashed data. While the CMS-DP has proved to improve on some aspects of CMS, it has the major drawback of arising from a "heuristic" proof that builds upon arguments tailored to the DP prior, namely arguments that are not usable for other nonparametric priors. In this paper, we present a "rigorous" proof of the CMS-DP that has the advantage of building upon arguments that are usable, in principle, within the broad class of nonparametric priors arising from normalized random measures. This first result leads to develop a novel learning-augmented CMS under power-law data streams, referred to as CMS-PYP, which relies on BNP modeling of the data stream of tokens via a Pitman-Yor process (PYP) prior. Under this more general BNP model, we apply the arguments of the "rigorous" proof of the CMS-DP, suitably adapted to the PYP prior, in order to compute the posterior distribution of a point query, given the hashed data. Some large sample asymptotic behaviours of the CMS-DP and the CMS-PYP are also investigated and discussed. Applications to synthetic and real data show that the CMS-PYP outperforms the CMS and the CMS-DP in estimating low-frequency tokens, and it is competitive with respect to a variation of the CMS designed for low-frequency tokens.

</details>

<details>

<summary>2022-03-14 02:57:28 - A Bayesian Precision Response-adaptive Phase II Clinical Trial Design for Radiotherapies with Competing Risk Survival Outcomes</summary>

- *Jina Park, Wenjing Hu, Ick Hoon Jin, Hao Liu, Yong Zang*

- `2203.06830v1` - [abs](http://arxiv.org/abs/2203.06830v1) - [pdf](http://arxiv.org/pdf/2203.06830v1)

> Many phase II clinical trials have used survival outcomes as the primary endpoints in recent decades. Suppose the radiotherapy is evaluated in a phase II trial using survival outcomes. In that case, the competing risk issue often arises because the time to disease progression can be censored by the time to normal tissue complications, and vice versa. Besides, much literature has examined that patients receiving the same radiotherapy dose may yield distinct responses due to their heterogeneous radiation susceptibility statuses. Therefore, the "one-dose-fit-all" strategy often fails, and it is more relevant to evaluate the subgroup-specific treatment effect with the subgroup defined by the radiation susceptibility status. In this paper, we propose a Bayesian precision phase II trial design evaluating the subgroup-specific treatment effects of radiotherapy. We use the cause-specific hazard approach to model the competing risk survival outcomes. We propose restricting the candidate radiation doses based on each patient's radiation susceptibility status. Only the clinically feasible personalized dose will be considered, which enhances the benefit for the patients in the trial. In addition, we propose a stratified Bayesian adaptive randomization scheme such that more patients will be randomized to the dose reporting more favorable survival outcomes. Numerical studies have shown that the proposed design performed well and outperformed the conventional design ignoring the competing risk issue.

</details>

<details>

<summary>2022-03-14 03:13:44 - Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search</summary>

- *Linnan Wang, Rodrigo Fonseca, Yuandong Tian*

- `2007.00708v2` - [abs](http://arxiv.org/abs/2007.00708v2) - [pdf](http://arxiv.org/pdf/2007.00708v2)

> High dimensional black-box optimization has broad applications but remains a challenging problem to solve. Given a set of samples $\{\vx_i, y_i\}$, building a global model (like Bayesian Optimization (BO)) suffers from the curse of dimensionality in the high-dimensional search space, while a greedy search may lead to sub-optimality. By recursively splitting the search space into regions with high/low function values, recent works like LaNAS shows good performance in Neural Architecture Search (NAS), reducing the sample complexity empirically. In this paper, we coin LA-MCTS that extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space using a few samples and their function values in an online fashion. While LaNAS uses linear partition and performs uniform sampling in each region, our LA-MCTS adopts a nonlinear decision boundary and learns a local model to pick good candidates. If the nonlinear partition function and the local model fits well with ground-truth black-box function, then good partitions and candidates can be reached with much fewer samples. LA-MCTS serves as a \emph{meta-algorithm} by using existing black-box optimizers (e.g., BO, TuRBO) as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems.

</details>

<details>

<summary>2022-03-14 03:30:58 - Beckmann's approach to multi-item multi-bidder auctions</summary>

- *Alexander Kolesnikov, Fedor Sandomirskiy, Aleh Tsyvinski, Alexander P. Zimin*

- `2203.06837v1` - [abs](http://arxiv.org/abs/2203.06837v1) - [pdf](http://arxiv.org/pdf/2203.06837v1)

> We consider the problem of revenue-maximizing Bayesian auction design with several i.i.d. bidders and several items. We show that the auction-design problem can be reduced to the problem of continuous optimal transportation introduced by Beckmann. We establish the strong duality between the two problems and demonstrate the existence of solutions. We then develop a new numerical approximation scheme that combines multi-to-single-agent reduction and the majorization theory insights to characterize the solution.

</details>

<details>

<summary>2022-03-14 06:50:57 - Asymptotic Behavior of Bayesian Generalization Error in Multinomial Mixtures</summary>

- *Takumi Watanabe, Sumio Watanabe*

- `2203.06884v1` - [abs](http://arxiv.org/abs/2203.06884v1) - [pdf](http://arxiv.org/pdf/2203.06884v1)

> Multinomial mixtures are widely used in the information engineering field, however, their mathematical properties are not yet clarified because they are singular learning models. In fact, the models are non-identifiable and their Fisher information matrices are not positive definite. In recent years, the mathematical foundation of singular statistical models are clarified by using algebraic geometric methods. In this paper, we clarify the real log canonical thresholds and multiplicities of the multinomial mixtures and elucidate their asymptotic behaviors of generalization error and free energy.

</details>

<details>

<summary>2022-03-14 10:55:55 - Bayesian sample size determination for diagnostic accuracy studies</summary>

- *Kevin J. Wilson, S. Faye Williamson, A. Joy Allen, Cameron J. Williams, Thomas P. Hellyer, B. Clare Lendrem*

- `2108.08594v2` - [abs](http://arxiv.org/abs/2108.08594v2) - [pdf](http://arxiv.org/pdf/2108.08594v2)

> The development of a new diagnostic test ideally follows a sequence of stages which, amongst other aims, evaluate technical performance. This includes an analytical validity study, a diagnostic accuracy study and an interventional clinical utility study. Current approaches to the design and analysis of the diagnostic accuracy study can suffer from prohibitively large sample sizes and interval estimates with undesirable properties. In this paper, we propose a novel Bayesian approach which takes advantage of information available from the analytical validity stage. We utilise assurance to calculate the required sample size based on the target width of a posterior probability interval and can choose to use or disregard the data from the analytical validity study when subsequently inferring measures of test accuracy. Sensitivity analyses are performed to assess the robustness of the proposed sample size to the choice of prior, and prior-data conflict is evaluated by comparing the data to the prior predictive distributions. We illustrate the proposed approach using a motivating real-life application involving a diagnostic test for ventilator associated pneumonia. Finally, we compare the properties of the proposed approach against commonly used alternatives. The results show that by making better use of existing data from earlier studies, the assurance-based approach can not only reduce the required sample size when compared to alternatives, but can also produce more reliable sample sizes for diagnostic accuracy studies.

</details>

<details>

<summary>2022-03-14 12:04:29 - Optimal designs for some bivariate cokriging models</summary>

- *Subhadra Dasgupta, Siuli Mukhopadhyay, Jonathan Keith*

- `2004.13967v4` - [abs](http://arxiv.org/abs/2004.13967v4) - [pdf](http://arxiv.org/pdf/2004.13967v4)

> This article focuses on the estimation and design aspects of a bivariate collocated cokriging experiment. For a large class of covariance matrices, a linear dependency criterion is identified, which allows the best linear unbiased estimator of the primary variable in a bivariate collocated cokriging setup to reduce to a univariate kriging estimator. Exact optimal designs for efficient prediction for such simple and ordinary reduced cokriging models with one-dimensional inputs are determined. Designs are found by minimizing the maximum and the integrated prediction variance, where the primary variable is an Ornstein-Uhlenbeck process. For simple and ordinary cokriging models with known covariance parameters, the equispaced design is shown to be optimal for both criterion functions. The more realistic scenario of unknown covariance parameters is addressed by assuming prior distributions on the parameter vector, thus adopting a Bayesian approach to the design problem. The equispaced design is proved to be the Bayesian optimal design for both criteria. The work is motivated by designing an optimal water monitoring system for an Indian river.

</details>

<details>

<summary>2022-03-14 13:46:12 - Bayesian Deep Learning via Subnetwork Inference</summary>

- *Erik Daxberger, Eric Nalisnick, James Urquhart Allingham, Javier Antorn, Jos Miguel Hernndez-Lobato*

- `2010.14689v4` - [abs](http://arxiv.org/abs/2010.14689v4) - [pdf](http://arxiv.org/pdf/2010.14689v4)

> The Bayesian paradigm has the potential to solve core issues of deep neural networks such as poor calibration and data inefficiency. Alas, scaling Bayesian inference to large weight spaces often requires restrictive approximations. In this work, we show that it suffices to perform inference over a small subset of model weights in order to obtain accurate predictive posteriors. The other weights are kept as point estimates. This subnetwork inference framework enables us to use expressive, otherwise intractable, posterior approximations over such subsets. In particular, we implement subnetwork linearized Laplace as a simple, scalable Bayesian deep learning method: We first obtain a MAP estimate of all weights and then infer a full-covariance Gaussian posterior over a subnetwork using the linearized Laplace approximation. We propose a subnetwork selection strategy that aims to maximally preserve the model's predictive uncertainty. Empirically, our approach compares favorably to ensembles and less expressive posterior approximations over full networks. Our proposed subnetwork (linearized) Laplace method is implemented within the laplace PyTorch library at https://github.com/AlexImmer/Laplace.

</details>

<details>

<summary>2022-03-14 13:51:02 - Consistent and scalable Bayesian joint variable and graph selection for disease diagnosis leveraging functional brain network</summary>

- *Xuan Cao, Kyoungjae Lee*

- `2203.07108v1` - [abs](http://arxiv.org/abs/2203.07108v1) - [pdf](http://arxiv.org/pdf/2203.07108v1)

> We consider the joint inference of regression coefficients and the inverse covariance matrix for covariates in high-dimensional probit regression, where the predictors are both relevant to the binary response and functionally related to one another. A hierarchical model with spike and slab priors over regression coefficients and the elements in the inverse covariance matrix is employed to simultaneously perform variable and graph selection. We establish joint selection consistency for both the variable and the underlying graph when the dimension of predictors is allowed to grow much larger than the sample size, which is the first theoretical result in the Bayesian literature. A scalable Gibbs sampler is derived that performs better in high-dimensional simulation studies compared with other state-of-art methods. We illustrate the practical impact and utilities of the proposed method via a functional MRI dataset, where both the regions of interest with altered functional activities and the underlying functional brain network are inferred and integrated together for stratifying disease risk.

</details>

<details>

<summary>2022-03-14 13:54:55 - Bayesian inference on hierarchical nonlocal priors in generalized linear models</summary>

- *Xuan Cao, Kyoungjae Lee*

- `2203.07110v1` - [abs](http://arxiv.org/abs/2203.07110v1) - [pdf](http://arxiv.org/pdf/2203.07110v1)

> Variable selection methods with nonlocal priors have been widely studied in linear regression models, and their theoretical and empirical performances have been reported. However, the crucial model selection properties for hierarchical nonlocal priors in high-dimensional generalized linear regression have rarely been investigated. In this paper, we consider a hierarchical nonlocal prior for high-dimensional logistic regression models and investigate theoretical properties of the posterior distribution. Specifically, a product moment (pMOM) nonlocal prior is imposed over the regression coefficients with an Inverse-Gamma prior on the tuning parameter. Under standard regularity assumptions, we establish strong model selection consistency in a high-dimensional setting, where the number of covariates is allowed to increase at a sub-exponential rate with the sample size. We implement the Laplace approximation for computing the posterior probabilities, and a modified shotgun stochastic search procedure is suggested for efficiently exploring the model space. We demonstrate the validity of the proposed method through simulation studies and an RNA-sequencing dataset for stratifying disease risk.

</details>

<details>

<summary>2022-03-14 16:32:30 - Laplace Redux -- Effortless Bayesian Deep Learning</summary>

- *Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, Philipp Hennig*

- `2106.14806v3` - [abs](http://arxiv.org/abs/2106.14806v3) - [pdf](http://arxiv.org/pdf/2106.14806v3)

> Bayesian formulations of deep learning have been shown to have compelling theoretical properties and offer practical functional benefits, such as improved predictive uncertainty quantification and model selection. The Laplace approximation (LA) is a classic, and arguably the simplest family of approximations for the intractable posteriors of deep neural networks. Yet, despite its simplicity, the LA is not as popular as alternatives like variational Bayes or deep ensembles. This may be due to assumptions that the LA is expensive due to the involved Hessian computation, that it is difficult to implement, or that it yields inferior results. In this work we show that these are misconceptions: we (i) review the range of variants of the LA including versions with minimal cost overhead; (ii) introduce "laplace", an easy-to-use software library for PyTorch offering user-friendly access to all major flavors of the LA; and (iii) demonstrate through extensive experiments that the LA is competitive with more popular alternatives in terms of performance, while excelling in terms of computational cost. We hope that this work will serve as a catalyst to a wider adoption of the LA in practical deep learning, including in domains where Bayesian approaches are not typically considered at the moment.

</details>

<details>

<summary>2022-03-14 17:42:58 - A Bayesian Nonparametric Approach to Species Sampling Problems with Ordering</summary>

- *Cecilia Balocchi, Federico Camerlenghi, Stefano Favaro*

- `2203.07342v1` - [abs](http://arxiv.org/abs/2203.07342v1) - [pdf](http://arxiv.org/pdf/2203.07342v1)

> Species-sampling problems (SSPs) refer to a vast class of statistical problems that, given an observable sample from an unknown population of individuals belonging to some species, call for estimating (discrete) functionals of the unknown species composition of additional unobservable samples. A common feature of SSPs is the invariance with respect to species labelling, i.e. species' labels are immaterial in defining the functional of interest, which is at the core of the development of the Bayesian nonparametric (BNP) approach to SSPs under the popular Pitman-Yor process (PYP) prior. In this paper, we consider SSPs that are not invariant to species labelling, in the sense that an ordering or ranking is assigned to species' labels, and we develop a BNP approach to such problems. In particular, inspired by the population genetics literature on age-ordered alleles' compositions, with a renowned interest in the frequency of the oldest allele, we study the following SSP with ordering: given an observable sample from unknown population of individuals belonging to some species (alleles), with species' labels being ordered according to weights (ages), estimate the frequencies of the first r order species' labels in an enlarged sample obtained by including additional unobservable samples. Our BNP approach relies on an ordered version of the PYP prior, which leads to an explicit posterior distribution of the first r order frequencies, with corresponding estimates being simple and computationally efficient. We apply our approach to the analysis of genetic variation, showing its effectiveness in the estimation of the frequency of the oldest allele, and then discuss other applications in the contexts of citations to academic articles and online purchases of items.

</details>

<details>

<summary>2022-03-14 18:00:03 - Accelerated Bayesian SED Modeling using Amortized Neural Posterior Estimation</summary>

- *ChangHoon Hahn, Peter Melchior*

- `2203.07391v1` - [abs](http://arxiv.org/abs/2203.07391v1) - [pdf](http://arxiv.org/pdf/2203.07391v1)

> State-of-the-art spectral energy distribution (SED) analyses use a Bayesian framework to infer the physical properties of galaxies from observed photometry or spectra. They require sampling from a high-dimensional space of SED model parameters and take $>10-100$ CPU hours per galaxy, which renders them practically infeasible for analyzing the $billions$ of galaxies that will be observed by upcoming galaxy surveys ($e.g.$ DESI, PFS, Rubin, Webb, and Roman). In this work, we present an alternative scalable approach to rigorous Bayesian inference using Amortized Neural Posterior Estimation (ANPE). ANPE is a simulation-based inference method that employs neural networks to estimate the posterior probability distribution over the full range of observations. Once trained, it requires no additional model evaluations to estimate the posterior. We present, and publicly release, ${\rm SED}{flow}$, an ANPE method to produce posteriors of the recent Hahn et al. (2022) SED model from optical photometry. ${\rm SED}{flow}$ takes ${\sim}1$ $second~per~galaxy$ to obtain the posterior distributions of 12 model parameters, all of which are in excellent agreement with traditional Markov Chain Monte Carlo sampling results. We also apply ${\rm SED}{flow}$ to 33,884 galaxies in the NASA-Sloan Atlas and publicly release their posteriors: see https://changhoonhahn.github.io/SEDflow.

</details>

<details>

<summary>2022-03-14 18:11:44 - Forecasting number of corner kicks taken in association football using overdispersed distribution</summary>

- *Stan Yip, Yinghong Zou, Ronald Tsz Hin Hung, Ka Fai Cedric Yiu*

- `2112.13001v2` - [abs](http://arxiv.org/abs/2112.13001v2) - [pdf](http://arxiv.org/pdf/2112.13001v2)

> This paper presents a novel compound Poisson regression model framework to forecast number of corner kicks taken in association football. Corner kick taken events are often decisive in the match outcome and embody serial correlation and clustered pattern. Providing parameter estimates with intuitive interpretation, a class of compound Poisson distribution including a Bayesian implementation of geometric-Poisson distribution is introduced. Apart from introducing a new statistical framework, utilisation of cross-market information, margin application methods to adjust bias in raw odds and the heterogeneity of overdispersion between matches are also discussed.

</details>

<details>

<summary>2022-03-14 18:48:31 - Local Exchangeability</summary>

- *Trevor Campbell, Saifuddin Syed, Chiao-Yu Yang, Michael I. Jordan, Tamara Broderick*

- `1906.09507v4` - [abs](http://arxiv.org/abs/1906.09507v4) - [pdf](http://arxiv.org/pdf/1906.09507v4)

> Exchangeability -- in which the distribution of an infinite sequence is invariant to reorderings of its elements -- implies the existence of a simple conditional independence structure that may be leveraged in the design of statistical models and inference procedures. In practice, however, this assumption is too strong an idealization; the distribution typically fails to be exactly invariant to permutations and de Finetti's representation theory does not apply. Thus there is the need for a distributional assumption that is both weak enough to hold in practice, and strong enough to guarantee a useful underlying representation. We introduce a relaxed notion of local exchangeability -- where swapping data associated with nearby covariates causes a bounded change in the distribution. We prove that locally exchangeable processes correspond to independent observations from an underlying measure-valued stochastic process. We thereby show that de Finetti's theorem is robust to perturbation and provide further justification for the Bayesian modelling approach. Using this main probabilistic result, we show that the local empirical measure of a finite collection of observations provides a consistent approximation of the underlying measure-valued process and Bayesian posterior predictive distributions. The paper concludes with applications of the main theoretical results to a model from Bayesian nonparametrics and covariate-dependent permutation tests.

</details>

<details>

<summary>2022-03-14 22:50:20 - The TAP free energy for high-dimensional linear regression</summary>

- *Jiaze Qiu, Subhabrata Sen*

- `2203.07539v1` - [abs](http://arxiv.org/abs/2203.07539v1) - [pdf](http://arxiv.org/pdf/2203.07539v1)

> We derive a variational representation for the log-normalizing constant of the posterior distribution in Bayesian linear regression with a uniform spherical prior and an i.i.d. Gaussian design. We work under the "proportional" asymptotic regime, where the number of observations and the number of features grow at a proportional rate. This rigorously establishes the Thouless-Anderson-Palmer (TAP) approximation arising from spin glass theory, and proves a conjecture of Krzakala et. al. (2014) in the special case of the spherical prior.

</details>

<details>

<summary>2022-03-15 00:41:28 - Knots and their effect on the tensile strength of lumber</summary>

- *Shuxian Fan, Samuel WK Wong, James V Zidek*

- `2201.03464v2` - [abs](http://arxiv.org/abs/2201.03464v2) - [pdf](http://arxiv.org/pdf/2201.03464v2)

> When assessing the strength of sawn lumber for use in engineering applications, the sizes and locations of knots are an important consideration. Knots are the most common visual characteristics of lumber, that result from the growth of tree branches. Large individual knots, as well as clusters of distinct knots, are known to have strength-reducing effects. However, industry grading rules that govern knots are informed by subjective judgment to some extent. Thus, the spatial interaction of knots and their relationship with lumber strength has not been fully understood. This paper reports the results of a study that investigated and modelled the strength-reducing effects of knots on a sample of Douglas Fir lumber. Experimental data were obtained by taking scans of lumber surfaces and applying tensile strength testing. The modelling approach presented incorporates all relevant knot information in a Bayesian framework, thereby contributing a more refined way of managing the quality of manufactured lumber.

</details>

<details>

<summary>2022-03-15 02:37:37 - Sampling Bias Correction for Supervised Machine Learning: A Bayesian Inference Approach with Practical Applications</summary>

- *Max Sklar*

- `2203.06239v2` - [abs](http://arxiv.org/abs/2203.06239v2) - [pdf](http://arxiv.org/pdf/2203.06239v2)

> Given a supervised machine learning problem where the training set has been subject to a known sampling bias, how can a model be trained to fit the original dataset? We achieve this through the Bayesian inference framework by altering the posterior distribution to account for the sampling function. We then apply this solution to binary logistic regression, and discuss scenarios where a dataset might be subject to intentional sample bias such as label imbalance. This technique is widely applicable for statistical inference on big data, from the medical sciences to image recognition to marketing. Familiarity with it will give the practitioner tools to improve their inference pipeline from data collection to model selection.

</details>

<details>

<summary>2022-03-15 02:50:07 - MoReL: Multi-omics Relational Learning</summary>

- *Arman Hasanzadeh, Ehsan Hajiramezanali, Nick Duffield, Xiaoning Qian*

- `2203.08149v1` - [abs](http://arxiv.org/abs/2203.08149v1) - [pdf](http://arxiv.org/pdf/2203.08149v1)

> Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as often existing data may be collected from different subjects under different conditions for each type of omics data. We propose a novel deep Bayesian generative model to efficiently infer a multi-partite graph encoding molecular interactions across such heterogeneous views, using a fused Gromov-Wasserstein (FGW) regularization between latent representations of corresponding views for integrative analysis. With such an optimal transport regularization in the deep Bayesian generative model, it not only allows incorporating view-specific side information, either with graph-structured or unstructured data in different views, but also increases the model flexibility with the distribution-based regularization. This allows efficient alignment of heterogeneous latent variable distributions to derive reliable interaction predictions compared to the existing point-based graph embedding methods. Our experiments on several real-world datasets demonstrate enhanced performance of MoReL in inferring meaningful interactions compared to existing baselines.

</details>

<details>

<summary>2022-03-15 09:59:50 - Estimating the potential to prevent locally acquired HIV infections in a UNAIDS Fast-Track City, Amsterdam</summary>

- *Alexandra Blenkinsop, Mlodie Monod, Ard van Sighem, Nikos Pantazis, Daniela Bezemer, Eline Op de Coul, Thijs van de Laar, Christophe Fraser, Maria Prins, Peter Reiss, Godelieve de Bree, Oliver Ratmann*

- `2203.07753v1` - [abs](http://arxiv.org/abs/2203.07753v1) - [pdf](http://arxiv.org/pdf/2203.07753v1)

> Amsterdam and other UNAIDS Fast-Track cities aim for zero new HIV infections. Utilising molecular and clinical data of the ATHENA observational HIV cohort, our primary aims are to estimate the proportion of undiagnosed HIV infections and the proportion of locally acquired infections in Amsterdam in 2014-2018, both in MSM and heterosexuals and Dutch-born and foreign-born individuals.   We located diagnosed HIV infections in Amsterdam using postcode data at time of registration to the cohort, and estimated their date of infection using clinical HIV data. We then inferred the proportion undiagnosed from the estimated times to diagnosis. To determine sources of Amsterdam infections, we used HIV sequences of people living with HIV (PLHIV) within a background of other Dutch and international sequences to phylogenetically reconstruct transmission chains. Frequent late diagnoses indicate that more recent phylogenetically observed chains are increasingly incomplete, and we use a Bayesian model to estimate the actual growth of Amsterdam transmission chains, and the proportion of locally acquired infections.   We estimate that 20% [95% CrI 18-22%] of infections acquired among MSM between 2014-2018 were undiagnosed by the start of 2019, and 44% [37-50%] among heterosexuals, with variation by place of birth. The estimated proportion of MSM infections in 2014-2018 that were locally acquired was 68% [61-74%], with no substantial differences by region of birth. In heterosexuals, this was 57% [41-71%] overall, with heterogeneity by place of birth.   The data indicate substantial potential to further curb local transmission, in both MSM and heterosexual Amsterdam residents. In 2014-2018 the largest proportion of local transmissions in Amsterdam are estimated to have occurred in foreign-born MSM, who would likely benefit most from intensified interventions.

</details>

<details>

<summary>2022-03-15 10:05:43 - Generative models and Bayesian inversion using Laplace approximation</summary>

- *Manuel Marschall, Gerd Wbbeler, Franko Schmhling, Clemens Elster*

- `2203.07755v1` - [abs](http://arxiv.org/abs/2203.07755v1) - [pdf](http://arxiv.org/pdf/2203.07755v1)

> The Bayesian approach to solving inverse problems relies on the choice of a prior. This critical ingredient allows the formulation of expert knowledge or physical constraints in a probabilistic fashion and plays an important role for the success of the inference. Recently, Bayesian inverse problems were solved using generative models as highly informative priors. Generative models are a popular tool in machine learning to generate data whose properties closely resemble those of a given database. Typically, the generated distribution of data is embedded in a low-dimensional manifold. For the inverse problem, a generative model is trained on a database that reflects the properties of the sought solution, such as typical structures of the tissue in the human brain in magnetic resonance (MR) imaging. The inference is carried out in the low-dimensional manifold determined by the generative model which strongly reduces the dimensionality of the inverse problem. However, this proceeding produces a posterior that admits no Lebesgue density in the actual variables and the accuracy reached can strongly depend on the quality of the generative model. For linear Gaussian models we explore an alternative Bayesian inference based on probabilistic generative models which is carried out in the original high-dimensional space. A Laplace approximation is employed to analytically derive the required prior probability density function induced by the generative model. Properties of the resulting inference are investigated. Specifically, we show that derived Bayes estimates are consistent, in contrast to the approach employing the low-dimensional manifold of the generative model. The MNIST data set is used to construct numerical experiments which confirm our theoretical findings.

</details>

<details>

<summary>2022-03-15 10:24:16 - Model Comparison in Approximate Bayesian Computation</summary>

- *Jan Boelts*

- `2203.11276v1` - [abs](http://arxiv.org/abs/2203.11276v1) - [pdf](http://arxiv.org/pdf/2203.11276v1)

> A common problem in natural sciences is the comparison of competing models in the light of observed data. Bayesian model comparison provides a statistically sound framework for this comparison based on the evidence each model provides for the data. However, this framework relies on the calculation of likelihood functions which are intractable for most models used in practice. Previous approaches in the field of Approximate Bayesian Computation (ABC) circumvent the evaluation of the likelihood and estimate the model evidence based on rejection sampling, but they are typically computationally intense. Here, I propose a new efficient method to perform Bayesian model comparison in ABC. Based on recent advances in posterior density estimation, the method approximates the posterior over models in parametric form. In particular, I train a mixture-density network to map features of the observed data to the posterior probability of the models. The performance is assessed with two examples. On a tractable model comparison problem, the underlying exact posterior probabilities are predicted accurately. In a use-case scenario from computational neuroscience -- the comparison between two ion channel models -- the underlying ground-truth model is reliably assigned a high posterior probability. Overall, the method provides a new efficient way to perform Bayesian model comparison on complex biophysical models independent of the model architecture.

</details>

<details>

<summary>2022-03-15 10:26:16 - Deep learning via message passing algorithms based on belief propagation</summary>

- *Carlo Lucibello, Fabrizio Pittorino, Gabriele Perugini, Riccardo Zecchina*

- `2110.14583v3` - [abs](http://arxiv.org/abs/2110.14583v3) - [pdf](http://arxiv.org/pdf/2110.14583v3)

> Message-passing algorithms based on the Belief Propagation (BP) equations constitute a well-known distributed computational scheme. It is exact on tree-like graphical models and has also proven to be effective in many problems defined on graphs with loops (from inference to optimization, from signal processing to clustering). The BP-based scheme is fundamentally different from stochastic gradient descent (SGD), on which the current success of deep networks is based. In this paper, we present and adapt to mini-batch training on GPUs a family of BP-based message-passing algorithms with a reinforcement field that biases distributions towards locally entropic solutions. These algorithms are capable of training multi-layer neural networks with discrete weights and activations with performance comparable to SGD-inspired heuristics (BinaryNet) and are naturally well-adapted to continual learning. Furthermore, using these algorithms to estimate the marginals of the weights allows us to make approximate Bayesian predictions that have higher accuracy than point-wise solutions.

</details>

<details>

<summary>2022-03-15 14:25:30 - Variational methods for simulation-based inference</summary>

- *Manuel Glckler, Michael Deistler, Jakob H. Macke*

- `2203.04176v2` - [abs](http://arxiv.org/abs/2203.04176v2) - [pdf](http://arxiv.org/pdf/2203.04176v2)

> We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.

</details>

<details>

<summary>2022-03-15 15:14:46 - Generalized Bayesian Likelihood-Free Inference Using Scoring Rules Estimators</summary>

- *Lorenzo Pacchiardi, Ritabrata Dutta*

- `2104.03889v5` - [abs](http://arxiv.org/abs/2104.03889v5) - [pdf](http://arxiv.org/pdf/2104.03889v5)

> We propose a framework for Bayesian Likelihood-Free Inference (LFI) based on Generalized Bayesian Inference. To define the generalized posterior, we use Scoring Rules (SRs), which evaluate probabilistic models given an observation. In LFI, we can sample from the model but not evaluate the likelihood; for this reason, we employ SRs with easy empirical estimators. Our framework includes novel approaches and popular LFI techniques (such as Bayesian Synthetic Likelihood) and enjoys posterior consistency in a well-specified setting when a strictly-proper SR is used (i.e., one whose expectation is uniquely minimized when the model corresponds to the data generating process). In general, our framework does not approximate the standard posterior; as such, it is possible to achieve outlier robustness, which we prove is the case for the Kernel and Energy Scores. We also discuss a strategy for tuning the learning rate in the generalized posterior suitable for the LFI setup. We run simulations studies with correlated pseudo-marginal Markov Chain Monte Carlo and compare with related approaches on standard benchmarks and challenging intractable-likelihood models from meteorology and ecology.

</details>

<details>

<summary>2022-03-15 17:10:16 - Regenerative Particle Thompson Sampling</summary>

- *Zeyu Zhou, Bruce Hajek, Nakjung Choi, Anwar Walid*

- `2203.08082v1` - [abs](http://arxiv.org/abs/2203.08082v1) - [pdf](http://arxiv.org/pdf/2203.08082v1)

> This paper proposes regenerative particle Thompson sampling (RPTS), a flexible variation of Thompson sampling. Thompson sampling itself is a Bayesian heuristic for solving stochastic bandit problems, but it is hard to implement in practice due to the intractability of maintaining a continuous posterior distribution. Particle Thompson sampling (PTS) is an approximation of Thompson sampling obtained by simply replacing the continuous distribution by a discrete distribution supported at a set of weighted static particles. We observe that in PTS, the weights of all but a few fit particles converge to zero. RPTS is based on the heuristic: delete the decaying unfit particles and regenerate new particles in the vicinity of fit surviving particles. Empirical evidence shows uniform improvement from PTS to RPTS and flexibility and efficacy of RPTS across a set of representative bandit problems, including an application to 5G network slicing.

</details>

<details>

<summary>2022-03-15 17:24:54 - An Experimental Design Perspective on Model-Based Reinforcement Learning</summary>

- *Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, Willie Neiswanger*

- `2112.05244v2` - [abs](http://arxiv.org/abs/2112.05244v2) - [pdf](http://arxiv.org/pdf/2112.05244v2)

> In many practical applications of RL, it is expensive to observe state transitions from the environment. For example, in the problem of plasma control for nuclear fusion, computing the next state for a given state-action pair requires querying an expensive transition function which can lead to many hours of computer simulation or dollars of scientific research. Such expensive data collection prohibits application of standard RL algorithms which usually require a large number of observations to learn. In this work, we address the problem of efficiently learning a policy while making a minimal number of state-action queries to the transition function. In particular, we leverage ideas from Bayesian optimal experimental design to guide the selection of state-action queries for efficient learning. We propose an acquisition function that quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process. At each iteration, our algorithm maximizes this acquisition function, to choose the most informative state-action pair to be queried, thus yielding a data-efficient RL approach. We experiment with a variety of simulated continuous control problems and show that our approach learns an optimal policy with up to $5$ -- $1,000\times$ less data than model-based RL baselines and $10^3$ -- $10^5\times$ less data than model-free RL baselines. We also provide several ablated comparisons which point to substantial improvements arising from the principled method of obtaining data.

</details>

<details>

<summary>2022-03-15 17:25:37 - Transfer Learning with Gaussian Processes for Bayesian Optimization</summary>

- *Petru Tighineanu, Kathrin Skubch, Paul Baireuther, Attila Reiss, Felix Berkenkamp, Julia Vinogradska*

- `2111.11223v2` - [abs](http://arxiv.org/abs/2111.11223v2) - [pdf](http://arxiv.org/pdf/2111.11223v2)

> Bayesian optimization is a powerful paradigm to optimize black-box functions based on scarce and noisy data. Its data efficiency can be further improved by transfer learning from related tasks. While recent transfer models meta-learn a prior based on large amount of data, in the low-data regime methods that exploit the closed-form posterior of Gaussian processes (GPs) have an advantage. In this setting, several analytically tractable transfer-model posteriors have been proposed, but the relative advantages of these methods are not well understood. In this paper, we provide a unified view on hierarchical GP models for transfer learning, which allows us to analyze the relationship between methods. As part of the analysis, we develop a novel closed-form boosted GP transfer model that fits between existing approaches in terms of complexity. We evaluate the performance of the different approaches in large-scale experiments and highlight strengths and weaknesses of the different transfer-learning methods.

</details>

<details>

<summary>2022-03-16 04:29:24 - Scalable marginalization of latent variables for correlated data</summary>

- *Mengyang Gu, Xubo Liu, Xinyi Fang, Sui Tang*

- `2203.08389v1` - [abs](http://arxiv.org/abs/2203.08389v1) - [pdf](http://arxiv.org/pdf/2203.08389v1)

> Marginalization of latent variables or nuisance parameters is a fundamental aspect of Bayesian inference and uncertainty quantification. In this work, we focus on scalable marginalization of latent variables in modeling correlated data, such as spatio-temporal or functional observations. We first introduce Gaussian processes (GPs) for modeling correlated data and highlight the computational challenge, where the computational complexity increases cubically fast along with the number of observations. We then review the connection between the state space model and GPs with Mat{\'e}rn covariance for temporal inputs. The Kalman filter and Rauch-Tung-Striebel smoother were introduced as a scalable marginalization technique of computing the likelihood and making predictions of GPs without approximation. We then introduce recent efforts on extending the scalable marginalization idea to linear model of coregionalization for multivariate correlated output and spatio-temporal observations. In the final part of this work, we introduce a novel marginalization technique to estimate interaction kernels and to forecast particle trajectories. The achievement lies in sparse representation of covariance function, then applying conjugate gradient for solving the computational challenges and improving predictive accuracy. The computational advances achieved in this work outline a wide range of applications in molecular dynamic simulation, cellular migration and agent based models.

</details>

<details>

<summary>2022-03-16 07:29:47 - Sparse Bayesian Inference on Positive-valued Data using Global-local Shrinkage Priors</summary>

- *Yasuyuki Hamura, Takahiro Onizuka, Shintaro Hashimoto, Shonosuke Sugasawa*

- `2203.08440v1` - [abs](http://arxiv.org/abs/2203.08440v1) - [pdf](http://arxiv.org/pdf/2203.08440v1)

> In various applications, we deal with high-dimensional positive-valued data that often exhibits sparsity. This paper develops a new class of continuous global-local shrinkage priors tailored to analyzing positive-valued data where most of the underlying means are concentrated around a certain value. Unlike existing shrinkage priors, our new prior is a shape-scale mixture of inverse-gamma distributions, which has a desirable interpretation of the form of the posterior mean. We show that the proposed prior has two desirable theoretical properties; Kullback-Leibler super-efficiency under sparsity and robust shrinkage rules for large observations. We propose an efficient sampling algorithm to carry out posterior inference. The performance of the proposed method is illustrated through simulation and two real data examples, the average length of hospital stay for COVID-19 in South Korea and adaptive variance estimation of gene expression data.

</details>

<details>

<summary>2022-03-16 09:32:55 - Bayesian Analysis of Formula One Race Results: Disentangling Driver Skill and Constructor Advantage</summary>

- *Erik-Jan van Kesteren, Tom Bergkamp*

- `2203.08489v1` - [abs](http://arxiv.org/abs/2203.08489v1) - [pdf](http://arxiv.org/pdf/2203.08489v1)

> Successful performance in Formula One is determined by combination of both the driver's skill and race-car constructor advantage. This makes key performance questions in the sport difficult to answer. For example, who is the best Formula One driver, which is the best constructor, and what is their relative contribution to success? In this paper, we answer these questions based on data from the hybrid era in Formula One (2014 - 2021 seasons). We present a novel Bayesian multilevel Beta regression method to model individual race success as the proportion of outperformed competitors. We show that our modelling approach describes our data well, which allows for precise inferences about driver skill and constructor advantage. We conclude that Hamilton and Verstappen are the best drivers in the hybrid era, the top-three teams (Mercedes, Ferrari, and Red Bull) clearly outperform other constructors, and around 86% of the variance in race results is explained by the constructor. We argue that this modeling approach may prove useful for sports beyond Formula One, as it creates performance ratings for independent components contributing to success.

</details>

<details>

<summary>2022-03-16 09:55:58 - Natural Posterior Network: Deep Bayesian Uncertainty for Exponential Family Distributions</summary>

- *Bertrand Charpentier, Oliver Borchert, Daniel Zgner, Simon Geisler, Stephan Gnnemann*

- `2105.04471v2` - [abs](http://arxiv.org/abs/2105.04471v2) - [pdf](http://arxiv.org/pdf/2105.04471v2)

> Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks.

</details>

<details>

<summary>2022-03-16 14:00:53 - Measurability of functionals and of ideal point forecasts</summary>

- *Tobias Fissler, Hajo Holzmann*

- `2203.08635v1` - [abs](http://arxiv.org/abs/2203.08635v1) - [pdf](http://arxiv.org/pdf/2203.08635v1)

> The ideal probabilistic forecast for a random variable $Y$ based on an information set $\mathcal{F}$ is the conditional distribution of $Y$ given $\mathcal{F}$. In the context of point forecasts aiming to specify a functional $T$ such as the mean, a quantile or a risk measure, the ideal point forecast is the respective functional applied to the conditional distribution. This paper provides a theoretical justification why this ideal forecast is actually a forecast, that is, an $\mathcal{F}$-measurable random variable. To that end, the appropriate notion of measurability of $T$ is clarified and this measurability is established for a large class of practically relevant functionals, including elicitable ones. More generally, the measurability of $T$ implies the measurability of any point forecast which arises by applying $T$ to a probabilistic forecast. Similar measurability results are established for proper scoring rules, the main tool to evaluate the predictive accuracy of probabilistic forecasts.

</details>

<details>

<summary>2022-03-16 14:20:31 - Bayesian Neural Network Priors Revisited</summary>

- *Vincent Fortuin, Adri Garriga-Alonso, Sebastian W. Ober, Florian Wenzel, Gunnar Rtsch, Richard E. Turner, Mark van der Wilk, Laurence Aitchison*

- `2102.06571v3` - [abs](http://arxiv.org/abs/2102.06571v3) - [pdf](http://arxiv.org/pdf/2102.06571v3)

> Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets.

</details>

<details>

<summary>2022-03-16 21:12:55 - Gibbs posterior concentration rates under sub-exponential type losses</summary>

- *Nicholas Syring, Ryan Martin*

- `2012.04505v6` - [abs](http://arxiv.org/abs/2012.04505v6) - [pdf](http://arxiv.org/pdf/2012.04505v6)

> Bayesian posterior distributions are widely used for inference, but their dependence on a statistical model creates some challenges. In particular, there may be lots of nuisance parameters that require prior distributions and posterior computations, plus a potentially serious risk of model misspecification bias. Gibbs posterior distributions, on the other hand, offer direct, principled, probabilistic inference on quantities of interest through a loss function, not a model-based likelihood. Here we provide simple sufficient conditions for establishing Gibbs posterior concentration rates when the loss function is of a sub-exponential type. We apply these general results in a range of practically relevant examples, including mean regression, quantile regression, and sparse high-dimensional classification. We also apply these techniques in an important problem in medical statistics, namely, estimation of a personalized minimum clinically important difference.

</details>

<details>

<summary>2022-03-17 00:39:47 - Lorenz map, inequality ordering and curves based on multidimensional rearrangements</summary>

- *Yanqin Fan, Marc Henry, Brendan Pass, Jorge A. Rivero*

- `2203.09000v1` - [abs](http://arxiv.org/abs/2203.09000v1) - [pdf](http://arxiv.org/pdf/2203.09000v1)

> We propose a multivariate extension of the Lorenz curve based on multivariate rearrangements of optimal transport theory. We define a vector Lorenz map as the integral of the vector quantile map associated to a multivariate resource allocation. Each component of the Lorenz map is the cumulative share of each resource, as in the traditional univariate case. The pointwise ordering of such Lorenz maps defines a new multivariate majorization order. We define a multi-attribute Gini index and complete ordering based on the Lorenz map. We propose the level sets of an Inverse Lorenz Function as a practical tool to visualize and compare inequality in two dimensions, and apply it to income-wealth inequality in the United States between 1989 and 2019.

</details>

<details>

<summary>2022-03-17 01:56:42 - Sensitivity analysis in longitudinal clinical trials via distributional imputation</summary>

- *Siyi Liu, Shu Yang, Yilong Zhang, Guanghan, Liu*

- `2203.09025v1` - [abs](http://arxiv.org/abs/2203.09025v1) - [pdf](http://arxiv.org/pdf/2203.09025v1)

> Missing data is inevitable in longitudinal clinical trials. Conventionally, the missing at random assumption is assumed to handle missingness, which however is unverifiable empirically. Thus, sensitivity analysis is critically important to assess the robustness of the study conclusions against untestable assumptions. Toward this end, regulatory agencies often request using imputation models such as return-to-baseline, control-based, and washout imputation. Multiple imputation is popular in sensitivity analysis; however, it may be inefficient and result in an unsatisfying interval estimation by Rubin's combining rule. We propose distributional imputation (DI) in sensitivity analysis, which imputes each missing value by samples from its target imputation model given the observed data. Drawn on the idea of Monte Carlo integration, the DI estimator solves the mean estimating equations of the imputed dataset. It is fully efficient with theoretical guarantees. Moreover, we propose weighted bootstrap to obtain a consistent variance estimator, taking into account the variabilities due to model parameter estimation and target parameter estimation. The finite-sample performance of DI inference is assessed in the simulation study. We apply the proposed framework to an antidepressant longitudinal clinical trial involving missing data to investigate the robustness of the treatment effect. Our proposed DI approach detects a statistically significant treatment effect in both the primary analysis and sensitivity analysis under certain prespecified sensitivity models in terms of the average treatment effect, the risk difference, and the quantile treatment effect in lower quantiles of the responses, uncovering the benefit of the test drug for curing depression.

</details>

<details>

<summary>2022-03-17 04:12:02 - Evaluating Posterior Distributions by Selectively Breeding Prior Samples</summary>

- *Cosma Rohilla Shalizi*

- `2203.09077v1` - [abs](http://arxiv.org/abs/2203.09077v1) - [pdf](http://arxiv.org/pdf/2203.09077v1)

> Using Markov chain Monte Carlo to sample from posterior distributions was the key innovation which made Bayesian data analysis practical. Notoriously, however, MCMC is hard to tune, hard to diagnose, and hard to parallelize. This pedagogical note explores variants on a universal {\em non}-Markov-chain Monte Carlo scheme for sampling from posterior distributions. The basic idea is to draw parameter values from the prior distributions, evaluate the likelihood of each draw, and then copy that draw a number of times proportional to its likelihood. The distribution after copying is an approximation to the posterior which becomes exact as the number of initial samples goes to infinity; the convergence of the approximation is easily analyzed, and is uniform over Glivenko-Cantelli classes. While not {\em entirely} practical, the schemes are straightforward to implement (a few lines of R), easily parallelized, and require no rejection, burn-in, convergence diagnostics, or tuning of any control settings. I provide references to the prior art which deals with some of the practical obstacles, at some cost in computational and analytical simplicity.

</details>

<details>

<summary>2022-03-17 07:51:52 - Covid19 Reproduction Number: Credibility Intervals by Blockwise Proximal Monte Carlo Samplers</summary>

- *Gersende Fort, Barbara Pascal, Patrice Abry, Nelly Pustelnik*

- `2203.09142v1` - [abs](http://arxiv.org/abs/2203.09142v1) - [pdf](http://arxiv.org/pdf/2203.09142v1)

> Monitoring the Covid19 pandemic constitutes a critical societal stake that received considerable research efforts. The intensity of the pandemic on a given territory is efficiently measured by the reproduction number, quantifying the rate of growth of daily new infections. Recently, estimates for the time evolution of the reproduction number were produced using an inverse problem formulation with a nonsmooth functional minimization. While it was designed to be robust to the limited quality of the Covid19 data (outliers, missing counts), the procedure lacks the ability to output credibility interval based estimates. This remains a severe limitation for practical use in actual pandemic monitoring by epidemiologists that the present work aims to overcome by use of Monte Carlo sampling. After interpretation of the functional into a Bayesian framework, several sampling schemes are tailored to adjust the nonsmooth nature of the resulting posterior distribution. The originality of the devised algorithms stems from combining a Langevin Monte Carlo sampling scheme with Proximal operators. Performance of the new algorithms in producing relevant credibility intervals for the reproduction number estimates and denoised counts are compared. Assessment is conducted on real daily new infection counts made available by the Johns Hopkins University. The interest of the devised monitoring tools are illustrated on Covid19 data from several different countries.

</details>

<details>

<summary>2022-03-17 11:12:34 - Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison</summary>

- *Tuomas Sivula, Mns Magnusson, Asael Alonzo Matamoros, Aki Vehtari*

- `2008.10296v3` - [abs](http://arxiv.org/abs/2008.10296v3) - [pdf](http://arxiv.org/pdf/2008.10296v3)

> Leave-one-out cross-validation (LOO-CV) is a popular method for comparing Bayesian models based on their estimated predictive performance on new, unseen, data. As leave-one-out cross-validation is based on finite observed data, there is uncertainty about the expected predictive performance on new data. By modeling this uncertainty when comparing two models, we can compute the probability that one model has a better predictive performance than the other. Modeling this uncertainty well is not trivial, and for example, it is known that the commonly used standard error estimate is often too small. We study the properties of the Bayesian LOO-CV estimator and the related uncertainty estimates when comparing two models. We provide new results of the properties both theoretically in the linear regression case and empirically for multiple different models and discuss the challenges of modeling the uncertainty. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection in the skewness of the individual leave-one-out terms and the distribution of the error of the Bayesian LOO-CV estimator. We show that it is possible that the problematic skewness of the error distribution, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide practical recommendations for the users of Bayesian LOO-CV for model comparison.

</details>

<details>

<summary>2022-03-17 11:32:00 - Optimal Bayesian estimation of Gaussian mixtures with growing number of components</summary>

- *Ilsang Ohn, Lizhen Lin*

- `2007.09284v2` - [abs](http://arxiv.org/abs/2007.09284v2) - [pdf](http://arxiv.org/pdf/2007.09284v2)

> We study Bayesian estimation of finite mixture models in a general setup where the number of components is unknown and allowed to grow with the sample size. An assumption on growing number of components is a natural one as the degree of heterogeneity present in the sample can grow and new components can arise as sample size increases, allowing full flexibility in modeling the complexity of data. This however will lead to a high-dimensional model which poses great challenges for estimation. We novelly employ the idea of a sample size dependent prior in a Bayesian model and establish a number of important theoretical results. We first show that under mild conditions on the prior, the posterior distribution concentrates around the true mixing distribution at a near optimal rate with respect to the Wasserstein distance. Under a separation condition on the true mixing distribution, we further show that a better and adaptive convergence rate can be achieved, and the number of components can be consistently estimated. Furthermore, we derive optimal convergence rates for the higher-order mixture models where the number of components diverges arbitrarily fast. In addition, we suggest a simple recipe for using Dirichlet process (DP) mixture prior for estimating the finite mixture models and provide theoretical guarantees. In particular, we provide a novel solution for adopting the number of clusters in a DP mixture model as an estimate of the number of components in a finite mixture model. Simulation study and real data applications are carried out demonstrating the utilities of our method.

</details>

<details>

<summary>2022-03-17 14:59:43 - Efficient Bayesian estimation and use of cut posterior in semiparametric hidden Markov models</summary>

- *Daniel Moss, Judith Rousseau*

- `2203.06081v2` - [abs](http://arxiv.org/abs/2203.06081v2) - [pdf](http://arxiv.org/pdf/2203.06081v2)

> We consider the problem of estimation in Hidden Markov models with finite state space and nonparametric emission distributions. Efficient estimators for the transition matrix are exhibited, and a semiparametric Bernstein-von Mises result is deduced, extending the work of Gassiat et al. (2018) on mixture models to the HMM setting. Following from this, we propose a modular approach using the cut posterior to jointly estimate the transition matrix and the emission densities. We derive a general theorem on contraction rates for this approach, in the spirit of the seminal work of Ghosal and van der Vaart (2007). We then show how this result may be applied to obtain a contraction rate result for the emission densities in our setting; a key intermediate step is an inversion inequality relating $L^1$ distance between the marginal densities to $L^1$ distance between the emissions. Finally, a contraction result for the smoothing probabilities is shown, which avoids the common approach of sample splitting. Simulations are provided which demonstrate both the theory and the ease of its implementation.

</details>

<details>

<summary>2022-03-17 15:10:27 - Bayesian Semiparametric Multivariate Density Deconvolution via Stochastic Rotation of Replicates</summary>

- *Arkaprava Roy, Abhra Sarkar*

- `2107.06436v2` - [abs](http://arxiv.org/abs/2107.06436v2) - [pdf](http://arxiv.org/pdf/2107.06436v2)

> We consider the problem of multivariate density deconvolution where the distribution of a random vector needs to be estimated from replicates contaminated with conditionally heteroscedastic measurement errors. We propose a conceptually straightforward yet fundamentally novel and highly robust approach to multivariate density deconvolution by stochastically rotating the replicates toward the corresponding true latent values. We also address the additionally significantly challenging problem of accommodating conditionally heteroscedastic measurement errors in this newly introduced framework. We take a Bayesian route to estimation and inference, implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of our analysis. Asymptotic convergence guarantees for the method are also established. We illustrate the method's empirical efficacy through simulation experiments and its practical utility in estimating the long-term joint average intakes of different dietary components from their measurement error contaminated 24-hour dietary recalls.

</details>

<details>

<summary>2022-03-17 15:10:42 - Data aggregation can lead to biased inferences in Bayesian linear mixed models</summary>

- *Daniel J. Schad, Bruno Nicenboim, Shravan Vasishth*

- `2203.02361v2` - [abs](http://arxiv.org/abs/2203.02361v2) - [pdf](http://arxiv.org/pdf/2203.02361v2)

> Bayesian linear mixed-effects models are increasingly being used in the cognitive sciences to perform null hypothesis tests, where a null hypothesis that an effect is zero is compared with an alternative hypothesis that the effect exists and is different from zero. While software tools for Bayes factor null hypothesis tests are easily accessible, how to specify the data and the model correctly is often not clear. In Bayesian approaches, many authors recommend data aggregation at the by-subject level and running Bayes factors on aggregated data. Here, we use simulation-based calibration for model inference to demonstrate that null hypothesis tests can yield biased Bayes factors, when computed from aggregated data. Specifically, when random slope variances differ (i.e., violated sphericity assumption), Bayes factors are too conservative for contrasts where the variance is small and they are too liberal for contrasts where the variance is large. Moreover, Bayes factors for by-subject aggregated data are biased (too liberal) when random item variance is present but ignored in the analysis. We also perform corresponding frequentist analyses (type I and II error probabilities) to illustrate that the same problems exist and are well known from frequentist tools. These problems can be circumvented by running Bayesian linear mixed-effects models on non-aggregated data such as on individual trials and by explicitly modeling the full random effects structure.   Reproducible code is available from https://osf.io/mjf47/.

</details>

<details>

<summary>2022-03-17 20:44:35 - Learning to Personalize Treatments When Agents Are Strategic</summary>

- *Evan Munro*

- `2011.06528v4` - [abs](http://arxiv.org/abs/2011.06528v4) - [pdf](http://arxiv.org/pdf/2011.06528v4)

> There is increasing interest in allocating treatments based on observed individual data: examples include targeted marketing, individualized credit offers, and heterogenous pricing. Treatment personalization introduces incentives for individuals to modify their behavior to obtain a better treatment. This shifts the distribution of covariates, requiring a new definition for the Conditional Average Treatment Effect (CATE) that makes explicit its dependence on how treatments are allocated. We provide necessary conditions that treatment rules under strategic behavior must meet. The optimal rule without strategic behavior allocates treatments only to those with a positive CATE. With strategic behavior, we show that the optimal rule can involve randomization, allocating treatments with less than 100\% probability even to those with a positive CATE induced by that rule. We propose a dynamic experiment based on Bayesian Optimization that converges to the optimal treatment allocation function without parametric assumptions on individual strategic behavior.

</details>

<details>

<summary>2022-03-17 20:49:12 - An alternative Interpretation of residual feed intake by phenotypic recursive relationships in dairy cattle</summary>

- *Xiao-Lin Wu, Kristen L. Parker Gaddis, Javier Burchard, H. Duane Norman, Ezequiel Nicolazzi, Erin E. Connor, John B. Cole, Joao Durr*

- `2203.09609v1` - [abs](http://arxiv.org/abs/2203.09609v1) - [pdf](http://arxiv.org/pdf/2203.09609v1)

> There has been an increasing interest in residual feed intake (RFI) as a measure of net feed efficiency in dairy cattle. RFI phenotypes are obtained as residuals from linear regression encompassing relevant factors (i.e., energy sinks) to account for body tissue mobilization. However, fitting energy sink phenotypes as regression variables in standard linear regression was criticized because phenotypes are subject to measurement errors. Multiple-trait models have been proposed which derive RFI by follow-up partial regression. By re-arranging the single-trait linear regression, we showed a causal RFI interpretation underlying the linear regression for RFI. It postulates recursive effects in energy allocation from energy sinks on dry matter intake, but the feedback or simultaneous effects are assumed to be nonexistent. A Bayesian recursive structural equation model was proposed for directly predicting RFI and energy sinks and estimating relevant genetic parameters simultaneously. A simplified Markov chain Monte Carlo algorithm that implemented the Bayesian recursive model was described. The recursive model is asymptotically equivalent to one-step linear regression for RFI, yet extends the analytical capacity to multiple-trait analysis. It is equivalent to Bayesian-implemented, multiple-trait model reparameterized based on Cholesky decomposition of phenotypic (co)variance matrix for evaluating RFI, but varied in assumptions about relationships between energy sinks.

</details>

<details>

<summary>2022-03-18 01:04:39 - Fast Bayesian Coresets via Subsampling and Quasi-Newton Refinement</summary>

- *Cian Naik, Judith Rousseau, Trevor Campbell*

- `2203.09675v1` - [abs](http://arxiv.org/abs/2203.09675v1) - [pdf](http://arxiv.org/pdf/2203.09675v1)

> Bayesian coresets approximate a posterior distribution by building a small weighted subset of the data points. Any inference procedure that is too computationally expensive to be run on the full posterior can instead be run inexpensively on the coreset, with results that approximate those on the full data. However, current approaches are limited by either a significant run-time or the need for the user to specify a low-cost approximation to the full posterior. We propose a Bayesian coreset construction algorithm that first selects a uniformly random subset of data, and then optimizes the weights using a novel quasi-Newton method. Our algorithm is simple to implement, does not require the user to specify a low-cost posterior approximation, and is the first to come with a general high-probability bound on the KL divergence of the output coreset posterior. Experiments demonstrate that the method provides orders of magnitude improvement in construction time against the state-of-the-art black-box method. Moreover, it provides significant improvements in coreset quality against alternatives with comparable construction times, with far less storage cost and user input required.

</details>

<details>

<summary>2022-03-18 07:58:37 - Modularized Bayesian analyses and cutting feedback in likelihood-free inference</summary>

- *Atlanta Chakraborty, David J. Nott, Christopher Drovandi, David T. Frazier, Scott A. Sisson*

- `2203.09782v1` - [abs](http://arxiv.org/abs/2203.09782v1) - [pdf](http://arxiv.org/pdf/2203.09782v1)

> There has been much recent interest in modifying Bayesian inference for misspecified models so that it is useful for specific purposes. One popular modified Bayesian inference method is "cutting feedback" which can be used when the model consists of a number of coupled modules, with only some of the modules being misspecified. Cutting feedback methods represent the full posterior distribution in terms of conditional and sequential components, and then modify some terms in such a representation based on the modular structure for specification or computation of a modified posterior distribution. The main goal of this is to avoid contamination of inferences for parameters of interest by misspecified modules. Computation for cut posterior distributions is challenging, and here we consider cutting feedback for likelihood-free inference based on Gaussian mixture approximations to the joint distribution of parameters and data summary statistics. We exploit the fact that marginal and conditional distributions of a Gaussian mixture are Gaussian mixtures to give explicit approximations to marginal or conditional posterior distributions so that we can easily approximate cut posterior analyses. The mixture approach allows repeated approximation of posterior distributions for different data based on a single mixture fit, which is important for model checks which aid in the decision of whether to "cut". A semi-modular approach to likelihood-free inference where feedback is partially cut is also developed. The benefits of the method are illustrated in two challenging examples, a collective cell spreading model and a continuous time model for asset returns with jumps.

</details>

<details>

<summary>2022-03-18 08:42:51 - Standard and reference-based conditional mean imputation</summary>

- *Marcel Wolbers, Alessandro Noci, Paul Delmar, Craig Gower-Page, Sean Yiu, Jonathan W. Bartlett*

- `2109.11162v3` - [abs](http://arxiv.org/abs/2109.11162v3) - [pdf](http://arxiv.org/pdf/2109.11162v3)

> Clinical trials with longitudinal outcomes typically include missing data due to missed assessments or structural missingness of outcomes after intercurrent events handled with a hypothetical strategy. Approaches based on Bayesian random multiple imputation and Rubin's rule for pooling results across multiple imputed datasets are increasingly used in order to align the analysis of these trials with the targeted estimand. We propose and justify deterministic conditional mean imputation combined with the jackknife for inference as an alternative approach. The method is applicable to imputations under a missing-at-random assumption as well as for reference-based imputation approaches. In an application and a simulation study, we demonstrate that it provides consistent treatment effect estimates with the Bayesian approach and reliable frequentist inference with accurate standard error estimation and type I error control. A further advantage of the method is that it does not rely on random sampling and is therefore replicable and unaffected by Monte Carlo error.

</details>

<details>

<summary>2022-03-18 09:11:11 - makemyprior: Intuitive Construction of Joint Priors for Variance Parameters in R</summary>

- *Ingeborg Gullikstad Hem, Geir-Arne Fuglstad, Andrea Riebler*

- `2105.09712v2` - [abs](http://arxiv.org/abs/2105.09712v2) - [pdf](http://arxiv.org/pdf/2105.09712v2)

> Priors allow us to robustify inference and to incorporate expert knowledge in Bayesian hierarchical models. This is particularly important when there are random effects that are hard to identify based on observed data. The challenge lies in understanding and controlling the joint influence of the priors for the variance parameters, and makemyprior is an R package that guides the formulation of joint prior distributions for variance parameters. A joint prior distribution is constructed based on a hierarchical decomposition of the total variance in the model along a tree, and takes the entire model structure into account. Users input their prior beliefs or express ignorance at each level of the tree. Prior beliefs can be general ideas about reasonable ranges of variance values and need not be detailed expert knowledge. The constructed priors lead to robust inference and guarantee proper posteriors. A graphical user interface facilitates construction and assessment of different choices of priors through visualization of the tree and joint prior. The package aims to expand the toolbox of applied researchers and make priors an active component in their Bayesian workflow.

</details>

<details>

<summary>2022-03-18 09:28:59 - Model Averaging based Semiparametric Modelling for Conditional Quantile Prediction</summary>

- *Chaohui Guo, Wenyang Zhang*

- `2203.09816v1` - [abs](http://arxiv.org/abs/2203.09816v1) - [pdf](http://arxiv.org/pdf/2203.09816v1)

> In real data analysis, the underlying model is usually unknown, modelling strategy plays a key role in the success of data analysis. Stimulated by the idea of model averaging, we propose a novel semiparametric modelling strategy for conditional quantile prediction, without assuming the underlying model is any specific parametric or semiparametric model. Thanks the optimality of the selected weights by cross-validation, the proposed modelling strategy results in a more accurate prediction than that based on some commonly used semiparametric models, such as the varying coefficient models and additive models. Asymptotic properties are established of the proposed modelling strategy together with its estimation procedure. Intensive simulation studies are conducted to demonstrate how well the proposed method works, compared with its alternatives under various circumstances. The results show the proposed method indeed leads to more accurate predictions than its alternatives. Finally, the proposed modelling strategy together with its prediction procedure are applied to the Boston housing data, which result in more accurate predictions of the quantiles of the house prices than that based on some commonly used alternative methods, therefore, present us a more accurate picture of the housing market in Boston.

</details>

<details>

<summary>2022-03-18 10:30:25 - The Bayesian Learning Rule</summary>

- *Mohammad Emtiyaz Khan, Hvard Rue*

- `2107.04562v2` - [abs](http://arxiv.org/abs/2107.04562v2) - [pdf](http://arxiv.org/pdf/2107.04562v2)

> We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.

</details>

<details>

<summary>2022-03-18 12:14:23 - Bayesian Spectral Deconvolution of X-Ray Absorption Near Edge Structure Discriminating High- and Low-Energy Domain</summary>

- *Shuhei Kashiwamura, Shun Katakami, Ryo Yamagami, Kazunori Iwamitsu, Hiroyuki Kumazoe, Kenji Nagata, Toshihiro Okajima, Ichiro Akai, Masato Okada*

- `2203.09895v1` - [abs](http://arxiv.org/abs/2203.09895v1) - [pdf](http://arxiv.org/pdf/2203.09895v1)

> In this paper, we propose a Bayesian spectral deconvolution considering the properties of peaks in different energy domains. Bayesian spectral deconvolution regresses spectral data into the sum of multiple basis functions. Conventional methods use a model that treats all peaks equally. However, in X-ray absorption near edge structure (XANES) spectra, the properties of the peaks differ depending on the energy domain, and the specific energy domain of XANES is essential in condensed matter physics. We propose a model that discriminates the energy domain and a prior distribution that reflects the physical properties. We compare the conventional and proposed models in computational efficiency, estimation accuracy, and model evidence. We demonstrate that our method effectively estimates the number of transition components in the important energy domain, on which the material scientists focus for mapping to the electronic transition analysis by first-principles simulation.

</details>

<details>

<summary>2022-03-18 12:21:52 - BCEA: An R Package for Cost-Effectiveness Analysis</summary>

- *Nathan Green, Anna Heath, Gianluca Baio*

- `2203.09901v1` - [abs](http://arxiv.org/abs/2203.09901v1) - [pdf](http://arxiv.org/pdf/2203.09901v1)

> We describe in detail how to perform health economic cost-effectiveness analyses (CEA) using the R package $\textbf{BCEA}$ (Bayesian Cost-Effectiveness Analysis). CEA consist of analytic approaches for combining costs and health consequences of intervention(s). These help to understand how much an intervention may cost (per unit of health gained) compared to an alternative intervention, such as a control or status quo. For resource allocation, a decision maker may wish to know if an intervention is cost saving, and if not then how much more would it cost to implement it compared to a less effective intervention.   Current guidance for cost-effectiveness analyses advocates the quantification of uncertainties which can be represented by random samples obtained from a probability sensitivity analysis or, more efficiently, a Bayesian model. $\textbf{BCEA}$ can be used to post-process the sampled costs and health impacts to perform advanced analyses producing standardised and highly customisable outputs. We present the features of the package, including its many functions and their practical application. $\textbf{BCEA}$ is valuable for statisticians and practitioners working in the field of health economic modelling wanting to simplify and standardise their workflow, for example in the preparation of dossiers in support of marketing authorisation, or academic and scientific publications.

</details>

<details>

<summary>2022-03-18 15:04:34 - Element-wise Estimation Error of Generalized Fused Lasso</summary>

- *Teng Zhang, Sabyasachi Chatterjee*

- `2203.04369v2` - [abs](http://arxiv.org/abs/2203.04369v2) - [pdf](http://arxiv.org/pdf/2203.04369v2)

> The main result of this article is that we obtain an elementwise error bound for the Fused Lasso estimator for any general convex loss function $\rho$. We then focus on the special cases when either $\rho$ is the square loss function (for mean regression) or is the quantile loss function (for quantile regression) for which we derive new pointwise error bounds. Even though error bounds for the usual Fused Lasso estimator and its quantile version have been studied before; our bound appears to be new. This is because all previous works bound a global loss function like the sum of squared error, or a sum of Huber losses in the case of quantile regression in Padilla and Chatterjee (2021). Clearly, element wise bounds are stronger than global loss error bounds as it reveals how the loss behaves locally at each point. Our element wise error bound also has a clean and explicit dependence on the tuning parameter $\lambda$ which informs the user of a good choice of $\lambda$. In addition, our bound is nonasymptotic with explicit constants and is able to recover almost all the known results for Fused Lasso (both mean and quantile regression) with additional improvements in some cases.

</details>

<details>

<summary>2022-03-18 15:17:30 - Priors in Bayesian Deep Learning: A Review</summary>

- *Vincent Fortuin*

- `2105.06868v3` - [abs](http://arxiv.org/abs/2105.06868v3) - [pdf](http://arxiv.org/pdf/2105.06868v3)

> While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on vague priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders, and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.

</details>

<details>

<summary>2022-03-18 16:27:41 - Econometric Inference on Large Bayesian Games with Heterogeneous Beliefs</summary>

- *Denis Kojevnikov, Kyungchul Song*

- `1404.2015v3` - [abs](http://arxiv.org/abs/1404.2015v3) - [pdf](http://arxiv.org/pdf/1404.2015v3)

> Econometric models of strategic interactions among people or firms have received a great deal of attention in the literature. Less attention has been paid to the role of the underlying assumptions about the way agents form beliefs about other agents. We focus on a single large Bayesian game with idiosyncratic strategic neighborhoods and develop an approach of empirical modeling which relaxes the assumption of rational expectations and allows the players to form beliefs differently from each other. By drawing on the main intuition of Kalai (2004), we introduce the notion of a hindsight regret, which measures each player's ex post value of other players' type information, and obtain its belief-free bound. Using this bound, we derive testable implications and develop a bootstrap inference procedure for the structural parameters. Our inference method is uniformly valid regardless of the size of strategic neighborhoods and tends to exhibit high power when the neighborhoods are large. We demonstrate the finite sample performance of the method through Monte Carlo simulations.

</details>

<details>

<summary>2022-03-18 18:27:14 - Bayesian structural learning of microbiota systems from count metagenomic data</summary>

- *Veronica Vinciotti, Pariya Behrouzi, Reza Mohammadi*

- `2203.10118v1` - [abs](http://arxiv.org/abs/2203.10118v1) - [pdf](http://arxiv.org/pdf/2203.10118v1)

> Metagenomics combined with high-resolution sequencing techniques have enabled researchers to study the genomes of entire microbial communities. Unraveling interactions between these communities is of vital importance to understand how microbes influence human health and disease. However, learning these interactions from microbiome data is challenging, due to the high dimensionality, discreteness, broad dispersion levels, compositionality and excess of zero counts that characterize these data. In this paper, we develop a copula graphical model for structure learning in these settings. In particular, we advocate the use of discrete Weibull regression for linking the marginal distributions to external covariates, which are often available in genomic studies but rarely used for network inference, coupled with a Gaussian copula to model the joint distribution of the counts. An efficient Bayesian procedure for structural learning is implemented in the R package BDgraph and returns inference of the marginals and of the dependency structure, providing simultaneous differential analysis and graph uncertainty estimates. A simulation study and a real data analysis of microbiome data show the usefulness of the proposed approach at inferring networks from high-dimensional count data in general, and its relevance in the context of microbiota data analyses in particular.

</details>

<details>

<summary>2022-03-20 05:28:03 - Getting more from your regression model: A free lunch?</summary>

- *David P. Hofmeyr*

- `2203.10459v1` - [abs](http://arxiv.org/abs/2203.10459v1) - [pdf](http://arxiv.org/pdf/2203.10459v1)

> We consider a simple approach for approximating detailed information about the conditional distribution of a real-valued response variable, given values for its covariates, using only the outputs from a standard regression model. We validate this approach by assessing its performance in the context of quantile regression; when applied to the outputs of linear, gradient boosted tree ensemble and random forest models. We find that it compares favourably to the standard approach for estimating quantile regression functions, especially for commonly selected tail probabilities, and is highly competitive with the quantile regression forest model, across a large collection of benchmark data sets.

</details>

<details>

<summary>2022-03-20 09:46:58 - Rank-based Bayesian variable selection for genome-wide transcriptomic analyses</summary>

- *Emilie Eliseussen, Thomas Fleischer, Valeria Vitelli*

- `2107.05072v2` - [abs](http://arxiv.org/abs/2107.05072v2) - [pdf](http://arxiv.org/pdf/2107.05072v2)

> Variable selection is crucial in high-dimensional omics-based analyses, since it is biologically reasonable to assume only a subset of non-noisy features contributes to the data structures. However, the task is particularly hard in an unsupervised setting, and a priori ad hoc variable selection is still a very frequent approach, despite the evident drawbacks and lack of reproducibility. We propose a Bayesian variable selection approach for rank-based unsupervised transcriptomic analysis. Making use of data rankings instead of the actual continuous measurements increases the robustness of conclusions when compared to classical statistical methods, and embedding variable selection into the inferential tasks allows complete reproducibility. Specifically, we develop a novel extension of the Bayesian Mallows model for variable selection that allows for a full probabilistic analysis, leading to coherent quantification of uncertainties. Simulation studies demonstrate the versatility and robustness of the proposed method in a variety of scenarios, as well as its superiority with respect to several competitors when varying the data dimension or data generating process. We use the novel approach to analyse genome-wide RNAseq gene expression data from ovarian cancer patients: several genes that affect cancer development are correctly detected in a completely unsupervised fashion, showing the usefulness of the method in the context of signature discovery for cancer genomics. Moreover, the possibility to also perform uncertainty quantification plays a key role in the subsequent biological investigation.

</details>

<details>

<summary>2022-03-20 19:19:16 - Bayesian Sequential Optimal Experimental Design for Nonlinear Models Using Policy Gradient Reinforcement Learning</summary>

- *Wanggang Shen, Xun Huan*

- `2110.15335v2` - [abs](http://arxiv.org/abs/2110.15335v2) - [pdf](http://arxiv.org/pdf/2110.15335v2)

> We present a mathematical framework and computational methods to optimally design a finite number of sequential experiments. We formulate this sequential optimal experimental design (sOED) problem as a finite-horizon partially observable Markov decision process (POMDP) in a Bayesian setting and with information-theoretic utilities. It is built to accommodate continuous random variables, general non-Gaussian posteriors, and expensive nonlinear forward models. sOED then seeks an optimal design policy that incorporates elements of both feedback and lookahead, generalizing the suboptimal batch and greedy designs. We solve for the sOED policy numerically via policy gradient (PG) methods from reinforcement learning, and derive and prove the PG expression for sOED. Adopting an actor-critic approach, we parameterize the policy and value functions using deep neural networks and improve them using gradient estimates produced from simulated episodes of designs and observations. The overall PG-sOED method is validated on a linear-Gaussian benchmark, and its advantages over batch and greedy designs are demonstrated through a contaminant source inversion problem in a convection-diffusion field.

</details>

<details>

<summary>2022-03-21 02:27:03 - Predicting Cricket Outcomes using Bayesian Priors</summary>

- *Mohammed Quazi, Joshua Clifford, Pavan Datta*

- `2203.10706v1` - [abs](http://arxiv.org/abs/2203.10706v1) - [pdf](http://arxiv.org/pdf/2203.10706v1)

> This research has developed a statistical modeling procedure to predict outcomes of future cricket tournaments. Proposed model provides an insight into the application of stratified survey sampling to the team selection pattern by incorporating individual players' performance history coupled with Bayesian priors not only against a particular opposition but also against any cricket playing nation - full member of International Cricket Council (ICC). A case study for the next ICC cricket world cup 2023 in India is provided, predictions are obtained for all participating teams against one another, and simulation results are discussed. The proposed statistical model is tested on 2020 Indian Premier League (IPL) season. The model predicted the top three finishers of IPL 2020 correctly, including the winners of the tournament, Mumbai Indians, and other positions with reasonable accuracy. The method can predict probabilities of winning for each participating team. This method can be extended to other cricket tournaments as well.

</details>

<details>

<summary>2022-03-21 08:43:39 - Low-rank statistical finite elements for scalable model-data synthesis</summary>

- *Connor Duffin, Edward Cripps, Thomas Stemler, Mark Girolami*

- `2109.04757v2` - [abs](http://arxiv.org/abs/2109.04757v2) - [pdf](http://arxiv.org/pdf/2109.04757v2)

> Statistical learning additions to physically derived mathematical models are gaining traction in the literature. A recent approach has been to augment the underlying physics of the governing equations with data driven Bayesian statistical methodology. Coined statFEM, the method acknowledges a priori model misspecification, by embedding stochastic forcing within the governing equations. Upon receipt of additional data, the posterior distribution of the discretised finite element solution is updated using classical Bayesian filtering techniques. The resultant posterior jointly quantifies uncertainty associated with the ubiquitous problem of model misspecification and the data intended to represent the true process of interest. Despite this appeal, computational scalability is a challenge to statFEM's application to high-dimensional problems typically experienced in physical and industrial contexts. This article overcomes this hurdle by embedding a low-rank approximation of the underlying dense covariance matrix, obtained from the leading order modes of the full-rank alternative. Demonstrated on a series of reaction-diffusion problems of increasing dimension, using experimental and simulated data, the method reconstructs the sparsely observed data-generating processes with minimal loss of information, in both the posterior mean and variance, paving the way for further integration of physical and probabilistic approaches to complex systems.

</details>

<details>

<summary>2022-03-21 12:28:46 - Which Model to Trust: Assessing the Influence of Models on the Performance of Reinforcement Learning Algorithms for Continuous Control Tasks</summary>

- *Giacomo Arcieri, David Wlfle, Eleni Chatzi*

- `2110.13079v2` - [abs](http://arxiv.org/abs/2110.13079v2) - [pdf](http://arxiv.org/pdf/2110.13079v2)

> The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.

</details>

<details>

<summary>2022-03-21 13:43:41 - Sequential time-window learning with approximate Bayesian computation: an application to epidemic forecasting</summary>

- *Joo Pedro Valeriano, Pedro Henrique Cintra, Gustavo Libotte, Igor Reis, Felipe Fontinele, Renato Silva, Sandra Malta*

- `2203.10982v1` - [abs](http://arxiv.org/abs/2203.10982v1) - [pdf](http://arxiv.org/pdf/2203.10982v1)

> The long duration of the COVID-19 pandemic allowed for multiple bursts in the infection and death rates, the so-called epidemic waves. This complex behavior is no longer tractable by simple compartmental model and requires more sophisticated mathematical techniques for analyzing epidemic data and generating reliable forecasts. In this work, we propose a framework for analyzing complex dynamical systems by dividing the data in consecutive time-windows to be separately analyzed. We fit parameters for each time-window through an Approximate Bayesian Computation (ABC) algorithm, and the posterior distribution of parameters obtained for one window is used as the prior distribution for the next window. This Bayesian learning approach is tested with data on COVID-19 cases in multiple countries and is shown to improve ABC performance and to produce good short-term forecasting.

</details>

<details>

<summary>2022-03-21 14:42:19 - A Bayesian Nonparametric System Reliability Model which Integrates Multiple Sources of Lifetime Information</summary>

- *Richard L. Warr, Jeremy M. Meyer, Jackson T. Curtis*

- `1412.4260v2` - [abs](http://arxiv.org/abs/1412.4260v2) - [pdf](http://arxiv.org/pdf/1412.4260v2)

> We present a Bayesian nonparametric system reliability model which scales well and provides a great deal of flexibility in modeling. The Bayesian approach naturally handles the disparate amounts of component and subsystem data that may exist. However, traditional Bayesian reliability models are quite computationally complex, relying on MCMC techniques. Our approach utilizes the conjugate properties of the beta-Stacy process, which is the fundamental building block of our model. These individual models are linked together using a method of moments estimation approach. This model is computationally fast, allows for right-censored data, and is used for estimating and predicting system reliability.

</details>

<details>

<summary>2022-03-21 14:57:44 - Formal Definitions of Conservative Probability Distribution Functions (PDFs)</summary>

- *Shane Lubold, Clark N. Taylor*

- `1912.06780v3` - [abs](http://arxiv.org/abs/1912.06780v3) - [pdf](http://arxiv.org/pdf/1912.06780v3)

> Under ideal conditions, the probability density function (PDF) of a random variable, such as a sensor measurement, would be well known and amenable to computation and communication tasks. However, this is often not the case, so the user looks for some other PDF that approximates the true but intractable PDF. Conservativeness is a commonly sought property of this approximating PDF, especially in distributed or unstructured data systems where the data being fused may contain un-known correlations. Roughly, a conservative approximation is one that overestimates the uncertainty of a system. While prior work has introduced some definitions of conservativeness, these definitions either apply only to normal distributions or violate some of the intuitive appeal of (Gaussian) conservative definitions. This work provides a general and intuitive definition of conservativeness that is applicable to any probability distribution that is a measure over $\mathbb{R}^m$ or an infinite subset thereof, including multi-modal and uniform distributions. Unfortunately, we show that this strong definition of conservative cannot be used to evaluate data fusion techniques. Therefore, we also describe a weaker definition of conservative and show it is preserved through common data fusion methods such as the linear and log-linear opinion pool, and homogeneous functionals, assuming the input distributions can be factored into independent and common distributions. In addition, we show that after fusion, weak conservativeness is preserved by Bayesian updates. These strong and weak definitions of conservativeness can help design and evaluate potential correlation-agnostic data fusion techniques.

</details>

<details>

<summary>2022-03-21 21:12:54 - Regret and Belief Complexity Trade-off in Gaussian Process Bandits via Information Thresholding</summary>

- *Amrit Singh Bedi, Dheeraj Peddireddy, Vaneet Aggarwal, Brian M. Sadler, Alec Koppel*

- `2003.10550v3` - [abs](http://arxiv.org/abs/2003.10550v3) - [pdf](http://arxiv.org/pdf/2003.10550v3)

> Bayesian optimization is a framework for global search via maximum a posteriori updates rather than simulated annealing, and has gained prominence for decision-making under uncertainty. In this work, we cast Bayesian optimization as a multi-armed bandit problem, where the payoff function is sampled from a Gaussian process (GP). Further, we focus on action selections via upper confidence bound (UCB) or expected improvement (EI) due to their prevalent use in practice. Prior works using GPs for bandits cannot allow the iteration horizon $T$ to be large, as the complexity of computing the posterior parameters scales cubically with the number of past observations. To circumvent this computational burden, we propose a simple statistical test: only incorporate an action into the GP posterior when its conditional entropy exceeds an $\epsilon$ threshold. Doing so permits us to precisely characterize the trade-off between regret bounds of GP bandit algorithms and complexity of the posterior distributions depending on the compression parameter $\epsilon$ for both discrete and continuous action sets. To best of our knowledge, this is the first result which allows us to obtain sublinear regret bounds while still maintaining sublinear growth rate of the complexity of the posterior which is linear in the existing literature. Moreover, a provably finite bound on the complexity could be achieved but the algorithm would result in $\epsilon$-regret which means $\textbf{Reg}_T/T \rightarrow \mathcal{O}(\epsilon)$ as $T\rightarrow \infty$. Experimentally, we observe state of the art accuracy and complexity trade-offs for GP bandit algorithms applied to global optimization, suggesting the merits of compressed GPs in bandit settings.

</details>

<details>

<summary>2022-03-21 23:02:50 - Preference Exploration for Efficient Bayesian Optimization with Multiple Outcomes</summary>

- *Zhiyuan Jerry Lin, Raul Astudillo, Peter I. Frazier, Eytan Bakshy*

- `2203.11382v1` - [abs](http://arxiv.org/abs/2203.11382v1) - [pdf](http://arxiv.org/pdf/2203.11382v1)

> We consider Bayesian optimization of expensive-to-evaluate experiments that generate vector-valued outcomes over which a decision-maker (DM) has preferences. These preferences are encoded by a utility function that is not known in closed form but can be estimated by asking the DM to express preferences over pairs of outcome vectors. To address this problem, we develop Bayesian optimization with preference exploration, a novel framework that alternates between interactive real-time preference learning with the DM via pairwise comparisons between outcomes, and Bayesian optimization with a learned compositional model of DM utility and outcomes. Within this framework, we propose preference exploration strategies specifically designed for this task, and demonstrate their performance via extensive simulation studies.

</details>

<details>

<summary>2022-03-22 03:28:40 - Bayesian outcome selection modelling</summary>

- *Khue-Dung Dang, Louise M. Ryan, Richard J. Cook, Tugba Akkaya-Hocagil, Sandra W. Jacobson, Joseph L. Jacobson*

- `2203.11439v1` - [abs](http://arxiv.org/abs/2203.11439v1) - [pdf](http://arxiv.org/pdf/2203.11439v1)

> Psychiatric and social epidemiology often involves assessing the effects of environmental exposure on outcomes that are difficult to measure directly. To address this problem, it is common to measure outcomes using a comprehensive battery of different tests thought to be related to a common, underlying construct of interest. In the application that motivates our work, for example, researchers wanted to assess the impact of in utero alcohol exposure on child cognition and neuropsychological development, which were evaluated using a range of different tests. Statistical analysis of the resulting multiple outcomes data can be challenging, not only because of the need to account for the correlation between outcomes measured on the same individual, but because it is often unclear, a priori, which outcomes are impacted by the exposure under study. While researchers will generally have some hypotheses about which outcomes are important, a framework is needed to help identify outcomes that are sensitive to the exposure and to quantify the associated treatment or exposure effects of interest. We propose such a framework using a modification of stochastic search variable selection (SSVS), a popular Bayesian variable selection model and use it to quantify an overall effect of the exposure on the affected outcomes. We investigate the performance of the method via simulation and illustrate its application to data from a study involving the effects of prenatal alcohol exposure on child cognition.

</details>

<details>

<summary>2022-03-22 07:45:19 - Spatially-Varying Bayesian Predictive Synthesis for Flexible and Interpretable Spatial Prediction</summary>

- *Danielle Cabel, Shonosuke Sugasawa, Masahiro Kato, Kosaku Takanashi, Kenichiro McAlinn*

- `2203.05197v2` - [abs](http://arxiv.org/abs/2203.05197v2) - [pdf](http://arxiv.org/pdf/2203.05197v2)

> Spatial data are characterized by their spatial dependence, which is often complex, non-linear, and difficult to capture with a single model. Significant levels of model uncertainty -- arising from these characteristics -- cannot be resolved by model selection or simple ensemble methods, as performances are not homogeneous. We address this issue by proposing a novel methodology that captures spatially-varying model uncertainty, which we call spatial Bayesian predictive synthesis. Our proposal is defined by specifying a latent factor spatially-varying coefficient model as the synthesis function, which enables model coefficients to vary over the region to achieve flexible spatial model ensembling. Two MCMC strategies are implemented for full uncertainty quantification, as well as a variational inference strategy for fast point inference. We also extend the estimations strategy for general responses. A finite sample theoretical guarantee is given for the predictive performance of our methodology, showing that the predictions are exact minimax. Through simulation examples and two real data applications, we demonstrate that our proposed spatial Bayesian predictive synthesis outperforms standard spatial models and advanced machine learning methods, in terms of predictive accuracy, while maintaining interpretability of the prediction mechanism.

</details>

<details>

<summary>2022-03-22 10:17:27 - Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming</summary>

- *Gabriel Riutort-Mayol, Paul-Christian Brkner, Michael R. Andersen, Arno Solin, Aki Vehtari*

- `2004.11408v2` - [abs](http://arxiv.org/abs/2004.11408v2) - [pdf](http://arxiv.org/pdf/2004.11408v2)

> Gaussian processes are powerful non-parametric probabilistic models for stochastic functions. However, the direct implementation entails a complexity that is computationally intractable when the number of observations is large, especially when estimated with fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we focus on a low-rank approximate Bayesian Gaussian processes, based on a basis function approximation via Laplace eigenfunctions for stationary covariance functions. The main contribution of this paper is a detailed analysis of the performance, and practical recommendations for how to select the number of basis functions and the boundary factor. Intuitive visualizations and recommendations, make it easier for users to improve approximation accuracy and computational performance. We also propose diagnostics for checking that the number of basis functions and the boundary factor are adequate given the data. The approach is simple and exhibits an attractive computational complexity due to its linear structure, and it is easy to implement in probabilistic programming frameworks. Several illustrative examples of the performance and applicability of the method in the probabilistic programming language Stan are presented together with the underlying Stan model code.

</details>

<details>

<summary>2022-03-22 15:04:35 - Bayesian Nonparametric Adjustment of Confounding</summary>

- *Chanmin Kim, Mauricio Tec, Corwin M Zigler*

- `2203.11798v1` - [abs](http://arxiv.org/abs/2203.11798v1) - [pdf](http://arxiv.org/pdf/2203.11798v1)

> Analysis of observational studies increasingly confronts the challenge of determining which of a possibly high-dimensional set of available covariates are required to satisfy the assumption of ignorable treatment assignment for estimation of causal effects. We propose a Bayesian nonparametric approach that simultaneously 1) prioritizes inclusion of adjustment variables in accordance with existing principles of confounder selection; 2) estimates causal effects in a manner that permits complex relationships among confounders, exposures, and outcomes; and 3) provides causal estimates that account for uncertainty in the nature of confounding. The proposal relies on specification of multiple Bayesian Additive Regression Trees models, linked together with a common prior distribution that accrues posterior selection probability to covariates on the basis of association with both the exposure and the outcome of interest. A set of extensive simulation studies demonstrates that the proposed method performs well relative to similarly-motivated methodologies in a variety of scenarios. We deploy the method to investigate the causal effect of emissions from coal-fired power plants on ambient air pollution concentrations, where the prospect of confounding due to local and regional meteorological factors introduces uncertainty around the confounding role of a high-dimensional set of measured variables. Ultimately, we show that the proposed method produces more efficient and more consistent results across adjacent years than alternative methods, lending strength to the evidence of the causal relationship between SO2 emissions and ambient particulate pollution.

</details>

<details>

<summary>2022-03-22 16:50:40 - Weibull Racing Survival Analysis with Competing Events, Left Truncation, and Time-varying Covariates</summary>

- *Quan Zhang, Yanxun Xu, Mei-Cheng Wang, Mingyuan Zhou*

- `1911.01827v3` - [abs](http://arxiv.org/abs/1911.01827v3) - [pdf](http://arxiv.org/pdf/1911.01827v3)

> We propose Bayesian nonparametric Weibull delegate racing (WDR) for survival analysis with competing events and achieve both model interpretability and flexibility. Utilizing a natural mechanism of surviving competing events, we assume a race among a potentially infinite number of sub-events. In doing this, WDR accommodates nonlinear covariate effects with no need of data transformation. Moreover, WDR is able to handle left truncation, time-varying covariates, different types of censoring, and missing event times or types. We develop an efficient MCMC algorithm based on Gibbs sampling for Bayesian inference and provide an \texttt{R} package. Synthetic data analysis and comparison with benchmark approaches demonstrate WDR's outstanding performance and parsimonious nonlinear modeling capacity. In addition, we analyze two real data sets and showcase advantages of WDR. Specifically, we study time to death of three types of lymphoma and show the potential of WDR in modeling nonlinear covariate effects and discovering new diseases. We also use WDR to investigate the age at onset of mild cognitive impairment and interpret the accelerating or decelerating effects of biomarkers on the progression of Alzheimer's disease.

</details>

<details>

<summary>2022-03-22 19:23:27 - Sequential Bayesian Registration for Functional Data</summary>

- *Yoonji Kim, Oksana A. Chkrebtii, Sebastian A. Kurtek*

- `2203.12005v1` - [abs](http://arxiv.org/abs/2203.12005v1) - [pdf](http://arxiv.org/pdf/2203.12005v1)

> In many modern applications, discretely-observed data may be naturally understood as a set of functions. Functional data often exhibit two confounded sources of variability: amplitude (y-axis) and phase (x-axis). The extraction of amplitude and phase, a process known as registration, is essential in exploring the underlying structure of functional data in a variety of areas, from environmental monitoring to medical imaging. Critically, such data are often gathered sequentially with new functional observations arriving over time. Despite this, most available registration procedures are only applicable to batch learning, leading to inefficient computation. To address these challenges, we introduce a Bayesian framework for sequential registration of functional data, which updates statistical inference as new sets of functions are assimilated. This Bayesian model-based sequential learning approach utilizes sequential Monte Carlo sampling to recursively update the alignment of observed functions while accounting for associated uncertainty. As a result, distributed computing, which is not generally an option in batch learning, significantly reduces computational cost. Simulation studies and comparisons to existing batch learning methods reveal that the proposed approach performs well even when the target posterior distribution has a challenging structure. We apply the proposed method to three real datasets: (1) functions of annual drought intensity near Kaweah River in California, (2) annual sea surface salinity functions near Null Island, and (3) PQRST complexes segmented from an electrocardiogram signal.

</details>

<details>

<summary>2022-03-23 00:44:50 - The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms</summary>

- *Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi*

- `2002.10121v3` - [abs](http://arxiv.org/abs/2002.10121v3) - [pdf](http://arxiv.org/pdf/2002.10121v3)

> We study a Bayesian $k$-armed bandit problem in many-armed regime, when $k \geq \sqrt{T}$, with $T$ the time horizon. We first show that subsampling is critical for designing optimal policies. Specifically, the standard UCB algorithm is sub-optimal while a subsampled UCB (SS-UCB), which samples $\Theta(\sqrt{T})$ arms and executes UCB on that subset, is rate-optimal. Despite theoretically optimal regret, SS-UCB numerically performs worse than a greedy algorithm that pulls the current empirically best arm each time. These empirical insights hold in a contextual setting as well, using simulations on real data. These results suggest a new form of free exploration in the many-armed regime that benefits greedy algorithms. We theoretically show that this source of free exploration is deeply connected to the distribution of a tail event for the prior distribution of arm rewards. This is a fundamentally distinct phenomenon from free exploration due to variation in covariates, as discussed in the recent literature on contextual bandits. Building on this result, we prove that the subsampled greedy algorithm is rate-optimal for Bernoulli bandits in many armed regime, and achieves sublinear regret with more general distributions. Taken together, our results suggest that practitioners may benefit from using greedy algorithms in the many-armed regime.

</details>

<details>

<summary>2022-03-23 05:06:29 - A Computationally Efficient Approach to Fully Bayesian Benchmarking</summary>

- *Taylor Okonek, Jon Wakefield*

- `2203.12195v1` - [abs](http://arxiv.org/abs/2203.12195v1) - [pdf](http://arxiv.org/pdf/2203.12195v1)

> In small area estimation, it is often necessary to resort to model-based methods in order to produce estimates in areas with little or no data. In many settings, we require that some aggregate of small area estimates agree with a national level estimate that may be considered more reliable, for internal consistency purposes. The process of enforcing this agreement is referred to as benchmarking, and while methods currently exist to perform benchmarking in many settings, few are ideal for applications with non-normal outcomes and many are computationally inefficient. Fully Bayesian benchmarking is a theoretically appealing approach insofar as we can obtain posterior distributions conditional on a benchmarking constraint. However, existing implementations are often computationally prohibitive. In this paper, we summarize existing benchmarking methods and their shortcomings in the setting of small area estimation with binary outcomes, and propose an approach in which an unbenchmarked method that produces samples can be combined with a rejection sampler to produce fully Bayesian benchmarked estimates in a computationally efficient way. To illustrate our approach, we provide comparisons of various benchmarking methods in applications to HIV prevalence and under-5 mortality estimation. Code implementing our methodology is available in the R package simultBench.

</details>

<details>

<summary>2022-03-23 08:59:25 - Bayesian Nonparametric Vector Autoregressive Models via a Logit Stick-breaking Prior: an Application to Child Obesity</summary>

- *Mario Beraha, Alessandra Guglielmi, Fernando A. Quintana, Maria de Iorio, Johan Gunnar Eriksson, Fabian Yap*

- `2203.12280v1` - [abs](http://arxiv.org/abs/2203.12280v1) - [pdf](http://arxiv.org/pdf/2203.12280v1)

> Overweight and obesity in adults are known to be associated with risks of metabolic and cardiovascular diseases. Because obesity is an epidemic, increasingly affecting children, it is important to understand if this condition persists from early life to childhood and if different patterns of obesity growth can be detected. Our motivation starts from a study of obesity over time in children from South Eastern Asia. Our main focus is on clustering obesity patterns after adjusting for the effect of baseline information. Specifically, we consider a joint model for height and weight patterns taken every 6 months from birth. We propose a novel model that facilitates clustering by combining a vector autoregressive sampling model with a dependent logit stick-breaking prior. Simulation studies show the superiority of the model to capture patterns, compared to other alternatives. We apply the model to the motivating dataset, and discuss the main features of the detected clusters. We also compare alternative models with ours in terms of predictive performances.

</details>

<details>

<summary>2022-03-23 14:13:58 - Stateful to Stateless: Modelling Stateless Ethereum</summary>

- *Sandra Johnson, David Hyland-Wood, Anders L Madsen, Kerrie Mengersen*

- `2203.12435v1` - [abs](http://arxiv.org/abs/2203.12435v1) - [pdf](http://arxiv.org/pdf/2203.12435v1)

> The concept of 'Stateless Ethereum' was conceived with the primary aim of mitigating Ethereum's unbounded state growth. The key facilitator of Stateless Ethereum is through the introduction of 'witnesses' into the ecosystem. The changes and potential consequences that these additional data packets pose on the network need to be identified and analysed to ensure that the Ethereum ecosystem can continue operating securely and efficiently. In this paper we propose a Bayesian Network model, a probabilistic graphical modelling approach, to capture the key factors and their interactions in Ethereum mainnet, the public Ethereum blockchain, focussing on the changes being introduced by Stateless Ethereum to estimate the health of the resulting Ethereum ecosystem. We use a mixture of empirical data and expert knowledge, where data are unavailable, to quantify the model. Based on the data and expert knowledge available to use at the time of modelling, the Ethereum ecosystem is expected to remain healthy following the introduction of Stateless Ethereum.

</details>

<details>

<summary>2022-03-23 15:10:18 - Separating the Wheat from the Chaff: Bayesian Regularization in Dynamic Social Networks</summary>

- *Diana Karimova, Joris Mulder, Roger Th. A. J. Leenders*

- `2203.12474v1` - [abs](http://arxiv.org/abs/2203.12474v1) - [pdf](http://arxiv.org/pdf/2203.12474v1)

> In recent years there has been an increasing interest in the use of relational event models for dynamic social network analysis. The basis of these models is the concept of an "event", defined as a triplet of time, sender, and receiver of some social interaction. The key question that relational event models aim to answer is what drives social interactions among actors. Researchers often consider a very large number of predictors in their studies (including exogenous variables, endogenous network effects, and various interaction effects). The problem is however that employing an excessive number of effects may lead to model overfitting and inflated Type-I error rates. Consequently, the fitted model can easily become overly complex and the implied social interaction behavior becomes difficult to interpret. A potential solution to this problem is to apply Bayesian regularization using shrinkage priors. In this paper, we propose Bayesian regularization methods for relational event models using four different priors: a flat prior model with no shrinkage effect, a ridge estimator with a normal prior, a Bayesian lasso with a Laplace prior, and a horseshoe estimator with a numerically constructed prior that has an asymptote at zero. We develop and use these models for both an actor-oriented relational event model and a dyad-oriented relational event model. We show how to apply Bayesian regularization methods for these models and provide insights about which method works best and guidelines how to apply them in practice. Our results show that shrinkage priors can reduce Type-I errors while keeping reasonably high predictive performance and yielding parsimonious models to explain social network behavior.

</details>

<details>

<summary>2022-03-23 15:47:23 - On predictive inference for intractable models via approximate Bayesian computation</summary>

- *Marko Jrvenp, Jukka Corander*

- `2203.12495v1` - [abs](http://arxiv.org/abs/2203.12495v1) - [pdf](http://arxiv.org/pdf/2203.12495v1)

> Approximate Bayesian computation (ABC) is commonly used for parameter estimation and model comparison for intractable simulator-based models whose likelihood function cannot be evaluated. In this paper we instead investigate the feasibility of ABC as a generic approximate method for predictive inference, in particular, for computing the posterior predictive distribution of future observations or missing data of interest. We consider three complementary ABC approaches for this goal, each based on different assumptions regarding which predictive density of the intractable model can be sampled from. The case where only simulation from the joint density of the observed and future data given the model parameters can be used for inference is given particular attention and it is shown that the ideal summary statistic in this setting is minimal predictive sufficient instead of merely minimal sufficient (in the ordinary sense). An ABC prediction approach that takes advantage of a certain latent variable representation is also investigated. We additionally show how common ABC sampling algorithms can be used in the predictive settings considered. Our main results are first illustrated by using simple time-series models that facilitate analytical treatment, and later by using two common intractable dynamic models.

</details>

<details>

<summary>2022-03-23 16:44:23 - Posterior Concentration Rates for Bayesian Penalized Splines</summary>

- *Paul Bach, Nadja Klein*

- `2109.04288v3` - [abs](http://arxiv.org/abs/2109.04288v3) - [pdf](http://arxiv.org/pdf/2109.04288v3)

> Despite their widespread use in practice, the asymptotic properties of Bayesian penalized splines have not been investigated so far. We close this gap and study posterior concentration rates for Bayesian penalized splines in a Gaussian nonparametric regression model. A key feature of the approach is the hyperprior on the smoothing variance, which allows for adaptive smoothing in practice but complicates the theoretical analysis considerably as it destroys conjugacy and precludes analytic expressions for the posterior moments. To derive our theoretical results, we rely on several new concepts including a carefully defined proper version of the partially improper penalized splines prior as well as an innovative spline estimator that projects the observations onto the first basis functions of a Demmler-Reinsch basis. Our results show that posterior concentration at near optimal rate can be achieved if the hyperprior on the smoothing variance strikes a fine balance between oversmoothing and undersmoothing, which can for instance be met by a Weibull hyperprior with shape parameter 1/2. We complement our theoretical results with empirical evidence demonstrating the adaptivity of the hyperprior in practice.

</details>

<details>

<summary>2022-03-23 21:58:45 - Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders</summary>

- *Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, Andrew Gordon Wilson*

- `2203.12742v1` - [abs](http://arxiv.org/abs/2203.12742v1) - [pdf](http://arxiv.org/pdf/2203.12742v1)

> Bayesian optimization is a gold standard for query-efficient continuous optimization. However, its adoption for drug and antibody sequence design has been hindered by the discrete, high-dimensional nature of the decision variables. We develop a new approach (LaMBO) which jointly trains a denoising autoencoder with a discriminative multi-task Gaussian process head, enabling gradient-based optimization of multi-objective acquisition functions in the latent space of the autoencoder. These acquisition functions allow LaMBO to balance the explore-exploit trade-off over multiple design rounds, and to balance objective tradeoffs by optimizing sequences at many different points on the Pareto frontier. We evaluate LaMBO on a small-molecule task based on the ZINC dataset and introduce a new large-molecule task targeting fluorescent proteins. In our experiments, LaMBO outperforms genetic optimizers and does not require a large pretraining corpus, demonstrating that Bayesian optimization is practical and effective for biological sequence design.

</details>

<details>

<summary>2022-03-23 23:59:03 - Kernel Robust Hypothesis Testing</summary>

- *Zhongchang Sun, Shaofeng Zou*

- `2203.12777v1` - [abs](http://arxiv.org/abs/2203.12777v1) - [pdf](http://arxiv.org/pdf/2203.12777v1)

> The problem of robust hypothesis testing is studied, where under the null and the alternative hypotheses, the data-generating distributions are assumed to be in some uncertainty sets, and the goal is to design a test that performs well under the worst-case distributions over the uncertainty sets. In this paper, uncertainty sets are constructed in a data-driven manner using kernel method, i.e., they are centered around empirical distributions of training samples from the null and alternative hypotheses, respectively; and are constrained via the distance between kernel mean embeddings of distributions in the reproducing kernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesian setting and the Neyman-Pearson setting are investigated. For the Bayesian setting where the goal is to minimize the worst-case error probability, an optimal test is firstly obtained when the alphabet is finite. When the alphabet is infinite, a tractable approximation is proposed to quantify the worst-case average error probability, and a kernel smoothing method is further applied to design test that generalizes to unseen samples. A direct robust kernel test is also proposed and proved to be exponentially consistent. For the Neyman-Pearson setting, where the goal is to minimize the worst-case probability of miss detection subject to a constraint on the worst-case probability of false alarm, an efficient robust kernel test is proposed and is shown to be asymptotically optimal. Numerical results are provided to demonstrate the performance of the proposed robust tests.

</details>

<details>

<summary>2022-03-24 10:03:01 - Knowledge Removal in Sampling-based Bayesian Inference</summary>

- *Shaopeng Fu, Fengxiang He, Dacheng Tao*

- `2203.12964v1` - [abs](http://arxiv.org/abs/2203.12964v1) - [pdf](http://arxiv.org/pdf/2203.12964v1)

> The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an {\it MCMC influence function} is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at \url{https://github.com/fshp971/mcmc-unlearning}.

</details>

<details>

<summary>2022-03-24 12:44:21 - pyABC: Efficient and robust easy-to-use approximate Bayesian computation</summary>

- *Yannik Schlte, Emmanuel Klinger, Emad Alamoudi, Jan Hasenauer*

- `2203.13043v1` - [abs](http://arxiv.org/abs/2203.13043v1) - [pdf](http://arxiv.org/pdf/2203.13043v1)

> The Python package pyABC provides a framework for approximate Bayesian computation (ABC), a likelihood-free parameter inference method popular in many research areas. At its core, it implements a sequential Monte-Carlo (SMC) scheme, with various algorithms to adapt to the problem structure and automatically tune hyperparameters. To scale to computationally expensive problems, it provides efficient parallelization strategies for multi-core and distributed systems. The package is highly modular and designed to be easily usable. In this major update to pyABC, we implement several advanced algorithms that facilitate efficient and robust inference on a wide range of data and model types. In particular, we implement algorithms to account for noise, to adaptively scale-normalize distance metrics, to robustly handle data outliers, to elucidate informative data points via regression models, to circumvent summary statistics via optimal transport based distances, and to avoid local optima in acceptance threshold sequences by predicting acceptance rate curves. Further, we provide, besides previously existing support of Python and R, interfaces in particular to the Julia language, the COPASI simulator, and the PEtab standard.

</details>

<details>

<summary>2022-03-24 16:34:24 - On a generalised form of subjective probability</summary>

- *Russell J. Bowater*

- `1810.10972v3` - [abs](http://arxiv.org/abs/1810.10972v3) - [pdf](http://arxiv.org/pdf/1810.10972v3)

> This paper is motivated by the questions of how to give the concept of probability an adequate real-world meaning, and how to explain a certain type of phenomenon that can be found, for instance, in Ellsberg's paradox. It attempts to answer these questions by constructing an alternative theory to one that was proposed in earlier papers on the basis of various important criticisms that were raised against this earlier theory. The conceptual principles of the corresponding definition of probability are laid out and explained in detail. In particular, what is required to fully specify a probability distribution under this definition is not just the distribution function of the variable concerned, but also an assessment of the internal and/or the external strength of this function relative to other distribution functions of interest. This way of defining probability is applied to various examples and problems including, perhaps most notably, to a long-running controversy concerning the distinction between Bayesian and fiducial inference. The characteristics of this definition of probability are carefully evaluated in terms of the issues that it sets out to address.

</details>

<details>

<summary>2022-03-24 23:58:38 - A Tutorial on Bayesian Data Assimilation</summary>

- *Colin Grudzien, Marc Bocquet*

- `2112.07704v3` - [abs](http://arxiv.org/abs/2112.07704v3) - [pdf](http://arxiv.org/pdf/2112.07704v3)

> This tutorial provides a broad introduction to Bayesian data assimilation that will be useful to practitioners, in interpreting algorithms and results, and for theoretical studies developing novel schemes with an understanding of the rich history of geophysical data assimilation and its current directions. The simple case of data assimilation in a 'perfect' model is primarily discussed for pedagogical purposes. Some mathematical results are derived at a high-level in order to illustrate key ideas about different estimators. However, the focus of this work is on the intuition behind these methods, where more formal and detailed treatments of the data assimilation problem can be found in the various references. In surveying a variety of widely used data assimilation schemes, the key message of this tutorial is how the Bayesian analysis provides a consistent framework for the estimation problem and how this allows one to formulate its solution in a variety of ways to exploit the operational challenges in the geosciences.

</details>

<details>

<summary>2022-03-25 11:54:38 - Invariant measures of disagreement with stochastic dominance</summary>

- *E. del Barrio, J. A. Cuesta-Albertos, C. Matran*

- `1804.02905v3` - [abs](http://arxiv.org/abs/1804.02905v3) - [pdf](http://arxiv.org/pdf/1804.02905v3)

> An essential feature of stochastic order is its invariance against increasing maps. In this paper, we analyze a family of invariant indices of disagreement with respect to stochastic dominance. The indices in this family admit the representation $\theta(F,G)=P(X>Y)$, where $(X,Y)$ is a random vector with marginal distribution functions $F$ and $G$. This includes the case of independent marginals, but also other interesting indices related to a contamination model or to a joint quantile representation. For some choices of $\theta$ the condition $\theta(F,G)=0$ is equivalent to stochastic dominance of $G$ over $F$. We show that the index associated to the contamination model achieves the minimal value within this family. The plug-in sample-based versions of these indices lead to the Mann-Whitney, the one-sided Kolmogorov-Smirnov, and the Galton statistics. For some of the most interesting indices this fact provides sufficient theoretical support for asymptotic inference. However, this is not the case for Galton's statistic, for which we provide additional theory for its resampling behaviour. We stress on the complementary roles of some of these indices, which beyond measuring disagreement with respect to stochastic order allow to describe the maximum possible difference in status of a value $x\in \mathbb{R}$ under $F$ or $G$. We apply these indices to some real data sets.

</details>

<details>

<summary>2022-03-25 13:11:26 - Dealing with collinearity in large-scale linear system identification using Bayesian regularization</summary>

- *Wenqi Cao, Gianluigi Pillonetto*

- `2203.13633v1` - [abs](http://arxiv.org/abs/2203.13633v1) - [pdf](http://arxiv.org/pdf/2203.13633v1)

> We consider the identification of large-scale linear and stable dynamic systems whose outputs may be the result of many correlated inputs. Hence, severe ill-conditioning may affect the estimation problem. This is a scenario often arising when modeling complex physical systems given by the interconnection of many sub-units where feedback and algebraic loops can be encountered. We develop a strategy based on Bayesian regularization where any impulse response is modeled as the realization of a zero-mean Gaussian process. The stable spline covariance is used to include information on smooth exponential decay of the impulse responses. We then design a new Markov chain Monte Carlo scheme that deals with collinearity and is able to efficiently reconstruct the posterior of the impulse responses. It is based on a variation of Gibbs sampling which updates possibly overlapping blocks of the parameter space on the basis of the level of collinearity affecting the different inputs. Numerical experiments are included to test the goodness of the approach where hundreds of impulse responses form the system and inputs correlation may be very high.

</details>

<details>

<summary>2022-03-25 13:54:37 - A Point Mass Proposal Method for Bayesian State-Space Model Fitting</summary>

- *Mary Llewellyn, Ruth King, Vctor Elvira, Gordon Ross*

- `2203.13649v1` - [abs](http://arxiv.org/abs/2203.13649v1) - [pdf](http://arxiv.org/pdf/2203.13649v1)

> State-space models (SSMs) are often used to model time series data where the observations depend on an unobserved latent process. However, inference on the process parameters of an SSM can be challenging, especially when the likelihood of the data given the parameters is not available in closed form. We focus on the problem of model fitting within a Bayesian framework, to which a variety of approaches have been applied, including MCMC using Bayesian data augmentation, sequential Monte Carlo (SMC) approximation, and particle MCMC algorithms, which combine SMC approximations and MCMC steps. However, these different methods can be inefficient because of sample impoverishment in the sequential Monte Carlo approximations and/or poor mixing in the MCMC steps. In this article, we propose an approach that borrows ideas from discrete hidden Markov models (HMMs) to provide an efficient MCMC with data augmentation approach, imputing the latent states within the algorithm. Our approach deterministically approximates the SSM by a discrete HMM, which is subsequently used as an MCMC proposal distribution for the latent states. We demonstrate that the algorithm provides an efficient alternative approach via two different case studies.

</details>

<details>

<summary>2022-03-25 14:50:08 - Conformal prediction beyond exchangeability</summary>

- *Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, Ryan J. Tibshirani*

- `2202.13415v2` - [abs](http://arxiv.org/abs/2202.13415v2) - [pdf](http://arxiv.org/pdf/2202.13415v2)

> Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use an algorithm that treats recent observations as more relevant, which would violate the assumption that data points are treated symmetrically. This paper proposes new methodology to deal with both aspects: we use weighted quantiles to introduce robustness against distribution drift, and design a new technique to allow for algorithms that do not treat data points symmetrically. Our algorithms are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. Finally, we demonstrate the practical utility of these new tools with simulations and real-data experiments.

</details>

<details>

<summary>2022-03-25 16:56:53 - Bayesian Uncertainty Quantification for Low-Rank Matrix Completion</summary>

- *Henry Shaowu Yuchi, Simon Mak, Yao Xie*

- `2101.01299v4` - [abs](http://arxiv.org/abs/2101.01299v4) - [pdf](http://arxiv.org/pdf/2101.01299v4)

> We consider the problem of uncertainty quantification for an unknown low-rank matrix $\mathbf{X}$, given a partial and noisy observation of its entries. This quantification of uncertainty is essential for many real-world problems, including image processing, satellite imaging, and seismology, providing a principled framework for validating scientific conclusions and guiding decision-making. However, existing literature has mainly focused on the completion (i.e., point estimation) of the matrix $\mathbf{X}$, with little work on investigating its uncertainty. To this end, we propose in this work a new Bayesian modeling framework, called BayeSMG, which parametrizes the unknown $\mathbf{X}$ via its underlying row and column subspaces. This Bayesian subspace parametrization enables efficient posterior inference on matrix subspaces, which represents interpretable phenomena in many applications. This can then be leveraged for improved matrix recovery. We demonstrate the effectiveness of BayeSMG over existing Bayesian matrix recovery methods in numerical experiments, image inpainting, and a seismic sensor network application.

</details>

<details>

<summary>2022-03-25 17:30:24 - Sequential matched randomization and a case for covariate-adaptive randomization</summary>

- *Jonathan J. Chipman, Lindsay Mayberry, Robert A. Greevy Jr.*

- `2203.13797v1` - [abs](http://arxiv.org/abs/2203.13797v1) - [pdf](http://arxiv.org/pdf/2203.13797v1)

> Background: Sequential Matched Randomization (SMR) is one of multiple recent covariate-adaptive randomization (CAR) procedures that utilize a distance matrix to improve covariate-balance and estimation efficiency. Randomization occurs within mates whose distance meet an a-priori, fixed similarity quantile of random distances. Methods: We extend SMR to allow multiple participants to be randomized simultaneously, to allow matches to break and rematch if a better match later enrolls (Sequential Rematched Randomization; SRR), and to use a dynamic threshold. In simplified settings which vary covariate distribution and association upon outcome, we compare end-study covariate-balance and estimator efficiency in SMR before and after extensions. In a real-world application, we compare covariate-balance, power, and estimator efficiency of SMR before and after extensions when adjusting for priority covariates and all covariates of interest. We compare with Complete Randomization (CR) and CR followed by a flexible, covariate-adjusted regression model. As side-by-side comparisons, we include stratified randomization, D$_A$ optimality biased coin design (D$_A$-BCD), and Pairwise Sequential Randomization (PSR). Results: In both the simplified and real-world application, we observe benefits of each extension upon covariate balance and estimator efficiency. In the real-world application, SRR with a dynamic threshold, D$_A$-BCD, and PSR provide greater power than CR followed by a covariate-adjusted regression model. Matching methods achieved greater covariate-balance when adjusting for all covariates yet greater power and efficiency when adjusting for priority covariates. Conclusion: We improve upon SMR and show the potential for CAR methods -- that adjusting for covariates in randomization can outperform covariate adjustment in a flexible regression model.

</details>

<details>

<summary>2022-03-26 15:21:20 - Model Selection for Maternal Hypertensive Disorders with Symmetric Hierarchical Dirichlet Processes</summary>

- *Beatrice Franzolini, Antonio Lijoi, Igor Prnster*

- `2203.15782v1` - [abs](http://arxiv.org/abs/2203.15782v1) - [pdf](http://arxiv.org/pdf/2203.15782v1)

> Hypertensive disorders of pregnancy occur in about 10% of pregnant women around the world. Though there is evidence that hypertension impacts maternal cardiac functions, the relation between hypertension and cardiac dysfunctions is only partially understood. The study of this relationship can be framed as a joint inferential problem on multiple populations, each corresponding to a different hypertensive disorder diagnosis, that combines multivariate information provided by a collection of cardiac function indexes. A Bayesian nonparametric approach seems particularly suited for this setup and we demonstrate it on a dataset consisting of transthoracic echocardiography results of a cohort of Indian pregnant women. We are able to perform model selection, provide density estimates of cardiac function indexes and a latent clustering of patients: these readily interpretable inferential outputs allow to single out modified cardiac functions in hypertensive patients compared to healthy subjects and progressively increased alterations with the severity of the disorder. The analysis is based on a Bayesian nonparametric model that relies on a novel hierarchical structure, called symmetric hierarchical Dirichlet process. This is suitably designed so that the mean parameters are identified and used for model selection across populations, a penalization for multiplicity is enforced, and the presence of unobserved relevant factors is investigated through a latent clustering of subjects. Posterior inference relies on a suitable Markov Chain Monte Carlo algorithm and the model behaviour is also showcased on simulated data.

</details>

<details>

<summary>2022-03-26 15:42:18 - Influential Observations in Bayesian Regression Tree Models</summary>

- *Matthew T. Pratola, Edward I. George, Robert E. McCulloch*

- `2203.14102v1` - [abs](http://arxiv.org/abs/2203.14102v1) - [pdf](http://arxiv.org/pdf/2203.14102v1)

> BCART (Bayesian Classification and Regression Trees) and BART (Bayesian Additive Regression Trees) are popular Bayesian regression models widely applicable in modern regression problems. Their popularity is intimately tied to the ability to flexibly model complex responses depending on high-dimensional inputs while simultaneously being able to quantify uncertainties. This ability to quantify uncertainties is key, as it allows researchers to perform appropriate inferential analyses in settings that have generally been too difficult to handle using the Bayesian approach. However, surprisingly little work has been done to evaluate the sensitivity of these modern regression models to violations of modeling assumptions. In particular, we will consider influential observations, which one reasonably would imagine to be common -- or at least a concern -- in the big-data setting. In this paper, we consider both the problem of detecting influential observations and adjusting predictions to not be unduly affected by such potentially problematic data. We consider two detection diagnostics for Bayesian tree models, one an analogue of Cook's distance and the other taking the form of a divergence measure, and then propose an importance sampling algorithm to re-weight previously sampled posterior draws so as to remove the effects of influential data in a computationally efficient manner. Finally, our methods are demonstrated on real-world data where blind application of the models can lead to poor predictions and inference.

</details>

<details>

<summary>2022-03-26 20:51:07 - Parameterizing and Simulating from Causal Models</summary>

- *Robin J. Evans, Vanessa Didelez*

- `2109.03694v2` - [abs](http://arxiv.org/abs/2109.03694v2) - [pdf](http://arxiv.org/pdf/2109.03694v2)

> Many statistical problems in causal inference involve a probability distribution other than the one from which data are actually observed; as an additional complication, the object of interest is often a marginal quantity of this other probability distribution. This creates many practical complications for statistical inference, even where the problem is non-parametrically identified. In particular, it is difficult to perform likelihood-based inference, or even to simulate from the model in a general way.   We introduce the `frugal parameterization', which places the causal effect of interest at its centre, and then build the rest of the model around it. We do this in a way that provides a recipe for constructing a regular, non-redundant parameterization using causal quantities of interest. In the case of discrete variables we can use odds ratios to complete the parameterization, while in the continuous case copulas are the natural choice; other possibilities are also discussed.   Our methods allow us to construct and simulate from models with parametrically specified causal distributions, and fit them using likelihood-based methods, including fully Bayesian approaches. Our proposal includes parameterizations for the average causal effect and effect of treatment on the treated, as well as other causal quantities of interest.

</details>

<details>

<summary>2022-03-26 21:39:08 - Robust Bayesian Inference for Big Data: Combining Sensor-based Records with Traditional Survey Data</summary>

- *Ali Rafei, Carol A. C. Flannagan, Brady T. West, Michael R. Elliott*

- `2101.07456v2` - [abs](http://arxiv.org/abs/2101.07456v2) - [pdf](http://arxiv.org/pdf/2101.07456v2)

> Big Data often presents as massive non-probability samples. Not only is the selection mechanism often unknown, but larger data volume amplifies the relative contribution of selection bias to total error. Existing bias adjustment approaches assume that the conditional mean structures have been correctly specified for the selection indicator or key substantive measures. In the presence of a reference probability sample, these methods rely on a pseudo-likelihood method to account for the sampling weights of the reference sample, which is parametric in nature. Under a Bayesian framework, handling the sampling weights is an even bigger hurdle. To further protect against model misspecification, we expand the idea of double robustness such that more flexible non-parametric methods, as well as Bayesian models, can be used for prediction. In particular, we employ Bayesian additive regression trees, which not only capture non-linear associations automatically but permit direct quantification of the uncertainty of point estimates through its posterior predictive draws. We apply our method to sensor-based naturalistic driving data from the second Strategic Highway Research Program using the 2017 National Household Travel Survey as a benchmark.

</details>

<details>

<summary>2022-03-27 11:18:56 - Marginalising over Stationary Kernels with Bayesian Quadrature</summary>

- *Saad Hamid, Sebastian Schulze, Michael A. Osborne, Stephen J. Roberts*

- `2106.07452v2` - [abs](http://arxiv.org/abs/2106.07452v2) - [pdf](http://arxiv.org/pdf/2106.07452v2)

> Marginalising over families of Gaussian Process kernels produces flexible model classes with well-calibrated uncertainty estimates. Existing approaches require likelihood evaluations of many kernels, rendering them prohibitively expensive for larger datasets. We propose a Bayesian Quadrature scheme to make this marginalisation more efficient and thereby more practical. Through use of the maximum mean discrepancies between distributions, we define a kernel over kernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel samples are selected by generalising an information-theoretic acquisition function for warped Bayesian Quadrature. We show that our framework achieves more accurate predictions with better calibrated uncertainty than state-of-the-art baselines, especially when given limited (wall-clock) time budgets.

</details>

<details>

<summary>2022-03-27 13:48:07 - An Extended Simplified Laplace strategy for Approximate Bayesian inference of Latent Gaussian Models using R-INLA</summary>

- *Cristian Chiuchiolo, Janet van Niekerk, Hvard Rue*

- `2203.14304v1` - [abs](http://arxiv.org/abs/2203.14304v1) - [pdf](http://arxiv.org/pdf/2203.14304v1)

> Various computational challenges arise when applying Bayesian inference approaches to complex hierarchical models. Sampling-based inference methods, such as Markov Chain Monte Carlo strategies, are renowned for providing accurate results but with high computational costs and slow or questionable convergence. On the contrary, approximate methods like the Integrated Nested Laplace Approximation (INLA) construct a deterministic approximation to the univariate posteriors through nested Laplace Approximations. This method enables fast inference performance in Latent Gaussian Models, which encode a large class of hierarchical models. R-INLA software mainly consists of three strategies to compute all the required posterior approximations depending on the accuracy requirements. The Simplified Laplace approximation (SLA) is the most attractive because of its speed performance since it is based on a Taylor expansion up to order three of a full Laplace Approximation. Here we enhance the methodology by simplifying the computations necessary for the skewness and modal configuration. Then we propose an expansion up to order four and use the Extended Skew Normal distribution as a new parametric fit. The resulting approximations to the marginal posterior densities are more accurate than those calculated with the SLA, with essentially no additional cost.

</details>

<details>

<summary>2022-03-27 17:38:12 - Robust and Efficient Bayesian Inference for Non-Probability Samples</summary>

- *Ali Rafei, Michael R. Elliott, Carol A. C. Flannagan*

- `2203.14355v1` - [abs](http://arxiv.org/abs/2203.14355v1) - [pdf](http://arxiv.org/pdf/2203.14355v1)

> The declining response rates in probability surveys along with the widespread availability of unstructured data has led to growing research into non-probability samples. Existing robust approaches are not well-developed for non-Gaussian outcomes and may perform poorly in presence of influential pseudo-weights. Furthermore, their variance estimator lacks a unified framework and rely often on asymptotic theory. To address these gaps, we propose an alternative Bayesian approach using a partially linear Gaussian process regression that utilizes a prediction model with a flexible function of the pseudo-inclusion probabilities to impute the outcome variable for the reference survey. By efficiency, we mean not only computational scalability but also superiority with respect to variance. We also show that Gaussian process regression behaves as a kernel matching technique based on the estimated propensity scores, which yields double robustness and lowers sensitivity to influential pseudo-weights. Using the simulated posterior predictive distribution, one can directly quantify the uncertainty of the proposed estimator and derive associated $95\%$ credible intervals. We assess the repeated sampling properties of our method in two simulation studies. The application of this study deals with modeling count data with varying exposures under a non-probability sample setting.

</details>

<details>

<summary>2022-03-27 19:07:01 - Posterior Predictive Propensity Scores and $p$-Values</summary>

- *Peng Ding, Tianyu Guo*

- `2202.08368v2` - [abs](http://arxiv.org/abs/2202.08368v2) - [pdf](http://arxiv.org/pdf/2202.08368v2)

> \citet{Rosenbaum83ps} introduced the notion of the propensity score and discussed its central role in causal inference with observational studies. Their paper, however, caused a fundamental incoherence with an early paper by \citet{Rubin78}, which showed that the propensity score does not play any role in the Bayesian analysis of unconfounded observational studies if the priors on the propensity score and outcome models are independent. Despite the serious efforts made in the literature, it is generally difficult to reconcile these contradicting results. We offer a simple approach to incorporating the propensity score in Bayesian causal inference based on the posterior predictive $p$-value. To motivate a simple procedure, we focus on the model with the strong null hypothesis of no causal effects for any units whatsoever. Computationally, the proposed posterior predictive $p$-value equals the classic $p$-value based on the Fisher randomization test averaged over the posterior predictive distribution of the propensity score. Moreover, using the studentized doubly robust estimator as the test statistic, the proposed $p$-value inherits the doubly robust property and is also asymptotically valid for testing the weak null hypothesis of zero average causal effect. Perhaps surprisingly, this Bayesianly motivated $p$-value can have better frequentist's finite-sample performance than the frequentist's $p$-value based on the asymptotic approximation especially when the propensity scores can take extreme values.

</details>

<details>

<summary>2022-03-27 22:25:56 - A Differentially Private Bayesian Approach to Replication Analysis</summary>

- *Chengxin Yang, Jerome P. Reiter*

- `2111.13378v6` - [abs](http://arxiv.org/abs/2111.13378v6) - [pdf](http://arxiv.org/pdf/2111.13378v6)

> Replication analysis is widely used in many fields of study. Once a research is published, other researchers will conduct analysis to assess the reliability of the published research. However, what if the data are confidential? In particular, if the data sets used for the studies are confidential, we cannot release the results of replication analyses to any entity without the permission to access the data sets, otherwise it may result in privacy leakage especially when the published study and replication studies are using similar or common data sets. In this paper, we present two methods for replication analysis. We illustrate the properties of our methods by a combination of theoretical analysis and simulation.

</details>

<details>

<summary>2022-03-28 03:22:10 - Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making</summary>

- *Jie Zhou, Botao Hao, Zheng Wen, Jingfei Zhang, Will Wei Sun*

- `2007.15788v2` - [abs](http://arxiv.org/abs/2007.15788v2) - [pdf](http://arxiv.org/pdf/2007.15788v2)

> Multi-dimensional online decision making plays a crucial role in many real applications such as online recommendation and digital marketing. In these problems, a decision at each time is a combination of choices from different types of entities. To solve it, we introduce stochastic low-rank tensor bandits, a class of bandits whose mean rewards can be represented as a low-rank tensor. We consider two settings, tensor bandits without context and tensor bandits with context. In the first setting, the platform aims to find the optimal decision with the highest expected reward, a.k.a, the largest entry of true reward tensor. In the second setting, some modes of the tensor are contexts and the rest modes are decisions, and the goal is to find the optimal decision given the contextual information. We propose two learning algorithms tensor elimination and tensor epoch-greedy for tensor bandits without context, and derive finite-time regret bounds for them. Comparing with existing competitive methods, tensor elimination has the best overall regret bound and tensor epoch-greedy has a sharper dependency on dimensions of the reward tensor. Furthermore, we develop a practically effective Bayesian algorithm called tensor ensemble sampling for tensor bandits with context. Numerical experiments back up our theoretical findings and show that our algorithms outperform various state-of-the-art approaches that ignore the tensor low-rank structure. In an online advertising application with contextual information, our tensor ensemble sampling reduces the cumulative regret by 75% compared to the benchmark method.

</details>

<details>

<summary>2022-03-28 09:05:31 - General Bayesian Loss Function Selection and the use of Improper Models</summary>

- *Jack Jewson, David Rossell*

- `2106.01214v2` - [abs](http://arxiv.org/abs/2106.01214v2) - [pdf](http://arxiv.org/pdf/2106.01214v2)

> Statisticians often face the choice between using probability models or a paradigm defined by minimising a loss function. Both approaches are useful and, if the loss can be re-cast into a proper probability model, there are many tools to decide which model or loss is more appropriate for the observed data, in the sense of explaining the data's nature. However, when the loss leads to an improper model, there are no principled ways to guide this choice. We address this task by combining the Hyv\"arinen score, which naturally targets infinitesimal relative probabilities, and general Bayesian updating, which provides a unifying framework for inference on losses and models. Specifically we propose the H-score, a general Bayesian selection criterion and prove that it consistently selects the (possibly improper) model closest to the data-generating truth in Fisher's divergence. We also prove that an associated H-posterior consistently learns optimal hyper-parameters featuring in loss functions, including a challenging tempering parameter in generalised Bayesian inference. As salient examples, we consider robust regression and non-parametric density estimation where popular loss functions define improper models for the data and hence cannot be dealt with using standard model selection tools. These examples illustrate advantages in robustness-efficiency trade-offs and provide a Bayesian implementation for kernel density estimation, opening a new avenue for Bayesian non-parametrics.

</details>

<details>

<summary>2022-03-28 13:49:46 - Black Box Variational Bayesian Model Averaging</summary>

- *Vojtech Kejzlar, Shrijita Bhattacharya, Mookyong Son, Tapabrata Maiti*

- `2106.12652v2` - [abs](http://arxiv.org/abs/2106.12652v2) - [pdf](http://arxiv.org/pdf/2106.12652v2)

> For many decades now, Bayesian Model Averaging (BMA) has been a popular framework to systematically account for model uncertainty that arises in situations when multiple competing models are available to describe the same or similar physical process. The implementation of this framework, however, comes with a multitude of practical challenges including posterior approximation via Markov Chain Monte Carlo and numerical integration. We present a Variational Bayesian Inference approach to BMA as a viable alternative to the standard solutions which avoids many of the aforementioned pitfalls. The proposed method is "black box" in the sense that it can be readily applied to many models with little to no model-specific derivation. We illustrate the utility of our variational approach on a suite of examples and discuss all the necessary implementation details. Fully documented Python code with all the examples is provided as well.

</details>

<details>

<summary>2022-03-28 14:28:02 - Flexible and efficient Bayesian pharmacometrics modeling using Stan and Torsten, Part I</summary>

- *Charles C. Margossian, Yi Zhang, William R. Gillespie*

- `2109.10184v3` - [abs](http://arxiv.org/abs/2109.10184v3) - [pdf](http://arxiv.org/pdf/2109.10184v3)

> Stan is an open-source probabilistic programing language, primarily designed to do Bayesian data analysis. Its main inference algorithm is an adaptive Hamiltonian Monte Carlo sampler, supported by state of the art gradient computation. Stan's strengths include efficient computation, an expressive language which offers a great deal of flexibility, and numerous diagnostics that allow modelers to check whether the inference is reliable. Torsten extends Stan with a suite of functions that facilitate the specification of pharmacokinetic and pharmacodynamic models, and makes it straightforward to specify a clinical event schedule. Part I of this tutorial demonstrates how to build, fit, and criticize standard pharmacokinetic and pharmacodynamic models using Stan and Torsten.

</details>

<details>

<summary>2022-03-28 14:32:37 - Statistic Selection and MCMC for Differentially Private Bayesian Estimation</summary>

- *Baris Alparslan, Sinan Yildirim*

- `2203.13377v2` - [abs](http://arxiv.org/abs/2203.13377v2) - [pdf](http://arxiv.org/pdf/2203.13377v2)

> This paper concerns differentially private Bayesian estimation of the parameters of a population distribution, when a statistic of a sample from that population is shared in noise to provide differential privacy.   This work mainly addresses two problems: (1) What statistic of the sample should be shared privately? For the first question, i.e., the one about statistic selection, we promote using the Fisher information. We find out that, the statistic that is most informative in a non-privacy setting may not be the optimal choice under the privacy restrictions. We provide several examples to support that point. We consider several types of data sharing settings and propose several Monte Carlo-based numerical estimation methods for calculating the Fisher information for those settings. The second question concerns inference: (2) Based on the shared statistics, how could we perform effective Bayesian inference? We propose several Markov chain Monte Carlo (MCMC) algorithms for sampling from the posterior distribution of the parameter given the noisy statistic. The proposed MCMC algorithms can be preferred over one another depending on the problem. For example, when the shared statistics is additive and added Gaussian noise, a simple Metropolis-Hasting algorithm that utilizes the central limit theorem is a decent choice. We propose more advanced MCMC algorithms for several other cases of practical relevance.   Our numerical examples involve comparing several candidate statistics to be shared privately. For each statistic, we perform Bayesian estimation based on the posterior distribution conditional on the privatized version of that statistic. We demonstrate that, the relative performance of a statistic, in terms of the mean squared error of the Bayesian estimator based on the corresponding privatized statistic, is adequately predicted by the Fisher information of the privatized statistic.

</details>

<details>

<summary>2022-03-28 14:44:52 - Extracting Image Characteristics to Predict Crowdfunding Success</summary>

- *S. J. Blanchard, T. J. Noseworthy, E. Pancer, M. Poole*

- `2203.14806v1` - [abs](http://arxiv.org/abs/2203.14806v1) - [pdf](http://arxiv.org/pdf/2203.14806v1)

> Despite an increase in the empirical study of crowdfunding platforms and the prevalence of visual information, operations management and marketing literature has yet to explore the role that image characteristics play in crowdfunding success. The authors of this manuscript begin by synthesizing literature on visual processing to identify several image characteristics that are likely to shape crowdfunding success. After detailing measures for each image characteristic, they use them as part of a machine-learning algorithm (Bayesian additive trees), along with project characteristics and textual information, to predict crowdfunding success. Results show that the inclusion of these image characteristics substantially improves prediction over baseline project variables, as well as textual features. Furthermore, image characteristic variables exhibit high importance, similar to variables linked to the number of pictures and number of videos. This research therefore offers valuable resources to researchers and managers who are interested in the role of visual information in ensuring new product success.

</details>

<details>

<summary>2022-03-28 15:14:14 - Empirical Bayes cumulative $\ell$-value multiple testing procedure for sparse sequences</summary>

- *Kweku Abraham, Ismael Castillo, Etienne Roquain*

- `2102.00929v3` - [abs](http://arxiv.org/abs/2102.00929v3) - [pdf](http://arxiv.org/pdf/2102.00929v3)

> In the sparse sequence model, we consider a popular Bayesian multiple testing procedure and investigate for the first time its behaviour from the frequentist point of view. Given a spike-and-slab prior on the high-dimensional sparse unknown parameter, one can easily compute posterior probabilities of coming from the spike, which correspond to the well known local-fdr values, also called $\ell$-values. The spike-and-slab weight parameter is calibrated in an empirical Bayes fashion, using marginal maximum likelihood. The multiple testing procedure under study, called here the cumulative $\ell$-value procedure, ranks coordinates according to their empirical $\ell$-values and thresholds so that the cumulative ranked sum does not exceed a user-specified level $t$.   We validate the use of this method from the multiple testing perspective: for alternatives of appropriately large signal strength, the false discovery rate (FDR) of the procedure is shown to converge to the target level $t$, while its false negative rate (FNR) goes to $0$. We complement this study by providing convergence rates for the method. Additionally, we prove that the $q$-value multiple testing procedure shares similar convergence rates in this model.

</details>

<details>

<summary>2022-03-29 00:16:32 - bayesassurance: An R package for calculating sample size and Bayesian assurance</summary>

- *Jane Pan, Sudipto Banerjee*

- `2203.15154v1` - [abs](http://arxiv.org/abs/2203.15154v1) - [pdf](http://arxiv.org/pdf/2203.15154v1)

> We present a bayesassurance R package that computes the Bayesian assurance under various settings characterized by different assumptions and objectives. The package offers a constructive set of simulation-based functions suitable for addressing a wide range of clinical trial study design problems. We provide a detailed description of the underlying framework embedded within each of the power and assurance functions and demonstrate their usage through a series of worked-out examples. Through these examples, we hope to corroborate the advantages that come with using a two-stage generalized structure. We also illustrate scenarios where the Bayesian assurance and frequentist power overlap, allowing the user to address both Bayesian and classical inference problems provided that the parameters are properly defined. All assurance-related functions included in this R package rely on a two-stage Bayesian method that assigns two distinct priors to evaluate the unconditional probability of observing a positive outcome, which in turn addresses subtle limitations that take place when using the standard single-prior approach.

</details>

<details>

<summary>2022-03-29 01:48:36 - Analysis of sloppiness in model simulations: unveiling parameter uncertainty when mathematical models are fitted to data</summary>

- *Gloria M. Monsalve-Bravo, Brodie A. J. Lawson, Christopher Drovandi, Kevin Burrage, Kevin S. Brown, Christopher M. Baker, Sarah A. Vollert, Kerrie Mengersen, Eve McDonald-Madden, Matthew P. Adams*

- `2203.15184v1` - [abs](http://arxiv.org/abs/2203.15184v1) - [pdf](http://arxiv.org/pdf/2203.15184v1)

> This work introduces a Bayesian approach to assess the sensitivity of model outputs to changes in parameter values, constrained by the combination of prior beliefs and data. This novel approach identifies stiff parameter combinations that strongly affect the quality of the model-data fit while simultaneously revealing which of these key parameter combinations are informed primarily from the data or are also substantively influenced by the priors. We focus on the very common context in complex systems where the amount and quality of data are low compared to the number of model parameters to be collectively estimated, and showcase the benefits of our technique for applications in biochemistry, ecology, and cardiac electrophysiology. We also show how stiff parameter combinations, once identified, uncover controlling mechanisms underlying the system being modeled and inform which of the model parameters need to be prioritized in future experiments for improved parameter inference from collective model-data fitting.

</details>

<details>

<summary>2022-03-29 07:22:08 - Towards Flexible Sparsity-Aware Modeling: Automatic Tensor Rank Learning Using The Generalized Hyperbolic Prior</summary>

- *Lei Cheng, Zhongtao Chen, Qingjiang Shi, Yik-Chung Wu, Sergios Theodoridis*

- `2009.02472v2` - [abs](http://arxiv.org/abs/2009.02472v2) - [pdf](http://arxiv.org/pdf/2009.02472v2)

> Tensor rank learning for canonical polyadic decomposition (CPD) has long been deemed as an essential yet challenging problem. In particular, since the tensor rank controls the complexity of the CPD model, its inaccurate learning would cause overfitting to noise or underfitting to the signal sources, and even destroy the interpretability of model parameters. However, the optimal determination of a tensor rank is known to be a non-deterministic polynomial-time hard (NP-hard) task. Rather than exhaustively searching for the best tensor rank via trial-and-error experiments, Bayesian inference under the Gaussian-gamma prior was introduced in the context of probabilistic CPD modeling, and it was shown to be an effective strategy for automatic tensor rank determination. This triggered flourishing research on other structured tensor CPDs with automatic tensor rank learning. On the other side of the coin, these research works also reveal that the Gaussian-gamma model does not perform well for high-rank tensors and/or low signal-to-noise ratios (SNRs). To overcome these drawbacks, in this paper, we introduce a more advanced generalized hyperbolic (GH) prior to the probabilistic CPD model, which not only includes the Gaussian-gamma model as a special case, but also is more flexible to adapt to different levels of sparsity. Based on this novel probabilistic model, an algorithm is developed under the framework of variational inference, where each update is obtained in a closed-form. Extensive numerical results, using synthetic data and real-world datasets, demonstrate the significantly improved performance of the proposed method in learning both low as well as high tensor ranks even for low SNR cases.

</details>

<details>

<summary>2022-03-29 09:50:04 - Multilevel Bayesian Deep Neural Networks</summary>

- *Neil K. Chada, Ajay Jasra, Kody J. H. Law, Sumeetpal S. Singh*

- `2203.12961v2` - [abs](http://arxiv.org/abs/2203.12961v2) - [pdf](http://arxiv.org/pdf/2203.12961v2)

> In this article we consider Bayesian inference associated to deep neural networks (DNNs) and in particular, trace-class neural network (TNN) priors which were proposed by Sell et al. [39]. Such priors were developed as more robust alternatives to classical architectures in the context of inference problems. For this work we develop multilevel Monte Carlo (MLMC) methods for such models. MLMC is a popular variance reduction technique, with particular applications in Bayesian statistics and uncertainty quantification. We show how a particular advanced MLMC method that was introduced in [4] can be applied to Bayesian inference from DNNs and establish mathematically, that the computational cost to achieve a particular mean square error, associated to posterior expectation computation, can be reduced by several orders, versus more conventional techniques. To verify such results we provide numerous numerical experiments on model problems arising in machine learning. These include Bayesian regression, as well as Bayesian classification and reinforcement learning.

</details>

<details>

<summary>2022-03-29 22:57:16 - Sensing Cox Processes via Posterior Sampling and Positive Bases</summary>

- *Mojmr Mutn, Andreas Krause*

- `2110.11181v2` - [abs](http://arxiv.org/abs/2110.11181v2) - [pdf](http://arxiv.org/pdf/2110.11181v2)

> We study adaptive sensing of Cox point processes, a widely used model from spatial statistics. We introduce three tasks: maximization of captured events, search for the maximum of the intensity function and learning level sets of the intensity function. We model the intensity function as a sample from a truncated Gaussian process, represented in a specially constructed positive basis. In this basis, the positivity constraint on the intensity function has a simple form. We show how an minimal description positive basis can be adapted to the covariance kernel, non-stationarity and make connections to common positive bases from prior works. Our adaptive sensing algorithms use Langevin dynamics and are based on posterior sampling (\textsc{Cox-Thompson}) and top-two posterior sampling (\textsc{Top2}) principles. With latter, the difference between samples serves as a surrogate to the uncertainty. We demonstrate the approach using examples from environmental monitoring and crime rate modeling, and compare it to the classical Bayesian experimental design approach.

</details>

<details>

<summary>2022-03-29 23:05:40 - Robust, Automated, and Accurate Black-box Variational Inference</summary>

- *Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, Jonathan H. Huggins*

- `2203.15945v1` - [abs](http://arxiv.org/abs/2203.15945v1) - [pdf](http://arxiv.org/pdf/2203.15945v1)

> Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust, Automated, and Accurate BBVI (RAABBVI), a framework for reliable BBVI optimization. RAABBVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RAABBVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leiber (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RAABBVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.

</details>

<details>

<summary>2022-03-29 23:50:34 - Parallel MCMC Without Embarrassing Failures</summary>

- *Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi*

- `2202.11154v2` - [abs](http://arxiv.org/abs/2202.11154v2) - [pdf](http://arxiv.org/pdf/2202.11154v2)

> Embarrassingly parallel Markov Chain Monte Carlo (MCMC) exploits parallel computing to scale Bayesian inference to large datasets by using a two-step approach. First, MCMC is run in parallel on (sub)posteriors defined on data partitions. Then, a server combines local results. While efficient, this framework is very sensitive to the quality of subposterior sampling. Common sampling problems such as missing modes or misrepresentation of low-density regions are amplified -- instead of being corrected -- in the combination phase, leading to catastrophic failures. In this work, we propose a novel combination strategy to mitigate this issue. Our strategy, Parallel Active Inference (PAI), leverages Gaussian Process (GP) surrogate modeling and active learning. After fitting GPs to subposteriors, PAI (i) shares information between GP surrogates to cover missing modes; and (ii) uses active sampling to individually refine subposterior approximations. We validate PAI in challenging benchmarks, including heavy-tailed and multi-modal posteriors and a real-world application to computational neuroscience. Empirical results show that PAI succeeds where previous methods catastrophically fail, with a small communication overhead.

</details>

<details>

<summary>2022-03-30 12:19:05 - Model parameters in Gaussian process interpolation: an empirical study of selection criteria</summary>

- *Sbastien Petit, Julien Bect, Paul Feliot, Emmanuel Vazquez*

- `2107.06006v3` - [abs](http://arxiv.org/abs/2107.06006v3) - [pdf](http://arxiv.org/pdf/2107.06006v3)

> This article revisits the fundamental problem of parameter selection for Gaussian process interpolation. By choosing the mean and the covariance functions of a Gaussian process within parametric families, the user obtains a family of Bayesian procedures to perform predictions about the unknown function, and must choose a member of the family that will hopefully provide good predictive performances. We base our study on the general concept of scoring rules, which provides an effective framework for building leave-one-out selection and validation criteria, and a notion of extended likelihood criteria based on an idea proposed by Fasshauer and co-authors in 2009, which makes it possible to recover standard selection criteria such as, for instance, the generalized cross-validation criterion. Under this setting, we empirically show on several test problems of the literature that the choice of an appropriate family of models is often more important than the choice of a particular selection criterion (e.g., the likelihood versus a leave-one-out selection criterion). Moreover, our numerical results show that the regularity parameter of a Mat{\'e}rn covariance can be selected effectively by most selection criteria.

</details>

<details>

<summary>2022-03-30 13:54:28 - Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice</summary>

- *Tobias Fissler, Christian Lorentzen, Michael Mayer*

- `2202.12780v2` - [abs](http://arxiv.org/abs/2202.12780v2) - [pdf](http://arxiv.org/pdf/2202.12780v2)

> One of the main tasks of actuaries and data scientists is to build good predictive models for certain phenomena such as the claim size or the number of claims in insurance. These models ideally exploit given feature information to enhance the accuracy of prediction. This user guide revisits and clarifies statistical techniques to assess the calibration or adequacy of a model on the one hand, and to compare and rank different models on the other hand. In doing so, it emphasises the importance of specifying the prediction target functional at hand a priori (e.g. the mean or a quantile) and of choosing the scoring function in model comparison in line with this target functional. Guidance for the practical choice of the scoring function is provided. Striving to bridge the gap between science and daily practice in application, it focuses mainly on the pedagogical presentation of existing results and of best practice. The results are accompanied and illustrated by two real data case studies on workers' compensation and customer churn.

</details>

<details>

<summary>2022-03-30 17:00:12 - Ordered community detection in directed networks</summary>

- *Tiago P. Peixoto*

- `2203.16460v1` - [abs](http://arxiv.org/abs/2203.16460v1) - [pdf](http://arxiv.org/pdf/2203.16460v1)

> We develop a method to infer community structure in directed networks where the groups are ordered in a latent one-dimensional hierarchy that determines the preferred edge direction. Our nonparametric Bayesian approach is based on a modification of the stochastic block model (SBM), which can take advantage of rank alignment and coherence to produce parsimonious descriptions of networks that combine ordered hierarchies with arbitrary mixing patterns between groups. Since our model also includes directed degree correction, we can use it to distinguish non-local hierarchical structure from local in- and out-degree imbalance -- thus removing a source of conflation present in most ranking methods. We also demonstrate how we can reliably compare with the results obtained with the unordered SBM variant to determine whether a hierarchical ordering is statistically warranted in the first place. We illustrate the application of our method on a wide variety of empirical networks across several domains.

</details>

<details>

<summary>2022-03-30 17:17:50 - On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification</summary>

- *Sanyam Kapoor, Wesley J. Maddox, Pavel Izmailov, Andrew Gordon Wilson*

- `2203.16481v1` - [abs](http://arxiv.org/abs/2203.16481v1) - [pdf](http://arxiv.org/pdf/2203.16481v1)

> Aleatoric uncertainty captures the inherent randomness of the data, such as measurement noise. In Bayesian regression, we often use a Gaussian observation model, where we control the level of aleatoric uncertainty with a noise variance parameter. By contrast, for Bayesian classification we use a categorical distribution with no mechanism to represent our beliefs about aleatoric uncertainty. Our work shows that explicitly accounting for aleatoric uncertainty significantly improves the performance of Bayesian neural networks. We note that many standard benchmarks, such as CIFAR, have essentially no aleatoric uncertainty. Moreover, we show data augmentation in approximate inference has the effect of softening the likelihood, leading to underconfidence and profoundly misrepresenting our honest beliefs about aleatoric uncertainty. Accordingly, we find that a cold posterior, tempered by a power greater than one, often more honestly reflects our beliefs about aleatoric uncertainty than no tempering -- providing an explicit link between data augmentation and cold posteriors. We show that we can match or exceed the performance of posterior tempering by using a Dirichlet observation model, where we explicitly control the level of aleatoric uncertainty, without any need for tempering.

</details>

<details>

<summary>2022-03-30 17:22:29 - Bayesian Deep Learning and a Probabilistic Perspective of Generalization</summary>

- *Andrew Gordon Wilson, Pavel Izmailov*

- `2002.08791v4` - [abs](http://arxiv.org/abs/2002.08791v4) - [pdf](http://arxiv.org/pdf/2002.08791v4)

> The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.

</details>

<details>

<summary>2022-03-30 18:33:13 - A Shared Parameter Model for Systolic Blood Pressure Accounting for Data Missing Not at Random in the HUNT Study</summary>

- *Aurora Christine Hofman, Lars Espeland, Ingelin Steinsland, Emma M. L. Ingestrm*

- `2203.16602v1` - [abs](http://arxiv.org/abs/2203.16602v1) - [pdf](http://arxiv.org/pdf/2203.16602v1)

> In this work, blood pressure eleven years ahead is modeled using data from a longitudinal population-based health survey, the Trondelag Health (HUNT) Study, while accounting for missing data due to dropout between consecutive surveys (20-50 %). We propose and validate a shared parameter model (SPM) in the Bayesian framework with age, sex, body mass index, and initial blood pressure as explanatory variables. Further, we propose a novel evaluation scheme to assess data missing not at random (MNAR) by comparing the predictive performance of the fitted SPM with and without conditioning on the missing process. The results demonstrate that the SPM is suitable for inference for a dataset of this size (cohort of 64385 participants) and structure. The SPM indicates data MNAR and gives different parameter estimates than a naive model assuming data missing at random. The SPM and naive models are compared based on predictive performance in a validation dataset. The naive model performs slightly better than the SPM for the present participants. This is in accordance with results from a simulation study based on the SPM where we find that the naive model performs better for the present participants, while the SPM performs better for the dropouts.

</details>

<details>

<summary>2022-03-30 19:06:55 - Quantifying the presence/absence of meso-scale structures in networks</summary>

- *Eric Yanchenko*

- `2203.16620v1` - [abs](http://arxiv.org/abs/2203.16620v1) - [pdf](http://arxiv.org/pdf/2203.16620v1)

> Meso-scale structures are network features where nodes with similar properties are grouped together instead of being treated individually. In this work, we provide formal and mathematical definitions of three such structures: assortative communities, disassortative communities and core-periphery. We then leverage these definitions and a Bayesian framework to quantify the presence/absence of each structure in a network. This allows for probabilistic statements about the network structure as well as uncertainty estimates of the group labels and edge probabilities. The method is applied to real-world networks, yielding provocative results about well-known network data sets.

</details>

<details>

<summary>2022-03-30 19:22:30 - A Bayesian framework for incorporating exposure uncertainty into health analyses with application to air pollution and stillbirth</summary>

- *Saskia Comess, Howard H. Chang, Joshua L. Warren*

- `2203.16627v1` - [abs](http://arxiv.org/abs/2203.16627v1) - [pdf](http://arxiv.org/pdf/2203.16627v1)

> Studies of the relationships between environmental exposures and adverse health outcomes often rely on a two-stage statistical modeling approach, where exposure is modeled/predicted in the first stage and used as input to a separately fit health outcome analysis in the second stage. Uncertainty in these predictions is frequently ignored, or accounted for in an overly simplistic manner, when estimating the associations of interest. Working in the Bayesian setting, we propose a flexible kernel density estimation (KDE) approach for fully utilizing posterior output from the first stage modeling/prediction to make accurate inference on the association between exposure and health in the second stage, derive the full conditional distributions needed for efficient model fitting, detail its connections with existing approaches, and compare its performance through simulation. Our KDE approach is shown to generally have improved performance across several settings and model comparison metrics. Using competing approaches, we investigate the association between lagged daily ambient fine particulate matter levels and stillbirth counts in New Jersey (2011-2015), observing an increase in risk with elevated exposure three days prior to delivery. The newly developed methods are available in the R package KDExp.

</details>

<details>

<summary>2022-03-30 20:39:46 - Hawkes Process Modeling of Block Arrivals in Bitcoin Blockchain</summary>

- *Rui Luo, Vikram Krishnamurthy, Erik Blasch*

- `2203.16666v1` - [abs](http://arxiv.org/abs/2203.16666v1) - [pdf](http://arxiv.org/pdf/2203.16666v1)

> The paper constructs a multi-variate Hawkes process model of Bitcoin block arrivals and price jumps. Hawkes processes are selfexciting point processes that can capture the self- and cross-excitation effects of block mining and Bitcoin price volatility. We use publicly available blockchain datasets to estimate the model parameters via maximum likelihood estimation. The results show that Bitcoin price volatility boost block mining rate and Bitcoin investment return demonstrates mean reversion. Quantile-Quantile plots show that the proposed Hawkes process model is a better fit to the blockchain datasets than a Poisson process model.

</details>

<details>

<summary>2022-03-30 21:52:11 - Eigenvector-Assisted Statistical Inference for Signal-Plus-Noise Matrix Models</summary>

- *Fangzheng Xie, Dingbo Wu*

- `2203.16688v1` - [abs](http://arxiv.org/abs/2203.16688v1) - [pdf](http://arxiv.org/pdf/2203.16688v1)

> In this paper, we develop a generalized Bayesian inference framework for a collection of signal-plus-noise matrix models arising in high-dimensional statistics and many applications. The framework is built upon an asymptotically unbiased estimating equation with the assistance of the leading eigenvectors of the data matrix. The solution to the estimating equation coincides with the maximizer of an appropriate statistical criterion function. The generalized posterior distribution is constructed by replacing the usual log-likelihood function in the Bayes formula with the criterion function. The proposed framework does not require the complete specification of the sampling distribution and is convenient for uncertainty quantification via a Markov Chain Monte Carlo sampler, circumventing the inconvenience of resampling the data matrix. Under mild regularity conditions, we establish the large sample properties of the estimating equation estimator and the generalized posterior distributions. In particular, the generalized posterior credible sets have the correct frequentist nominal coverage probability provided that the so-called generalized information equality holds. The validity and usefulness of the proposed framework are demonstrated through the analysis of synthetic datasets and the real-world ENZYMES network datasets.

</details>

<details>

<summary>2022-03-31 00:05:19 - Parameter estimation and uncertainty quantification using information geometry</summary>

- *Jesse A Sharp, Alexander P Browning, Kevin Burrage, Matthew J Simpson*

- `2111.12201v3` - [abs](http://arxiv.org/abs/2111.12201v3) - [pdf](http://arxiv.org/pdf/2111.12201v3)

> In this work we: (1) review likelihood-based inference for parameter estimation and the construction of confidence regions; and, (2) explore the use of techniques from information geometry, including geodesic curves and Riemann scalar curvature, to supplement typical techniques for uncertainty quantification such as Bayesian methods, profile likelihood, asymptotic analysis and bootstrapping. These techniques from information geometry provide data-independent insights into uncertainty and identifiability, and can be used to inform data collection decisions. All code used in this work to implement the inference and information geometry techniques is available on GitHub.

</details>

<details>

<summary>2022-03-31 09:27:59 - MBORE: Multi-objective Bayesian Optimisation by Density-Ratio Estimation</summary>

- *George De Ath, Tinkle Chugh, Alma A. M. Rahat*

- `2203.16912v1` - [abs](http://arxiv.org/abs/2203.16912v1) - [pdf](http://arxiv.org/pdf/2203.16912v1)

> Optimisation problems often have multiple conflicting objectives that can be computationally and/or financially expensive. Mono-surrogate Bayesian optimisation (BO) is a popular model-based approach for optimising such black-box functions. It combines objective values via scalarisation and builds a Gaussian process (GP) surrogate of the scalarised values. The location which maximises a cheap-to-query acquisition function is chosen as the next location to expensively evaluate. While BO is an effective strategy, the use of GPs is limiting. Their performance decreases as the problem input dimensionality increases, and their computational complexity scales cubically with the amount of data. To address these limitations, we extend previous work on BO by density-ratio estimation (BORE) to the multi-objective setting. BORE links the computation of the probability of improvement acquisition function to that of probabilistic classification. This enables the use of state-of-the-art classifiers in a BO-like framework. In this work we present MBORE: multi-objective Bayesian optimisation by density-ratio estimation, and compare it to BO across a range of synthetic and real-world benchmarks. We find that MBORE performs as well as or better than BO on a wide variety of problems, and that it outperforms BO on high-dimensional and real-world problems.

</details>

<details>

<summary>2022-03-31 11:58:49 - A Bayesian network model for predicting cardiovascular risk</summary>

- *J. M. Ordovas, D. Rios Insua, A. Santos-Lozano, A. Lucia, A. Torres, A. Kosgodagan, J. M. Camacho*

- `2112.13963v2` - [abs](http://arxiv.org/abs/2112.13963v2) - [pdf](http://arxiv.org/pdf/2112.13963v2)

> We propose a Bayesian network model to make inferences and predictions about cardiovascular risk. Both the structure and the probability tables in the underlying model are built using a large dataset collected in Spain from annual work health assessments, with uncertainty characterized through posterior distributions. We illustrate its use for public health practice, policy and research purposes. A freely available version of the software is included in an Appendix.

</details>

<details>

<summary>2022-03-31 16:00:14 - Advances in Importance Sampling</summary>

- *Vctor Elvira, Luca Martino*

- `2102.05407v3` - [abs](http://arxiv.org/abs/2102.05407v3) - [pdf](http://arxiv.org/pdf/2102.05407v3)

> Importance sampling (IS) is a Monte Carlo technique for the approximation of intractable distributions and integrals with respect to them. The origin of IS dates from the early 1950s. In the last decades, the rise of the Bayesian paradigm and the increase of the available computational resources have propelled the interest in this theoretically sound methodology. In this paper, we first describe the basic IS algorithm and then revisit the recent advances in this methodology. We pay particular attention to two sophisticated lines. First, we focus on multiple IS (MIS), the case where more than one proposal is available. Second, we describe adaptive IS (AIS), the generic methodology for adapting one or more proposals.

</details>

<details>

<summary>2022-03-31 16:20:02 - Unbiased Parameter Inference for a Class of Partially Observed Levy-Process Models</summary>

- *Hamza Ruzayqat, Ajay Jasra*

- `2112.13874v2` - [abs](http://arxiv.org/abs/2112.13874v2) - [pdf](http://arxiv.org/pdf/2112.13874v2)

> We consider the problem of static Bayesian inference for partially observed Levy-process models. We develop a methodology which allows one to infer static parameters and some states of the process, without a bias from the time-discretization of the afore-mentioned Levy process. The unbiased method is exceptionally amenable to parallel implementation and can be computationally efficient relative to competing approaches. We implement the method on S & P 500 log-return daily data and compare it to some Markov chain Monte Carlo (MCMC) algorithm.

</details>

<details>

<summary>2022-03-31 17:46:49 - Recommender Systems meet Mechanism Design</summary>

- *Yang Cai, Constantinos Daskalakis*

- `2110.12558v2` - [abs](http://arxiv.org/abs/2110.12558v2) - [pdf](http://arxiv.org/pdf/2110.12558v2)

> Machine learning has developed a variety of tools for learning and representing high-dimensional distributions with structure. Recent years have also seen big advances in designing multi-item mechanisms. Akin to overfitting, however, these mechanisms can be extremely sensitive to the Bayesian prior that they target, which becomes problematic when that prior is only approximately known. At the same time, even if access to the exact Bayesian prior is given, it is known that optimal or even approximately optimal multi-item mechanisms run into sample, computational, representation and communication intractability barriers.   We consider a natural class of multi-item mechanism design problems with very large numbers of items, but where the bidders' value distributions can be well-approximated by a topic model akin to those used in recommendation systems with very large numbers of possible recommendations. We propose a mechanism design framework for this setting, building on a recent robustification framework by Brustle et al., which disentangles the statistical challenge of estimating a multi-dimensional prior from the task of designing a good mechanism for it, and robustifies the performance of the latter against the estimation error of the former. We provide an extension of this framework appropriate for our setting, which allows us to exploit the expressive power of topic models to reduce the effective dimensionality of the mechanism design problem and remove the dependence of its computational, communication and representation complexity on the number of items.

</details>

<details>

<summary>2022-03-31 18:33:51 - Integrating Biological Knowledge in Kernel-Based Analyses of Environmental Mixtures and Health</summary>

- *Glen McGee, Ander Wilson, Brent A Coull, Thomas F Webster*

- `2204.00040v1` - [abs](http://arxiv.org/abs/2204.00040v1) - [pdf](http://arxiv.org/pdf/2204.00040v1)

> A key goal of environmental health research is to assess the risk posed by mixtures of pollutants. As epidemiologic studies of mixtures can be expensive to conduct, it behooves researchers to incorporate prior knowledge about mixtures into their analyses. This work extends the Bayesian multiple index model (BMIM), which assumes the exposure-response function is a non-parametric function of a set of linear combinations of pollutants formed with a set of exposure-specific weights. The framework is attractive because it combines the flexibility of response-surface methods with the interpretability of linear index models. We propose three strategies to incorporate prior toxicological knowledge into construction of indices in a BMIM: (a) constraining index weights, (b) structuring index weights by exposure transformations, and (c) placing informative priors on the index weights. We propose a novel prior specification that combines spike-and-slab variable selection with informative Dirichlet distribution based on relative potency factors often derived from previous toxicological studies. In simulations we show that the proposed priors improve inferences when prior information is correct and can protect against misspecification suffered by naive toxicological models when prior information is incorrect. Moreover, different strategies may be mixed-and-matched for different indices to suit available information (or lack thereof). We demonstrate the proposed methods on an analysis of data from the National Health and Nutrition Examination Survey and incorporate prior information on relative chemical potencies obtained from toxic equivalency factors available in the literature.

</details>

<details>

<summary>2022-03-31 19:27:41 - Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination</summary>

- *Tilmann Gneiting, Johannes Resin*

- `2108.03210v2` - [abs](http://arxiv.org/abs/2108.03210v2) - [pdf](http://arxiv.org/pdf/2108.03210v2)

> Model diagnostics and forecast evaluation are two sides of the same coin. A common principle is that fitted or predicted distributions ought to be calibrated or reliable, ideally in the sense of auto-calibration, where the outcome is a random draw from the posited distribution. For binary responses, this is the universal concept of reliability. For real-valued outcomes, a general theory of calibration has been elusive, despite a recent surge of interest in distributional regression and machine learning. We develop a framework rooted in probability theory, which gives rise to hierarchies of calibration, and applies to both predictive distributions and stand-alone point forecasts. In a nutshell, a prediction - distributional or single-valued - is conditionally T-calibrated if it can be taken at face value in terms of the functional T. Whenever T is defined via an identification function - as in the cases of threshold (non) exceedance probabilities, quantiles, expectiles, and moments - auto-calibration implies T-calibration. We introduce population versions of T-reliability diagrams and revisit a score decomposition into measures of miscalibration (MCB), discrimination (DSC), and uncertainty (UNC). In empirical settings, stable and efficient estimators of T-reliability diagrams and score components arise via nonparametric isotonic regression and the pool-adjacent-violators algorithm. For in-sample model diagnostics, we propose a universal coefficient of determination, $$\text{R}^\ast = \frac{\text{DSC}-\text{MCB}}{\text{UNC}},$$ that nests and reinterprets the classical $\text{R}^2$ in least squares (mean) regression and its natural analogue $\text{R}^1$ in quantile regression, yet applies to T-regression in general, with MCB $\geq 0$, DSC $\geq 0$, and $\text{R}^\ast \in [0,1]$ under modest conditions.

</details>

<details>

<summary>2022-03-31 22:52:43 - VFDS: Variational Foresight Dynamic Selection in Bayesian Neural Networks for Efficient Human Activity Recognition</summary>

- *Randy Ardywibowo, Shahin Boluki, Zhangyang Wang, Bobak Mortazavi, Shuai Huang, Xiaoning Qian*

- `2204.00130v1` - [abs](http://arxiv.org/abs/2204.00130v1) - [pdf](http://arxiv.org/pdf/2204.00130v1)

> In many machine learning tasks, input features with varying degrees of predictive capability are acquired at varying costs. In order to optimize the performance-cost trade-off, one would select features to observe a priori. However, given the changing context with previous observations, the subset of predictive features to select may change dynamically. Therefore, we face the challenging new problem of foresight dynamic selection (FDS): finding a dynamic and light-weight policy to decide which features to observe next, before actually observing them, for overall performance-cost trade-offs. To tackle FDS, this paper proposes a Bayesian learning framework of Variational Foresight Dynamic Selection (VFDS). VFDS learns a policy that selects the next feature subset to observe, by optimizing a variational Bayesian objective that characterizes the trade-off between model performance and feature cost. At its core is an implicit variational distribution on binary gates that are dependent on previous observations, which will select the next subset of features to observe. We apply VFDS on the Human Activity Recognition (HAR) task where the performance-cost trade-off is critical in its practice. Extensive results demonstrate that VFDS selects different features under changing contexts, notably saving sensory costs while maintaining or improving the HAR accuracy. Moreover, the features that VFDS dynamically select are shown to be interpretable and associated with the different activity types. We will release the code.

</details>


## 2022-04

<details>

<summary>2022-04-01 05:46:59 - Same environment, stratified impacts? Air pollution, extreme temperatures, and birth weight in south China</summary>

- *Xiaoying Liu, Jere R. Behrman, Emily Hannum, Fan Wang, Qingguo Zhao*

- `2204.00219v1` - [abs](http://arxiv.org/abs/2204.00219v1) - [pdf](http://arxiv.org/pdf/2204.00219v1)

> This paper investigates whether associations between birth weight and prenatal ambient environmental conditions--pollution and extreme temperatures--differ by 1) maternal education; 2) children's innate health; and 3) interactions between these two. We link birth records from Guangzhou, China, during a period of high pollution, to ambient air pollution (PM10 and a composite measure) and extreme temperature data. We first use mean regressions to test whether, overall, maternal education is an "effect modifier" in the relationships between ambient air pollution, extreme temperature, and birth weight. We then use conditional quantile regressions to test for effect heterogeneity according to the unobserved innate vulnerability of babies after conditioning on other confounders. Results show that 1) the negative association between ambient exposures and birth weight is twice as large at lower conditional quantiles of birth weights as at the median; 2) the protection associated with college-educated mothers with respect to pollution and extreme heat is heterogeneous and potentially substantial: between 0.02 and 0.34 standard deviations of birth weights, depending on the conditional quantiles; 3) this protection is amplified under more extreme ambient conditions and for infants with greater unobserved innate vulnerabilities.

</details>

<details>

<summary>2022-04-01 09:01:20 - Scalable Semi-Modular Inference with Variational Meta-Posteriors</summary>

- *Chris U. Carmona, Geoff K. Nicholls*

- `2204.00296v1` - [abs](http://arxiv.org/abs/2204.00296v1) - [pdf](http://arxiv.org/pdf/2204.00296v1)

> The Cut posterior and related Semi-Modular Inference are Generalised Bayes methods for Modular Bayesian evidence combination. Analysis is broken up over modular sub-models of the joint posterior distribution. Model-misspecification in multi-modular models can be hard to fix by model elaboration alone and the Cut posterior and SMI offer a way round this. Information entering the analysis from misspecified modules is controlled by an influence parameter $\eta$ related to the learning rate. This paper contains two substantial new methods. First, we give variational methods for approximating the Cut and SMI posteriors which are adapted to the inferential goals of evidence combination. We parameterise a family of variational posteriors using a Normalising Flow for accurate approximation and end-to-end training. Secondly, we show that analysis of models with multiple cuts is feasible using a new Variational Meta-Posterior. This approximates a family of SMI posteriors indexed by $\eta$ using a single set of variational parameters.

</details>

<details>

<summary>2022-04-01 10:00:08 - Distilling importance sampling</summary>

- *Dennis Prangle, Cecilia Viscardi*

- `1910.03632v4` - [abs](http://arxiv.org/abs/1910.03632v4) - [pdf](http://arxiv.org/pdf/1910.03632v4)

> Many complicated Bayesian posteriors are difficult to approximate by either sampling or optimisation methods. Therefore we propose a novel approach combining features of both, making use of a flexible parameterised family of densities, typically a normalising flow. We start with a density from this family approximating a highly tempered posterior. This is used as a proposal density in importance sampling to produce a weighted sample from a less tempered posterior. This sample is then used in optimisation to update the parameters of the density, which we view as "distilling" the importance sampling results. We iterate these steps, gradually reducing the tempering, eventually reaching a good approximation to the posterior. We illustrate our method in three challenging examples, on queuing, epidemiology, and inference for stochastic differential equations. These cover applications in both likelihood-based and likelihood-free inference.

</details>

<details>

<summary>2022-04-01 15:26:57 - Wind Farm Layout Optimisation using Set Based Multi-objective Bayesian Optimisation</summary>

- *Tinkle Chugh, Endi Ymeraj*

- `2203.17065v2` - [abs](http://arxiv.org/abs/2203.17065v2) - [pdf](http://arxiv.org/pdf/2203.17065v2)

> Wind energy is one of the cleanest renewable electricity sources and can help in addressing the challenge of climate change. One of the drawbacks of wind-generated energy is the large space necessary to install a wind farm; this arises from the fact that placing wind turbines in a limited area would hinder their productivity and therefore not be economically convenient. This naturally leads to an optimisation problem, which has three specific challenges: (1) multiple conflicting objectives (2) computationally expensive simulation models and (3) optimisation over design sets instead of design vectors. The first and second challenges can be addressed by using surrogate-assisted e.g.\ Bayesian multi-objective optimisation. However, the traditional Bayesian optimisation cannot be applied as the optimisation function in the problem relies on design sets instead of design vectors. This paper extends the applicability of Bayesian multi-objective optimisation to set based optimisation for solving the wind farm layout problem. We use a set-based kernel in Gaussian process to quantify the correlation between wind farms (with a different number of turbines). The results on the given data set of wind energy and direction clearly show the potential of using set-based Bayesian multi-objective optimisation.

</details>

<details>

<summary>2022-04-01 16:08:00 - Detecting changes in dynamic social networks using multiply-labeled movement data</summary>

- *Zaineb L. Boulil, John W. Durban, Holly Fearnbach, Trevor W. Joyce, Samantha G. M. Leander, Henry R. Scharf*

- `2204.00542v1` - [abs](http://arxiv.org/abs/2204.00542v1) - [pdf](http://arxiv.org/pdf/2204.00542v1)

> The social structure of an animal population can often influence movement and inform researchers on a species' behavioral tendencies. Animal social networks can be studied through movement data; however, modern sources of data can have identification issues that result in multiply-labeled individuals. Since all available social movement models rely on unique labels, we extend an existing Bayesian hierarchical movement model in a way that makes use of a latent social network and accommodates multiply-labeled movement data (MLMD). We apply our model to drone-measured movement data from Risso's dolphins (Grampus griseus) and estimate the effects of sonar exposure on the dolphins' social structure. Our proposed framework can be applied to MLMD for various social movement applications.

</details>

<details>

<summary>2022-04-01 16:20:57 - Decomposition of Differences in Distribution under Sample Selection and the Gender Wage Gap</summary>

- *Santiago Pereda-Fernndez*

- `2204.00551v1` - [abs](http://arxiv.org/abs/2204.00551v1) - [pdf](http://arxiv.org/pdf/2204.00551v1)

> I address the decomposition of the differences between the distribution of outcomes of two groups when individuals self-select themselves into participation. I differentiate between the decomposition for participants and the entire population, highlighting how the primitive components of the model affect each of the distributions of outcomes. Additionally, I introduce two ancillary decompositions that help uncover the sources of differences in the distribution of unobservables and participation between the two groups. The estimation is done using existing quantile regression methods, for which I show how to perform uniformly valid inference. I illustrate these methods by revisiting the gender wage gap, finding that changes in female participation and self-selection have been the main drivers for reducing the gap.

</details>

<details>

<summary>2022-04-01 18:41:34 - Bayesian Non-Homogeneous Hidden Markov Model with Variable Selection for Investigating Drivers of Seizure Risk Cycling</summary>

- *Emily T. Wang, Sharon Chiang, Zulfi Haneef, Vikram R. Rao, Robert Moss, Marina Vannucci*

- `2204.00651v1` - [abs](http://arxiv.org/abs/2204.00651v1) - [pdf](http://arxiv.org/pdf/2204.00651v1)

> A major issue in the clinical management of epilepsy is the unpredictability of seizures. Yet, traditional approaches to seizure forecasting and risk assessment in epilepsy rely heavily on raw seizure frequencies, which are a stochastic measurement of seizure risk. We consider a Bayesian non-homogeneous hidden Markov model for unsupervised clustering of zero-inflated seizure count data. The proposed model allows for a probabilistic estimate of the sequence of seizure risk states at the individual level. It also offers significant improvement over prior approaches by incorporating a variable selection prior for the identification of clinical covariates that drive seizure risk changes and accommodating highly granular data. For inference, we implement an efficient sampler that employs stochastic search and data augmentation techniques. We evaluate model performance on simulated seizure count data. We then demonstrate the clinical utility of the proposed model by analyzing daily seizure count data from 133 patients with Dravet syndrome collected through the Seizure Tracker TM system, a patient-reported electronic seizure diary. We report on the dynamics of seizure risk cycling, including validation of several known pharmacologic relationships. We also uncover novel findings characterizing the presence and volatility of risk states in Dravet syndrome, which may directly inform counseling to reduce the unpredictability of seizures for patients with this devastating cause of epilepsy.

</details>

<details>

<summary>2022-04-01 22:08:03 - Strategies for Safe Multi-Armed Bandits with Logarithmic Regret and Risk</summary>

- *Tianrui Chen, Aditya Gangrade, Venkatesh Saligrama*

- `2204.00706v1` - [abs](http://arxiv.org/abs/2204.00706v1) - [pdf](http://arxiv.org/pdf/2204.00706v1)

> We investigate a natural but surprisingly unstudied approach to the multi-armed bandit problem under safety risk constraints. Each arm is associated with an unknown law on safety risks and rewards, and the learner's goal is to maximise reward whilst not playing unsafe arms, as determined by a given threshold on the mean risk.   We formulate a pseudo-regret for this setting that enforces this safety constraint in a per-round way by softly penalising any violation, regardless of the gain in reward due to the same. This has practical relevance to scenarios such as clinical trials, where one must maintain safety for each round rather than in an aggregated sense.   We describe doubly optimistic strategies for this scenario, which maintain optimistic indices for both safety risk and reward. We show that schema based on both frequentist and Bayesian indices satisfy tight gap-dependent logarithmic regret bounds, and further that these play unsafe arms only logarithmically many times in total. This theoretical analysis is complemented by simulation studies demonstrating the effectiveness of the proposed schema, and probing the domains in which their use is appropriate.

</details>

<details>

<summary>2022-04-01 23:25:15 - Robust Particle Density Tempering for State Space Models</summary>

- *David Gunawan, Robert Kohn, Minh Ngoc Tran*

- `1805.00649v3` - [abs](http://arxiv.org/abs/1805.00649v3) - [pdf](http://arxiv.org/pdf/1805.00649v3)

> Density tempering (also called density annealing) is a sequential Monte Carlo approach to Bayesian inference for general state models; it is an alternative to Markov chain Monte Carlo. When applied to state space models, it moves a collection of parameters and latent states (which are called particles) through a number of stages, with each stage having its own target distribution. The particles are initially generated from a distribution that is easy to sample from, e.g. the prior; the target at the final stage is the posterior distribution. Tempering is usually carried out either in batch mode, involving all the data at each stage, or sequentially with observations added at each stage, which is called data tempering. Our paper proposes efficient Markov moves for generating the parameters and states for each stage of particle based density tempering. This allows the proposed SMC methods to increase (scale up) the number of parameters and states that can be handled. Most of the current literature uses a pseudo-marginal Markov move step with the states integrated out, and the parameters generated by a random walk proposal; although this strategy is general, it is very inefficient when the states or parameters are high dimensional. We also build on the work of Dufays (2016) and make data tempering more robust to outliers and structural changes for models with intractable likelihoods by adding batch tempering at each stage. The performance of the proposed methods is evaluated using univariate stochastic volatility models with outliers and structural breaks and high dimensional factor stochastic volatility models having both many parameters and many latent states.

</details>

<details>

<summary>2022-04-02 05:39:57 - Variational message passing for online polynomial NARMAX identification</summary>

- *Wouter Kouw, Albert Podusenko, Magnus Koudahl, Maarten Schoukens*

- `2204.00769v1` - [abs](http://arxiv.org/abs/2204.00769v1) - [pdf](http://arxiv.org/pdf/2204.00769v1)

> We propose a variational Bayesian inference procedure for online nonlinear system identification. For each output observation, a set of parameter posterior distributions is updated, which is then used to form a posterior predictive distribution for future outputs. We focus on the class of polynomial NARMAX models, which we cast into probabilistic form and represent in terms of a Forney-style factor graph. Inference in this graph is efficiently performed by a variational message passing algorithm. We show empirically that our variational Bayesian estimator outperforms an online recursive least-squares estimator, most notably in small sample size settings and low noise regimes, and performs on par with an iterative least-squares estimator trained offline.

</details>

<details>

<summary>2022-04-02 06:32:19 - Distributional Gradient Boosting Machines</summary>

- *Alexander Mrz, Thomas Kneib*

- `2204.00778v1` - [abs](http://arxiv.org/abs/2204.00778v1) - [pdf](http://arxiv.org/pdf/2204.00778v1)

> We present a unified probabilistic gradient boosting framework for regression tasks that models and predicts the entire conditional distribution of a univariate response variable as a function of covariates. Our likelihood-based approach allows us to either model all conditional moments of a parametric distribution, or to approximate the conditional cumulative distribution function via Normalizing Flows. As underlying computational backbones, our framework is based on XGBoost and LightGBM. Modelling and predicting the entire conditional distribution greatly enhances existing tree-based gradient boosting implementations, as it allows to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. Empirical results show that our framework achieves state-of-the-art forecast accuracy.

</details>

<details>

<summary>2022-04-02 10:53:12 - Inverse uncertainty quantification of a mechanical model of arterial tissue with surrogate modeling</summary>

- *Salome Kakhaia, Pavel Zun, Dongwei Ye, Valeria Krzhizhanovskaya*

- `2204.02167v1` - [abs](http://arxiv.org/abs/2204.02167v1) - [pdf](http://arxiv.org/pdf/2204.02167v1)

> Disorders of coronary arteries lead to severe health problems such as atherosclerosis, angina, heart attack and even death. Considering the clinical significance of coronary arteries, an efficient computer model is a vital step towards tissue engineering, enhancing the research of coronary diseases, and developing medical treatment and interventional tools. In this work, we apply inverse uncertainty quantification to a microscale agent-based arterial tissue model, a component of the 3D multiscale model of in-stent restenosis (ISR3D). IUQ provides calibration of the arterial tissue model to achieve realistic mechanical behaviour in line with the experimental data measured from the tissue's macroscopic behaviour. Bayesian calibration with bias term correction is applied as an IUQ technique to reduce the uncertainty of unknown polynomial coefficients of the attractive force function and achieve agreement with the experimental data based on the uniaxial strain tests of arterial tissue. Due to the high computational costs of the ISR3D model, the Gaussian process (GP) regression surrogate model is introduced to ensure the feasibility of the IUQ computations. The result is an IUQ methodology to calibrate a model with uncertain parameters and a microscale agent-based model of arterial tissue, which produces mechanical behaviour in line with the experimental data.

</details>

<details>

<summary>2022-04-03 17:59:37 - Bayesian estimation of topological features of persistence diagrams</summary>

- *Asael Fabian Martnez*

- `2204.01127v1` - [abs](http://arxiv.org/abs/2204.01127v1) - [pdf](http://arxiv.org/pdf/2204.01127v1)

> Persistent homology is a common technique in topological data analysis providing geometrical and topological information about the sample space. All this information, known as topological features, is summarized in persistence diagrams, and the main interest is in identifying the most persisting ones since they correspond to the Betti number values. Given the randomness inherent in the sampling process, and the complex structure of the space where persistence diagrams take values, estimation of Betti numbers is not straightforward. The approach followed in this work makes use of features' lifetimes and provides a full Bayesian clustering model, based on random partitions, in order to estimate Betti numbers. A simulation study is also presented.

</details>

<details>

<summary>2022-04-04 01:47:51 - First-Order Bayesian Regret Analysis of Thompson Sampling</summary>

- *Sbastien Bubeck, Mark Sellke*

- `1902.00681v3` - [abs](http://arxiv.org/abs/1902.00681v3) - [pdf](http://arxiv.org/pdf/1902.00681v3)

> We address online combinatorial optimization when the player has a prior over the adversary's sequence of losses. In this framework, Russo and Van Roy proposed an information-theoretic analysis of Thompson Sampling based on the information ratio, resulting in optimal worst-case regret bounds. In this paper we introduce three novel ideas to this line of work. First we propose a new quantity, the scale-sensitive information ratio, which allows us to obtain more refined first-order regret bounds (i.e., bounds of the form $\sqrt{L^*}$ where $L^*$ is the loss of the best combinatorial action). Second we replace the entropy over combinatorial actions by a coordinate entropy, which allows us to obtain the first optimal worst-case bound for Thompson Sampling in the combinatorial setting. Finally, we introduce a novel link between Bayesian agents and frequentist confidence intervals. Combining these ideas we show that the classical multi-armed bandit first-order regret bound $\tilde{O}(\sqrt{d L^*})$ still holds true in the more challenging and more general semi-bandit scenario. This latter result improves the previous state of the art bound $\tilde{O}(\sqrt{(d+m^3)L^*})$ by Lykouris, Sridharan and Tardos.   Moreover we sharpen these results with two technical ingredients. The first leverages a recent insight of Zimmert and Lattimore to replace Shannon entropy with more refined potential functions in the analysis. The second is a \emph{Thresholded} Thompson sampling algorithm, which slightly modifies the original algorithm by never playing low-probability actions. This thresholding results in fully $T$-independent regret bounds when $L^*$ is almost surely upper-bounded, which we show does not hold for ordinary Thompson sampling.

</details>

<details>

<summary>2022-04-04 03:59:39 - Minimax Confidence Intervals for the Sliced Wasserstein Distance</summary>

- *Tudor Manole, Sivaraman Balakrishnan, Larry Wasserman*

- `1909.07862v3` - [abs](http://arxiv.org/abs/1909.07862v3) - [pdf](http://arxiv.org/pdf/1909.07862v3)

> Motivated by the growing popularity of variants of the Wasserstein distance in statistics and machine learning, we study statistical inference for the Sliced Wasserstein distance--an easily computable variant of the Wasserstein distance. Specifically, we construct confidence intervals for the Sliced Wasserstein distance which have finite-sample validity under no assumptions or under mild moment assumptions. These intervals are adaptive in length to the regularity of the underlying distributions. We also bound the minimax risk of estimating the Sliced Wasserstein distance, and as a consequence establish that the lengths of our proposed confidence intervals are minimax optimal over appropriate distribution classes. To motivate the choice of these classes, we also study minimax rates of estimating a distribution under the Sliced Wasserstein distance. These theoretical findings are complemented with a simulation study demonstrating the deficiencies of the classical bootstrap, and the advantages of our proposed methods. We also show strong correspondences between our theoretical predictions and the adaptivity of our confidence interval lengths in simulations. We conclude by demonstrating the use of our confidence intervals in the setting of simulator-based likelihood-free inference. In this setting, contrasting popular approximate Bayesian computation methods, we develop uncertainty quantification methods with rigorous frequentist coverage guarantees.

</details>

<details>

<summary>2022-04-04 09:49:27 - The Tangent Exponential Model</summary>

- *Anthony C. Davison, Nancy Reid*

- `2106.10496v2` - [abs](http://arxiv.org/abs/2106.10496v2) - [pdf](http://arxiv.org/pdf/2106.10496v2)

> The likelihood function is central to both frequentist and Bayesian formulations of parametric statistical inference, and large-sample approximations to the sampling distributions of estimators and test statistics, and to posterior densities, are widely used in practice. Improved approximations have been widely studied and can provide highly accurate inferences when samples are small or there are many nuisance parameters. This article reviews improved approximations based on the tangent exponential model developed in a series of articles by D.~A.~S.~Fraser and co-workers, attempting to explain the theoretical basis of this model and to provide a guide to the associated literature, including a partially-annotated bibliography.

</details>

<details>

<summary>2022-04-04 10:36:18 - Non-crossing convex quantile regression</summary>

- *Sheng Dai, Timo Kuosmanen, Xun Zhou*

- `2204.01371v1` - [abs](http://arxiv.org/abs/2204.01371v1) - [pdf](http://arxiv.org/pdf/2204.01371v1)

> Quantile crossing is a common phenomenon in shape constrained nonparametric quantile regression. A recent study by Wang et al. (2014) has proposed to address this problem by imposing non-crossing constraints to convex quantile regression. However, the non-crossing constraints may violate an intrinsic quantile property. This paper proposes a penalized convex quantile regression approach that can circumvent quantile crossing while better maintaining the quantile property. A Monte Carlo study demonstrates the superiority of the proposed penalized approach in addressing the quantile crossing problem.

</details>

<details>

<summary>2022-04-04 13:51:06 - Logical coherence in Bayesian simultaneous three-way hypothesis tests</summary>

- *Bernardo F. Reimann, Rafael Izbicki, Julio M. Stern, Rafael B. Stern, Luis G. Esteves*

- `2204.01495v1` - [abs](http://arxiv.org/abs/2204.01495v1) - [pdf](http://arxiv.org/pdf/2204.01495v1)

> This paper studies whether Bayesian simultaneous three-way hypothesis tests can be logically coherent. Two types of results are obtained. First, under the standard error-wise constant loss, only for a limited set of models can a Bayes simultaneous test be logically coherent. Second, if more general loss functions are used, then it is possible to obtain Bayes simultaneous tests that are always logically coherent. An explicit example of such a loss function is provided.

</details>

<details>

<summary>2022-04-04 15:00:44 - Towards Federated Bayesian Network Structure Learning with Continuous Optimization</summary>

- *Ignavier Ng, Kun Zhang*

- `2110.09356v2` - [abs](http://arxiv.org/abs/2110.09356v2) - [pdf](http://arxiv.org/pdf/2110.09356v2)

> Traditionally, Bayesian network structure learning is often carried out at a central site, in which all data is gathered. However, in practice, data may be distributed across different parties (e.g., companies, devices) who intend to collectively learn a Bayesian network, but are not willing to disclose information related to their data owing to privacy or security concerns. In this work, we present a federated learning approach to estimate the structure of Bayesian network from data that is horizontally partitioned across different parties. We develop a distributed structure learning method based on continuous optimization, using the alternating direction method of multipliers (ADMM), such that only the model parameters have to be exchanged during the optimization process. We demonstrate the flexibility of our approach by adopting it for both linear and nonlinear cases. Experimental results on synthetic and real datasets show that it achieves an improved performance over the other methods, especially when there is a relatively large number of clients and each has a limited sample size.

</details>

<details>

<summary>2022-04-04 15:24:56 - Low Tree-Rank Bayesian Vector Autoregression Model</summary>

- *Zeyu Yuwen, George Michailidis, Zhengwu Zhang, Leo L. Duan*

- `2204.01573v1` - [abs](http://arxiv.org/abs/2204.01573v1) - [pdf](http://arxiv.org/pdf/2204.01573v1)

> Vector autoregressions have been widely used for modeling and analysis of multivariate time series data. In high-dimensional settings, model parameter regularization schemes inducing sparsity have achieved good forecasting performances. However, in many data applications such as those in neuroscience, the graph estimates from existing methods still tend to be quite dense and difficult to interpret, unless we made compromise in the goodness-of-fit. To address this dilemma, in this article we propose to incorporate a commonly used structural assumption -- that the ground-truth graph should be largely connected, in the sense that it should only contain at most a few components. We take a Bayesian approach and develop a novel tree-rank prior for the regression coefficients. Specifically, this prior forces the non-zero coefficients to appear only on the union of a few spanning trees. Since each spanning tree connects $p$ nodes with only $(p-1)$ edges, this prior effectively achieves both high connectivity and high sparsity. In analyzing test-retest functional magnetic resonance imaging data, our model produces a much more interpretable graph estimate, compared to popular existing approaches. In addition, we show appealing properties of this new method, such as differentiable computation, mild stability conditions and posterior consistency.

</details>

<details>

<summary>2022-04-04 19:40:00 - Bayesian Inference with the l1-ball Prior: Solving Combinatorial Problems with Exact Zeros</summary>

- *Maoran Xu, Leo L. Duan*

- `2006.01340v4` - [abs](http://arxiv.org/abs/2006.01340v4) - [pdf](http://arxiv.org/pdf/2006.01340v4)

> The l1-regularization is very popular in high dimensional statistics -- it changes a combinatorial problem of choosing which subset of the parameter are zero, into a simple continuous optimization. Using a continuous prior concentrated near zero, the Bayesian counterparts are successful in quantifying the uncertainty in the variable selection problems; nevertheless, the lack of exact zeros makes it difficult for broader problems such as the change-point detection and rank selection. Inspired by the duality of the l1-regularization as a constraint onto an l1-ball, we propose a new prior by projecting a continuous distribution onto the l1-ball. This creates a positive probability on the ball boundary, which contains both continuous elements and exact zeros. Unlike the spike-and-slab prior, this l1-ball projection is continuous and differentiable almost surely, making the posterior estimation amenable to the Hamiltonian Monte Carlo algorithm. We examine the properties, such as the volume change due to the projection, the connection to the combinatorial prior, the minimax concentration rate in the linear problem. We demonstrate the usefulness of exact zeros that simplify the combinatorial problems, such as the change-point detection in time series, the dimension selection of mixture model and the low-rank-plus-sparse change detection in the medical images.

</details>

<details>

<summary>2022-04-05 09:15:51 - Asymptotic Theory for Moderate Deviations from the Unit Boundary in Quantile Autoregressive Time Series</summary>

- *Christis Katsouris*

- `2204.02073v1` - [abs](http://arxiv.org/abs/2204.02073v1) - [pdf](http://arxiv.org/pdf/2204.02073v1)

> We establish the asymptotic theory in quantile autoregression when the model parameter is specified with respect to moderate deviations from the unit boundary of the form (1 + c / k) with a convergence sequence that diverges at a rate slower than the sample size n. Then, extending the framework proposed by Phillips and Magdalinos (2007), we consider the limit theory for the near-stationary and the near-explosive cases when the model is estimated with a conditional quantile specification function and model parameters are quantile-dependent. Additionally, a Bahadur-type representation and limiting distributions based on the M-estimators of the model parameters are derived. Specifically, we show that the serial correlation coefficient converges in distribution to a ratio of two independent random variables. Monte Carlo simulations illustrate the finite-sample performance of the estimation procedure under investigation.

</details>

<details>

<summary>2022-04-05 11:35:23 - Modeling time evolving COVID-19 uncertainties with density dependent asymptomatic infections and social reinforcement</summary>

- *Qing Liu, Longbing Cao*

- `2108.10029v2` - [abs](http://arxiv.org/abs/2108.10029v2) - [pdf](http://arxiv.org/pdf/2108.10029v2)

> The COVID-19 pandemic has posed significant challenges in modeling its complex epidemic transmissions, infection and contagion, which are very different from known epidemics. The challenges in quantifying COVID-19 complexities include effectively modeling its process and data uncertainties. The uncertainties are embedded in implicit and high-proportional undocumented infections, asymptomatic contagion, social reinforcement of infections, and various quality issues in the reported data. These uncertainties become even more apparent in the first two months of the COVID-19 pandemic, when the relevant knowledge, case reporting and testing were all limited. Here we introduce a novel hybrid approach Susceptible-Undocumented infected-Documented infected-Recovered (SUDR) model. First, SUDR (1) characterizes and distinguishes Undocumented (U) and Documented (D) infections commonly seen during COVID-19 incubation periods and asymptomatic infections. Second, SUDR characterizes the probabilistic density of infections by capturing exogenous processes. Lastly, SUDR approximates the density likelihood of COVID-19 prevalence over time by incorporating Bayesian inference into SUDR. Different from existing COVID-19 models, SUDR characterizes the undocumented infections during unknown transmission processes. To capture the uncertainties of temporal transmission and social reinforcement during COVID-19 contagion, the transmission rate is modeled by a time-varying density function of undocumented infectious cases. By sampling from the mean-field posterior distribution with reasonable priors, SUDR handles the randomness, noise and sparsity of COVID-19 observations widely seen in the public COVID-19 case data. The results demonstrate a deeper quantitative understanding of the above uncertainties, in comparison with classic SIR, time-dependent SIR, and probabilistic SIR models.

</details>

<details>

<summary>2022-04-05 12:57:34 - Semiparametric Approach to Estimation of Marginal and Quantile Effects</summary>

- *Seong-ho Lee, Yanyuan Ma, Elvezio Ronchetti*

- `2204.02170v1` - [abs](http://arxiv.org/abs/2204.02170v1) - [pdf](http://arxiv.org/pdf/2204.02170v1)

> We consider a semiparametric generalized linear model and study estimation of both marginal and quantile effects in this model. We propose an approximate maximum likelihood estimator, and rigorously establish the consistency, the asymptotic normality, and the semiparametric efficiency of our method in both the marginal effect and the quantile effect estimation. Simulation studies are conducted to illustrate the finite sample performance, and we apply the new tool to analyze a Swiss non-labor income data and discover a new interesting predictor.

</details>

<details>

<summary>2022-04-05 15:06:44 - Methods for Combining Probability and Nonprobability Samples Under Unknown Overlaps</summary>

- *Terrance D. Savitsky, Matthew R. Williams, Julie Gershunskaya, Vladislav Beresovsky, Nels G. Johnson*

- `2204.02271v1` - [abs](http://arxiv.org/abs/2204.02271v1) - [pdf](http://arxiv.org/pdf/2204.02271v1)

> Nonprobability (convenience) samples are increasingly sought to stabilize estimations for one or more population variables of interest that are performed using a randomized survey (reference) sample by increasing the effective sample size. Estimation of a population quantity derived from a convenience sample will typically result in bias since the distribution of variables of interest in the convenience sample is different from the population. A recent set of approaches estimates conditional (on sampling design predictors) inclusion probabilities for convenience sample units by specifying reference sample-weighted pseudo likelihoods. This paper introduces a novel approach that derives the propensity score for the observed sample as a function of conditional inclusion probabilities for the reference and convenience samples as our main result. Our approach allows specification of an exact likelihood for the observed sample. We construct a Bayesian hierarchical formulation that simultaneously estimates sample propensity scores and both conditional and reference sample inclusion probabilities for the convenience sample units. We compare our exact likelihood with the pseudo likelihoods in a Monte Carlo simulation study.

</details>

<details>

<summary>2022-04-05 15:42:51 - Aggregating distribution forecasts from deep ensembles</summary>

- *Benedikt Schulz, Sebastian Lerch*

- `2204.02291v1` - [abs](http://arxiv.org/abs/2204.02291v1) - [pdf](http://arxiv.org/pdf/2204.02291v1)

> The importance of accurately quantifying forecast uncertainty has motivated much recent research on probabilistic forecasting. In particular, a variety of deep learning approaches has been proposed, with forecast distributions obtained as output of neural networks. These neural network-based methods are often used in the form of an ensemble based on multiple model runs from different random initializations, resulting in a collection of forecast distributions that need to be aggregated into a final probabilistic prediction. With the aim of consolidating findings from the machine learning literature on ensemble methods and the statistical literature on forecast combination, we address the question of how to aggregate distribution forecasts based on such deep ensembles. Using theoretical arguments, simulation experiments and a case study on wind gust forecasting, we systematically compare probability- and quantile-based aggregation methods for three neural network-based approaches with different forecast distribution types as output. Our results show that combining forecast distributions can substantially improve the predictive performance. We propose a general quantile aggregation framework for deep ensembles that shows superior performance compared to a linear combination of the forecast densities. Finally, we investigate the effects of the ensemble size and derive recommendations of aggregating distribution forecasts from deep ensembles in practice.

</details>

<details>

<summary>2022-04-05 15:50:26 - Theoretical properties of Bayesian Student-$t$ linear regression</summary>

- *Philippe Gagnon, Yoshiko Hayashi*

- `2204.02299v1` - [abs](http://arxiv.org/abs/2204.02299v1) - [pdf](http://arxiv.org/pdf/2204.02299v1)

> Student-$t$ linear regression is a commonly used alternative to the normal model in Bayesian analysis when one wants to gain robustness against outliers. The assumption of heavy-tailed error distribution makes the model more adapted to a potential presence of outliers by assigning higher probabilities to extreme values. Even though the Student-$t$ model is often used in practice, not a lot is known about its theoretical properties. In this paper, we aim to fill some gaps by providing analyses in two different asymptotic scenarios. In the first one, outliers are considered to be further and further away from the bulk of the data. The analysis allows to characterize the limiting posterior distribution, a distribution in which a trace of the outliers is present, making the approach partially robust. The impact of the trace is seen to increase with the degrees of freedom of the Student-$t$ distribution assumed. The second asymptotic scenario is one where the sample size increases and the normal model is the true generating process to be able to compare the efficiency of the robust estimator to the ordinary-least-squares one when the latter is the benchmark. The asymptotic efficiency is comparable, in the sense that the variance of the robust estimator is inflated but only by a factor, and this factor converges to 1 as the degrees of freedom increase. The trade-off between robustness and efficiency controlled through the degrees of freedom is thus precisely characterized (at least asymptotically).

</details>

<details>

<summary>2022-04-05 16:58:41 - Bayesian Quantile Regression for Longitudinal Count Data</summary>

- *Sanket Jantre*

- `2204.02344v1` - [abs](http://arxiv.org/abs/2204.02344v1) - [pdf](http://arxiv.org/pdf/2204.02344v1)

> This work introduces Bayesian quantile regression modeling framework for the analysis of longitudinal count data. In this model, the response variable is not continuous and hence an artificial smoothing of counts is incorporated. The Bayesian implementation utilizes the normal-exponential mixture representation of the asymmetric Laplace distribution for the response variable. An efficient Gibbs sampling algorithm is derived for fitting the model to the data. The model is illustrated through simulation studies and implemented in an application drawn from neurology. Model comparison demonstrates the practical utility of the proposed model.

</details>

<details>

<summary>2022-04-05 19:29:55 - CATVI: Conditional and Adaptively Truncated Variational Inference for Hierarchical Bayesian Nonparametric Models</summary>

- *Yirui Liu, Xinghao Qiao, Jessica Lam*

- `2001.04508v2` - [abs](http://arxiv.org/abs/2001.04508v2) - [pdf](http://arxiv.org/pdf/2001.04508v2)

> Current variational inference methods for hierarchical Bayesian nonparametric models can neither characterize the correlation structure among latent variables due to the mean-field setting, nor infer the true posterior dimension because of the universal truncation. To overcome these limitations, we propose the conditional and adaptively truncated variational inference method (CATVI) by maximizing the nonparametric evidence lower bound and integrating Monte Carlo into the variational inference framework. CATVI enjoys several advantages over traditional methods, including a smaller divergence between variational and true posteriors, reduced risk of underfitting or overfitting, and improved prediction accuracy. Empirical studies on three large datasets reveal that CATVI applied in Bayesian nonparametric topic models substantially outperforms competing models, providing lower perplexity and clearer topic-words clustering.

</details>

<details>

<summary>2022-04-05 21:07:47 - Discovering and forecasting extreme events via active learning in neural operators</summary>

- *Ethan Pickering, George Em Karniadakis, Themistoklis P. Sapsis*

- `2204.02488v1` - [abs](http://arxiv.org/abs/2204.02488v1) - [pdf](http://arxiv.org/pdf/2204.02488v1)

> Extreme events in society and nature, such as pandemic spikes or rogue waves, can have catastrophic consequences. Characterizing extremes is difficult as they occur rarely, arise from seemingly benign conditions, and belong to complex and often unknown infinite-dimensional systems. Such challenges render attempts at characterizing them as moot. We address each of these difficulties by combining novel training schemes in Bayesian experimental design (BED) with an ensemble of deep neural operators (DNOs). This model-agnostic framework pairs a BED scheme that actively selects data for quantifying extreme events with an ensemble of DNOs that approximate infinite-dimensional nonlinear operators. We find that not only does this framework clearly beat Gaussian processes (GPs) but that 1) shallow ensembles of just two members perform best; 2) extremes are uncovered regardless of the state of initial data (i.e. with or without extremes); 3) our method eliminates "double-descent" phenomena; 4) the use of batches of suboptimal acquisition points compared to step-by-step global optima does not hinder BED performance; and 5) Monte Carlo acquisition outperforms standard minimizers in high-dimensions. Together these conclusions form the foundation of an AI-assisted experimental infrastructure that can efficiently infer and pinpoint critical situations across many domains, from physical to societal systems.

</details>

<details>

<summary>2022-04-06 10:33:08 - A new Bayesian discrepancy measure</summary>

- *Francesco Bertolino, Mara Manca, Monica Musio, Walter Racugno, Laura Ventura*

- `2105.13716v3` - [abs](http://arxiv.org/abs/2105.13716v3) - [pdf](http://arxiv.org/pdf/2105.13716v3)

> A Bayesian Discrepancy Measure (BDM) is proposed for parametric models to evaluate the distance of a given hypothesis with respect to the available information (prior law and data). The proposed measure of evidence has properties of consistency and invariance. After having presented some similarities and differences between the test associated to the BDM and other testing procedures, the analysis of some multiple parameter case is illustrated, showing the properties of the BDM. Among them, conceptual and interpretive simplicity.

</details>

<details>

<summary>2022-04-06 10:45:19 - The computational cost of blocking for sampling discretely observed diffusions</summary>

- *Marcin Mider, Paul A. Jenkins, Murray Pollock, Gareth O. Roberts*

- `2009.10440v2` - [abs](http://arxiv.org/abs/2009.10440v2) - [pdf](http://arxiv.org/pdf/2009.10440v2)

> Many approaches for conducting Bayesian inference on discretely observed diffusions involve imputing diffusion bridges between observations. This can be computationally challenging in settings in which the temporal horizon between subsequent observations is large, due to the poor scaling of algorithms for simulating bridges as observation distance increases. It is common in practical settings to use a blocking scheme, in which the path is split into a (user-specified) number of overlapping segments and a Gibbs sampler is employed to update segments in turn. Substituting the independent simulation of diffusion bridges for one obtained using blocking introduces an inherent trade-off: we are now imputing shorter bridges at the cost of introducing a dependency between subsequent iterations of the bridge sampler. This is further complicated by the fact that there are a number of possible ways to implement the blocking scheme, each of which introduces a different dependency structure between iterations. Although blocking schemes have had considerable empirical success in practice, there has been no analysis of this trade-off nor guidance to practitioners on the particular specifications that should be used to obtain a computationally efficient implementation. In this article we conduct this analysis and demonstrate that the expected computational cost of a blocked path-space rejection sampler applied to Brownian bridges scales asymptotically at a cubic rate with respect to the observation distance and that this rate is linear in the case of the Ornstein-Uhlenbeck process. Numerical experiments suggest applicability both of the results of our paper and of the guidance we provide beyond the class of linear diffusions considered.

</details>

<details>

<summary>2022-04-06 10:54:11 - GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes</summary>

- *Mateus Maia, Keefe Murphy, Andrew C. Parnell*

- `2204.02112v2` - [abs](http://arxiv.org/abs/2204.02112v2) - [pdf](http://arxiv.org/pdf/2204.02112v2)

> The Bayesian additive regression trees (BART) model is an ensemble method extensively and successfully used in regression tasks due to its consistently strong predictive performance and its ability to quantify uncertainty. BART combines "weak" tree models through a set of shrinkage priors, whereby each tree explains a small portion of the variability in the data. However, the lack of smoothness and the absence of a covariance structure over the observations in standard BART can yield poor performance in cases where such assumptions would be necessary. We propose Gaussian processes Bayesian additive regression trees (GP-BART) as an extension of BART which assumes Gaussian process (GP) priors for the predictions of each terminal node among all trees. We illustrate our model on simulated and real data and compare its performance to traditional modelling approaches, outperforming them in many scenarios. An implementation of our method is available in the R package rGPBART available at: https://github.com/MateusMaiaDS/gpbart

</details>

<details>

<summary>2022-04-06 13:14:19 - Markov cohort state-transition model: A multinomial distribution representation</summary>

- *Rowan Iskandar, Cassandra Berns*

- `2204.02805v1` - [abs](http://arxiv.org/abs/2204.02805v1) - [pdf](http://arxiv.org/pdf/2204.02805v1)

> Markov cohort state-transition models have been the standard approach for simulating the prognosis of patients or, more generally, the life trajectories of individuals over a time period. Current approaches for estimating the variance of a Markov model using a Monte Carlo sampling or a master equation representation are computationally expensive and analytically difficult to express and solve. We introduce an alternative representation of a Markov model in the form of a multinomial distribution. We derive this representation from principles and then verify its veracity in a simulation exercise. This representation provides an exact and fast approach to compute the variance and a way to estimate transition probabilities in a Bayesian setting.

</details>

<details>

<summary>2022-04-06 13:45:25 - Importance Weighting Approach in Kernel Bayes' Rule</summary>

- *Liyuan Xu, Yutian Chen, Arnaud Doucet, Arthur Gretton*

- `2202.02474v2` - [abs](http://arxiv.org/abs/2202.02474v2) - [pdf](http://arxiv.org/pdf/2202.02474v2)

> We study a nonparametric approach to Bayesian computation via feature means, where the expectation of prior features is updated to yield expected posterior features, based on regression from kernel or neural net features of the observations. All quantities involved in the Bayesian update are learned from observed data, making the method entirely model-free. The resulting algorithm is a novel instance of a kernel Bayes' rule (KBR). Our approach is based on importance weighting, which results in superior numerical stability to the existing approach to KBR, which requires operator inversion. We show the convergence of the estimator using a novel consistency analysis on the importance weighting estimator in the infinity norm. We evaluate our KBR on challenging synthetic benchmarks, including a filtering problem with a state-space model involving high dimensional image observations. The proposed method yields uniformly better empirical performance than the existing KBR, and competitive performance with other competing methods.

</details>

<details>

<summary>2022-04-06 13:48:26 - Bayesian inference for asymptomatic COVID-19 infection rates</summary>

- *Dexter Cahoy, Joseph Sedransk*

- `2203.14381v2` - [abs](http://arxiv.org/abs/2203.14381v2) - [pdf](http://arxiv.org/pdf/2203.14381v2)

> To strengthen inferences meta analyses are commonly used to summarize information from a set of independent studies. In some cases, though, the data may not satisfy the assumptions underlying the meta analysis. Using three Bayesian methods that have a more general structure than the common meta analytic ones, we can show the extent and nature of the pooling that is justified statistically. In this paper, we re-analyze data from several reviews whose objective is to make inference about the COVID-19 asymptomatic infection rate. When it is unlikely that all of the true effect sizes come from a single source researchers should be cautious about pooling the data from all of the studies. Our findings and methodology are applicable to other COVID-19 outcome variables, and more generally.

</details>

<details>

<summary>2022-04-06 13:53:08 - Optimal Bayesian design for model discrimination via classification</summary>

- *Markus Hainy, David J. Price, Olivier Restif, Christopher Drovandi*

- `1809.05301v3` - [abs](http://arxiv.org/abs/1809.05301v3) - [pdf](http://arxiv.org/pdf/1809.05301v3)

> Performing optimal Bayesian design for discriminating between competing models is computationally intensive as it involves estimating posterior model probabilities for thousands of simulated datasets. This issue is compounded further when the likelihood functions for the rival models are computationally expensive. A new approach using supervised classification methods is developed to perform Bayesian optimal model discrimination design. This approach requires considerably fewer simulations from the candidate models than previous approaches using approximate Bayesian computation. Further, it is easy to assess the performance of the optimal design through the misclassification error rate. The approach is particularly useful in the presence of models with intractable likelihoods but can also provide computational advantages when the likelihoods are manageable.

</details>

<details>

<summary>2022-04-06 15:31:14 - Robust Estimation of Conditional Factor Models</summary>

- *Qihui Chen*

- `2204.00801v2` - [abs](http://arxiv.org/abs/2204.00801v2) - [pdf](http://arxiv.org/pdf/2204.00801v2)

> This paper develops estimation and inference methods for conditional quantile factor models. We first introduce a simple sieve estimation, and establish asymptotic properties of the estimators under large $N$. We then provide a bootstrap procedure for estimating the distributions of the estimators. We also provide two consistent estimators for the number of factors. The methods allow us not only to estimate conditional factor structures of distributions of asset returns utilizing characteristics, but also to conduct robust inference in conditional factor models, which enables us to analyze the cross section of asset returns with heavy tails. We apply the methods to analyze the cross section of individual US stock returns.

</details>

<details>

<summary>2022-04-06 15:44:56 - Vecchia-approximated Deep Gaussian Processes for Computer Experiments</summary>

- *Annie Sauer, Andrew Cooper, Robert B. Gramacy*

- `2204.02904v1` - [abs](http://arxiv.org/abs/2204.02904v1) - [pdf](http://arxiv.org/pdf/2204.02904v1)

> Deep Gaussian processes (DGPs) upgrade ordinary GPs through functional composition, in which intermediate GP layers warp the original inputs, providing flexibility to model non-stationary dynamics. Two DGP regimes have emerged in recent literature. A "big data" regime, prevalent in machine learning, favors approximate, optimization-based inference for fast, high-fidelity prediction. A "small data" regime, preferred for computer surrogate modeling, deploys posterior integration for enhanced uncertainty quantification (UQ). We aim to bridge this gap by expanding the capabilities of Bayesian DGP posterior inference through the incorporation of the Vecchia approximation, allowing linear computational scaling without compromising accuracy or UQ. We are motivated by surrogate modeling of simulation campaigns with upwards of 100,000 runs -- a size too large for previous fully-Bayesian implementations -- and demonstrate prediction and UQ superior to that of "big data" competitors. All methods are implemented in the "deepgp" package on CRAN.

</details>

<details>

<summary>2022-04-06 18:19:29 - Statistical Model Criticism of Variational Auto-Encoders</summary>

- *Claartje Barkhof, Wilker Aziz*

- `2204.03030v1` - [abs](http://arxiv.org/abs/2204.03030v1) - [pdf](http://arxiv.org/pdf/2204.03030v1)

> We propose a framework for the statistical evaluation of variational auto-encoders (VAEs) and test two instances of this framework in the context of modelling images of handwritten digits and a corpus of English text. Our take on evaluation is based on the idea of statistical model criticism, popular in Bayesian data analysis, whereby a statistical model is evaluated in terms of its ability to reproduce statistics of an unknown data generating process from which we can obtain samples. A VAE learns not one, but two joint distributions over a shared sample space, each exploiting a choice of factorisation that makes sampling tractable in one of two directions (latent-to-data, data-to-latent). We evaluate samples from these distributions, assessing their (marginal) fit to the observed data and our choice of prior, and we also evaluate samples through a pipeline that connects the two distributions starting from a data sample, assessing whether together they exploit and reveal latent factors of variation that are useful to a practitioner. We show that this methodology offers possibilities for model selection qualitatively beyond intrinsic evaluation metrics and at a finer granularity than commonly used statistics can offer.

</details>

<details>

<summary>2022-04-07 03:09:36 - Bayesian vector autoregressive analysis of macroeconomic and transport influences on urban traffic accidents</summary>

- *Jieling Jin*

- `2204.03177v1` - [abs](http://arxiv.org/abs/2204.03177v1) - [pdf](http://arxiv.org/pdf/2204.03177v1)

> The macro influencing factors analysis of urban traffic safety is important to guide the direction of urban development to reduce the frequency of traffic accidents. In this study, a Bayesian vector autoregressive(BVAR) model was developed to exploring the impact of six macro-level economic and transport factors, including population, GDP, private vehicle ownership, bus ownership, subway rail mileage and road average speed on traffic accidents with the small sample size transport annual report data in Beijing. The results show that the BVAR model was suitable for time series analysis of traffic accidents in small sample situations. In macroeconomic factors, GDP growth was considered to reduce the number of traffic accidents in the long term, while population growth had a positive effect on traffic accidents in the short term. With the respect to macro-transport factors, road average speed and private vehicle ownership was perceived to increase traffic accidents in long duration, whereas bus ownership and subway rail mileage had long-term negative effects, with the greatest positive effect for road average speed and the greatest negative effect for subway rail mileage. This study suggests that government departments can reduce the number of traffic accidents by increasing investment in public transportation infrastructures, limiting private vehicles and road speed.

</details>

<details>

<summary>2022-04-07 04:59:15 - Robust Model-based Inference for Non-Probability Samples</summary>

- *Ali Rafei, Michael R. Elliott, Carol A. C. Flannagan*

- `2204.03215v1` - [abs](http://arxiv.org/abs/2204.03215v1) - [pdf](http://arxiv.org/pdf/2204.03215v1)

> With the ubiquitous availability of unstructured data, growing attention is paid as how to adjust for selection bias in such non-probability samples. The majority of the robust estimators proposed by prior literature are either fully or partially design-based, which may lead to inefficient estimates if outlying (pseudo-)weights are present. In addition, correctly reflecting the uncertainty of the adjusted estimator remains a challenge when the available reference survey is complex in the sample design. This article proposes a fully model-based method for inference using non-probability samples where the goal is to predict the outcome variable for the entire population units. We employ a Bayesian bootstrap method with Rubin's combing rules to derive the adjusted point and interval estimates. Using Gaussian process regression, our method allows for kernel matching between the non-probability sample units and population units based on the estimated selection propensities when the outcome model is misspecified. The repeated sampling properties of our method are evaluated through two Monte Carlo simulation studies. Finally, we examine it on a real-world non-probability sample with the aim to estimate crash-attributed injury rates in different body regions in the United States.

</details>

<details>

<summary>2022-04-07 06:58:24 - Amortized Auto-Tuning: Cost-Efficient Bayesian Transfer Optimization for Hyperparameter Recommendation</summary>

- *Yuxin Xiao, Eric P. Xing, Willie Neiswanger*

- `2106.09179v2` - [abs](http://arxiv.org/abs/2106.09179v2) - [pdf](http://arxiv.org/pdf/2106.09179v2)

> With the surge in the number of hyperparameters and training times of modern machine learning models, hyperparameter tuning is becoming increasingly expensive. However, after assessing 40 tuning methods systematically, we find that each faces certain limitations. In particular, methods that speed up tuning via knowledge transfer typically require the final performance of hyperparameters and do not focus on low-fidelity information. As we demonstrate empirically, this common practice is suboptimal and can incur an unnecessary use of resources. It is more cost-efficient to instead leverage low-fidelity tuning observations to measure inter-task similarity and transfer knowledge from existing to new tasks accordingly. However, performing multi-fidelity tuning comes with its own challenges in the transfer setting: the noise in additional observations and the need for performance forecasting. Therefore, we propose and conduct a thorough analysis of a multi-task multi-fidelity Bayesian optimization framework, which leads to the best instantiation--amortized auto-tuning (AT2). We further present an offline-computed 27-task hyperparameter recommendation (HyperRec) database to serve the community. Extensive experiments on HyperRec and other real-world databases illustrate the effectiveness of our AT2 method.

</details>

<details>

<summary>2022-04-07 08:36:37 - Calibration of a bumble bee foraging model using Approximate Bayesian Computation</summary>

- *Charlotte Baey, Henrik G. Smith, Maj Rundlf, Ola Olsson, Yann Clough, Ullrika Sahlin*

- `2204.03287v1` - [abs](http://arxiv.org/abs/2204.03287v1) - [pdf](http://arxiv.org/pdf/2204.03287v1)

> 1. Challenging calibration of complex models can be approached by using prior knowledge on the parameters. However, the natural choice of Bayesian inference can be computationally heavy when relying on Markov Chain Monte Carlo (MCMC) sampling. When the likelihood of the data is intractable, alternative Bayesian methods have been proposed. Approximate Bayesian Computation (ABC) only requires sampling from the data generative model, but may be problematic when the dimension of the data is high.   2. We studied alternative strategies to handle high dimensional data in ABC applied to the calibration of a spatially explicit foraging model for \textit{Bombus terrestris}. The first step consisted in building a set of summary statistics carrying enough biological meaning, i.e. as much as the original data, and then applying ABC on this set. Two ABC strategies, the use of regression adjustment leading to the production of ABC posterior samples, and the use of machine learning approaches to approximate ABC posterior quantiles, were compared with respect to coverage of model estimates and true parameter values. The comparison was made on simulated data as well as on data from two field studies.   3. Results from simulated data showed that some model parameters were easier to calibrate than others. Approaches based on random forests in general performed better on simulated data. They also performed well on field data, even though the posterior predictive distribution exhibited a higher variance. Nonlinear regression adjustment performed better than linear ones, and the classical ABC rejection algorithm performed badly.   4. ABC is an interesting and appealing approach for the calibration of complex models in biology, such as spatially explicit foraging models. However, while ABC methods are easy to implement, they require considerable tuning.

</details>

<details>

<summary>2022-04-07 10:05:57 - Flexible Amortized Variational Inference in qBOLD MRI</summary>

- *Ivor J. A. Simpson, Ashley McManamon, Balzs rzsik, Alan J. Stone, Nicholas P. Blockley, Iris Asllani, Alessandro Colasanti, Mara Cercignani*

- `2203.05845v2` - [abs](http://arxiv.org/abs/2203.05845v2) - [pdf](http://arxiv.org/pdf/2203.05845v2)

> Streamlined qBOLD acquisitions enable experimentally straightforward observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred; however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume (DBV) are more ambiguously determined from the data. As such, existing inference methods tend to yield very noisy and underestimated OEF maps, while overestimating DBV.   This work describes a novel probabilistic machine learning approach that can infer plausible distributions of OEF and DBV. Initially, we create a model that produces informative voxelwise prior distribution based on synthetic training data. Contrary to prior work, we model the joint distribution of OEF and DBV through a scaled multivariate logit-Normal distribution, which enables the values to be constrained within a plausible range. The prior distribution model is used to train an efficient amortized variational Bayesian inference model. This model learns to infer OEF and DBV by predicting real image data, with few training data required, using the signal equations as a forward model.   We demonstrate that our approach enables the inference of smooth OEF and DBV maps, with a physiologically plausible distribution that can be adapted through specification of an informative prior distribution. Other benefits include model comparison (via the evidence lower bound) and uncertainty quantification for identifying image artefacts. Results are demonstrated on a small study comparing subjects undergoing hyperventilation and at rest. We illustrate that the proposed approach allows measurement of gray matter differences in OEF and DBV and enables voxelwise comparison between conditions, where we observe significant increases in OEF and $R_2^\prime$ during hyperventilation.

</details>

<details>

<summary>2022-04-07 12:06:40 - A spatial modeling framework for monitoring surveys with different sampling protocols with a case study for bird abundance in mid-Scandinavia</summary>

- *Jorge Sicacha-Parada, Diego Pavon-Jordan, Ingelin Steinsland, Roel May, Brd Stokke, Ingar Jostein ien*

- `2104.05751v3` - [abs](http://arxiv.org/abs/2104.05751v3) - [pdf](http://arxiv.org/pdf/2104.05751v3)

> We present a new methodology to model total abundance by merging count data information from surveys with different sampling protocols. The proposed methods are used for data from national breeding bird monitoring programs in Norway and Sweden. Each census collects abundance data following two different sampling protocols in each country, i.e. these protocols provides data from four different sampling processes. The modeling framework assumes a common Gaussian Random Field shared by both the observed and true abundance with either a linear or a relaxed linear association between them. The models account for particularities of each sampling protocol by including terms that affect each observation process, i.e. accounting for differences in observation units and detectability. Bayesian inference is performed using the Integrated Nested Laplace Approximation (INLA) and the Stochastic Partial Differential Equation (SPDE) approach for spatial modeling. We also present the results of a simulation study based on the empirical census data from mid-Scandinavia to assess the performance of the models under model misspecification. Finally, maps of the total expected abundance of birds in our study region in mid-Scandinavia are presented with uncertainty estimates.

</details>

<details>

<summary>2022-04-07 15:52:24 - Bidimensional linked matrix factorization for pan-omics pan-cancer analysis</summary>

- *Eric F. Lock, Jun Young Park, Katherine A. Hoadley*

- `2002.02601v2` - [abs](http://arxiv.org/abs/2002.02601v2) - [pdf](http://arxiv.org/pdf/2002.02601v2)

> Several modern applications require the integration of multiple large data matrices that have shared rows and/or columns. For example, cancer studies that integrate multiple omics platforms across multiple types of cancer, pan-omics pan-cancer analysis, have extended our knowledge of molecular heterogenity beyond what was observed in single tumor and single platform studies. However, these studies have been limited by available statistical methodology. We propose a flexible approach to the simultaneous factorization and decomposition of variation across such bidimensionally linked matrices, BIDIFAC+. This decomposes variation into a series of low-rank components that may be shared across any number of row sets (e.g., omics platforms) or column sets (e.g., cancer types). This builds on a growing literature for the factorization and decomposition of linked matrices, which has primarily focused on multiple matrices that are linked in one dimension (rows or columns) only. Our objective function extends nuclear norm penalization, is motivated by random matrix theory, gives an identifiable decomposition under relatively mild conditions, and can be shown to give the mode of a Bayesian posterior distribution. We apply BIDIFAC+ to pan-omics pan-cancer data from TCGA, identifying shared and specific modes of variability across 4 different omics platforms and 29 different cancer types.

</details>

<details>

<summary>2022-04-07 16:53:28 - Synthesizing longitudinal cortical thickness estimates with a flexible and hierarchical multivariate measurement-error model</summary>

- *Jesse W. Birchfield, Nicholas J. Tustison, Andrew J. Holbrook*

- `2204.03576v1` - [abs](http://arxiv.org/abs/2204.03576v1) - [pdf](http://arxiv.org/pdf/2204.03576v1)

> MRI-based entorhinal cortical thickness (eCT) measurements predict cognitive decline in Alzheimer's disease (AD) with low cost and minimal invasiveness. Two prominent imaging paradigms, FreeSurfer (FS) and Advanced Normalization Tools (ANTs), feature multiple pipelines for extracting region-specific eCT measurements from raw MRI, but the sheer complexity of these pipelines makes it difficult to choose between pipelines, compare results between pipelines, and characterize uncertainty in pipeline estimates. Worse yet, the EC is particularly difficult to image, leading to variations in thickness estimates between pipelines that overwhelm physiologicl variations predictive of AD. We examine the eCT outputs of seven different pipelines on MRIs from the Alzheimer's Disease Neuroimaging Initiative. Because of both theoretical and practical limitations, we have no gold standard by which to evaluate them. Instead, we use a Bayesian hierarchical model to combine the estimates. The resulting posterior distribution yields high-probability idealized eCT values that account for inherent uncertainty through a flexible multivariate error model that supports different constant offsets, standard deviations, tailedness, and correlation structures between pipelines. Our hierarchical model directly relates idealized eCTs to clinical outcomes in a way that propagates eCT estimation uncertainty to clinical estimates while accounting for longitudinal structure in the data. Surprisingly, even though it incorporates greater uncertainty in the predictor and regularization provided by the prior, the combined model reveals a stronger association between eCT and cognitive capacity than do nonhierarchical models based on data from single pipelines alone.

</details>

<details>

<summary>2022-04-08 01:00:49 - Divergent Effects of Factors on Crashes under Autonomous and Conventional Driving Modes Using A Hierarchical Bayesian Approach</summary>

- *Weixi Ren, Bo Yu, Yuren Chen, Kun Gao, Shan Bao*

- `2108.02422v2` - [abs](http://arxiv.org/abs/2108.02422v2) - [pdf](http://arxiv.org/pdf/2108.02422v2)

> Influencing factors on crashes involved with autonomous vehicles (AVs) have been paid increasing attention. However, there is a lack of comparative analyses between influencing factors on crashes of AVs and human-driven vehicles. To fill this research gap, the study aims to explore the divergent effects of factors on crashes under autonomous and conventional driving modes. This study obtained 154 publicly available autonomous vehicle crash data (70 for the autonomous driving mode and 84 for the conventional driving mode), and 36 explanatory variables were extracted from three categories, including environment, roads, and vehicles. Then, a hierarchical Bayesian approach was applied to analyze the impacting factors on crash type and severity under both driving modes. The results showed that some factors affected both driving modes, but their degrees were different. For example, the presence of turning movement had a greater impact on the crash severity under the conventional driving mode, while the presence of turning movement led to a larger decrease in the likelihood of rear-end crashes under the autonomous driving mode. More influencing factors only had a significant impact on one of the driving modes. For example, in the autonomous driving mode, two sidewalks decreased the severity of crashes, and on-street parking was positively associated with rear-end crashes, but they were not significant in the conventional driving mode. This study could contribute to the understanding and development of autonomous driving systems and the better coordination between autonomous driving and conventional driving.

</details>

<details>

<summary>2022-04-08 13:15:53 - Transformation-Invariant Learning of Optimal Individualized Decision Rules with Time-to-Event Outcomes</summary>

- *Yu Zhou, Lan Wang, Rui Song, Tuoyi Zhao*

- `2204.04052v1` - [abs](http://arxiv.org/abs/2204.04052v1) - [pdf](http://arxiv.org/pdf/2204.04052v1)

> In many important applications of precision medicine, the outcome of interest is time to an event (e.g., death, relapse of disease) and the primary goal is to identify the optimal individualized decision rule (IDR) to prolong survival time. Existing work in this area have been mostly focused on estimating the optimal IDR to maximize the We propose a new robust framework for estimating an optimal static or dynamic IDR with time-to-event outcomes based on an easy-to-interpret quantile criterion. The new method does not need to specify an outcome regression model and is robust for heavy-tailed distribution. The estimation problem corresponds to a nonregular M-estimation problem with both finite and infinite-dimensional nuisance parameters. Employing advanced empirical process techniques, we establish the statistical theory of the estimated parameter indexing the optimal IDR. Furthermore, we prove a novel result that the proposed approach can consistently estimate the optimal value function under mild conditions even when the optimal IDR is non-unique, which happens in the challenging setting of exceptional laws. We also propose a smoothed resampling procedure for inference. The proposed methods are implemented in the R-package QTOCen. We demonstrate the performance of the proposed new methods via extensive Monte Carlo studies and a real data application.restricted mean survival time in the population.

</details>

<details>

<summary>2022-04-08 13:43:08 - General Bayesian $L^2$ calibration of mathematical models</summary>

- *Antony M. Overstall, James M. McGree*

- `2103.01132v2` - [abs](http://arxiv.org/abs/2103.01132v2) - [pdf](http://arxiv.org/pdf/2103.01132v2)

> A general Bayesian method for $L^2$ calibration of a mathematical model is presented. General Bayesian inference starts with the specification of a loss function. Then, the log-likelihood in Bayes' theorem is replaced by the negative loss. While the minimiser of the loss function is unchanged by, for example, multiplying the loss by a constant, the same is not true of the resulting general posterior distribution. To address this problem in the context of $L^2$ calibration of mathematical models, different automatic scalings of the general Bayesian posterior are proposed. These are based on equating asymptotic properties of the general Bayesian posterior and the minimiser of the $L^2$ loss. The approach is validated and compared to traditional Bayesian calibration on a range of synthetic and real applications.

</details>

<details>

<summary>2022-04-08 14:14:53 - Covariance-Free Sparse Bayesian Learning</summary>

- *Alexander Lin, Andrew H. Song, Berkin Bilgic, Demba Ba*

- `2105.10439v2` - [abs](http://arxiv.org/abs/2105.10439v2) - [pdf](http://arxiv.org/pdf/2105.10439v2)

> Sparse Bayesian learning (SBL) is a powerful framework for tackling the sparse coding problem while also providing uncertainty quantification. The most popular inference algorithms for SBL exhibit prohibitively large computational costs for high-dimensional problems due to the need to maintain a large covariance matrix. To resolve this issue, we introduce a new method for accelerating SBL inference -- named covariance-free expectation maximization (CoFEM) -- that avoids explicit computation of the covariance matrix. CoFEM solves multiple linear systems to obtain unbiased estimates of the posterior statistics needed by SBL. This is accomplished by exploiting innovations from numerical linear algebra such as preconditioned conjugate gradient and a little-known diagonal estimation rule. For a large class of compressed sensing matrices, we provide theoretical justifications for why our method scales well in high-dimensional settings. Through simulations, we show that CoFEM can be up to thousands of times faster than existing baselines without sacrificing coding accuracy. Through applications to calcium imaging deconvolution and multi-contrast MRI reconstruction, we show that CoFEM enables SBL to tractably tackle high-dimensional sparse coding problems of practical interest.

</details>

<details>

<summary>2022-04-08 18:57:00 - Assessing Statistical Disclosure Risk for Differentially Private, Hierarchical Count Data, with Application to the 2020 U.S. Decennial Census</summary>

- *Zeki Kazan, Jerome Reiter*

- `2204.04253v1` - [abs](http://arxiv.org/abs/2204.04253v1) - [pdf](http://arxiv.org/pdf/2204.04253v1)

> We propose Bayesian methods to assess the statistical disclosure risk of data released under zero-concentrated differential privacy, focusing on settings with a strong hierarchical structure and categorical variables with many levels. Risk assessment is performed by hypothesizing Bayesian intruders with various amounts of prior information and examining the distance between their posteriors and priors. We discuss applications of these risk assessment methods to differentially private data releases from the 2020 decennial census and perform simulation studies using public individual-level data from the 1940 decennial census. Among these studies, we examine how the data holder's choice of privacy parameter affects the disclosure risk and quantify the increase in risk when a hypothetical intruder incorporates substantial amounts of hierarchical information.

</details>

<details>

<summary>2022-04-08 19:39:37 - Conformalized Frequency Estimation from Sketched Data</summary>

- *Matteo Sesia, Stefano Favaro*

- `2204.04270v1` - [abs](http://arxiv.org/abs/2204.04270v1) - [pdf](http://arxiv.org/pdf/2204.04270v1)

> A flexible conformal inference method is developed to construct confidence intervals for the frequencies of queried objects in a very large data set, based on the information contained in a much smaller sketch of those data. The approach is completely data-adaptive and makes no use of any knowledge of the population distribution or of the inner workings of the sketching algorithm; instead, it constructs provably valid frequentist confidence intervals under the sole assumption of data exchangeability. Although the proposed solution is much more broadly applicable, this paper explicitly demonstrates its use in combination with the famous count-min sketch algorithm and a non-linear variation thereof to facilitate the exposition. The performance is compared to that of existing frequentist and Bayesian alternatives through several experiments with synthetic data as well as with real data sets consisting of SARS-CoV-2 DNA sequences and classic English literature.

</details>

<details>

<summary>2022-04-09 02:52:07 - Multi-output calibration of a honeycomb seal via on-site surrogates</summary>

- *Jiangeng Huang, Robert B. Gramacy*

- `2102.00391v3` - [abs](http://arxiv.org/abs/2102.00391v3) - [pdf](http://arxiv.org/pdf/2102.00391v3)

> We consider large-scale industrial computer model calibration, combining multi-output simulation with limited physical observation, involved in the development of a honeycomb seal. Toward that end, we adopt a localized sampling and emulation strategy called "on-site surrogates (OSSs)", designed to cope with the amalgamated challenges of high-dimensional inputs, large-scale simulation campaigns, and nonstationary response surfaces. In previous applications, OSSs were one-at-a-time affairs for multiple outputs leading to dissonance in calibration efforts for a common parameter set across outputs for the honeycomb. We demonstrate that a principal-components representation, adapted from ordinary Gaussian process surrogate modeling to the OSS setting, can resolve this tension. With a two-pronged - optimization-based and fully Bayesian - approach, we show how pooled information across outputs can reduce uncertainty and enhance (statistical and computational) efficiency in calibrated parameters for the honeycomb relative to the previous, "data-poor" univariate analog.

</details>

<details>

<summary>2022-04-09 16:41:13 - Generalised Gaussian Process Latent Variable Models (GPLVM) with Stochastic Variational Inference</summary>

- *Vidhi Lalchand, Aditya Ravuri, Neil D. Lawrence*

- `2202.12979v2` - [abs](http://arxiv.org/abs/2202.12979v2) - [pdf](http://arxiv.org/pdf/2202.12979v2)

> Gaussian process latent variable models (GPLVM) are a flexible and non-linear approach to dimensionality reduction, extending classical Gaussian processes to an unsupervised learning context. The Bayesian incarnation of the GPLVM Titsias and Lawrence, 2010] uses a variational framework, where the posterior over latent variables is approximated by a well-behaved variational family, a factorized Gaussian yielding a tractable lower bound. However, the non-factories ability of the lower bound prevents truly scalable inference. In this work, we study the doubly stochastic formulation of the Bayesian GPLVM model amenable with minibatch training. We show how this framework is compatible with different latent variable formulations and perform experiments to compare a suite of models. Further, we demonstrate how we can train in the presence of massively missing data and obtain high-fidelity reconstructions. We demonstrate the model's performance by benchmarking against the canonical sparse GPLVM for high-dimensional data examples.

</details>

<details>

<summary>2022-04-10 02:41:27 - Bayesian Learning of Graph Substructures</summary>

- *Willem van den Boom, Maria De Iorio, Alexandros Beskos*

- `2203.11664v2` - [abs](http://arxiv.org/abs/2203.11664v2) - [pdf](http://arxiv.org/pdf/2203.11664v2)

> Graphical models provide a powerful methodology for learning the conditional independence structure in multivariate data. Inference is often focused on estimating individual edges in the latent graph. Nonetheless, there is increasing interest in inferring more complex structures, such as communities, for multiple reasons, including more effective information retrieval and better interpretability. Stochastic blockmodels offer a powerful tool to detect such structure in a network. We thus propose to exploit advances in random graph theory and embed them within the graphical models framework. A consequence of this approach is the propagation of the uncertainty in graph estimation to large-scale structure learning. We consider Bayesian nonparametric stochastic blockmodels as priors on the graph. We extend such models to consider clique-based blocks and to multiple graph settings introducing a novel prior process based on a dependent Dirichlet process. Moreover, we devise a tailored computation strategy of Bayes factors for block structure based on the Savage-Dickey ratio to test for presence of larger structure in a graph. We demonstrate our approach in simulations as well as on real data applications in finance and transcriptomics.

</details>

<details>

<summary>2022-04-10 04:45:29 - Bandit Change-Point Detection for Real-Time Monitoring High-Dimensional Data Under Sampling Control</summary>

- *Wanrong Zhang, Yajun Mei*

- `2009.11891v2` - [abs](http://arxiv.org/abs/2009.11891v2) - [pdf](http://arxiv.org/pdf/2009.11891v2)

> In many real-world problems of real-time monitoring high-dimensional streaming data, one wants to detect an undesired event or change quickly once it occurs, but under the sampling control constraint in the sense that one might be able to only observe or use selected components data for decision-making per time step in the resource-constrained environments. In this paper, we propose to incorporate multi-armed bandit approaches into sequential change-point detection to develop an efficient bandit change-point detection algorithm based on the limiting Bayesian approach to incorporate a prior knowledge of potential changes. Our proposed algorithm, termed Thompson-Sampling-Shiryaev-Roberts-Pollak (TSSRP), consists of two policies per time step: the adaptive sampling policy applies the Thompson Sampling algorithm to balance between exploration for acquiring long-term knowledge and exploitation for immediate reward gain, and the statistical decision policy fuses the local Shiryaev-Roberts-Pollak statistics to determine whether to raise a global alarm by sum shrinkage techniques. Extensive numerical simulations and case studies demonstrate the statistical and computational efficiency of our proposed TSSRP algorithm.

</details>

<details>

<summary>2022-04-10 12:52:02 - Parallelized integrated nested Laplace approximations for fast Bayesian inference</summary>

- *Lisa Gaedke-Merzhuser, Janet van Niekerk, Olaf Schenk, Hvard Rue*

- `2204.04678v1` - [abs](http://arxiv.org/abs/2204.04678v1) - [pdf](http://arxiv.org/pdf/2204.04678v1)

> There is a growing demand for performing larger-scale Bayesian inference tasks, arising from greater data availability and higher-dimensional model parameter spaces. In this work we present parallelization strategies for the methodology of integrated nested Laplace approximations (INLA), a popular framework for performing approximate Bayesian inference on the class of Latent Gaussian models. Our approach makes use of nested OpenMP parallelism, a parallel line search procedure using robust regression in INLA's optimization phase and the state-of-the-art sparse linear solver PARDISO. We leverage mutually independent function evaluations in the algorithm as well as advanced sparse linear algebra techniques. This way we can flexibly utilize the power of today's multi-core architectures. We demonstrate the performance of our new parallelization scheme on a number of different real-world applications. The introduction of parallelism leads to speedups of a factor 10 and more for all larger models. Our work is already integrated in the current version of the open-source R-INLA package, making its improved performance conveniently available to all users.

</details>

<details>

<summary>2022-04-10 20:07:23 - Information-theoretic Online Memory Selection for Continual Learning</summary>

- *Shengyang Sun, Daniele Calandriello, Huiyi Hu, Ang Li, Michalis Titsias*

- `2204.04763v1` - [abs](http://arxiv.org/abs/2204.04763v1) - [pdf](http://arxiv.org/pdf/2204.04763v1)

> A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the \textit{surprise} and the \textit{learnability} criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of \textit{the timing to update the memory}, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy.

</details>

<details>

<summary>2022-04-11 01:20:35 - RMFGP: Rotated Multi-fidelity Gaussian process with Dimension Reduction for High-dimensional Uncertainty Quantification</summary>

- *Jiahao Zhang, Shiqi Zhang, Guang Lin*

- `2204.04819v1` - [abs](http://arxiv.org/abs/2204.04819v1) - [pdf](http://arxiv.org/pdf/2204.04819v1)

> Multi-fidelity modelling arises in many situations in computational science and engineering world. It enables accurate inference even when only a small set of accurate data is available. Those data often come from a high-fidelity model, which is computationally expensive. By combining the realizations of the high-fidelity model with one or more low-fidelity models, the multi-fidelity method can make accurate predictions of quantities of interest. This paper proposes a new dimension reduction framework based on rotated multi-fidelity Gaussian process regression and a Bayesian active learning scheme when the available precise observations are insufficient. By drawing samples from the trained rotated multi-fidelity model, the so-called supervised dimension reduction problems can be solved following the idea of the sliced average variance estimation (SAVE) method combined with a Gaussian process regression dimension reduction technique. This general framework we develop can effectively solve high-dimensional problems while the data are insufficient for applying traditional dimension reduction methods. Moreover, a more accurate surrogate Gaussian process model of the original problem can be obtained based on our trained model. The effectiveness of the proposed rotated multi-fidelity Gaussian process(RMFGP) model is demonstrated in four numerical examples. The results show that our method has better performance in all cases and uncertainty propagation analysis is performed for last two cases involving stochastic partial differential equations.

</details>

<details>

<summary>2022-04-11 01:27:41 - Markov categories, causal theories, and the do-calculus</summary>

- *Yimu Yin, Jiji Zhang*

- `2204.04821v1` - [abs](http://arxiv.org/abs/2204.04821v1) - [pdf](http://arxiv.org/pdf/2204.04821v1)

> We give a category-theoretic treatment of causal models that formalizes the syntax for causal reasoning over a directed acyclic graph (DAG) by associating a free Markov category with the DAG in a canonical way. This framework enables us to define and study important concepts in causal reasoning from an abstract and "purely causal" point of view, such as causal independence/separation, causal conditionals, and decomposition of intervention effects. Our results regarding these concepts abstract away from the details of the commonly adopted causal models such as (recursive) structural equation models or causal Bayesian networks. They are therefore more widely applicable and in a way conceptually clearer. Our results are also intimately related to Judea Pearl's celebrated do-calculus, and yield a syntactic version of a core part of the calculus that is inherited in all causal models. In particular, it induces a simpler and specialized version of Pearl's do-calculus in the context of causal Bayesian networks, which we show is as strong as the full version.

</details>

<details>

<summary>2022-04-11 02:59:13 - Nonparametric Bayes Differential Analysis of Multigroup DNA Methylation Data</summary>

- *Chiyu Gu, Veerabhadran Baladandayuthapani, Subharup Guha*

- `2204.04840v1` - [abs](http://arxiv.org/abs/2204.04840v1) - [pdf](http://arxiv.org/pdf/2204.04840v1)

> DNA methylation datasets in cancer studies are comprised of sample measurements on a large number of genomic locations called cytosine-phosphate-guanine (CpG) sites with complex correlation structures. A fundamental goal of these investigations is the development of statistical techniques that identify disease genomic signatures across multiple patient groups determined by different experimental or biological conditions. We propose BayesDiff, a nonparametric Bayesian approach for differential analysis relying on a novel class of first order mixture models called the Sticky Poisson-Dirichlet process or two-restaurant two-cuisine franchise (2R2CF). The BayesDiff methodology flexibly utilizes information from all CpG sites, adaptively accommodating any serial dependence in the data corresponding to the widely varying inter-probe distances, to perform simultaneous inferences about the differential genomic signature of the patient groups. In simulation studies, we demonstrate the effectiveness of the BayesDiff procedure relative to existing statistical techniques for differential DNA methylation. The methodology is applied to analyze a gastrointestinal (GI) cancer DNA methylation dataset that displays both serial correlations and interaction patterns. The results support and complement known aspects of DNA methylation and gene association in upper GI cancers.

</details>

<details>

<summary>2022-04-11 03:04:53 - Nonparametric Bayes Differential Analysis for Dependent Multigroup Data with Application to DNA Methylation Analyses in Cancer</summary>

- *Chiyu Gu, Veerabhadran Baladandayuthapani, Subharup Guha*

- `1710.10713v6` - [abs](http://arxiv.org/abs/1710.10713v6) - [pdf](http://arxiv.org/pdf/1710.10713v6)

> Modern cancer genomics datasets involve widely varying sizes and scales, measurement variables, and correlation structures. A fundamental analytical goal in these high-throughput studies is the development of general statistical techniques that can cleanly sift the signal from noise in identifying disease-specific genomic signatures across a set of experimental or biological conditions. We propose BayesDiff, a nonparametric Bayesian approach based on a novel class of first order mixture models, called the Sticky Poisson-Dirichlet process or multicuisine restaurant franchise. The BayesDiff methodology flexibly utilizes information from all the measurements and adaptively accommodates any serial dependence in the data, accounting for the inter-probe distances, to perform simultaneous inferences on the variables. The technique is applied to analyze a DNA methylation gastrointestinal (GI) cancer dataset, which displays both serial correlations and complex interaction patterns. Our analyses and results both support and complement known aspects of DNA methylation and gene association in upper GI cancers. In simulation studies, we demonstrate the effectiveness of the BayesDiff procedure relative to existing techniques for differential DNA methylation.

</details>

<details>

<summary>2022-04-11 09:51:14 - Empirical Bayes inference for the block maxima method</summary>

- *Simone A Padoan, Stefano Rizzelli*

- `2204.04981v1` - [abs](http://arxiv.org/abs/2204.04981v1) - [pdf](http://arxiv.org/pdf/2204.04981v1)

> The block maxima method is one of the most popular approaches for extreme value analysis with independent and identically distributed observations in the domain of attraction of an extreme value distribution. The lack of a rigorous study on the Bayesian inference in this context has limited its use for statistical analysis of extremes. In this paper we propose an empirical Bayes procedure for inference on the block maxima law and its related quantities. We show that the posterior distributions of the tail index of the data distribution and of the return levels (representative of future extreme episodes) satisfy a number of important theoretical properties. These guarantee the reliability of posterior-based inference and extend to the posterior predictive distribution, the key tool in Bayesian probabilistic forecasting. Posterior computations are readily obtained via an efficient adaptive Metropolis-Hasting type of algorithm. Simulations show its excellent inferential performances already with modest sample sizes. The utility of our proposal is showcased analysing extreme winds generated by hurricanes in the Atlantic basin.

</details>

<details>

<summary>2022-04-11 12:31:10 - Bayesian Quantile Matching Estimation</summary>

- *Rajbir-Singh Nirwan, Nils Bertschinger*

- `2008.06423v2` - [abs](http://arxiv.org/abs/2008.06423v2) - [pdf](http://arxiv.org/pdf/2008.06423v2)

> Due to increased awareness of data protection and corresponding laws many data, especially involving sensitive personal information, are not publicly accessible. Accordingly, many data collecting agencies only release aggregated data, e.g. providing the mean and selected quantiles of population distributions. Yet, research and scientific understanding, e.g. for medical diagnostics or policy advice, often relies on data access. To overcome this tension, we propose a Bayesian method for learning from quantile information. Being based on order statistics of finite samples our method adequately and correctly reflects the uncertainty of empirical quantiles. After outlining the theory, we apply our method to simulated as well as real world examples. In addition, we provide a python-based package that implements the proposed model.

</details>

<details>

<summary>2022-04-11 16:19:00 - Bayesian inferences on uncertain ranks and orderings: Application to ranking players and lineups</summary>

- *Andres F. Barrientos, Deborshee Sen, Garritt L Page, David B Dunson*

- `1907.04842v4` - [abs](http://arxiv.org/abs/1907.04842v4) - [pdf](http://arxiv.org/pdf/1907.04842v4)

> It is common to be interested in rankings or order relationships among entities. In complex settings where one does not directly measure a univariate statistic upon which to base ranks, such inferences typically rely on statistical models having entity-specific parameters. These can be treated as random effects in hierarchical models characterizing variation among the entities. In this paper, we are particularly interested in the problem of ranking basketball players in terms of their contribution to team performance. Using data from the United States National Basketball Association (NBA), we find that many players have similar latent ability levels, making any single estimated ranking highly misleading. The current literature fails to provide summaries of order relationships that adequately account for uncertainty. Motivated by this, we propose a Bayesian strategy for characterizing uncertainty in inferences on order relationships among players and lineups. Our approach adapts to scenarios in which uncertainty in ordering is high by producing more conservative results that improve interpretability. This is achieved through a reward function within a decision theoretic framework. We apply our approach to data from the 2009-10 NBA season.

</details>

<details>

<summary>2022-04-11 16:22:51 - Mixture-of-experts VAEs can disregard variation in surjective multimodal data</summary>

- *Jannik Wolff, Tassilo Klein, Moin Nabi, Rahul G. Krishnan, Shinichi Nakajima*

- `2204.05229v1` - [abs](http://arxiv.org/abs/2204.05229v1) - [pdf](http://arxiv.org/pdf/2204.05229v1)

> Machine learning systems are often deployed in domains that entail data from multiple modalities, for example, phenotypic and genotypic characteristics describe patients in healthcare. Previous works have developed multimodal variational autoencoders (VAEs) that generate several modalities. We consider subjective data, where single datapoints from one modality (such as class labels) describe multiple datapoints from another modality (such as images). We theoretically and empirically demonstrate that multimodal VAEs with a mixture of experts posterior can struggle to capture variability in such surjective data.

</details>

<details>

<summary>2022-04-11 17:34:11 - Uncertainty quantification and estimation in differential dynamic microscopy</summary>

- *Mengyang Gu, Yimin Luo, Yue He, Matthew E. Helgeson, Megan T. Valentine*

- `2105.01200v4` - [abs](http://arxiv.org/abs/2105.01200v4) - [pdf](http://arxiv.org/pdf/2105.01200v4)

> Differential dynamic microscopy (DDM) is a form of video image analysis that combines the sensitivity of scattering and the direct visualization benefits of microscopy. DDM is broadly useful in determining dynamical properties including the intermediate scattering function for many spatiotemporally correlated systems. Despite its straightforward analysis, DDM has not been fully adopted as a routine characterization tool, largely due to computational cost and lack of algorithmic robustness. We present statistical analysis that quantifies the noise, reduces the computational order and enhances the robustness of DDM analysis. We propagate the image noise through the Fourier analysis, which allows us to comprehensively study the bias in different estimators of model parameters, and we derive a different way to detect whether the bias is negligible. Furthermore, through use of Gaussian process regression (GPR), we find that predictive samples of the image structure function require only around 0.5%-5% of the Fourier transforms of the observed quantities. This vastly reduces computational cost, while preserving information of the quantities of interest, such as quantiles of the image scattering function, for subsequent analysis. The approach, which we call DDM with uncertainty quantification (DDM-UQ), is validated using both simulations and experiments with respect to accuracy and computational efficiency, as compared with conventional DDM and multiple particle tracking. Overall, we propose that DDM-UQ lays the foundation for important new applications of DDM, as well as to high-throughput characterization. We implement the fast computation tool in a new, publicly available MATLAB software package.

</details>

<details>

<summary>2022-04-11 17:57:53 - Hierarchical Bayesian Persuasion: Importance of Vice Presidents</summary>

- *Majid Mahzoon*

- `2204.05304v1` - [abs](http://arxiv.org/abs/2204.05304v1) - [pdf](http://arxiv.org/pdf/2204.05304v1)

> We study strategic information transmission in a hierarchical setting where information gets transmitted through a chain of agents up to a decision maker whose action is of importance to every agent. This situation could arise whenever an agent can communicate to the decision maker only through a chain of intermediaries, for example, an entry-level worker and the CEO in a firm, or an official in the bottom of the chain of command and the president in a government. Each agent can decide to conceal part or all the information she receives. Proving we can focus on simple equilibria, where the only player who conceals information is the first one, we provide a tractable recursive characterization of the equilibrium outcome, and show that it could be inefficient. Interestingly, in the binary-action case, regardless of the number of intermediaries, there are a few pivotal ones who determine the amount of information communicated to the decision maker. In this case, our results underscore the importance of choosing a pivotal vice president for maximizing the payoff of the CEO or president.

</details>

<details>

<summary>2022-04-12 05:27:40 - Online Structural Change-point Detection of High-dimensional Streaming Data via Dynamic Sparse Subspace Learning</summary>

- *Ruiyu Xu, Jianguo Wu, Xiaowei Yue, Yongxiang Li*

- `2009.11713v3` - [abs](http://arxiv.org/abs/2009.11713v3) - [pdf](http://arxiv.org/pdf/2009.11713v3)

> High-dimensional streaming data are becoming increasingly ubiquitous in many fields. They often lie in multiple low-dimensional subspaces, and the manifold structures may change abruptly on the time scale due to pattern shift or occurrence of anomalies. However, the problem of detecting the structural changes in a real-time manner has not been well studied. To fill this gap, we propose a dynamic sparse subspace learning approach for online structural change-point detection of high-dimensional streaming data. A novel multiple structural change-point model is proposed and the asymptotic properties of the estimators are investigated. A tuning method based on Bayesian information criterion and change-point detection accuracy is proposed for penalty coefficients selection. An efficient Pruned Exact Linear Time based algorithm is proposed for online optimization and change-point detection. The effectiveness of the proposed method is demonstrated through several simulation studies and a real case study on gesture data for motion tracking.

</details>

<details>

<summary>2022-04-12 08:02:07 - Strategic model reduction by analysing model sloppiness: a case study in coral calcification</summary>

- *Sarah A. Vollert, Christopher Drovandi, Gloria M. Monsalve-Bravo, Matthew P. Adams*

- `2204.05602v1` - [abs](http://arxiv.org/abs/2204.05602v1) - [pdf](http://arxiv.org/pdf/2204.05602v1)

> It can be difficult to identify ways to reduce the complexity of large models whilst maintaining predictive power, particularly where there are hidden parameter interdependencies. Here, we demonstrate that the analysis of model sloppiness can be a new invaluable tool for strategically simplifying complex models. Such an analysis identifies parameter combinations which strongly and/or weakly inform model behaviours, yet the approach has not previously been used to inform model reduction. Using a case study on a coral calcification model calibrated to experimental data, we show how the analysis of model sloppiness can strategically inform model simplifications which maintain predictive power. Additionally, when comparing various approaches to analysing sloppiness, we find that Bayesian methods can be advantageous when unambiguous identification of the best-fit model parameters is a challenge for standard optimisation procedures.

</details>

<details>

<summary>2022-04-12 08:23:18 - Normalized Power Prior Bayesian Analysis</summary>

- *Keying Ye, Zifei Han, Yuyan Duan, Tianyu Bai*

- `2204.05615v1` - [abs](http://arxiv.org/abs/2204.05615v1) - [pdf](http://arxiv.org/pdf/2204.05615v1)

> The elicitation of power priors, based on the availability of historical data, is realized by raising the likelihood function of the historical data to a fractional power {\delta}, which quantifies the degree of discounting of the historical information in making inference with the current data. When {\delta} is not pre-specified and is treated as random, it can be estimated from the data using Bayesian updating paradigm. However, in the original form of the joint power prior Bayesian approach, certain positive constants before the likelihood of the historical data could be multiplied when different settings of sufficient statistics are employed. This would change the power priors with different constants, and hence the likelihood principle is violated. In this article, we investigate a normalized power prior approach which obeys the likelihood principle and is a modified form of the joint power prior. The optimality properties of the normalized power prior in the sense of minimizing the weighted Kullback-Leibler divergence is investigated. By examining the posteriors of several commonly used distributions, we show that the discrepancy between the historical and the current data can be well quantified by the power parameter under the normalized power prior setting. Efficient algorithms to compute the scale factor is also proposed. In addition, we illustrate the use of the normalized power prior Bayesian analysis with three data examples, and provide an implementation with an R package NPP.

</details>

<details>

<summary>2022-04-12 10:30:44 - A Bayesian spatio-temporal analysis of markets during the Finnish 1860s famine</summary>

- *Tiia-Maria Pasanen, Miikka Voutilainen, Jouni Helske, Harri Hgmander*

- `2106.06268v3` - [abs](http://arxiv.org/abs/2106.06268v3) - [pdf](http://arxiv.org/pdf/2106.06268v3)

> We develop a Bayesian spatio-temporal model to study pre-industrial grain market integration during the Finnish famine of the 1860s. Our model takes into account several problematic features often present when analysing multiple spatially interdependent time series. For example, compared with the error correction methodology commonly applied in econometrics, our approach allows simultaneous modelling of multiple interdependent time series avoiding cumbersome statistical testing needed to predetermine the market leader as a point of reference. Furthermore, introducing a flexible spatio-temporal structure enables analysing detailed regional and temporal dynamics of the market mechanisms. Applying the proposed method, we detected spatially asymmetric "price ripples" that spread out from the shock origin. We corroborated the existing literature on the speedier adjustment to emerging price differentials during the famine, but we observed this principally in urban markets. This hastened return to long-run equilibrium means faster and longer travel of price shocks, implying prolonged out-of-equilibrium dynamics, proliferated influence of market shocks, and, importantly, a wider spread of famine conditions.

</details>

<details>

<summary>2022-04-12 18:29:31 - Approximate Bayesian Computation via Classification</summary>

- *Yuexi Wang, Tetsuya Kaji, Veronika Rokov*

- `2111.11507v3` - [abs](http://arxiv.org/abs/2111.11507v3) - [pdf](http://arxiv.org/pdf/2111.11507v3)

> Approximate Bayesian Computation (ABC) enables statistical inference in simulator-based models whose likelihoods are difficult to calculate but easy to simulate from. ABC constructs a kernel-type approximation to the posterior distribution through an accept/reject mechanism which compares summary statistics of real and simulated data. To obviate the need for summary statistics, we directly compare empirical distributions with a Kullback-Leibler (KL) divergence estimator obtained via contrastive learning. In particular, we blend flexible machine learning classifiers within ABC to automate fake/real data comparisons. We consider the traditional accept/reject kernel as well as an exponential weighting scheme which does not require the ABC acceptance threshold. Our theoretical results show that the rate at which our ABC posterior distributions concentrate around the true parameter depends on the estimation error of the classifier. We derive limiting posterior shape results and find that, with a properly scaled exponential kernel, asymptotic normality holds. We demonstrate the usefulness of our approach on simulated examples as well as real data in the context of stock volatility estimation.

</details>

<details>

<summary>2022-04-12 18:36:53 - Evolutionary shift detection with ensemble variable selection</summary>

- *Wensha Zhang, Toby Kenney, Lam Si Tung Ho*

- `2204.06032v1` - [abs](http://arxiv.org/abs/2204.06032v1) - [pdf](http://arxiv.org/pdf/2204.06032v1)

> 1. Abrupt environmental changes can lead to evolutionary shifts in trait evolution. Identifying these shifts is an important step in understanding the evolutionary history of phenotypes.   2. We propose an ensemble variable selection method (R package ELPASO) for the evolutionary shift detection task and compare it with existing methods (R packages l1ou and PhylogeneticEM) under several scenarios.   3. The performances of methods are highly dependent on the selection criterion. When the signal sizes are small, the methods using the Bayesian information criterion (BIC) have better performances. And when the signal sizes are large enough, the methods using the phylogenetic Bayesian information criterion (pBIC) (Khabbazian et al., 2016) have better performance. Moreover, the performance is heavily impacted by measurement error and tree reconstruction error.   4. Ensemble method + pBIC tends to perform less conservatively than l1ou + pBIC, and Ensemble method + BIC is more conservatively than l1ou + BIC. PhylogeneticEM is even more conservative with small signal sizes and falls between l1ou + pBIC and Ensemble method + BIC with large signal sizes. The results can differ between the methods, but none clearly outperforms the others. By applying multiple methods to a single dataset, we can access the robustness of each detected shift, based on the agreement among methods.

</details>

<details>

<summary>2022-04-12 19:00:00 - The sparse Polynomial Chaos expansion: a fully Bayesian approach with joint priors on the coefficients and global selection of terms</summary>

- *Paul-Christian Brkner, Ilja Krker, Sergey Oladyshkin, Wolfgang Nowak*

- `2204.06043v1` - [abs](http://arxiv.org/abs/2204.06043v1) - [pdf](http://arxiv.org/pdf/2204.06043v1)

> Polynomial chaos expansion (PCE) is a versatile tool widely used in uncertainty quantification and machine learning, but its successful application depends strongly on the accuracy and reliability of the resulting PCE-based response surface. High accuracy typically requires high polynomial degrees, demanding many training points especially in high-dimensional problems through the curse of dimensionality. So-called sparse PCE concepts work with a much smaller selection of basis polynomials compared to conventional PCE approaches and can overcome the curse of dimensionality very efficiently, but have to pay specific attention to their strategies of choosing training points. Furthermore, the approximation error resembles an uncertainty that most existing PCE-based methods do not estimate. In this study, we develop and evaluate a fully Bayesian approach to establish the PCE representation via joint shrinkage priors and Markov chain Monte Carlo. The suggested Bayesian PCE model directly aims to solve the two challenges named above: achieving a sparse PCE representation and estimating uncertainty of the PCE itself. The embedded Bayesian regularizing via the joint shrinkage prior allows using higher polynomial degrees for given training points due to its ability to handle underdetermined situations, where the number of considered PCE coefficients could be much larger than the number of available training points. We also explore multiple variable selection methods to construct sparse PCE expansions based on the established Bayesian representations, while globally selecting the most meaningful orthonormal polynomials given the available training data. We demonstrate the advantages of our Bayesian PCE and the corresponding sparsity-inducing methods on several benchmarks.

</details>

<details>

<summary>2022-04-12 22:01:08 - Specifying Prior Distributions in Reliability Applications</summary>

- *Qinglong Tian, Colin Lewis-Beck, Jarad Niemi, William Meeker*

- `2204.06099v1` - [abs](http://arxiv.org/abs/2204.06099v1) - [pdf](http://arxiv.org/pdf/2204.06099v1)

> Especially when facing reliability data with limited information (e.g., a small number of failures), there are strong motivations for using Bayesian inference methods. These include the option to use information from physics-of-failure or previous experience with a failure mode in a particular material to specify an informative prior distribution. Another advantage is the ability to make statistical inferences without having to rely on specious (when the number of failures is small) asymptotic theory needed to justify non-Bayesian methods. Users of non-Bayesian methods are faced with multiple methods of constructing uncertainty intervals (Wald, likelihood, and various bootstrap methods) that can give substantially different answers when there is little information in the data. For Bayesian inference, there is only one method -- but it is necessary to provide a prior distribution to fully specify the model. Much work has been done to find default or objective prior distributions that will provide inference methods with good (and in some cases exact) frequentist coverage properties. This paper reviews some of this work and provides, evaluates, and illustrates principled extensions and adaptations of these methods to the practical realities of reliability data (e.g., non-trivial censoring).

</details>

<details>

<summary>2022-04-13 02:26:18 - Efficient Non-parametric Bayesian Hawkes Processes</summary>

- *Rui Zhang, Christian Walder, Marian-Andrei Rizoiu, Lexing Xie*

- `1810.03730v5` - [abs](http://arxiv.org/abs/1810.03730v5) - [pdf](http://arxiv.org/pdf/1810.03730v5)

> In this paper, we develop an efficient nonparametric Bayesian estimation of the kernel function of Hawkes processes. The non-parametric Bayesian approach is important because it provides flexible Hawkes kernels and quantifies their uncertainty. Our method is based on the cluster representation of Hawkes processes. Utilizing the finite support assumption of the Hawkes process, we efficiently sample random branching structures and thus, we split the Hawkes process into clusters of Poisson processes. We derive two algorithms -- a block Gibbs sampler and a maximum a posteriori estimator based on expectation maximization -- and we show that our methods have a linear time complexity, both theoretically and empirically. On synthetic data, we show our methods to be able to infer flexible Hawkes triggering kernels. On two large-scale Twitter diffusion datasets, we show that our methods outperform the current state-of-the-art in goodness-of-fit and that the time complexity is linear in the size of the dataset. We also observe that on diffusions related to online videos, the learned kernels reflect the perceived longevity for different content types such as music or pets videos.

</details>

<details>

<summary>2022-04-13 04:30:37 - A Study on the Power Parameter in Power Prior Bayesian Analysis</summary>

- *Zifei Han, Keying Ye, Min Wang*

- `2204.06165v1` - [abs](http://arxiv.org/abs/2204.06165v1) - [pdf](http://arxiv.org/pdf/2204.06165v1)

> The power prior and its variations have been proven to be a useful class of informative priors in Bayesian inference due to their flexibility in incorporating the historical information by raising the likelihood of the historical data to a fractional power {\delta}. The derivation of the marginal likelihood based on the original power prior,and its variation, the normalized power prior, introduces a scaling factor C({\delta}) in the form of a prior predictive distribution with powered likelihood. In this paper, we show that the scaling factor might be infinite for some positive {\delta} with conventionally used initial priors, which would change the admissible set of the power parameter. This result seems to have been almost completely ignored in the literature. We then illustrate that such a phenomenon may jeopardize the posterior inference under the power priors when the initial prior of the model parameters is improper. The main findings of this paper suggest that special attention should be paid when the suggested level of borrowing is close to 0, while the actual optimum might be below the suggested value. We use a normal linear model as an example for illustrative purposes.

</details>

<details>

<summary>2022-04-13 08:22:31 - Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity</summary>

- *Arber Qoku, Florian Buettner*

- `2204.06242v1` - [abs](http://arxiv.org/abs/2204.06242v1) - [pdf](http://arxiv.org/pdf/2204.06242v1)

> Many real-world systems are described not only by data from a single source but via multiple data views. For example, in genomic medicine, a patient can be described by data from different molecular layers. This raises the need for multi-view models that are able to disentangle variation within and across data views in an interpretable manner. Latent variable models with structured sparsity are a commonly used tool to address this modeling task but interpretability is cumbersome since it requires a direct inspection and interpretation of each factor via a specialized domain expert. Here, we propose MuVI, a novel approach for domain-informed multi-view latent variable models, facilitating the analysis of multi-view data in an inherently explainable manner. We demonstrate that our model (i) is able to integrate noisy domain expertise in form of feature sets, (ii) is robust to noise in the encoded domain knowledge, (iii) results in identifiable factors and (iv) is able to infer interpretable and biologically meaningful axes of variation in a real-world multi-view dataset of cancer patients.

</details>

<details>

<summary>2022-04-13 09:12:42 - Noise, fake news, and tenacious Bayesians</summary>

- *Dorje C. Brody*

- `2110.03432v3` - [abs](http://arxiv.org/abs/2110.03432v3) - [pdf](http://arxiv.org/pdf/2110.03432v3)

> A modelling framework, based on the theory of signal processing, for characterising the dynamics of systems driven by the unravelling of information is outlined, and is applied to describe the process of decision making. The model input of this approach is the specification of the flow of information. This enables the representation of (i) reliable information, (ii) noise, and (iii) disinformation, in a unified framework. Because the approach is designed to characterise the dynamics of the behaviour of people, it is possible to quantify the impact of information control, including those resulting from the dissemination of disinformation. It is shown that if a decision maker assigns an exceptionally high weight on one of the alternative realities, then under the Bayesian logic their perception hardly changes in time even if evidences presented indicate that this alternative corresponds to a false reality. Thus confirmation bias need not be incompatible with Bayesian updating. By observing the role played by noise in other areas of natural sciences, where noise is used to excite the system away from false attractors, a new approach to tackle the dark forces of fake news is proposed.

</details>

<details>

<summary>2022-04-13 09:44:42 - Utilizing variational autoencoders in the Bayesian inverse problem of photoacoustic tomography</summary>

- *Teemu Sahlstrm, Tanja Tarvainen*

- `2204.06270v1` - [abs](http://arxiv.org/abs/2204.06270v1) - [pdf](http://arxiv.org/pdf/2204.06270v1)

> There has been an increasing interest in utilizing machine learning methods in inverse problems and imaging. Most of the work has, however, concentrated on image reconstruction problems, and the number of studies regarding the full solution of the inverse problem is limited. In this work, we study a machine learning based approach for the Bayesian inverse problem of photoacoustic tomography. We develop an approach for estimating the posterior distribution in photoacoustic tomography using an approach based on the variational autoencoder. The approach is evaluated with numerical simulations and compared to the solution of the inverse problem using a Bayesian approach.

</details>

<details>

<summary>2022-04-13 11:36:37 - Investigating the efficiency of marginalising over discrete parameters in Bayesian computations</summary>

- *Wen Zhang, Jeffrey Pullin, Lyle Gurrin, Damjan Vukcevic*

- `2204.06313v1` - [abs](http://arxiv.org/abs/2204.06313v1) - [pdf](http://arxiv.org/pdf/2204.06313v1)

> Bayesian analysis methods often use some form of iterative simulation such as Monte Carlo computation. Models that involve discrete variables can sometime pose a challenge, either because the methods used do not support such variables (e.g. Hamiltonian Monte Carlo) or because the presence of such variables can slow down the computation. A common workaround is to marginalise the discrete variables out of the model. While it is reasonable to expect that such marginalisation would also lead to more time-efficient computations, to our knowledge this has not been demonstrated beyond a few specialised models.   We explored the impact of marginalisation on the computational efficiency for a few simple statistical models. Specifically, we considered two- and three-component Gaussian mixture models, and also the Dawid-Skene model for categorical ratings. We explored each with two software implementations of Markov chain Monte Carlo techniques: JAGS and Stan. We directly compared marginalised and non-marginalised versions of the same model using the samplers on the same software.   Our results show that marginalisation on its own does not necessarily boost performance. Nevertheless, the best performance was usually achieved with Stan, which requires marginalisation. We conclude that there is no simple answer to whether or not marginalisation is helpful. It is not necessarily the case that, when turned 'on', this technique can be assured to provide computational benefit independent of other factors, nor is it likely to be the model component that has the largest impact on computational efficiency.

</details>

<details>

<summary>2022-04-13 11:49:34 - Scalable and Accurate Variational Bayes for High-Dimensional Binary Regression Models</summary>

- *Augusto Fasano, Daniele Durante, Giacomo Zanella*

- `1911.06743v6` - [abs](http://arxiv.org/abs/1911.06743v6) - [pdf](http://arxiv.org/pdf/1911.06743v6)

> Modern methods for Bayesian regression beyond the Gaussian response setting are often computationally impractical or inaccurate in high dimensions. In fact, as discussed in recent literature, bypassing such a trade-off is still an open problem even in routine binary regression models, and there is limited theory on the quality of variational approximations in high-dimensional settings. To address this gap, we study the approximation accuracy of routinely-used mean-field variational Bayes solutions in high-dimensional probit regression with Gaussian priors, obtaining novel and practically relevant results on the pathological behavior of such strategies in uncertainty quantification, point estimation and prediction. Motivated by these results, we further develop a new partially-factorized variational approximation for the posterior of the probit coefficients which leverages a representation with global and local variables but, unlike for classical mean-field assumptions, it avoids a fully factorized approximation, and instead assumes a factorization only for the local variables. We prove that the resulting approximation belongs to a tractable class of unified skew-normal densities that crucially incorporates skewness and, unlike for state-of-the-art mean-field solutions, converges to the exact posterior density as p goes to infinity. To solve the variational optimization problem, we derive a tractable CAVI algorithm that easily scales to p in the tens of thousands, and provably requires a number of iterations converging to 1 as p goes to infinity. Such findings are also illustrated in extensive empirical studies where our novel solution is shown to improve the approximation accuracy of mean-field variational Bayes for any n and p, with the magnitude of these gains being remarkable in those high-dimensional p>n settings where state-of-the-art methods are computationally impractical.

</details>

<details>

<summary>2022-04-13 14:18:54 - Bayesian Integrals on Toric Varieties</summary>

- *Michael Borinsky, Anna-Laura Sattelberger, Bernd Sturmfels, Simon Telen*

- `2204.06414v1` - [abs](http://arxiv.org/abs/2204.06414v1) - [pdf](http://arxiv.org/pdf/2204.06414v1)

> We explore the positive geometry of statistical models in the setting of toric varieties. Our focus lies on models for discrete data that are parameterized in terms of Cox coordinates. We develop a geometric theory for computations in Bayesian statistics, such as evaluating marginal likelihood integrals and sampling from posterior distributions. These are based on a tropical sampling method for evaluating Feynman integrals in physics. We here extend that method from projective spaces to arbitrary toric varieties.

</details>

<details>

<summary>2022-04-13 15:48:45 - A flexible approach for variable selection in large-scale healthcare database studies with missing covariate and outcome data</summary>

- *Jung-Yi Joyce Lin, Liangyuan Hu, Chuyue Huang, Steven Lawrence, Usha Govindarajulu*

- `2107.09730v2` - [abs](http://arxiv.org/abs/2107.09730v2) - [pdf](http://arxiv.org/pdf/2107.09730v2)

> Prior work has shown that combining bootstrap imputation with tree-based machine learning variable selection methods can provide good performances achievable on fully observed data when covariate and outcome data are missing at random (MAR). This approach however is computationally expensive, especially on large-scale datasets. We propose an inference-based method, called RR-BART, which leverages the likelihood-based Bayesian machine learning technique, Bayesian additive regression trees, and uses Rubin's rule to combine the estimates and variances of the variable importance measures on multiply imputed datasets for variable selection in the presence of MAR data. We conduct a representative simulation study to investigate the practical operating characteristics of RR-BART, and compare it with the bootstrap imputation based methods. We further demonstrate the methods via a case study of risk factors for 3-year incidence of metabolic syndrome among middle-aged women using data from the Study of Women's Health Across the Nation (SWAN). The simulation study suggests that even in complex conditions of nonlinearity and nonadditivity with a large percentage of missingness, RR-BART can reasonably recover both prediction and variable selection performances, achievable on the fully observed data. RR-BART provides the best performance that the bootstrap imputation based methods can achieve with the optimal selection threshold value. In addition, RR-BART demonstrates a substantially stronger ability of detecting discrete predictors. Furthermore, RR-BART offers substantial computational savings. When implemented on the SWAN data, RR-BART adds to the literature by selecting a set of predictors that had been less commonly identified as risk factors but had substantial biological justifications.

</details>

<details>

<summary>2022-04-13 18:55:23 - A Stochastic Process Model for Time Warping Functions</summary>

- *Yijia Ma, Xinyu Zhou, Wei Wu*

- `2201.09970v2` - [abs](http://arxiv.org/abs/2201.09970v2) - [pdf](http://arxiv.org/pdf/2201.09970v2)

> Time warping function provides a mathematical representation to measure phase variability in functional data. Recent studies have developed various approaches to estimate optimal warping between functions and provide non-Euclidean models. However, a principled, linear, generative model on time warping functions is still under-explored. This is a highly challenging problem because the space of warping functions is non-linear with the conventional Euclidean metric. To address this problem, we propose a stochastic process model for time warping functions, where the key is to define a linear, inner-product structure on the time warping space and then transform the warping functions into a sub-space of the $\mathbb L^2$ Euclidean space. With certain constraints on the warping functions, this transformation is an isometric isomorphism. In the transformed space, we adopt the $\mathbb L^2$ basis in the Hilbert space for representation. This new framework can easily build generative model on time warping by using different types of stochastic process. It can also be used to conduct statistical inferences such as functional PCA, functional ANOVA, and functional regressions. Furthermore, we demonstrate the effectiveness of this new framework by using it as a new prior in the Bayesian registration, and propose an efficient gradient method to address the important maximum a posteriori estimation. We illustrate the new Bayesian method using simulations which properly characterize nonuniform and correlated constraints in the time domain. Finally, we apply the new framework to the famous Berkeley growth data and obtain reasonable results on modeling, resampling, group comparison, and classification analysis.

</details>

<details>

<summary>2022-04-13 19:02:32 - Infinite Hidden Markov Models for Multiple Multivariate Time Series with Missing Data</summary>

- *Lauren Hoskovec, Matthew D. Koslovsky, Kirsten Koehler, Nicholas Good, Jennifer L. Peel, John Volckens, Ander Wilson*

- `2204.06610v1` - [abs](http://arxiv.org/abs/2204.06610v1) - [pdf](http://arxiv.org/pdf/2204.06610v1)

> Exposure to air pollution is associated with increased morbidity and mortality. Recent technological advancements permit the collection of time-resolved personal exposure data. Such data are often incomplete with missing observations and exposures below the limit of detection, which limit their use in health effects studies. In this paper we develop an infinite hidden Markov model for multiple asynchronous multivariate time series with missing data. Our model is designed to include covariates that can inform transitions among hidden states. We implement beam sampling, a combination of slice sampling and dynamic programming, to sample the hidden states, and a Bayesian multiple imputation algorithm to impute missing data. In simulation studies, our model excels in estimating hidden states and state-specific means and imputing observations that are missing at random or below the limit of detection. We validate our imputation approach on data from the Fort Collins Commuter Study. We show that the estimated hidden states improve imputations for data that are missing at random compared to existing approaches. In a case study of the Fort Collins Commuter Study, we describe the inferential gains obtained from our model including improved imputation of missing data and the ability to identify shared patterns in activity and exposure among repeated sampling days for individuals and among distinct individuals.

</details>

<details>

<summary>2022-04-13 22:31:40 - A Bayesian Machine Learning Approach for Estimating Heterogeneous Survivor Causal Effects: Applications to a Critical Care Trial</summary>

- *Xinyuan Chen, Michael O. Harhay, Guangyu Tong, Fan Li*

- `2204.06657v1` - [abs](http://arxiv.org/abs/2204.06657v1) - [pdf](http://arxiv.org/pdf/2204.06657v1)

> Motivated by the Acute Respiratory Distress Syndrome Network (ARDSNetwork) ARDS respiratory management (ARMA) trial, we developed a flexible Bayesian machine learning approach to estimate the average causal effect and heterogeneous causal effects among the always-survivors stratum when clinical outcomes are subject to truncation. We adopted Bayesian additive regression trees (BART) to flexibly specify separate models for the potential outcomes and latent strata membership. In the analysis of the ARMA trial, we found that the low tidal volume treatment had an overall benefit for participants sustaining acute lung injuries on the outcome of time to returning home, but substantial heterogeneity in treatment effects among the always-survivors, driven most strongly by sex and the alveolar-arterial oxygen gradient at baseline (a physiologic measure of lung function and source of hypoxemia). These findings illustrate how the proposed methodology could guide the prognostic enrichment of future trials in the field. We also demonstrated through a simulation study that our proposed Bayesian machine learning approach outperforms other parametric methods in reducing the estimation bias in both the average causal effect and heterogeneous causal effects for always-survivors.

</details>

<details>

<summary>2022-04-14 00:58:37 - Data Augmentation for Bayesian Deep Learning</summary>

- *Yuexi Wang, Nicholas G. Polson, Vadim O. Sokolov*

- `1903.09668v3` - [abs](http://arxiv.org/abs/1903.09668v3) - [pdf](http://arxiv.org/pdf/1903.09668v3)

> Deep Learning (DL) methods have emerged as one of the most powerful tools for functional approximation and prediction. While the representation properties of DL have been well studied, uncertainty quantification remains challenging and largely unexplored. Data augmentation techniques are a natural approach to provide uncertainty quantification and to incorporate stochastic Monte Carlo search into stochastic gradient descent (SGD) methods. The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains. We use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. This allows variants of the expectation-maximization and MCMC algorithms to be brought to bear on these high dimensional nonlinear deep learning models. To demonstrate our methodology, we develop data augmentation algorithms for a variety of commonly used activation functions: logit, ReLU, leaky ReLU and SVM. Our methodology is compared to traditional stochastic gradient descent with back-propagation. Our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed. We illustrate our methodology on a number of standard datasets. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2022-04-14 02:29:42 - A positive-definiteness-assured block Gibbs sampler for Bayesian graphical models with shrinkage priors</summary>

- *Sakae Oya, Teruo Nakatsuma*

- `2001.04657v5` - [abs](http://arxiv.org/abs/2001.04657v5) - [pdf](http://arxiv.org/pdf/2001.04657v5)

> Although the block Gibbs sampler for the Bayesian graphical LASSO proposed by Wang (2012) has been widely applied and extended to various shrinkage priors in recent years, it has a less noticeable but possibly severe disadvantage that the positive definiteness of a precision matrix in the Gaussian graphical model is not guaranteed in each cycle of the Gibbs sampler. Specifically, if the dimension of the precision matrix exceeds the sample size, the positive definiteness of the precision matrix will be barely satisfied and the Gibbs sampler will almost surely fail. In this paper, we propose modifying the original block Gibbs sampler so that the precision matrix never fails to be positive definite by sampling it exactly from the domain of the positive definiteness. As we have shown in the Monte Carlo experiments, this modification not only stabilizes the sampling procedure but also significantly improves the performance of the parameter estimation and graphical structure learning. We also apply our proposed algorithm to a graphical model of the monthly return data in which the number of stocks exceeds the sample period, demonstrating its stability and scalability.

</details>

<details>

<summary>2022-04-14 07:35:17 - A new avenue for Bayesian inference with INLA</summary>

- *Janet van Niekerk, Elias Krainski, Denis Rustand, Haavard Rue*

- `2204.06797v1` - [abs](http://arxiv.org/abs/2204.06797v1) - [pdf](http://arxiv.org/pdf/2204.06797v1)

> Integrated Nested Laplace Approximations (INLA) has been a successful approximate Bayesian inference framework since its proposal by Rue et al. (2009). The increased computational efficiency and accuracy when compared with sampling-based methods for Bayesian inference like MCMC methods, are some contributors to its success. Ongoing research in the INLA methodology and implementation thereof in the R package R-INLA, ensures continued relevance for practitioners and improved performance and applicability of INLA. The era of big data and some recent research developments, presents an opportunity to reformulate some aspects of the classic INLA formulation, to achieve even faster inference, improved numerical stability and scalability. The improvement is especially noticeable for data-rich models. We demonstrate the efficiency gains with various examples of data-rich models, like Cox's proportional hazards model, an item-response theory model, a spatial model including prediction, and a 3-dimensional model for fMRI data.

</details>

<details>

<summary>2022-04-14 07:59:20 - Objective Bayesian approach to the Jeffreys-Lindley paradox</summary>

- *Andrew Fowlie*

- `2012.04879v2` - [abs](http://arxiv.org/abs/2012.04879v2) - [pdf](http://arxiv.org/pdf/2012.04879v2)

> We consider the Jeffreys-Lindley paradox from an objective Bayesian perspective by attempting to find priors representing complete indifference to sample size in the problem. This means that we ensure that the prior for the unknown mean and the prior predictive for the $t$-statistic are independent of the sample size. If successful, this would lead to Bayesian model comparison that was independent of sample size and ameliorate the paradox. Unfortunately, it leads to an improper scale-invariant prior for the unknown mean. We show, however, that a truncated scale-invariant prior delays the dependence on sample size, which could be practically significant. Lastly, we shed light on the paradox by relating it to the fact that the scale-invariant prior is improper.

</details>

<details>

<summary>2022-04-14 10:55:03 - Sparse Interaction Neighborhood Selection for Markov Random Fields via Reversible Jump and Pseudoposteriors</summary>

- *Victor Freguglia, Nancy Lopes Garcia*

- `2204.05933v2` - [abs](http://arxiv.org/abs/2204.05933v2) - [pdf](http://arxiv.org/pdf/2204.05933v2)

> We consider the problem of estimating the interacting neighborhood of a Markov Random Field model with finite support and homogeneous pairwise interactions based on relative positions of a two-dimensional lattice. Using a Bayesian framework, we propose a Reversible Jump Monte Carlo Markov Chain algorithm that jumps across subsets of a maximal range neighborhood, allowing us to perform model selection based on a marginal pseudoposterior distribution of models.

</details>

<details>

<summary>2022-04-14 11:40:51 - Optimized Population Monte Carlo</summary>

- *Vctor Elvira, milie Chouzenoux*

- `2204.06891v1` - [abs](http://arxiv.org/abs/2204.06891v1) - [pdf](http://arxiv.org/pdf/2204.06891v1)

> Adaptive importance sampling (AIS) methods are increasingly used for the approximation of distributions and related intractable integrals in the context of Bayesian inference. Population Monte Carlo (PMC) algorithms are a subclass of AIS methods, widely used due to their ease in the adaptation. In this paper, we propose a novel algorithm that exploits the benefits of the PMC framework and includes more efficient adaptive mechanisms, exploiting geometric information of the target distribution. In particular, the novel algorithm adapts the location and scale parameters of a set of importance densities (proposals). At each iteration, the location parameters are adapted by combining a versatile resampling strategy (i.e., using the information of previous weighted samples) with an advanced optimization-based scheme. Local second-order information of the target distribution is incorporated through a preconditioning matrix acting as a scaling metric onto a gradient direction. A damped Newton approach is adopted to ensure robustness of the scheme. The resulting metric is also used to update the scale parameters of the proposals. We discuss several key theoretical foundations for the proposed approach. Finally, we show the successful performance of the proposed method in three numerical examples, involving challenging distributions.

</details>

<details>

<summary>2022-04-14 12:13:34 - Robust Bayesian inference in complex models with possibility theory</summary>

- *Jeremie Houssineau, David J. Nott*

- `2204.06911v1` - [abs](http://arxiv.org/abs/2204.06911v1) - [pdf](http://arxiv.org/pdf/2204.06911v1)

> We propose a general solution to the problem of robust Bayesian inference in complex settings where outliers may be present. In practice, the automation of robust Bayesian analyses is important in the many applications involving large and complex datasets. The proposed solution relies on a reformulation of Bayesian inference based on possibility theory, and leverages the observation that, in this context, the marginal likelihood of the data assesses the consistency between prior and likelihood rather than model fitness. Our approach does not require additional parameters in its simplest form and has a limited impact on the computational complexity when compared to non-robust solutions. The generality of our solution is demonstrated via applications on simulated and real data including matrix estimation and change-point detection.

</details>

<details>

<summary>2022-04-14 14:01:11 - Modelling Non-Smooth Signals with Complex Spectral Structure</summary>

- *Wessel P. Bruinsma, Martin Tegnr, Richard E. Turner*

- `2203.06997v2` - [abs](http://arxiv.org/abs/2203.06997v2) - [pdf](http://arxiv.org/pdf/2203.06997v2)

> The Gaussian Process Convolution Model (GPCM; Tobar et al., 2015a) is a model for signals with complex spectral structure. A significant limitation of the GPCM is that it assumes a rapidly decaying spectrum: it can only model smooth signals. Moreover, inference in the GPCM currently requires (1) a mean-field assumption, resulting in poorly calibrated uncertainties, and (2) a tedious variational optimisation of large covariance matrices. We redesign the GPCM model to induce a richer distribution over the spectrum with relaxed assumptions about smoothness: the Causal Gaussian Process Convolution Model (CGPCM) introduces a causality assumption into the GPCM, and the Rough Gaussian Process Convolution Model (RGPCM) can be interpreted as a Bayesian nonparametric generalisation of the fractional Ornstein-Uhlenbeck process. We also propose a more effective variational inference scheme, going beyond the mean-field assumption: we design a Gibbs sampler which directly samples from the optimal variational solution, circumventing any variational optimisation entirely. The proposed variations of the GPCM are validated in experiments on synthetic and real-world data, showing promising results.

</details>

<details>

<summary>2022-04-14 17:54:25 - The Current State of Undergraduate Bayesian Education and Recommendations for the Future</summary>

- *Mine Dogucu, Jingchen Hu*

- `2109.00848v2` - [abs](http://arxiv.org/abs/2109.00848v2) - [pdf](http://arxiv.org/pdf/2109.00848v2)

> As a result of the increased emphasis on mis- and over-use of $p$-values in scientific research and the rise in popularity of Bayesian statistics, Bayesian education is becoming more important at the undergraduate level. With the advances in computing tools, Bayesian statistics is also becoming more accessible for the undergraduates. This study focuses on analyzing Bayesian courses for the undergraduates. We explored whether an undergraduate Bayesian course is offered in our sample of 152 high-ranking research universities and liberal arts colleges. For each identified Bayesian course, we examined how it fits into the institution's undergraduate curricula, such as majors and prerequisites. Through a series of course syllabi analyses, we explored the topics covered and their popularity in these courses, and the adopted teaching and learning tools, such as software. This paper presents our findings on the current practices of teaching full Bayesian courses at the undergraduate level. Based on our findings, we provide recommendations for programs that may consider offering Bayesian courses to their students.

</details>

<details>

<summary>2022-04-14 18:36:31 - Bayesian Nonparametrics for Sparse Dynamic Networks</summary>

- *Cian Naik, Francois Caron, Judith Rousseau, Yee Whye Teh, Konstantina Palla*

- `1607.01624v2` - [abs](http://arxiv.org/abs/1607.01624v2) - [pdf](http://arxiv.org/pdf/1607.01624v2)

> In this paper we propose a Bayesian nonparametric approach to modelling sparse time-varying networks. A positive parameter is associated to each node of a network, which models the sociability of that node. Sociabilities are assumed to evolve over time, and are modelled via a dynamic point process model. The model is able to capture long term evolution of the sociabilities. Moreover, it yields sparse graphs, where the number of edges grows subquadratically with the number of nodes. The evolution of the sociabilities is described by a tractable time-varying generalised gamma process. We provide some theoretical insights into the model and apply it to three datasets: a simulated network, a network of hyperlinks between communities on Reddit, and a network of co-occurences of words in Reuters news articles after the September 11th attacks.

</details>

<details>

<summary>2022-04-14 19:56:03 - Hierarchical Embedded Bayesian Additive Regression Trees</summary>

- *Bruna Wundervald, Andrew Parnell, Katarina Domijan*

- `2204.07207v1` - [abs](http://arxiv.org/abs/2204.07207v1) - [pdf](http://arxiv.org/pdf/2204.07207v1)

> We propose a simple yet powerful extension of Bayesian Additive Regression Trees which we name Hierarchical Embedded BART (HE-BART). The model allows for random effects to be included at the terminal node level of a set of regression trees, making HE-BART a non-parametric alternative to mixed effects models which avoids the need for the user to specify the structure of the random effects in the model, whilst maintaining the prediction and uncertainty calibration properties of standard BART. Using simulated and real-world examples, we demonstrate that this new extension yields superior predictions for many of the standard mixed effects models' example data sets, and yet still provides consistent estimates of the random effect variances. In a future version of this paper, we outline its use in larger, more advanced data sets and structures.

</details>

<details>

<summary>2022-04-15 13:52:50 - A Statistical Decision-Theoretical Perspective on the Two-Stage Approach to Parameter Estimation</summary>

- *Braghadeesh Lakshminarayanan, Cristian R. Rojas*

- `2204.00036v2` - [abs](http://arxiv.org/abs/2204.00036v2) - [pdf](http://arxiv.org/pdf/2204.00036v2)

> One of the most important problems in system identification and statistics is how to estimate the unknown parameters of a given model. Optimization methods and specialized procedures, such as Empirical Minimization (EM) can be used in case the likelihood function can be computed. For situations where one can only simulate from a parametric model, but the likelihood is difficult or impossible to evaluate, a technique known as the Two-Stage (TS) Approach can be applied to obtain reliable parametric estimates. Unfortunately, there is currently a lack of theoretical justification for TS. In this paper, we propose a statistical decision-theoretical derivation of TS, which leads to Bayesian and Minimax estimators. We also show how to apply the TS approach on models for independent and identically distributed samples, by computing quantiles of the data as a first step, and using a linear function as the second stage. The proposed method is illustrated via numerical simulations.

</details>

<details>

<summary>2022-04-15 14:38:21 - Proximal nested sampling for high-dimensional Bayesian model selection</summary>

- *Xiaohao Cai, Jason D. McEwen, Marcelo Pereyra*

- `2106.03646v2` - [abs](http://arxiv.org/abs/2106.03646v2) - [pdf](http://arxiv.org/pdf/2106.03646v2)

> Bayesian model selection provides a powerful framework for objectively comparing models directly from observed data, without reference to ground truth data. However, Bayesian model selection requires the computation of the marginal likelihood (model evidence), which is computationally challenging, prohibiting its use in many high-dimensional Bayesian inverse problems. With Bayesian imaging applications in mind, in this work we present the proximal nested sampling methodology to objectively compare alternative Bayesian imaging models for applications that use images to inform decisions under uncertainty. The methodology is based on nested sampling, a Monte Carlo approach specialised for model comparison, and exploits proximal Markov chain Monte Carlo techniques to scale efficiently to large problems and to tackle models that are log-concave and not necessarily smooth (e.g., involving l_1 or total-variation priors). The proposed approach can be applied computationally to problems of dimension O(10^6) and beyond, making it suitable for high-dimensional inverse imaging problems. It is validated on large Gaussian models, for which the likelihood is available analytically, and subsequently illustrated on a range of imaging problems where it is used to analyse different choices of dictionary and measurement model.

</details>

<details>

<summary>2022-04-16 08:23:48 - A Variational Approach to Bayesian Phylogenetic Inference</summary>

- *Cheng Zhang, Frederick A. Matsen IV*

- `2204.07747v1` - [abs](http://arxiv.org/abs/2204.07747v1) - [pdf](http://arxiv.org/pdf/2204.07747v1)

> Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo (MCMC) with simple proposal mechanisms. This hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper, we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We propose combining subsplit Bayesian networks, an expressive graphical model for tree topology distributions, and a structured amortization of the branch lengths over tree topologies for a suitable variational family of distributions. We train the variational approximation via stochastic gradient ascent and adopt gradient estimators for continuous and discrete variational parameters separately to deal with the composite latent space of phylogenetic models. We show that our variational approach provides competitive performance to MCMC, while requiring much less computation due to a more efficient exploration mechanism enabled by variational inference. Experiments on a benchmark of challenging real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.

</details>

<details>

<summary>2022-04-16 13:02:39 - Nested smoothing algorithms for inference and tracking of heterogeneous multi-scale state-space systems</summary>

- *Sara Prez-Vieites, Harold Molina-Bulla, Joaquin Miguez*

- `2204.07795v1` - [abs](http://arxiv.org/abs/2204.07795v1) - [pdf](http://arxiv.org/pdf/2204.07795v1)

> Multi-scale problems, where variables of interest evolve in different time-scales and live in different state-spaces. can be found in many fields of science. Here, we introduce a new recursive methodology for Bayesian inference that aims at estimating the static parameters and tracking the dynamic variables of these kind of systems. Although the proposed approach works in rather general multi-scale systems, for clarity we analyze the case of a heterogeneous multi-scale model with 3 time-scales (static parameters, slow dynamic state variables and fast dynamic state variables). The proposed scheme, based on nested filtering methodology of P\'erez-Vieites et al. (2018), combines three intertwined layers of filtering techniques that approximate recursively the joint posterior probability distribution of the parameters and both sets of dynamic state variables given a sequence of partial and noisy observations. We explore the use of sequential Monte Carlo schemes in the first and second layers while we use an unscented Kalman filter to obtain a Gaussian approximation of the posterior probability distribution of the fast variables in the third layer. Some numerical results are presented for a stochastic two-scale Lorenz 96 model with unknown parameters.

</details>

<details>

<summary>2022-04-16 18:32:33 - PAC-Bayesian Based Adaptation for Regularized Learning</summary>

- *Prem Talwai, David Simchi-Levi*

- `2204.07856v1` - [abs](http://arxiv.org/abs/2204.07856v1) - [pdf](http://arxiv.org/pdf/2204.07856v1)

> In this paper, we propose a PAC-Bayesian \textit{a posteriori} parameter selection scheme for adaptive regularized regression in Hilbert scales under general, unknown source conditions. We demonstrate that our approach is adaptive to misspecification, and achieves the optimal learning rate under subgaussian noise. Unlike existing parameter selection schemes, the computational complexity of our approach is independent of sample size. We derive minimax adaptive rates for a new, broad class of Tikhonov-regularized learning problems under general, misspecified source conditions, that notably do not require any conventional a priori assumptions on kernel eigendecay. Using the theory of interpolation, we demonstrate that the spectrum of the Mercer operator can be inferred in the presence of "tight" $L^{\infty}$ embeddings of suitable Hilbert scales. Finally, we prove, that under a $\Delta_2$ condition on the smoothness index functions, our PAC-Bayesian scheme can indeed achieve minimax rates. We discuss applications of our approach to statistical inverse problems and oracle-efficient contextual bandit algorithms.

</details>

<details>

<summary>2022-04-17 13:59:29 - Free gs-monoidal categories and free Markov categories</summary>

- *Tobias Fritz, Wendong Liang*

- `2204.02284v2` - [abs](http://arxiv.org/abs/2204.02284v2) - [pdf](http://arxiv.org/pdf/2204.02284v2)

> Categorical probability has recently seen significant advances through the formalism of Markov categories, within which several classical theorems have been proven in entirely abstract categorical terms. Closely related to Markov categories are gs-monoidal categories, also known as CD categories. These omit a condition that implements the normalization of probability. Extending work of Corradini and Gadducci, we construct free gs-monoidal and free Markov categories generated by a collection of morphisms of arbitrary arity and coarity. For free gs-monoidal categories, this comes in the form of an explicit combinatorial description of their morphisms as structured cospans of labeled hypergraphs. These can be thought of as a formalization of gs-monoidal string diagrams ($=$term graphs) as a combinatorial data structure. We formulate the appropriate $2$-categorical universal property based on ideas of Walters and prove that our categories satisfy it.   We expect our free categories to be relevant for computer implementations and we also argue that they can be used as statistical causal models generalizing Bayesian networks.

</details>

<details>

<summary>2022-04-17 14:08:59 - Randomized Maximum Likelihood via High-Dimensional Bayesian Optimization</summary>

- *Valentin Breaz, Richard Wilkinson*

- `2204.08022v1` - [abs](http://arxiv.org/abs/2204.08022v1) - [pdf](http://arxiv.org/pdf/2204.08022v1)

> Randomized Maximum Likelihood (RML) is an approximate posterior sampling methodology, widely used in Bayesian inverse problems with complex forward models, particularly in petroleum engineering applications. The procedure involves solving a multi-objective optimization problem, which can be challenging in high-dimensions and when there are constraints on computational costs. We propose a new methodology for tackling the RML optimization problem based on the high-dimensional Bayesian optimization literature. By sharing data between the different objective functions, we are able to implement RML at a greatly reduced computational cost. We demonstrate the benefits of our methodology in comparison with the solutions obtained by alternative optimization methods on a variety of synthetic and real-world problems, including medical and fluid dynamics applications. Furthermore, we show that the samples produced by our method cover well the high-posterior density regions in all of the experiments.

</details>

<details>

<summary>2022-04-18 18:19:23 - Subset selection for linear mixed models</summary>

- *Daniel R. Kowal*

- `2107.12890v2` - [abs](http://arxiv.org/abs/2107.12890v2) - [pdf](http://arxiv.org/pdf/2107.12890v2)

> Linear mixed models (LMMs) are instrumental for regression analysis with structured dependence, such as grouped, clustered, or multilevel data. However, selection among the covariates--while accounting for this structured dependence--remains a challenge. We introduce a Bayesian decision analysis for subset selection with LMMs. Using a Mahalanobis loss function that incorporates the structured dependence, we derive optimal linear coefficients for (i) any given subset of variables and (ii) all subsets of variables that satisfy a cardinality constraint. Crucially, these estimates inherit shrinkage or regularization and uncertainty quantification from the underlying Bayesian model, and apply for any well-specified Bayesian LMM. More broadly, our decision analysis strategy deemphasizes the role of a single "best" subset, which is often unstable and limited in its information content, and instead favors a collection of near-optimal subsets. This collection is summarized by key member subsets and variable-specific importance metrics. Customized subset search and out-of-sample approximation algorithms are provided for more scalable computing. These tools are applied to simulated data and a longitudinal physical activity dataset, and demonstrate excellent prediction, estimation, and selection ability.

</details>

<details>

<summary>2022-04-19 01:27:00 - A surrogate-based reliability analysis method of the motion of large flexible space structures</summary>

- *Dongyu Zhao*

- `2110.04137v2` - [abs](http://arxiv.org/abs/2110.04137v2) - [pdf](http://arxiv.org/pdf/2110.04137v2)

> Satellites and their instruments are subject to the motion stability throughout their lifetimes. The reliability of the large flexible space structures (LFSS) is particularly important for the motion stability of satellites and their instruments. In this paper, the reliability analysis of large flexible space structures is conducted based on Bayesian support vector regression (SVR). The kinematic model of a typical large flexible space structure is first established. Based on the kinematic model, the surrogate model of the motion of the large flexible space structure is then developed to further reduce the computational cost. Finally, the reliability analysis is conducted using the surrogate model. The proposed method shows high accuracy and efficiency for the reliability assessments of the typical large flexible space structure and can be further developed for other LFSS.

</details>

<details>

<summary>2022-04-19 06:16:14 - Independence Testing for Bounded Degree Bayesian Network</summary>

- *Arnab Bhattacharyya, Clment L. Canonne, Joy Qiping Yang*

- `2204.08690v1` - [abs](http://arxiv.org/abs/2204.08690v1) - [pdf](http://arxiv.org/pdf/2204.08690v1)

> We study the following independence testing problem: given access to samples from a distribution $P$ over $\{0,1\}^n$, decide whether $P$ is a product distribution or whether it is $\varepsilon$-far in total variation distance from any product distribution. For arbitrary distributions, this problem requires $\exp(n)$ samples. We show in this work that if $P$ has a sparse structure, then in fact only linearly many samples are required. Specifically, if $P$ is Markov with respect to a Bayesian network whose underlying DAG has in-degree bounded by $d$, then $\tilde{\Theta}(2^{d/2}\cdot n/\varepsilon^2)$ samples are necessary and sufficient for independence testing.

</details>

<details>

<summary>2022-04-19 10:09:17 - A Bayesian Decision Support System in Energy Systems Planning</summary>

- *Victoria Volodina, Nikki Sonenberg, Peter Challenor, Jim Q. Smith*

- `2204.05035v2` - [abs](http://arxiv.org/abs/2204.05035v2) - [pdf](http://arxiv.org/pdf/2204.05035v2)

> Gaussian Process (GP) emulators are widely used to approximate complex computer model behaviour across the input space. Motivated by the problem of coupling computer models, recently progress has been made in the theory of the analysis of networks of connected GP emulators. In this paper, we combine these recent methodological advances with classical state-space models to construct a Bayesian decision support system. This approach gives a coherent probability model that produces predictions with the measure of uncertainty in terms of two first moments and enables the propagation of uncertainty from individual decision components.   This methodology is used to produce a decision support tool for a UK county council considering low carbon technologies to transform its infrastructure to reach a net-zero carbon target. In particular, we demonstrate how to couple information from an energy model, a heating demand model, and gas and electricity price time-series to quantitatively assess the impact on operational costs of various policy choices and changes in the energy market.

</details>

<details>

<summary>2022-04-19 16:35:44 - CPU- and GPU-based Distributed Sampling in Dirichlet Process Mixtures for Large-scale Analysis</summary>

- *Or Dinari, Raz Zamir, John W. Fisher III, Oren Freifeld*

- `2204.08988v1` - [abs](http://arxiv.org/abs/2204.08988v1) - [pdf](http://arxiv.org/pdf/2204.08988v1)

> In the realm of unsupervised learning, Bayesian nonparametric mixture models, exemplified by the Dirichlet Process Mixture Model (DPMM), provide a principled approach for adapting the complexity of the model to the data. Such models are particularly useful in clustering tasks where the number of clusters is unknown. Despite their potential and mathematical elegance, however, DPMMs have yet to become a mainstream tool widely adopted by practitioners. This is arguably due to a misconception that these models scale poorly as well as the lack of high-performance (and user-friendly) software tools that can handle large datasets efficiently. In this paper we bridge this practical gap by proposing a new, easy-to-use, statistical software package for scalable DPMM inference. More concretely, we provide efficient and easily-modifiable implementations for high-performance distributed sampling-based inference in DPMMs where the user is free to choose between either a multiple-machine, multiple-core, CPU implementation (written in Julia) and a multiple-stream GPU implementation (written in CUDA/C++). Both the CPU and GPU implementations come with a common (and optional) python wrapper, providing the user with a single point of entry with the same interface. On the algorithmic side, our implementations leverage a leading DPMM sampler from (Chang and Fisher III, 2013). While Chang and Fisher III's implementation (written in MATLAB/C++) used only CPU and was designed for a single multi-core machine, the packages we proposed here distribute the computations efficiently across either multiple multi-core machines or across mutiple GPU streams. This leads to speedups, alleviates memory and storage limitations, and lets us fit DPMMs to significantly larger datasets and of higher dimensionality than was possible previously by either (Chang and Fisher III, 2013) or other DPMM methods.

</details>

<details>

<summary>2022-04-19 17:57:36 - A stochastic Stein Variational Newton method</summary>

- *Alex Leviyev, Joshua Chen, Yifei Wang, Omar Ghattas, Aaron Zimmerman*

- `2204.09039v1` - [abs](http://arxiv.org/abs/2204.09039v1) - [pdf](http://arxiv.org/pdf/2204.09039v1)

> Stein variational gradient descent (SVGD) is a general-purpose optimization-based sampling algorithm that has recently exploded in popularity, but is limited by two issues: it is known to produce biased samples, and it can be slow to converge on complicated distributions. A recently proposed stochastic variant of SVGD (sSVGD) addresses the first issue, producing unbiased samples by incorporating a special noise into the SVGD dynamics such that asymptotic convergence is guaranteed. Meanwhile, Stein variational Newton (SVN), a Newton-like extension of SVGD, dramatically accelerates the convergence of SVGD by incorporating Hessian information into the dynamics, but also produces biased samples. In this paper we derive, and provide a practical implementation of, a stochastic variant of SVN (sSVN) which is both asymptotically correct and converges rapidly. We demonstrate the effectiveness of our algorithm on a difficult class of test problems -- the Hybrid Rosenbrock density -- and show that sSVN converges using three orders of magnitude fewer gradient evaluations of the log likelihood than its stochastic SVGD counterpart. Our results show that sSVN is a promising approach to accelerating high-precision Bayesian inference tasks with modest-dimension, $d\sim\mathcal{O}(10)$.

</details>

<details>

<summary>2022-04-19 18:04:36 - On the Dynamics of Inference and Learning</summary>

- *David S. Berman, Jonathan J. Heckman, Marc Klinger*

- `2204.12939v1` - [abs](http://arxiv.org/abs/2204.12939v1) - [pdf](http://arxiv.org/pdf/2204.12939v1)

> Statistical Inference is the process of determining a probability distribution over the space of parameters of a model given a data set. As more data becomes available this probability distribution becomes updated via the application of Bayes' theorem. We present a treatment of this Bayesian updating process as a continuous dynamical system. Statistical inference is then governed by a first order differential equation describing a trajectory or flow in the information geometry determined by a parametric family of models. We solve this equation for some simple models and show that when the Cram\'{e}r-Rao bound is saturated the learning rate is governed by a simple $1/T$ power-law, with $T$ a time-like variable denoting the quantity of data. The presence of hidden variables can be incorporated in this setting, leading to an additional driving term in the resulting flow equation. We illustrate this with both analytic and numerical examples based on Gaussians and Gaussian Random Processes and inference of the coupling constant in the 1D Ising model. Finally we compare the qualitative behaviour exhibited by Bayesian flows to the training of various neural networks on benchmarked data sets such as MNIST and CIFAR10 and show how that for networks exhibiting small final losses the simple power-law is also satisfied.

</details>

<details>

<summary>2022-04-19 18:43:00 - Choosing the number of factors in factor analysis with incomplete data via a hierarchical Bayesian information criterion</summary>

- *Jianhua Zhao, Changchun Shang, Shulan Li, Ling Xin, Philip L. H. Yu*

- `2204.09086v1` - [abs](http://arxiv.org/abs/2204.09086v1) - [pdf](http://arxiv.org/pdf/2204.09086v1)

> The Bayesian information criterion (BIC), defined as the observed data log likelihood minus a penalty term based on the sample size $N$, is a popular model selection criterion for factor analysis with complete data. This definition has also been suggested for incomplete data. However, the penalty term based on the `complete' sample size $N$ is the same no matter whether in a complete or incomplete data case. For incomplete data, there are often only $N_i<N$ observations for variable $i$, which means that using the `complete' sample size $N$ implausibly ignores the amounts of missing information inherent in incomplete data. Given this observation, a novel criterion called hierarchical BIC (HBIC) for factor analysis with incomplete data is proposed. The novelty is that it only uses the actual amounts of observed information, namely $N_i$'s, in the penalty term. Theoretically, it is shown that HBIC is a large sample approximation of variational Bayesian (VB) lower bound, and BIC is a further approximation of HBIC, which means that HBIC shares the theoretical consistency of BIC. Experiments on synthetic and real data sets are conducted to access the finite sample performance of HBIC, BIC, and related criteria with various missing rates. The results show that HBIC and BIC perform similarly when the missing rate is small, but HBIC is more accurate when the missing rate is not small.

</details>

<details>

<summary>2022-04-19 21:16:58 - CobBO: Coordinate Backoff Bayesian Optimization with Two-Stage Kernels</summary>

- *Jian Tan, Niv Nayman, Mengchang Wang*

- `2101.05147v3` - [abs](http://arxiv.org/abs/2101.05147v3) - [pdf](http://arxiv.org/pdf/2101.05147v3)

> Bayesian optimization is a popular method for optimizing expensive black-box functions. Yet it oftentimes struggles in high dimensions where the computation could be prohibitively heavy. To alleviate this problem, we introduce Coordinate backoff Bayesian Optimization (CobBO) with two-stage kernels. During each round, the first stage uses a simple coarse kernel that sacrifices the approximation accuracy for computational efficiency. It captures the global landscape by purposely smoothing away local fluctuations. Then, in the second stage of the same round, past observed points in the full space are projected to the selected subspace to form virtual points. These virtual points, along with the means and variances of their unknown function values estimated using the simple kernel of the first stage, are fitted to a more sophisticated kernel model in the second stage. Within the selected low dimensional subspace, the computational cost of conducting Bayesian optimization therein becomes affordable. To further enhance the performance, a sequence of consecutive observations in the same subspace are collected, which can effectively refine the approximation of the function. This refinement lasts until a stopping rule is met determining when to back off from a certain subspace and switch to another. This decoupling significantly reduces the computational burden in high dimensions, which fully leverages the observations in the whole space rather than only relying on observations in each coordinate subspace. Extensive evaluations show that CobBO finds solutions comparable to or better than other state-of-the-art methods for dimensions ranging from tens to hundreds, while reducing both the trial complexity and computational costs.

</details>

<details>

<summary>2022-04-20 12:58:44 - Estimating Software Reliability Using Size-biased Modelling</summary>

- *Soumen Dey, Ashis Kumar Chakraborty*

- `2202.08107v3` - [abs](http://arxiv.org/abs/2202.08107v3) - [pdf](http://arxiv.org/pdf/2202.08107v3)

> Software reliability estimation is one of the most active areas of research in software testing. Since time between failures (TBF) has often been challenging to record, software testing data are commonly recorded as test-case-wise in a discrete set up. We have developed a Bayesian generalised linear mixed model (GLMM) based on software testing detection data and a size-biased strategy which not only estimates the software reliability, but also estimates the total number of bugs present in the software. Our approach provides a flexible, unified modelling framework and can be adopted to various real-life situations. We have assessed the performance of our model via simulation study and found that each of the key parameters could be estimated with a satisfactory level of accuracy. We have also applied our model to two empirical software testing data sets. While there can be other fields of study for application of our model (e.g., hydrocarbon exploration), we anticipate that our novel modelling approach to estimate software reliability could be very useful for the users and can potentially be a key tool in the field of software reliability estimation.

</details>

<details>

<summary>2022-04-20 18:53:34 - A majorization-minimization algorithm for nonnegative binary matrix factorization</summary>

- *Paul Magron, Cdric Fvotte*

- `2204.09741v1` - [abs](http://arxiv.org/abs/2204.09741v1) - [pdf](http://arxiv.org/pdf/2204.09741v1)

> This paper tackles the problem of decomposing binary data using matrix factorization. We consider the family of mean-parametrized Bernoulli models, a class of generative models that are well suited for modeling binary data and enables interpretability of the factors. We factorize the Bernoulli parameter and consider an additional Beta prior on one of the factors to further improve the model's expressive power. While similar models have been proposed in the literature, they only exploit the Beta prior as a proxy to ensure a valid Bernoulli parameter in a Bayesian setting; in practice it reduces to a uniform or uninformative prior. Besides, estimation in these models has focused on costly Bayesian inference. In this paper, we propose a simple yet very efficient majorization-minimization algorithm for maximum a posteriori estimation. Our approach leverages the Beta prior whose parameters can be tuned to improve performance in matrix completion tasks. Experiments conducted on three public binary datasets show that our approach offers an excellent trade-off between prediction performance, computational complexity, and interpretability.

</details>

<details>

<summary>2022-04-20 19:02:27 - Conditional Hierarchical Bayesian Tucker Decomposition for Genetic Data Analysis</summary>

- *Adam Sandler, Diego Klabjan, Yuan Luo*

- `1911.12426v4` - [abs](http://arxiv.org/abs/1911.12426v4) - [pdf](http://arxiv.org/pdf/1911.12426v4)

> We develop methods for reducing the dimensionality of large data sets, common in biomedical applications. Learning about patients using genetic data often includes more features than observations, which makes direct supervised learning difficult. One method of reducing the feature space is to use latent Dirichlet allocation to group genetic variants in an unsupervised manner. Latent Dirichlet allocation describes a patient as a mixture of topics corresponding to genetic variants. This can be generalized as a Bayesian tensor decomposition to account for multiple feature variables. Our most significant contributions are with hierarchical topic modeling. We design distinct methods of incorporating hierarchical topic modeling, based on nested Chinese restaurant processes and Pachinko Allocation Machine, into Bayesian tensor decomposition. We apply these models to examine patients with one of four common types of cancer (breast, lung, prostate, and colorectal) and siblings with and without autism spectrum disorder. We linked the genes with their biological pathways and combine this information into a tensor of patients, counts of their genetic variants, and the genes' membership in pathways. We find that our trained models outperform baseline models, with respect to coherence, by up to 40%.

</details>

<details>

<summary>2022-04-21 01:48:56 - Computationally Efficient and Statistically Optimal Robust Low-rank Matrix and Tensor Estimation</summary>

- *Yinan Shen, Jingyang Li, Jian-Feng Cai, Dong Xia*

- `2203.00953v3` - [abs](http://arxiv.org/abs/2203.00953v3) - [pdf](http://arxiv.org/pdf/2203.00953v3)

> Low-rank matrix estimation under heavy-tailed noise is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a novel Riemannian sub-gradient (RsGrad) algorithm which is not only computationally efficient with linear convergence but also is statistically optimal, be the noise Gaussian or heavy-tailed. Convergence theory is established for a general framework and specific applications to absolute loss, Huber loss, and quantile loss are investigated. Compared with existing non-convex methods, ours reveals a surprising phenomenon of dual-phase convergence. In phase one, RsGrad behaves as in a typical non-smooth optimization that requires gradually decaying stepsizes. However, phase one only delivers a statistically sub-optimal estimator which is already observed in the existing literature. Interestingly, during phase two, RsGrad converges linearly as if minimizing a smooth and strongly convex objective function and thus a constant stepsize suffices. Underlying the phase-two convergence is the smoothing effect of random noise to the non-smooth robust losses in an area close but not too close to the truth. Lastly, RsGrad is applicable for low-rank tensor estimation under heavy-tailed noise where a statistically optimal rate is attainable with the same phenomenon of dual-phase convergence, and a novel shrinkage-based second-order moment method is guaranteed to deliver a warm initialization. Numerical simulations confirm our theoretical discovery and showcase the superiority of RsGrad over prior methods.

</details>

<details>

<summary>2022-04-21 03:36:40 - The $$-augmented model for Bayesian semiparametric inference on functional parameters</summary>

- *Vivian Y. Meng, David A. Stephens*

- `2204.09862v1` - [abs](http://arxiv.org/abs/2204.09862v1) - [pdf](http://arxiv.org/pdf/2204.09862v1)

> Semiparametric Bayesian inference has so far relied on models for the observable that partition into two parts, one being parametric and the other nonparametric, with the target parameter being dependent on the parametric component. While a partitioned structure makes specification of the marginal prior on the target parameter simple to perform, it often arises from conditional modelling which is subject to misspecification and ultimately a lack of consistency. We introduce a new type of semiparametric model to allow easy prior specification for a parameter that is defined as a functional of the distribution for the observable. Our semiparametric model is obtained as an extension of nonparametric models that are consistent under very general conditions. This type of Bayesian semiparametric model can be used to obtain Bayesian versions of Frequentist estimators that are defined as functionals of the empirical distribution. This gives us new opportunities to conduct Bayesian analysis in problems where Frequentist estimators exist but not well-accepted likelihoods.

</details>

<details>

<summary>2022-04-21 06:08:50 - Functional Horseshoe Smoothing for Functional Trend Estimation</summary>

- *Tomoya Wakayama, Shonosuke Sugasawa*

- `2204.09898v1` - [abs](http://arxiv.org/abs/2204.09898v1) - [pdf](http://arxiv.org/pdf/2204.09898v1)

> Due to developments in instruments and computers, functional observations are increasingly popular. However, effective methodologies for flexibly estimating the underlying trends with valid uncertainty quantification for a sequence of functional data (e.g. functional time series) are still scarce. In this work, we develop a locally adaptive smoothing method, called functional horseshoe smoothing, by introducing a shrinkage prior to the general order of differences of functional variables. This allows us to capture abrupt changes by taking advantage of the shrinkage capability and also to assess uncertainty by Bayesian inference. The fully Bayesian framework also allows the selection of the number of basis functions via the posterior predictive loss. Also, by taking advantage of the nature of functional data, this method is able to handle heterogeneously observed data without data augmentation. We show the theoretical properties of the proposed prior distribution and the posterior mean, and finally demonstrate them through simulation studies and applications to a real-world dataset.

</details>

<details>

<summary>2022-04-21 07:28:10 - Strong posterior contraction rates via Wasserstein dynamics</summary>

- *Emanuele Dolera, Stefano Favaro, Edoardo Mainini*

- `2203.10754v2` - [abs](http://arxiv.org/abs/2203.10754v2) - [pdf](http://arxiv.org/pdf/2203.10754v2)

> In this paper, we develop a novel approach to posterior contractions rates (PCRs), for both finite-dimensional (parametric) and infinite-dimensional (nonparametric) Bayesian models. Critical to our approach is the combination of an assumption of local Lipschitz-continuity for the posterior distribution with a dynamic formulation of the Wasserstein distance, here referred to as Wasserstein dynamics, which allows to set forth a connection between the problem of establishing PCRs and some classical problems in mathematical analysis, probability theory and mathematical statistics: the Laplace method for approximating integrals, Sanov's large deviation principles in the Wasserstein distance, rates of convergence of the mean Glivenko-Cantelli theorem, and estimates of weighted Poincar\'e-Wirtinger constants. Under dominated Bayesian models, we present two main results: i) a theorem on PCRs for the regular infinite-dimensional exponential family of statistical models; ii) a theorem on PCRs for a general dominated statistical model. Some applications of our results are presented for the regular parametric model, the multinomial model, the finite-dimensional and the infinite-dimensional logistic-Gaussian model and the infinite-dimensional linear regression. In general, our results lead to optimal PCRs in finite dimension, whereas in infinite dimension it is shown how the prior distribution may affect PCRs. With regards to infinite-dimensional Bayesian models for density estimation, our approach to PCRs is the first to consider strong norm distances on parameter spaces of functions, such as Sobolev-like norms, as most of the approaches in the classical (frequentist) and Bayesian literature deal with spaces of density functions endowed with $\mathrm{L}^p$ norms or the Hellinger distance.

</details>

<details>

<summary>2022-04-21 14:56:08 - Testing the equality of two coefficients of variation: a new Bayesian approach</summary>

- *Francesco Bertolino, Silvia Columbu, Mara Manca, Monica Musio*

- `2204.10147v1` - [abs](http://arxiv.org/abs/2204.10147v1) - [pdf](http://arxiv.org/pdf/2204.10147v1)

> The use of testing procedures for comparing two coefficients of variation (CVs) of independent populations is not extensively explored in the Bayesian context. We propose to address this issue through a test based on a measure of evidence, the Bayesian Discrepancy Measure, recently introduced in the literature. Computing the Bayesian Discrepancy Measure is straightforward when the CVs depend on a single parameter of the distribution. In contrast, it becomes more difficult when this simplification does not occur since more parameters are involved, requiring often the use of MCMC methods. We derive the Bayesian Discrepancy Measure and the related test by considering a variety of distribution assumptions with multiparametric CVs and apply them to real datasets. As far as we know, some of the examined problems have not yet been covered in the literature.

</details>

<details>

<summary>2022-04-21 15:09:48 - Bayesian Learning via Neural Schrdinger-Fllmer Flows</summary>

- *Francisco Vargas, Andrius Ovsianas, David Fernandes, Mark Girolami, Neil D. Lawrence, Nikolas Nsken*

- `2111.10510v8` - [abs](http://arxiv.org/abs/2111.10510v8) - [pdf](http://arxiv.org/pdf/2111.10510v8)

> In this work we explore a new framework for approximate Bayesian inference in large datasets based on stochastic control (i.e. Schr\"odinger bridges). We advocate stochastic control as a finite time and low variance alternative to popular steady-state methods such as stochastic gradient Langevin dynamics (SGLD). Furthermore, we discuss and adapt the existing theoretical guarantees of this framework and establish connections to already existing VI routines in SDE-based models.

</details>

<details>

<summary>2022-04-21 18:06:36 - Online, Informative MCMC Thinning with Kernelized Stein Discrepancy</summary>

- *Cole Hawkins, Alec Koppel, Zheng Zhang*

- `2201.07130v2` - [abs](http://arxiv.org/abs/2201.07130v2) - [pdf](http://arxiv.org/pdf/2201.07130v2)

> A fundamental challenge in Bayesian inference is efficient representation of a target distribution. Many non-parametric approaches do so by sampling a large number of points using variants of Markov Chain Monte Carlo (MCMC). We propose an MCMC variant that retains only those posterior samples which exceed a KSD threshold, which we call KSD Thinning. We establish the convergence and complexity tradeoffs for several settings of KSD Thinning as a function of the KSD threshold parameter, sample size, and other problem parameters. Finally, we provide experimental comparisons against other online nonparametric Bayesian methods that generate low-complexity posterior representations, and observe superior consistency/complexity tradeoffs. Code is available at github.com/colehawkins/KSD-Thinning.

</details>

<details>

<summary>2022-04-21 21:58:32 - Data Privacy Protection and Utility Preservation through Bayesian Data Synthesis: A Case Study on Airbnb Listings</summary>

- *Shijie Guo, Jingchen Hu*

- `2109.08511v2` - [abs](http://arxiv.org/abs/2109.08511v2) - [pdf](http://arxiv.org/pdf/2109.08511v2)

> When releasing record-level data containing sensitive information to the public, the data disseminator is responsible for protecting the privacy of every record in the dataset, simultaneously preserving important features of the data for users' analyses. These goals can be achieved by data synthesis, where confidential data are replaced with synthetic data that are simulated based on statistical models estimated on the confidential data. In this paper, we present a data synthesis case study, where synthetic values of price and the number of available days in a sample of the New York Airbnb Open Data are created for privacy protection. One sensitive variable, the number of available days of an Airbnb listing, has a large amount of zero-valued records and also truncated at the two ends. We propose a zero-inflated truncated Poisson regression model for its synthesis. We utilize a sequential synthesis approach to further synthesize the sensitive price variable. The resulting synthetic data are evaluated for its utility preservation and privacy protection, the latter in the form of disclosure risks. Furthermore, we propose methods to investigate how uncertainties in intruder's knowledge would influence the identification disclosure risks of the synthetic data. In particular, we explore several realistic scenarios of uncertainties in intruder's knowledge of available information and evaluate their impacts on the resulting identification disclosure risks.

</details>

<details>

<summary>2022-04-22 03:18:32 - On the Robustness of Second-Price Auctions in Prior-Independent Mechanism Design</summary>

- *Jerry Anunrojwong, Santiago Balseiro, Omar Besbes*

- `2204.10478v1` - [abs](http://arxiv.org/abs/2204.10478v1) - [pdf](http://arxiv.org/pdf/2204.10478v1)

> Classical Bayesian mechanism design relies on the common prior assumption, but the common prior is often not available in practice. We study the design of prior-independent mechanisms that relax this assumption: the seller is selling an indivisible item to $n$ buyers such that the buyers' valuations are drawn from a joint distribution that is unknown to both the buyers and the seller; buyers do not need to form beliefs about competitors, and the seller assumes the distribution is adversarially chosen from a specified class. We measure performance through the worst-case regret, or the difference between the expected revenue achievable with perfect knowledge of buyers' valuations and the actual mechanism revenue.   We study a broad set of classes of valuation distributions that capture a wide spectrum of possible dependencies: independent and identically distributed (i.i.d.) distributions, mixtures of i.i.d. distributions, affiliated and exchangeable distributions, exchangeable distributions, and all joint distributions. We derive in quasi closed form the minimax values and the associated optimal mechanism. In particular, we show that the first three classes admit the same minimax regret value, which is decreasing with the number of competitors, while the last two have the same minimax regret equal to that of the case $n = 1$. Furthermore, we show that the minimax optimal mechanisms have a simple form across all settings: a second-price auction with random reserve prices, which shows its robustness in prior-independent mechanism design. En route to our results, we also develop a principled methodology to determine the form of the optimal mechanism and worst-case distribution via first-order conditions that should be of independent interest in other minimax problems.

</details>

<details>

<summary>2022-04-22 08:53:54 - A piece-wise constant approximation for non-conjugate Gaussian Process models</summary>

- *Sarem Seitz*

- `2204.10575v1` - [abs](http://arxiv.org/abs/2204.10575v1) - [pdf](http://arxiv.org/pdf/2204.10575v1)

> Gaussian Processes (GPs) are a versatile and popular method in Bayesian Machine Learning. A common modification are Sparse Variational Gaussian Processes (SVGPs) which are well suited to deal with large datasets. While GPs allow to elegantly deal with Gaussian-distributed target variables in closed form, their applicability can be extended to non-Gaussian data as well. These extensions are usually impossible to treat in closed form and hence require approximate solutions. This paper proposes to approximate the inverse-link function, which is necessary when working with non-Gaussian likelihoods, by a piece-wise constant function. It will be shown that this yields a closed form solution for the corresponding SVGP lower bound. In addition, it is demonstrated how the piece-wise constant function itself can be optimized, resulting in an inverse-link function that can be learnt from the data at hand.

</details>

<details>

<summary>2022-04-22 11:29:38 - A robust Bayesian bias-adjusted random effects model for consideration of uncertainty about bias terms in evidence synthesis</summary>

- *Ivette Raices Cruz, Matthias C. M. Troffaes, Johan Lindstrm, Ullrika Sahlin*

- `2204.10645v1` - [abs](http://arxiv.org/abs/2204.10645v1) - [pdf](http://arxiv.org/pdf/2204.10645v1)

> Meta-analysis is a statistical method used in evidence synthesis for combining, analyzing and summarizing studies that have the same target endpoint and aims to derive a pooled quantitative estimate using fixed and random effects models or network models. Differences among included studies depend on variations in target populations (i.e. heterogeneity) and variations in study quality due to study design and execution (i.e. bias). The risk of bias is usually assessed qualitatively using critical appraisal, and quantitative bias analysis can be used to evaluate the influence of bias on the quantity of interest. We propose a way to consider ignorance or ambiguity in how to quantify bias terms in a bias analysis by characterizing bias with imprecision (as bounds on probability) and use robust Bayesian analysis to estimate the overall effect. Robust Bayesian analysis is here seen as Bayesian updating performed over a set of coherent probability distributions, where the set emerges from a set of bias terms. We show how the set of bias terms can be specified based on judgments on the relative magnitude of biases (i.e., low, unclear and high risk of bias) in one or several domains of the Cochrane's risk of bias table. For illustration, we apply a robust Bayesian bias-adjusted random effects model to an already published meta-analysis on the effect of Rituximab for rheumatoid arthritis from the Cochrane Database of Systematic Reviews.

</details>

<details>

<summary>2022-04-22 12:00:08 - Sequential Learning and Economic Benefits from Dynamic Term Structure Models</summary>

- *Tomasz Dubiel-Teleszynski, Konstantinos Kalogeropoulos, Nikolaos Karouzakis*

- `2204.10658v1` - [abs](http://arxiv.org/abs/2204.10658v1) - [pdf](http://arxiv.org/pdf/2204.10658v1)

> This paper explores the statistical and economic importance of restrictions on the dynamics of risk compensation, from the perspective of a real-time Bayesian learner who predicts bond excess returns using a dynamic term structure model (DTSM). We propose a novel methodological framework that successfully handles sequential model search and parameter estimation over the restriction space landscape in real time, allowing investors to revise their beliefs when new information arrives, thus informing their asset allocation and maximizing their expected utility. Our setup provides the entire predictive density of returns, allowing us to revisit the evident puzzling behaviour between statistical predictability and meaningful out-of-sample economic benefits for bond investors. Empirical results reveal the importance of different sets of restrictions across market conditions and monetary policy actions. Furthermore, our results reinforce the argument of sparsity in the market price of risk specification since we find strong evidence of out-of-sample predictability only for those models that allow for level risk to be priced. Most importantly, such statistical evidence is turned into economically significant utility gains, across prediction horizons. The sequential version of the stochastic search variable selection (SSVS) scheme developed offers an important diagnostic as it monitors potential changes in the importance of different risk prices over time and provides further improvement during periods of macroeconomic uncertainty, where results are more pronounced.

</details>

<details>

<summary>2022-04-22 12:47:32 - Bayesian mixed-effect models for independent dynamic social network data</summary>

- *Fabio Vieira, Roger Leenders, Daniel McFarland, Joris Mulder*

- `2204.10676v1` - [abs](http://arxiv.org/abs/2204.10676v1) - [pdf](http://arxiv.org/pdf/2204.10676v1)

> Relational event or time-stamped social network data have become increasingly available over the years. Accordingly, statistical methods for such data have also surfaced. These techniques are based on log-linear models of the rates of interactions in a social network via actor covariates and network statistics. Particularly, the use of survival analysis concepts has stimulated the development of powerful methods over the past decade. These models mainly focus on the analysis of single networks. To date, there are few models that can deal with multiple relational event networks jointly. In this paper, we propose a new Bayesian hierarchical model for multiple relational event sequences. This approach allows inferences at the actor level, which are useful in understanding which effects guide actors' preferences in social interactions. We also present Bayes factors for hypothesis testing in this class of models. In addition, a new Bayes factor to test random-effect structures is developed. In this test, we let the prior be determined by the data, alleviating the issue of employing improper priors in Bayes factors and thus preventing the use of ad-hoc choices in absence of prior information. We use data of classroom interactions among high school students to illustrate the proposed methods.

</details>

<details>

<summary>2022-04-22 16:01:35 - Non-parametric calibration of multiple related radiocarbon determinations and their calendar age summarisation</summary>

- *Timothy J Heaton*

- `2109.15024v2` - [abs](http://arxiv.org/abs/2109.15024v2) - [pdf](http://arxiv.org/pdf/2109.15024v2)

> Due to fluctuations in past radiocarbon ($^{14}$C) levels, calibration is required to convert $^{14}$C determinations $X_i$ into calendar ages $\theta_i$. In many studies, we wish to calibrate a set of related samples taken from the same site or context, which have calendar ages drawn from the same shared, but unknown, density $f(\theta)$. Calibration of $X_1, \ldots, X_n$ can be improved significantly by incorporating the knowledge that the samples are related. Furthermore, summary estimates of the underlying shared $f(\theta)$ can provide valuable information on changes in population size/activity over time. Most current approaches require a parametric specification for $f(\theta)$ which is often not appropriate. We develop a rigorous non-parametric Bayesian approach using a Dirichlet process mixture model, with slice sampling to address the multimodality typical within $^{14}$C calibration. Our approach simultaneously calibrates the set of $^{14}$C determinations and provides a predictive estimate for the underlying calendar age of a future sample. We show, in a simulation study, the improvement in calendar age estimation when jointly calibrating related samples using our approach, compared with calibration of each $^{14}$C determination independently. We also illustrate the use of the predictive calendar age estimate to provide insight on activity levels over time using three real-life case studies.

</details>

<details>

<summary>2022-04-22 16:17:35 - MCMC-driven importance samplers</summary>

- *F. Llorente, E. Curbelo, L. Martino, V. Elvira, D. Delgado*

- `2105.02579v4` - [abs](http://arxiv.org/abs/2105.02579v4) - [pdf](http://arxiv.org/pdf/2105.02579v4)

> Monte Carlo sampling methods are the standard procedure for approximating complicated integrals of multidimensional posterior distributions in Bayesian inference. In this work, we focus on the class of Layered Adaptive Importance Sampling (LAIS) scheme, which is a family of adaptive importance samplers where Markov chain Monte Carlo algorithms are employed to drive an underlying multiple importance sampling scheme. The modular nature of LAIS allows for different possible implementations, yielding a variety of different performance and computational costs. In this work, we propose different enhancements of the classical LAIS setting in order to increase the efficiency and reduce the computational cost, of both upper and lower layers. The different variants address computational challenges arising in real-world applications, for instance with highly concentrated posterior distributions. Furthermore, we introduce different strategies for designing cheaper schemes, for instance, recycling samples generated in the upper layer and using them in the final estimators in the lower layer. Different numerical experiments, considering several challenging scenarios, show the benefits of the proposed schemes comparing with benchmark methods presented in the literature.

</details>

<details>

<summary>2022-04-22 17:18:12 - Bayesian operator inference for data-driven reduced-order modeling</summary>

- *Mengwu Guo, Shane A. McQuarrie, Karen E. Willcox*

- `2204.10829v1` - [abs](http://arxiv.org/abs/2204.10829v1) - [pdf](http://arxiv.org/pdf/2204.10829v1)

> This work proposes a Bayesian inference method for the reduced-order modeling of time-dependent systems. Informed by the structure of governing equations, the task of learning a reduced-order model from data is posed as a Bayesian inversion problem with Gaussian prior and likelihood. The operators defining the reduced-order model, rather than being chosen deterministically, are characterized probabilistically as posterior Gaussian distributions. This embeds uncertainty into the reduced-order model, and hence the predictions subsequently issued by the reduced-order model are endowed with uncertainty. The learned reduced-order models are computationally efficient, which enables Monte Carlo sampling over the posterior distributions of reduced-order operators. Furthermore, the proposed Bayesian framework provides a statistical interpretation of the Tikhonov regularization incorporated in the operator inference, and the empirical Bayes approach of maximum marginal likelihood suggests a selection algorithm for the regularization hyperparameters. The proposed method is demonstrated by two examples: the compressible Euler equations with noise-corrupted observations, and a single-injector combustion process.

</details>

<details>

<summary>2022-04-22 21:02:34 - Bayesian Spatiotemporal Modeling for Inverse Problems</summary>

- *Shiwei Lan, Shuyi Li, Mirjeta Pasha*

- `2204.10929v1` - [abs](http://arxiv.org/abs/2204.10929v1) - [pdf](http://arxiv.org/pdf/2204.10929v1)

> Inverse problems with spatiotemporal observations are ubiquitous in scientific studies and engineering applications. In these spatiotemporal inverse problems, observed multivariate time series are used to infer parameters of physical or biological interests. Traditional solutions for these problems often ignore the spatial or temporal correlations in the data (static model), or simply model the data summarized over time (time-averaged model). In either case, the data information that contains the spatiotemporal interactions is not fully utilized for parameter learning, which leads to insufficient modeling in these problems. In this paper, we apply Bayesian models based on spatiotemporal Gaussian processes (STGP) to the inverse problems with spatiotemporal data and show that the spatial and temporal information provides more effective parameter estimation and uncertainty quantification (UQ). We demonstrate the merit of Bayesian spatiotemporal modeling for inverse problems compared with traditional static and time-averaged approaches using a time-dependent advection-diffusion partial different equation (PDE) and three chaotic ordinary differential equations (ODE). We also provide theoretic justification for the superiority of spatiotemporal modeling to fit the trajectories even it appears cumbersome (e.g. for chaotic dynamics).

</details>

<details>

<summary>2022-04-23 00:26:57 - SEIRDV Model for QATAR COVID-19 Outbreak: A Case Study</summary>

- *Elizabeth Amona, Edward Boone, Ryad Ghanam*

- `2204.10961v1` - [abs](http://arxiv.org/abs/2204.10961v1) - [pdf](http://arxiv.org/pdf/2204.10961v1)

> The Covid-19 outbreak of 2020 has required many governments to develop mathematical-statistical models of the outbreak for policy and planning purposes. This work provides a tutorial on building a compartmental model using Susceptibles, Exposed, Infected, Recovered, Deaths and Vaccinated (SEIRDV) status through time. A Bayesian Framework is utilized to perform both parameter estimation and predictions. This model uses interventions to quantify the impact of various government attempts to slow the spread of the virus. Predictions are also made to determine when the peak Active Infections will occur.

</details>

<details>

<summary>2022-04-23 00:37:53 - Local Gaussian process extrapolation for BART models with applications to causal inference</summary>

- *Meijiang Wang, Jingyu He, P. Richard Hahn*

- `2204.10963v1` - [abs](http://arxiv.org/abs/2204.10963v1) - [pdf](http://arxiv.org/pdf/2204.10963v1)

> Bayesian additive regression trees (BART) is a semi-parametric regression model offering state-of-the-art performance on out-of-sample prediction. Despite this success, standard implementations of BART typically provide inaccurate prediction and overly narrow prediction intervals at points outside the range of the training data. This paper proposes a novel extrapolation strategy that grafts Gaussian processes to the leaf nodes in BART for predicting points outside the range of the observed data. The new method is compared to standard BART implementations and recent frequentist resampling-based methods for predictive inference. We apply the new approach to a challenging problem from causal inference, wherein for some regions of predictor space, only treated or untreated units are observed (but not both). In simulations studies, the new approach boasts superior performance compared to popular alternatives, such as Jackknife+.

</details>

<details>

<summary>2022-04-23 05:54:36 - TREGO: a Trust-Region Framework for Efficient Global Optimization</summary>

- *Youssef Diouane, Victor Picheny, Rodolphe Le Riche, Alexandre Scotto Di Perrotolo*

- `2101.06808v4` - [abs](http://arxiv.org/abs/2101.06808v4) - [pdf](http://arxiv.org/pdf/2101.06808v4)

> Efficient Global Optimization (EGO) is the canonical form of Bayesian optimization that has been successfully applied to solve global optimization of expensive-to-evaluate black-box problems. However, EGO struggles to scale with dimension, and offers limited theoretical guarantees. In this work, a trust-region framework for EGO (TREGO) is proposed and analyzed. TREGO alternates between regular EGO steps and local steps within a trust region. By following a classical scheme for the trust region (based on a sufficient decrease condition), the proposed algorithm enjoys global convergence properties, while departing from EGO only for a subset of optimization steps. Using extensive numerical experiments based on the well-known COCO {bound constrained problems}, we first analyze the sensitivity of TREGO to its own parameters, then show that the resulting algorithm is consistently outperforming EGO and getting competitive with other state-of-the-art black-box optimization methods.

</details>

<details>

<summary>2022-04-23 06:06:22 - The Statistical Complexity of Interactive Decision Making</summary>

- *Dylan J. Foster, Sham M. Kakade, Jian Qian, Alexander Rakhlin*

- `2112.13487v2` - [abs](http://arxiv.org/abs/2112.13487v2) - [pdf](http://arxiv.org/pdf/2112.13487v2)

> A fundamental challenge in interactive learning and decision making, ranging from bandit problems to reinforcement learning, is to provide sample-efficient, adaptive learning algorithms that achieve near-optimal regret. This question is analogous to the classical problem of optimal (supervised) statistical learning, where there are well-known complexity measures (e.g., VC dimension and Rademacher complexity) that govern the statistical complexity of learning. However, characterizing the statistical complexity of interactive learning is substantially more challenging due to the adaptive nature of the problem. The main result of this work provides a complexity measure, the Decision-Estimation Coefficient, that is proven to be both necessary and sufficient for sample-efficient interactive learning. In particular, we provide:   1. a lower bound on the optimal regret for any interactive decision making problem, establishing the Decision-Estimation Coefficient as a fundamental limit.   2. a unified algorithm design principle, Estimation-to-Decisions (E2D), which transforms any algorithm for supervised estimation into an online algorithm for decision making. E2D attains a regret bound matching our lower bound, thereby achieving optimal sample-efficient learning as characterized by the Decision-Estimation Coefficient.   Taken together, these results constitute a theory of learnability for interactive decision making. When applied to reinforcement learning settings, the Decision-Estimation Coefficient recovers essentially all existing hardness results and lower bounds. More broadly, the approach can be viewed as a decision-theoretic analogue of the classical Le Cam theory of statistical estimation; it also unifies a number of existing approaches -- both Bayesian and frequentist.

</details>

<details>

<summary>2022-04-23 10:31:08 - SIReN-VAE: Leveraging Flows and Amortized Inference for Bayesian Networks</summary>

- *Jacobie Mouton, Steve Kroon*

- `2204.11847v1` - [abs](http://arxiv.org/abs/2204.11847v1) - [pdf](http://arxiv.org/pdf/2204.11847v1)

> Initial work on variational autoencoders assumed independent latent variables with simple distributions. Subsequent work has explored incorporating more complex distributions and dependency structures: including normalizing flows in the encoder network allows latent variables to entangle non-linearly, creating a richer class of distributions for the approximate posterior, and stacking layers of latent variables allows more complex priors to be specified for the generative model. This work explores incorporating arbitrary dependency structures, as specified by Bayesian networks, into VAEs. This is achieved by extending both the prior and inference network with graphical residual flows - residual flows that encode conditional independence by masking the weight matrices of the flow's residual blocks. We compare our model's performance on several synthetic datasets and show its potential in data-sparse settings.

</details>

<details>

<summary>2022-04-23 11:07:13 - $$BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization</summary>

- *Carl Hvarfner, Danny Stoll, Artur Souza, Marius Lindauer, Frank Hutter, Luigi Nardi*

- `2204.11051v1` - [abs](http://arxiv.org/abs/2204.11051v1) - [pdf](http://arxiv.org/pdf/2204.11051v1)

> Bayesian optimization (BO) has become an established framework and popular tool for hyperparameter optimization (HPO) of machine learning (ML) algorithms. While known for its sample-efficiency, vanilla BO can not utilize readily available prior beliefs the practitioner has on the potential location of the optimum. Thus, BO disregards a valuable source of information, reducing its appeal to ML practitioners. To address this issue, we propose $\pi$BO, an acquisition function generalization which incorporates prior beliefs about the location of the optimum in the form of a probability distribution, provided by the user. In contrast to previous approaches, $\pi$BO is conceptually simple and can easily be integrated with existing libraries and many acquisition functions. We provide regret bounds when $\pi$BO is applied to the common Expected Improvement acquisition function and prove convergence at regular rates independently of the prior. Further, our experiments show that $\pi$BO outperforms competing approaches across a wide suite of benchmarks and prior characteristics. We also demonstrate that $\pi$BO improves on the state-of-the-art performance for a popular deep learning task, with a 12.5 $\times$ time-to-accuracy speedup over prominent BO approaches.

</details>

<details>

<summary>2022-04-23 15:50:07 - Bayesian Inference and Partial Identification in Multi-Treatment Causal Inference with Unobserved Confounding</summary>

- *Jiajing Zheng, Alexander D'Amour, Alexander Franks*

- `2111.07973v2` - [abs](http://arxiv.org/abs/2111.07973v2) - [pdf](http://arxiv.org/pdf/2111.07973v2)

> In causal estimation problems, the parameter of interest is often only partially identified, implying that the parameter cannot be recovered exactly, even with infinite data. Here, we study Bayesian inference for partially identified treatment effects in multi-treatment causal inference problems with unobserved confounding. In principle, inferring the partially identified treatment effects is natural under the Bayesian paradigm, but the results can be highly sensitive to parameterization and prior specification, often in surprising ways. It is thus essential to understand which aspects of the conclusions about treatment effects are driven entirely by the prior specification. We use a so-called transparent parameterization to contextualize the effects of more interpretable scientifically motivated prior specifications on the multiple effects. We demonstrate our analysis in an example quantifying the effects of gene expression levels on mouse obesity.

</details>

<details>

<summary>2022-04-23 23:58:08 - Dimension-free Mixing for High-dimensional Bayesian Variable Selection</summary>

- *Quan Zhou, Jun Yang, Dootika Vats, Gareth O. Roberts, Jeffrey S. Rosenthal*

- `2105.05719v3` - [abs](http://arxiv.org/abs/2105.05719v3) - [pdf](http://arxiv.org/pdf/2105.05719v3)

> Yang et al. (2016) proved that the symmetric random walk Metropolis--Hastings algorithm for Bayesian variable selection is rapidly mixing under mild high-dimensional assumptions. We propose a novel MCMC sampler using an informed proposal scheme, which we prove achieves a much faster mixing time that is independent of the number of covariates, under the same assumptions. To the best of our knowledge, this is the first high-dimensional result which rigorously shows that the mixing rate of informed MCMC methods can be fast enough to offset the computational cost of local posterior evaluation. Motivated by the theoretical analysis of our sampler, we further propose a new approach called "two-stage drift condition" to studying convergence rates of Markov chains on general state spaces, which can be useful for obtaining tight complexity bounds in high-dimensional settings. The practical advantages of our algorithm are illustrated by both simulation studies and real data analysis.

</details>

<details>

<summary>2022-04-25 11:12:10 - Robust inference for non-destructive one-shot device testing under step-stress model with exponential lifetimes</summary>

- *Narayanaswamy Balakrishnan, Elena Castilla, Mara Jaenada, Leandro Pardo*

- `2204.11560v1` - [abs](http://arxiv.org/abs/2204.11560v1) - [pdf](http://arxiv.org/pdf/2204.11560v1)

> One-shot devices analysis involves an extreme case of interval censoring, wherein one can only know whether the failure time is either before or after the test time. Some kind of one-shot devices do not get destroyed when tested, and so can continue within the experiment, providing extra information for inference, if they did not fail before an inspection time. In addition, their reliability can be rapidly estimated via accelerated life tests (ALTs) by running the tests at varying and higher stress levels than working conditions. In particular, step-stress tests allow the experimenter to increase the stress levels at pre-fixed times gradually during the life-testing experiment. The cumulative exposure model is commonly assumed for step-stress models, relating the lifetime distribution of units at one stress level to the lifetime distributions at preceding stress levels. In this paper,vwe develop robust estimators and Z-type test statistics based on the density power divergence (DPD) for testing linear null hypothesis for non-destructive one-shot devices under the step-stress ALTs with exponential lifetime distribution. We study asymptotic and robustness properties of the estimators and test statistics, yielding point estimation and confidence intervals for different lifetime characteristic such as reliability, distribution quantiles and mean lifetime of the devices. A simulation study is carried out to assess the performance of the methods of inference developed here and some real-life data sets are analyzed finally for illustrative purpose.

</details>

<details>

<summary>2022-04-25 14:07:04 - Optimal day-ahead offering strategy for large producers based on market price response learning</summary>

- *Antnio Alcntara, Carlos Ruiz*

- `2204.11672v1` - [abs](http://arxiv.org/abs/2204.11672v1) - [pdf](http://arxiv.org/pdf/2204.11672v1)

> In day-ahead electricity markets based on uniform marginal pricing, small variations in the offering and bidding curves may substantially modify the resulting market outcomes. In this work, we deal with the problem of finding the optimal offering curve for a risk-averse profit-maximizing generating company (GENCO) in a data-driven context. In particular, a large GENCO's market share may imply that her offering strategy can alter the marginal price formation, which can be used to increase profit. We tackle this problem from a novel perspective. First, we propose a optimization-based methodology to summarize each GENCO's step-wise supply curves into a subset of representative price-energy blocks. Then, the relationship between the market price and the resulting energy block offering prices is modeled through a Bayesian linear regression approach, which also allows us to generate stochastic scenarios for the sensibility of the market towards the GENCO strategy, represented by the regression coefficient probabilistic distributions. Finally, this predictive model is embedded in the stochastic optimization model by employing a constraint learning approach. Results show how allowing the GENCO to deviate from her true marginal costs renders significant changes in her profits and the market marginal price. Furthermore, these results have also been tested in an out-of-sample validation setting, showing how this optimal offering strategy is also effective in a real-world market contest.

</details>

<details>

<summary>2022-04-25 15:48:05 - Scaling Up Bayesian Uncertainty Quantification for Inverse Problems using Deep Neural Networks</summary>

- *Shiwei Lan, Shuyi Li, Babak Shahbaba*

- `2101.03906v2` - [abs](http://arxiv.org/abs/2101.03906v2) - [pdf](http://arxiv.org/pdf/2101.03906v2)

> Due to the importance of uncertainty quantification (UQ), Bayesian approach to inverse problems has recently gained popularity in applied mathematics, physics, and engineering. However, traditional Bayesian inference methods based on Markov Chain Monte Carlo (MCMC) tend to be computationally intensive and inefficient for such high dimensional problems. To address this issue, several methods based on surrogate models have been proposed to speed up the inference process. More specifically, the calibration-emulation-sampling (CES) scheme has been proven to be successful in large dimensional UQ problems. In this work, we propose a novel CES approach for Bayesian inference based on deep neural network models for the emulation phase. The resulting algorithm is computationally more efficient and more robust against variations in the training set. Further, by using an autoencoder (AE) for dimension reduction, we have been able to speed up our Bayesian inference method up to three orders of magnitude. Overall, our method, henceforth called \emph{Dimension-Reduced Emulative Autoencoder Monte Carlo (DREAMC)} algorithm, is able to scale Bayesian UQ up to thousands of dimensions for inverse problems. Using two low-dimensional (linear and nonlinear) inverse problems we illustrate the validity of this approach. Next, we apply our method to two high-dimensional numerical examples (elliptic and advection-diffussion) to demonstrate its computational advantages over existing algorithms.

</details>

<details>

<summary>2022-04-25 16:06:16 - Optimal Discrete Decisions when Payoffs are Partially Identified</summary>

- *Timothy Christensen, Hyungsik Roger Moon, Frank Schorfheide*

- `2204.11748v1` - [abs](http://arxiv.org/abs/2204.11748v1) - [pdf](http://arxiv.org/pdf/2204.11748v1)

> We derive optimal statistical decision rules for discrete choice problems when the decision maker is unable to discriminate among a set of payoff distributions. In this problem, the decision maker must confront both model uncertainty (about the identity of the true payoff distribution) and statistical uncertainty (the set of payoff distributions must be estimated). We derive "efficient-robust decision rules" which minimize maximum risk or regret over the set of payoff distributions and which use the data to learn efficiently about features of the set of payoff distributions germane to the choice problem. We discuss implementation of these decision rules via the bootstrap and Bayesian methods, for both parametric and semiparametric models. Using a limits of experiments framework, we show that efficient-robust decision rules are optimal and can dominate seemingly natural alternatives. We present applications to treatment assignment using observational data and optimal pricing in environments with rich unobserved heterogeneity.

</details>

<details>

<summary>2022-04-25 16:25:51 - All You Need is a Good Functional Prior for Bayesian Deep Learning</summary>

- *Ba-Hien Tran, Simone Rossi, Dimitrios Milios, Maurizio Filippone*

- `2011.12829v2` - [abs](http://arxiv.org/abs/2011.12829v2) - [pdf](http://arxiv.org/pdf/2011.12829v2)

> The Bayesian treatment of neural networks dictates that a prior distribution is specified over their weight and bias parameters. This poses a challenge because modern neural networks are characterized by a large number of parameters, and the choice of these priors has an uncontrolled effect on the induced functional prior, which is the distribution of the functions obtained by sampling the parameters from their prior distribution. We argue that this is a hugely limiting aspect of Bayesian deep learning, and this work tackles this limitation in a practical and effective way. Our proposal is to reason in terms of functional priors, which are easier to elicit, and to "tune" the priors of neural network parameters in a way that they reflect such functional priors. Gaussian processes offer a rigorous framework to define prior distributions over functions, and we propose a novel and robust framework to match their prior with the functional prior of neural networks based on the minimization of their Wasserstein distance. We provide vast experimental evidence that coupling these priors with scalable Markov chain Monte Carlo sampling offers systematically large performance improvements over alternative choices of priors and state-of-the-art approximate Bayesian deep learning approaches. We consider this work a considerable step in the direction of making the long-standing challenge of carrying out a fully Bayesian treatment of neural networks, including convolutional neural networks, a concrete possibility.

</details>

<details>

<summary>2022-04-25 16:46:47 - Bayesian estimation of in-game home team win probability for college basketball</summary>

- *Jason Maddox, Ryan Sides, Jane Harvill*

- `2204.11777v1` - [abs](http://arxiv.org/abs/2204.11777v1) - [pdf](http://arxiv.org/pdf/2204.11777v1)

> Two new Bayesian methods for estimating and predicting in-game home team win probabilities are proposed. The first method has a prior that adjusts as a function of lead differential and time elapsed. The second is an adjusted version of the first, where the adjustment is a linear combination of the Bayesian estimator with a time-weighted pre-game win probability. The proposed methods are compared to existing methods, showing the new methods perform better for both estimation and prediction. The utility is illustrated via an application to the 2016 NCAA Division 1 Championship game.

</details>

<details>

<summary>2022-04-26 05:26:52 - Deep Quantile Regression for Uncertainty Estimation in Unsupervised and Supervised Lesion Detection</summary>

- *Haleh Akrami, Anand Joshi, Sergul Aydore, Richard Leahy*

- `2109.09374v2` - [abs](http://arxiv.org/abs/2109.09374v2) - [pdf](http://arxiv.org/pdf/2109.09374v2)

> Despite impressive state-of-the-art performance on a wide variety of machine learning tasks, deep learning methods can produce over-confident predictions, particularly with limited training data. Therefore, quantifying uncertainty is particularly important in critical applications such as lesion detection and clinical diagnosis, where a realistic assessment of uncertainty is essential in determining surgical margins, disease status and appropriate treatment. In this work, we propose a novel approach that uses quantile regression for quantifying aleatoric uncertainty in both supervised and unsupervised lesion detection problems. The resulting confidence intervals can be used for lesion detection and segmentation. In the unsupervised setting, we combine quantile regression with the Variational AutoEncoder (VAE). Here we address the problem of quantifying uncertainty in the images that are reconstructed by the VAE as the basis for principled outlier or lesion detection. The VAE models the output as a conditionally independent Gaussian characterized by its mean and variance. Unfortunately, joint optimization of both mean and variance in the VAE leads to the well-known problem of shrinkage or underestimation of variance. Here we describe an alternative Quantile-Regression VAE (QR-VAE) that avoids this variance shrinkage problem by directly estimating conditional quantiles for the input image. Using the estimated quantiles, we compute the conditional mean and variance for the input image from which we then detect outliers by thresholding at a false-discovery-rate corrected p-value. In the supervised setting, we develop binary quantile regression (BQR) for the supervised lesion segmentation task. We show how BQR can be used to capture uncertainty in lesion boundaries in a manner that characterizes expert disagreement.

</details>

<details>

<summary>2022-04-26 08:42:52 - Nonparametric Multiple-Output Center-Outward Quantile Regression</summary>

- *Eustasio del Barrio, Alberto Gonzalez Sanz, Marc Hallin*

- `2204.11756v2` - [abs](http://arxiv.org/abs/2204.11756v2) - [pdf](http://arxiv.org/pdf/2204.11756v2)

> Based on the novel concept of multivariate center-outward quantiles introduced recently in Chernozhukov et al. (2017) and Hallin et al. (2021), we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content irrespective of the underlying distribution; their graphs constitute nested center-outward quantile regression tubes. Empirical counterparts of these concepts are constructed, yielding interpretable empirical regions and contours which are shown to consistently reconstruct their population versions in the Pompeiu-Hausdorff topology. Our method is entirely non-parametric and performs well in simulations including heteroskedasticity and nonlinear trends; its power as a data-analytic tool is illustrated on some real datasets.

</details>

<details>

<summary>2022-04-26 08:47:04 - A Differentially Private Probabilistic Framework for Modeling the Variability Across Federated Datasets of Heterogeneous Multi-View Observations</summary>

- *Irene Balelli, Santiago Silva, Marco Lorenzi*

- `2204.07352v2` - [abs](http://arxiv.org/abs/2204.07352v2) - [pdf](http://arxiv.org/pdf/2204.07352v2)

> We propose a novel federated learning paradigm to model data variability among heterogeneous clients in multi-centric studies. Our method is expressed through a hierarchical Bayesian latent variable model, where client-specific parameters are assumed to be realization from a global distribution at the master level, which is in turn estimated to account for data bias and variability across clients. We show that our framework can be effectively optimized through expectation maximization (EM) over latent master's distribution and clients' parameters. We also introduce formal differential privacy (DP) guarantees compatibly with our EM optimization scheme. We tested our method on the analysis of multi-modal medical imaging data and clinical scores from distributed clinical datasets of patients affected by Alzheimer's disease. We demonstrate that our method is robust when data is distributed either in iid and non-iid manners, even when local parameters perturbation is included to provide DP guarantees. Moreover, the variability of data, views and centers can be quantified in an interpretable manner, while guaranteeing high-quality data reconstruction as compared to state-of-the-art autoencoding models and federated learning schemes. The code is available at https://gitlab.inria.fr/epione/federated-multi-views-ppca.

</details>

<details>

<summary>2022-04-26 12:30:10 - Grand canonical ensembles of sparse networks and Bayesian inference</summary>

- *Ginestra Bianconi*

- `2204.06466v2` - [abs](http://arxiv.org/abs/2204.06466v2) - [pdf](http://arxiv.org/pdf/2204.06466v2)

> Maximum entropy network ensembles have been very successful in modelling sparse network topologies and in solving challenging inference problems. However the sparse maximum entropy network models proposed so far have fixed number of nodes and are typically not exchangeable. Here we consider hierarchical models for exchangeable networks in the sparse limit, i.e. with the total number of links scaling linearly with the total number of nodes. The approach is grand canonical, i.e. the number of nodes of the network is not fixed a priori: it is finite but can be arbitrarily large. In this way the grand canonical network ensembles circumvent the difficulties in treating infinite sparse exchangeable networks which according to the Aldous-Hoover theorem must vanish. The approach can treat networks with given degree distribution or networks with given distribution of latent variables. When only a subgraph induced by a subset of nodes is known, this model allows a Bayesian estimation of the network size and the degree sequence (or the sequence of latent variables) of the entire network which can be used for network reconstruction.

</details>

<details>

<summary>2022-04-26 15:48:24 - PAC-Bayes training for neural networks: sparsity and uncertainty quantification</summary>

- *Maximilian F. Steffen, Mathias Trabs*

- `2204.12392v1` - [abs](http://arxiv.org/abs/2204.12392v1) - [pdf](http://arxiv.org/pdf/2204.12392v1)

> We study the Gibbs posterior distribution from PAC-Bayes theory for sparse deep neural nets in a nonparametric regression setting. To access the posterior distribution, an efficient MCMC algorithm based on backpropagation is constructed. The training yields a Bayesian neural network with a joint distribution on the network parameters. Using a mixture over uniform priors on sparse sets of networks weights, we prove an oracle inequality which shows that the method adapts to the unknown regularity and hierarchical structure of the regression function. Studying the Gibbs posterior distribution from a frequentist Bayesian perspective, we analyze the diameter and show high coverage probability of the resulting credible sets. The method is illustrated in a simulation example.

</details>

<details>

<summary>2022-04-26 16:02:25 - Knowledge Transfer in Engineering Fleets: Hierarchical Bayesian Modelling for Multi-Task Learning</summary>

- *L. A. Bull, M. Dhada, O. Steinert, T. Lindgren, A. K. Parlikad, A. B. Duncan, M. Girolami*

- `2204.12404v1` - [abs](http://arxiv.org/abs/2204.12404v1) - [pdf](http://arxiv.org/pdf/2204.12404v1)

> We propose a population-level analysis to address issues of data sparsity when building predictive models of engineering infrastructure. By sharing information between similar assets, hierarchical Bayesian modelling is used to improve the survival analysis of a truck fleet (hazard curves) and power prediction in a wind farm (power curves). In each example, a set of correlated functions are learnt over the asset fleet, in a combined inference, to learn a population model. Parameter estimation is improved when sub-fleets of assets are allowed to share correlated information at different levels in the hierarchy. In turn, groups with incomplete data automatically borrow statistical strength from those that are data-rich. The correlations can be inspected to inform which assets share information for which effect (i.e. parameter).

</details>

<details>

<summary>2022-04-26 23:55:14 - Multivariate and regression models for directional data based on projected Plya trees</summary>

- *Luis E. Nieto-Barajas*

- `2204.12635v1` - [abs](http://arxiv.org/abs/2204.12635v1) - [pdf](http://arxiv.org/pdf/2204.12635v1)

> Projected distributions have proved to be useful in the study of circular and directional data. Although any multivariate distribution can be used to produce a projected model, these distributions are typically parametric. In this article we consider a multivariate P\'olya tree on $R^k$ and project it to the unit hypersphere $S^k$ to define a new Bayesian nonparametric model for directional data. We study the properties of the proposed model and in particular, concentrate on the implied conditional distributions of some directions given the others to define a directional-directional regression model. We also define a multivariate linear regression model with P\'olya tree error and project it to define a linear-directional regression model. We obtain the posterior characterisation of all models and show their performance with simulated and real datasets.

</details>

<details>

<summary>2022-04-27 17:38:13 - Variational Kalman Filtering with Hinf-Based Correction for Robust Bayesian Learning in High Dimensions</summary>

- *Niladri Das, Jed A. Duersch, Thomas A. Catanach*

- `2204.13089v1` - [abs](http://arxiv.org/abs/2204.13089v1) - [pdf](http://arxiv.org/pdf/2204.13089v1)

> In this paper, we address the problem of convergence of sequential variational inference filter (VIF) through the application of a robust variational objective and Hinf-norm based correction for a linear Gaussian system. As the dimension of state or parameter space grows, performing the full Kalman update with the dense covariance matrix for a large scale system requires increased storage and computational complexity, making it impractical. The VIF approach, based on mean-field Gaussian variational inference, reduces this burden through the variational approximation to the covariance usually in the form of a diagonal covariance approximation. The challenge is to retain convergence and correct for biases introduced by the sequential VIF steps. We desire a framework that improves feasibility while still maintaining reasonable proximity to the optimal Kalman filter as data is assimilated. To accomplish this goal, a Hinf-norm based optimization perturbs the VIF covariance matrix to improve robustness. This yields a novel VIF- Hinf recursion that employs consecutive variational inference and Hinf based optimization steps. We explore the development of this method and investigate a numerical example to illustrate the effectiveness of the proposed filter.

</details>

<details>

<summary>2022-04-27 18:32:37 - Adaptive Bayesian Changepoint Analysis and Local Outlier Scoring</summary>

- *Haoxuan Wu, David S. Matteson*

- `2011.09437v3` - [abs](http://arxiv.org/abs/2011.09437v3) - [pdf](http://arxiv.org/pdf/2011.09437v3)

> We introduce global-local shrinkage priors into a Bayesian dynamic linear model to adaptively estimate both changepoints and local outliers in a novel model we call Adaptive Bayesian Changepoints with Outliers (ABCO). We utilize a state-space approach to identify a dynamic signal in the presence of outliers and measurement error with stochastic volatility. We find that global state equation parameters are inadequate for most real applications and we include local parameters to track noise at each time-step. This setup provides a flexible framework to detect unspecified changepoints in complex series, such as those with large interruptions in local trends, with robustness to outliers and heteroskedastic noise. ABCO may also be used as a robust Bayesian trend filter that can reconstruct interrupted time series. We detail the extension of our approach to time-varying parameter estimation within dynamic regression analysis to identify structural breaks. Finally, we compare our algorithm against several alternatives to demonstrate its efficacy in diverse simulation scenarios and three empirical examples.

</details>

<details>

<summary>2022-04-27 19:14:13 - Impulse response estimation via flexible local projections</summary>

- *Haroon Mumtaz, Michele Piffer*

- `2204.13150v1` - [abs](http://arxiv.org/abs/2204.13150v1) - [pdf](http://arxiv.org/pdf/2204.13150v1)

> This paper introduces a flexible local projection that generalizes the model by Jord\'a (2005) to a non-parametric setting using Bayesian Additive Regression Trees. Monte Carlo experiments show that our BART-LP model is able to capture non-linearities in the impulse responses. Our first application shows that the fiscal multiplier is stronger in recession than in expansion only in response to contractionary fiscal shocks, but not in response to expansionary fiscal shocks. We then show that financial shocks generate effects on the economy that increase more than proportionately in the size of the shock when the shock is negative, but not when the shock is positive.

</details>

<details>

<summary>2022-04-27 19:58:26 - R-MBO: A Multi-surrogate Approach for Preference Incorporation in Multi-objective Bayesian Optimisation</summary>

- *Tinkle Chugh*

- `2204.13166v1` - [abs](http://arxiv.org/abs/2204.13166v1) - [pdf](http://arxiv.org/pdf/2204.13166v1)

> Many real-world multi-objective optimisation problems rely on computationally expensive function evaluations. Multi-objective Bayesian optimisation (BO) can be used to alleviate the computation time to find an approximated set of Pareto optimal solutions. In many real-world problems, a decision-maker has some preferences on the objective functions. One approach to incorporate the preferences in multi-objective BO is to use a scalarising function and build a single surrogate model (mono-surrogate approach) on it. This approach has two major limitations. Firstly, the fitness landscape of the scalarising function and the objective functions may not be similar. Secondly, the approach assumes that the scalarising function distribution is Gaussian, and thus a closed-form expression of an acquisition function e.g., expected improvement can be used. We overcome these limitations by building independent surrogate models (multi-surrogate approach) on each objective function and show that the distribution of the scalarising function is not Gaussian. We approximate the distribution using Generalised value distribution. We present an a-priori multi-surrogate approach to incorporate the desirable objective function values (or reference point) as the preferences of a decision-maker in multi-objective BO. The results and comparison with the existing mono-surrogate approach on benchmark and real-world optimisation problems show the potential of the proposed approach.

</details>

<details>

<summary>2022-04-28 07:54:23 - Performance analysis of greedy algorithms for minimising a Maximum Mean Discrepancy</summary>

- *Luc Pronzato*

- `2101.07564v2` - [abs](http://arxiv.org/abs/2101.07564v2) - [pdf](http://arxiv.org/pdf/2101.07564v2)

> We analyse the performance of several iterative algorithms for the quantisation of a probability measure $\mu$, based on the minimisation of a Maximum Mean Discrepancy (MMD). Our analysis includes kernel herding, greedy MMD minimisation and Sequential Bayesian Quadrature (SBQ). We show that the finite-sample-size approximation error, measured by the MMD, decreases as $1/n$ for SBQ and also for kernel herding and greedy MMD minimisation when using a suitable step-size sequence. The upper bound on the approximation error is slightly better for SBQ, but the other methods are significantly faster, with a computational cost that increases only linearly with the number of points selected. This is illustrated by two numerical examples, with the target measure $\mu$ being uniform (a space-filling design application) and with $\mu$ a Gaussian mixture. They suggest that the bounds derived in the paper are overly pessimistic, in particular for SBQ. The sources of this pessimism are identified but seem difficult to counter.

</details>

<details>

<summary>2022-04-28 09:59:37 - Frequentist perspective on robust parameter estimation using the ensemble Kalman filter</summary>

- *Sebastian Reich*

- `2201.00611v2` - [abs](http://arxiv.org/abs/2201.00611v2) - [pdf](http://arxiv.org/pdf/2201.00611v2)

> Standard maximum likelihood or Bayesian approaches to parameter estimation of stochastic differential equations are not robust to perturbations in the continuous-in-time data. In this note, we give a rather elementary explanation of this observation in the context of continuous-time parameter estimation using an ensemble Kalman filter. We employ the frequentist perspective to shed new light on two robust estimation techniques; namely subsampling the data and rough path corrections. We illustrate our findings through a simple numerical experiment.

</details>

<details>

<summary>2022-04-28 11:10:48 - Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces</summary>

- *Alexander Terenin*

- `2202.10613v3` - [abs](http://arxiv.org/abs/2202.10613v3) - [pdf](http://arxiv.org/pdf/2202.10613v3)

> Bayesian learning using Gaussian processes provides a foundational framework for making decisions in a manner that balances what is known with what could be learned by gathering data. In this dissertation, we develop techniques for broadening the applicability of Gaussian processes. This is done in two ways. Firstly, we develop pathwise conditioning techniques for Gaussian processes, which allow one to express posterior random functions as prior random functions plus a dependent update term. We introduce a wide class of efficient approximations built from this viewpoint, which can be randomly sampled once in advance, and evaluated at arbitrary locations without any subsequent stochasticity. This key property improves efficiency and makes it simpler to deploy Gaussian process models in decision-making settings. Secondly, we develop a collection of Gaussian process models over non-Euclidean spaces, including Riemannian manifolds and graphs. We derive fully constructive expressions for the covariance kernels of scalar-valued Gaussian processes on Riemannian manifolds and graphs. Building on these ideas, we describe a formalism for defining vector-valued Gaussian processes on Riemannian manifolds. The introduced techniques allow all of these models to be trained using standard computational methods. In total, these contributions make Gaussian processes easier to work with and allow them to be used within a wider class of domains in an effective and principled manner. This, in turn, makes it possible to potentially apply Gaussian processes to novel decision-making settings.

</details>

<details>

<summary>2022-04-28 11:44:29 - Interpretable collective intelligence of non-rational human agents</summary>

- *Alexey V. Osipov, Nikolay N. Osipov*

- `2204.13424v1` - [abs](http://arxiv.org/abs/2204.13424v1) - [pdf](http://arxiv.org/pdf/2204.13424v1)

> We outline how to create a mechanism that provides an optimal way to elicit, from an arbitrary group of experts, the probability of the truth of an arbitrary logical proposition together with collective information that has an explicit form and interprets this probability. Such a system could, in particular, incentivize experts from all over the world to collectively solve scientific or medical problems in a very efficient manner. In our main considerations about real experts, they are not assumed to be Bayesian and their behavior is described by utilities that satisfy the von Neumann-Morgenstern axioms only locally.

</details>

<details>

<summary>2022-04-28 12:58:39 - Revisiting Bayesian Autoencoders with MCMC</summary>

- *Rohitash Chandra, Mahir Jain, Manavendra Maharana, Pavel N. Krivitsky*

- `2104.05915v2` - [abs](http://arxiv.org/abs/2104.05915v2) - [pdf](http://arxiv.org/pdf/2104.05915v2)

> Autoencoders gained popularity in the deep learning revolution given their ability to compress data and provide dimensionality reduction. Although prominent deep learning methods have been used to enhance autoencoders, the need to provide robust uncertainty quantification remains a challenge. This has been addressed with variational autoencoders so far. Bayesian inference via Markov Chain Monte Carlo (MCMC) sampling has faced several limitations for large models; however, recent advances in parallel computing and advanced proposal schemes have opened routes less traveled. This paper presents Bayesian autoencoders powered by MCMC sampling implemented using parallel computing and Langevin-gradient proposal distribution. The results indicate that the proposed Bayesian autoencoder provides similar performance accuracy when compared to related methods in the literature. Furthermore, it provides uncertainty quantification in the reduced data representation. This motivates further applications of the Bayesian autoencoder framework for other deep learning models.

</details>

<details>

<summary>2022-04-28 13:53:19 - Variational Inference with NoFAS: Normalizing Flow with Adaptive Surrogate for Computationally Expensive Models</summary>

- *Yu Wang, Fang Liu, Daniele E. Schiavazzi*

- `2108.12657v2` - [abs](http://arxiv.org/abs/2108.12657v2) - [pdf](http://arxiv.org/pdf/2108.12657v2)

> Fast inference of numerical model parameters from data is an important prerequisite to generate predictive models for a wide range of applications. Use of sampling-based approaches such as Markov chain Monte Carlo may become intractable when each likelihood evaluation is computationally expensive. New approaches combining variational inference with normalizing flow are characterized by a computational cost that grows only linearly with the dimensionality of the latent variable space, and rely on gradient-based optimization instead of sampling, providing a more efficient approach for Bayesian inference about the model parameters. Moreover, the cost of frequently evaluating an expensive likelihood can be mitigated by replacing the true model with an offline trained surrogate model, such as neural networks. However, this approach might generate significant bias when the surrogate is insufficiently accurate around the posterior modes. To reduce the computational cost without sacrificing inferential accuracy, we propose Normalizing Flow with Adaptive Surrogate (NoFAS), an optimization strategy that alternatively updates the normalizing flow parameters and surrogate model parameters. We also propose an efficient sample weighting scheme for surrogate model training that preserves global accuracy while effectively capturing high posterior density regions. We demonstrate the inferential and computational superiority of NoFAS against various benchmarks, including cases where the underlying model lacks identifiability. The source code and numerical experiments used for this study are available at https://github.com/cedricwangyu/NoFAS.

</details>

<details>

<summary>2022-04-28 15:06:48 - On the Use of $L$-functionals in Regression Models</summary>

- *Ola Hssjer, Mns Karlsson*

- `2204.13552v1` - [abs](http://arxiv.org/abs/2204.13552v1) - [pdf](http://arxiv.org/pdf/2204.13552v1)

> In this paper we survey and unify a large class or $L$-functionals of the conditional distribution of the response variable in regression models. This includes robust measures of location, scale, skewness, and heavytailedness of the response, conditionally on covariates. We generalize the concepts of $L$-moments (Sittinen, 1969), $L$-skewness, and $L$-kurtosis (Hosking, 1990) and introduce order numbers for a large class of $L$-functionals through orthogonal series expansions of quantile functions. In particular, we motivate why location, scale, skewness, and heavytailedness have order numbers 1, 2, (3,2), and (4,2) respectively and describe how a family of $L$-functionals, with different order numbers, is constructed from Legendre, Hermite, Laguerre or other types of polynomials. Our framework is applied to models where the relationship between quantiles of the response and the covariates follow a transformed linear model, with a link function that determines the appropriate class of $L$-functionals. In this setting, the distribution of the response is treated parametrically or nonparametrically, and the response variable is either censored/truncated or not. We also provide a unified asymptotic theory of estimates of $L$-functionals, and illustrate our approach by analyzing the arrival time distribution of migrating birds. In this context a novel version of the coefficient of determination is introduced, which makes use of the abovementioned orthogonal series expansion.

</details>

<details>

<summary>2022-04-28 15:23:17 - Uncertainty Quantification for nonparametric regression using Empirical Bayesian neural networks</summary>

- *Stefan Franssen, Botond Szab*

- `2204.12735v2` - [abs](http://arxiv.org/abs/2204.12735v2) - [pdf](http://arxiv.org/pdf/2204.12735v2)

> We propose a new, two-step empirical Bayes-type of approach for neural networks. We show in context of the nonparametric regression model that the procedure (up to a logarithmic factor) provides optimal recovery of the underlying functional parameter of interest and provides Bayesian credible sets with frequentist coverage guarantees. The approach requires fitting the neural network only once, hence it is substantially faster than Bootstrapping type approaches. We demonstrate the applicability of our method over synthetic data, observing good estimation properties and reliable uncertainty quantification.

</details>

<details>

<summary>2022-04-28 16:09:51 - Constrained Conditional Moment Restriction Models</summary>

- *Victor Chernozhukov, Whitney K. Newey, Andres Santos*

- `1509.06311v3` - [abs](http://arxiv.org/abs/1509.06311v3) - [pdf](http://arxiv.org/pdf/1509.06311v3)

> Shape restrictions have played a central role in economics as both testable implications of theory and sufficient conditions for obtaining informative counterfactual predictions. In this paper we provide a general procedure for inference under shape restrictions in identified and partially identified models defined by conditional moment restrictions. Our test statistics and proposed inference methods are based on the minimum of the generalized method of moments (GMM) objective function with and without shape restrictions. Uniformly valid critical values are obtained through a bootstrap procedure that approximates a subset of the true local parameter space. In an empirical analysis of the effect of childbearing on female labor supply, we show that employing shape restrictions in linear instrumental variables (IV) models can lead to shorter confidence regions for both local and average treatment effects. Other applications we discuss include inference for the variability of quantile IV treatment effects and for bounds on average equivalent variation in a demand model with general heterogeneity. We find in Monte Carlo examples that the critical values are conservatively accurate and that tests about objects of interest have good power relative to unrestricted GMM.

</details>

<details>

<summary>2022-04-28 16:19:50 - Bernstein - von Mises theorem and misspecified models: a review</summary>

- *Natalia Bochkina*

- `2204.13614v1` - [abs](http://arxiv.org/abs/2204.13614v1) - [pdf](http://arxiv.org/pdf/2204.13614v1)

> This is a review of asymptotic and non-asymptotic behaviour of Bayesian methods under model specification. In particular we focus on consistency, i.e. convergence of the posterior distribution to the point mass at the best parametric approximation to the true model, and conditions for it to be locally Gaussian around this point. For well specified regular models, variance of the Gaussian approximation coincides with the Fisher information, making Bayesian inference asymptotically efficient. In this review, we discuss how this is affected by model misspecification. We also discuss approaches to adjust Bayesian inference to make it asymptotically efficient under model misspecification.

</details>

<details>

<summary>2022-04-29 10:35:59 - Statistical applications of contrastive learning</summary>

- *Michael U. Gutmann, Steven Kleinegesse, Benjamin Rhodes*

- `2204.13999v1` - [abs](http://arxiv.org/abs/2204.13999v1) - [pdf](http://arxiv.org/pdf/2204.13999v1)

> The likelihood function plays a crucial role in statistical inference and experimental design. However, it is computationally intractable for several important classes of statistical models, including energy-based models and simulator-based models. Contrastive learning is an intuitive and computationally feasible alternative to likelihood-based learning. We here first provide an introduction to contrastive learning and then show how we can use it to derive methods for diverse statistical problems, namely parameter estimation for energy-based models, Bayesian inference for simulator-based models, as well as experimental design.

</details>

<details>

<summary>2022-04-29 11:52:25 - Geophysical Inversion and Optimal Transport</summary>

- *Malcolm Sambridge, Andrew Jackson, Andrew P. Valentine*

- `2204.14027v1` - [abs](http://arxiv.org/abs/2204.14027v1) - [pdf](http://arxiv.org/pdf/2204.14027v1)

> We propose a new approach to measuring the agreement between two oscillatory time series, such as seismic waveforms, and demonstrate that it can be employed effectively in inverse problems. Our approach is based on Optimal Transport theory and the Wasserstein distance, with a novel transformation of the time series to ensure that necessary normalisation and positivity conditions are met. Our measure is differentiable, and can readily be employed within an optimization framework. We demonstrate performance with a variety of synthetic examples, including seismic source inversion, and observe substantially better convergence properties than achieved with conventional $L_2$ misfits. We also briefly discuss the relationship between Optimal Transport and Bayesian inference.

</details>

<details>

<summary>2022-04-29 13:39:39 - Bayesian Information Criterion for Event-based Multi-trial Ensemble data</summary>

- *Kaidi Shao, Nikos K. Logothetis, Michel Besserve*

- `2204.14096v1` - [abs](http://arxiv.org/abs/2204.14096v1) - [pdf](http://arxiv.org/pdf/2204.14096v1)

> Transient recurring phenomena are ubiquitous in many scientific fields like neuroscience and meteorology. Time inhomogenous Vector Autoregressive Models (VAR) may be used to characterize peri-event system dynamics associated with such phenomena, and can be learned by exploiting multi-dimensional data gathering samples of the evolution of the system in multiple time windows comprising, each associated with one occurrence of the transient phenomenon, that we will call "trial". However, optimal VAR model order selection methods, commonly relying on the Akaike or Bayesian Information Criteria (AIC/BIC), are typically not designed for multi-trial data. Here we derive the BIC methods for multi-trial ensemble data which are gathered after the detection of the events. We show using simulated bivariate AR models that the multi-trial BIC is able to recover the real model order. We also demonstrate with simulated transient events and real data that the multi-trial BIC is able to estimate a sufficiently small model order for dynamic system modeling.

</details>

<details>

<summary>2022-04-29 14:18:12 - Inverse Probability Weighting: the Missing Link between Survey Sampling and Evidence Estimation</summary>

- *Jyotishka Datta, Nicholas Polson*

- `2204.14121v1` - [abs](http://arxiv.org/abs/2204.14121v1) - [pdf](http://arxiv.org/pdf/2204.14121v1)

> We consider the class of inverse probability weight (IPW) estimators, including the popular Horwitz-Thompson and Hajek estimators used routinely in survey sampling, causal inference and evidence estimation for Bayesian computation. We focus on the 'weak paradoxes' for these estimators due to two counterexamples by Basu (1988) and Wasserman (2004) and investigate the two natural Bayesian answers to this problem: one based on binning and smoothing : a 'Bayesian sieve' and the other based on a conjugate hierarchical model that allows borrowing information via exchangeability. We show that the two Bayesian estimators achieve lower mean squared errors in Wasserman's example compared to simple IPW estimators via simulation studies on a broad range of parameter configurations. We prove posterior consistency for the Bayes estimator and show how it requires fewer assumptions on the inclusion probabilities. We also revisit the connection between the different problems where improved or adaptive IPW estimators will be useful, including survey sampling, evidence estimation strategies such as Conditional Monte Carlo, Riemannian sum, Trapezoidal rules and vertical likelihood, as well as average treatment effect estimation in causal inference.

</details>

<details>

<summary>2022-04-29 16:44:11 - Incorporating Actor Heterogeneity into Large Network Models through Variational Approximations</summary>

- *Nadja Klein, Gran Kauermann*

- `2204.14214v1` - [abs](http://arxiv.org/abs/2204.14214v1) - [pdf](http://arxiv.org/pdf/2204.14214v1)

> The analysis of network data has gained considerable interest in the recent years. This also includes the analysis of large, high dimensional networks with hundreds and thousands of nodes. While Exponential Random Graph Models (ERGMs) serve as workhorse for network data analyses, their applicability to very large networks is problematic via classical inference such as maximum likelihood or fully Bayesian estimation due to scaling and instability issues. The latter trace from the fact, that classical network statistics consider nodes as exchangeable, i.e. actors in the network are assumed to be homogeneous. This is often questionable and one way to circumvent the restrictive assumption is to include actor specific random effects which account for unobservable heterogeneity. This in turn however increases the number of unknowns considerably making the model highly-parameterized. As a solution even for very large networks we propose a scalable approach based on variational approximations, which not only leads to numerically stable estimation but is also applicable to high-dimensional directed as well as undirected networks. We furthermore show that including node specific covariates can reduce node heterogeneity which we facilitate through versatile prior formulations. We demonstrate the procedure in two complex examples, namely Facebook data and data from an international arms trading network.

</details>

<details>

<summary>2022-04-29 22:24:44 - Bayesian Benefit Risk Analysis</summary>

- *Konstantinos Vamvourellis, Konstantinos Kalogeropoulos, Lawrence Phillips*

- `2205.00093v1` - [abs](http://arxiv.org/abs/2205.00093v1) - [pdf](http://arxiv.org/pdf/2205.00093v1)

> The process of approving and assessing new drugs is often quite complicated, mainly due to the fact that multiple criteria need to be considered. A standard way to proceed is with benefit risk analysis, often under the Bayesian paradigm to account for uncertainty and combine data with expert judgement, which is operationalised via multi-criteria decision analysis (MCDA) scores. The procedure is based on a suitable model to accommodate key features of the data, which are typically of mixed type and potentially depended, with factor models providing a standard choice. The contribution of this paper is threefold: first, we extend the family of existing structured factor models. Second, we provide a framework for choosing between them, which combines fit and out-of-sample predictive performance. Third, we present a sequential estimation framework that can offer multiple benefits: (i) it allows us to efficiently re-estimate MCDA scores of different drugs each time new data become available, thus getting an idea on potential fluctuations in differences between them, (ii) it can provide information on potential early stopping in cases of evident conclusions, thus reducing unnecessary further exposure to undesirable treatments; (iii) it can potentially allow to assign treatment groups dynamically based on research objectives. A drawback of sequential estimation is the increased computational time, but this can be mitigated by efficient sequential Monte Carlo schemes which we tailor in this paper to the context of Bayesian benefit risk analysis. The developed methodology is illustrated on real data on Type II diabetes patients who were administered Metformin (MET), Rosiglitazone (RSG) and a combination of the two (AVM).

</details>

<details>

<summary>2022-04-29 22:52:09 - On Unspanned Latent Risks in Dynamic Term Structure Models</summary>

- *Tomasz Dubiel-Teleszynski, Konstantinos Kalogeropoulos, Nikolaos Karouzakis*

- `2205.00098v1` - [abs](http://arxiv.org/abs/2205.00098v1) - [pdf](http://arxiv.org/pdf/2205.00098v1)

> We explore the importance of information hidden from the yield curve and assess how valuable the unspanned risks are to a real-time Bayesian investor seeking to forecast excess bond returns and maximise her utility. We propose a novel class of arbitrage-free unspanned Dynamic Term Structure Models (DTSM), that embed a stochastic market price of risk specification. We develop a suitable Sequential Monte Carlo (SMC) inferential and prediction scheme that guarantees joint identification of parameters and latent states and takes into account all relevant uncertainties. We find that latent factors contain significant predictive power above and beyond the yield curve, providing improvement to the out-of-sample predictive performance of models, especially at shorter maturities. Most importantly, they are capable of exploiting information hidden from the yield curve and translate the evident statistical predictability into significant utility gains, out-of-sample. The hidden component associated with slope risk is countercyclical and links with real activity.

</details>

<details>

<summary>2022-04-30 01:00:30 - Accelerated Parallel Non-conjugate Sampling for Bayesian Non-parametric Models</summary>

- *Michael Minyi Zhang, Sinead A. Williamson, Fernando Perez-Cruz*

- `1705.07178v7` - [abs](http://arxiv.org/abs/1705.07178v7) - [pdf](http://arxiv.org/pdf/1705.07178v7)

> Inference of latent feature models in the Bayesian nonparametric setting is generally difficult, especially in high dimensional settings, because it usually requires proposing features from some prior distribution. In special cases, where the integration is tractable, we can sample new feature assignments according to a predictive likelihood. We present a novel method to accelerate the mixing of latent variable model inference by proposing feature locations based on the data, as opposed to the prior. First, we introduce an accelerated feature proposal mechanism that we show is a valid MCMC algorithm for posterior inference. Next, we propose an approximate inference strategy to perform accelerated inference in parallel. A two-stage algorithm that combines the two approaches provides a computationally attractive method that can quickly reach local convergence to the posterior distribution of our model, while allowing us to exploit parallelization.

</details>

<details>

<summary>2022-04-30 03:19:41 - Improved Approximation to First-Best Gains-from-Trade</summary>

- *Yumou Fei*

- `2205.00140v1` - [abs](http://arxiv.org/abs/2205.00140v1) - [pdf](http://arxiv.org/pdf/2205.00140v1)

> We study the two-agent single-item bilateral trade. Ideally, the trade should happen whenever the buyer's value for the item exceeds the seller's cost. However, the classical result of Myerson and Satterthwaite showed that no mechanism can achieve this without violating one of the Bayesian incentive compatibility, individual rationality and weakly balanced budget conditions. This motivates the study of approximating the trade-whenever-socially-beneficial mechanism, in terms of the expected gains-from-trade. Recently, Deng, Mao, Sivan, and Wang showed that the random-offerer mechanism achieves at least a 1/8.23 approximation. We improve this lower bound to 1/3.15 in this paper. We also determine the exact worst-case approximation ratio of the seller-pricing mechanism assuming the distribution of the buyer's value satisfies the monotone hazard rate property.

</details>

<details>

<summary>2022-04-30 09:17:27 - A new approach to posterior contraction rates via Wasserstein dynamics</summary>

- *Emanuele Dolera, Stefano Favaro, Edoardo Mainini*

- `2011.14425v2` - [abs](http://arxiv.org/abs/2011.14425v2) - [pdf](http://arxiv.org/pdf/2011.14425v2)

> This paper presents a new approach to the classical problem of quantifying posterior contraction rates (PCRs) in Bayesian statistics. Our approach relies on Wasserstein distance, and it leads to two main contributions which improve on the existing literature of PCRs. The first contribution exploits the dynamic formulation of Wasserstein distance, for short referred to as Wasserstein dynamics, in order to establish PCRs under dominated Bayesian statistical models. As a novelty with respect to existing approaches to PCRs, Wasserstein dynamics allows us to circumvent the use of sieves in both stating and proving PCRs, and it sets forth a natural connection between PCRs and three well-known classical problems in statistics and probability theory: the speed of mean Glivenko-Cantelli convergence, the estimation of weighted Poincar\'e-Wirtinger constants and Sanov large deviation principle for Wasserstein distance. The second contribution combines the use of Wasserstein distance with a suitable sieve construction to establish PCRs under full Bayesian nonparametric models. As a novelty with respect to existing literature of PCRs, our second result provides with the first treatment of PCRs under non-dominated Bayesian models. Applications of our results are presented for some classical Bayesian statistical models, e.g., regular parametric models, infinite-dimensional exponential families, linear regression in infinite dimension and nonparametric models under Dirichlet process priors.

</details>

<details>

<summary>2022-04-30 10:14:22 - Regression-Adjusted Estimation of Quantile Treatment Effects under Covariate-Adaptive Randomizations</summary>

- *Liang Jiang, Peter C. B. Phillips, Yubo Tao, Yichong Zhang*

- `2105.14752v3` - [abs](http://arxiv.org/abs/2105.14752v3) - [pdf](http://arxiv.org/pdf/2105.14752v3)

> Datasets from field experiments with covariate-adaptive randomizations (CARs) usually contain extra covariates in addition to the strata indicators. We propose to incorporate these additional covariates via auxiliary regressions in the estimation and inference of unconditional quantile treatment effects (QTEs) under CARs. We establish the consistency and limit distribution of the regression-adjusted QTE estimator and prove that the use of multiplier bootstrap inference is non-conservative under CARs. The auxiliary regression may be estimated parametrically, nonparametrically, or via regularization when the data are high-dimensional. Even when the auxiliary regression is misspecified, the proposed bootstrap inferential procedure still achieves the nominal rejection probability in the limit under the null. When the auxiliary regression is correctly specified, the regression-adjusted estimator achieves the minimum asymptotic variance. We also discuss forms of adjustments that can improve the efficiency of the QTE estimators. The finite sample performance of the new estimation and inferential methods is studied in simulations and an empirical application to a well-known dataset concerned with expanding access to basic bank accounts on savings is reported.

</details>

<details>

<summary>2022-04-30 18:23:06 - Bayesian Models for Multivariate Difference Boundary Detection in Areal Data</summary>

- *Leiwen Gao, Sudipto Banerjee, Beate Ritz*

- `2205.00318v1` - [abs](http://arxiv.org/abs/2205.00318v1) - [pdf](http://arxiv.org/pdf/2205.00318v1)

> Regional aggregates of health outcomes over delineated administrative units (e.g., states, counties, zip codes), or areal units, are widely used by epidemiologists to map mortality or incidence rates and capture geographic variation. To capture health disparities over regions, we seek "difference boundaries" that separate neighboring regions with significantly different spatial effects. Matters are more challenging with multiple outcomes over each unit, where we capture dependence among diseases as well as across the areal units. Here, we address multivariate difference boundary detection for correlated diseases. We formulate the problem in terms of Bayesian pairwise multiple comparisons and seek the posterior probabilities of neighboring spatial effects being different. To achieve this, we endow the spatial random effects with a discrete probability law using a class of multivariate areally-referenced Dirichlet process (MARDP) models that accommodate spatial and inter-disease dependence. We evaluate our method through simulation studies and detect difference boundaries for multiple cancers using data from the Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute.

</details>


## 2022-05

<details>

<summary>2022-05-01 05:46:13 - A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness</summary>

- *Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zack Nado, Jasper Snoek, Dustin Tran, Balaji Lakshminarayanan*

- `2205.00403v1` - [abs](http://arxiv.org/abs/2205.00403v1) - [pdf](http://arxiv.org/pdf/2205.00403v1)

> Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks, SNGP outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning. Code is open-sourced at https://github.com/google/uncertainty-baselines

</details>

<details>

<summary>2022-05-01 07:32:33 - Smoothed quantile regression for censored residual life</summary>

- *Kyu Hyun Kim, Daniel J. Caplan, Sangwook Kang*

- `2205.00413v1` - [abs](http://arxiv.org/abs/2205.00413v1) - [pdf](http://arxiv.org/pdf/2205.00413v1)

> We consider a regression modeling of the quantiles of residual life, remaining lifetime at a specific time. We propose a smoothed induced version of the existing non-smooth estimating equations approaches for estimating regression parameters. The proposed estimating equations are smooth in regression parameters, so solutions can be readily obtained via standard numerical algorithms. Moreover, the smoothness in the proposed estimating equations enables one to obtain a robust sandwich-type covariance estimator of regression estimators aided by an efficient resampling method. To handle data subject to right censoring, the inverse probability of censoring weight are used as weights. The consistency and asymptotic normality of the proposed estimator are established. Extensive simulation studies are conducted to validate the proposed estimator's performance in various finite samples settings. We apply the proposed method to dental study data evaluating the longevity of dental restorations.

</details>

<details>

<summary>2022-05-02 06:17:10 - Controlled Discovery and Localization of Signals via Bayesian Linear Programming</summary>

- *Asher Spector, Lucas Janson*

- `2203.17208v2` - [abs](http://arxiv.org/abs/2203.17208v2) - [pdf](http://arxiv.org/pdf/2203.17208v2)

> Scientists often must simultaneously discover signals and localize them as precisely as possible. For instance, in genetic fine-mapping, high correlations between nearby genetic variants make it hard to identify the exact locations of causal variants. So the statistical task is to output as many disjoint regions containing a signal as possible, each as small as possible, while controlling false positives. Similar problems arise in any application where signals cannot be perfectly localized, such as locating stars in astronomical surveys and change point detection in time series data. Our first contribution is to propose a notion of resolution-adjusted power for such problems. Second, we introduce Bayesian Linear Programming (BLiP), a Bayesian method for jointly detecting and localizing signals. BLiP overcomes an extremely high-dimensional and non-convex problem to verifiably nearly maximize expected power while provably controlling false positives. BLiP is very computationally efficient and can wrap around nearly any Bayesian model and algorithm. Applying BLiP to existing state-of-the-art analyses of UK Biobank data (for genetic fine-mapping) and the Sloan Digital Sky Survey (for astronomical point source detection) increased resolution-adjusted power by 30-120% in just a few minutes of computation. BLiP is implemented in the new packages pyblip (Python) and blipr (R).

</details>

<details>

<summary>2022-05-02 07:12:12 - Variational Bayesian Inference for a Polytomous-Attribute Saturated Diagnostic Classification Model with Parallel Computing</summary>

- *Motonori Oka, Shun Saso, Kensuke Okada*

- `2107.01865v3` - [abs](http://arxiv.org/abs/2107.01865v3) - [pdf](http://arxiv.org/pdf/2107.01865v3)

> As a statistical tool to assist formative assessments in educational settings, diagnostic classification models (DCMs) have been increasingly used to provide diagnostic information regarding examinees' attributes. DCMs often adopt a dichotomous division such as the mastery and non-mastery of attributes to express the mastery states of attributes. However, many practical settings involve different levels of mastery states rather than a simple dichotomy in a single attribute. Although this practical demand can be addressed by polytomous-attribute DCMs, their computational cost in a Markov chain Monte Carlo estimation impedes their large-scale application due to the larger number of polytomous-attribute mastery patterns than that of binary-attribute ones. This study considers a scalable Bayesian estimation method for polytomous-attribute DCMs and developed a variational Bayesian (VB) algorithm for a polytomous-attribute saturated DCM -- a generalization of polytomous-attribute DCMs -- by building on the existing literature on polytomous-attribute DCMs and VB for binary-attribute DCMs. Furthermore, we proposed the configuration of parallel computing for the proposed VB algorithm to achieve better computational efficiency. Monte Carlo simulations revealed that our method exhibited the high performance in parameter recovery under a wide range of conditions. An empirical example is used to demonstrate the utility of our method.

</details>

<details>

<summary>2022-05-02 10:42:34 - Policy Implications of Statistical Estimates: A General Bayesian Decision-Theoretic Model for Binary Outcomes</summary>

- *Akisato Suzuki*

- `2008.10903v4` - [abs](http://arxiv.org/abs/2008.10903v4) - [pdf](http://arxiv.org/pdf/2008.10903v4)

> How should we evaluate the effect of a policy on the likelihood of an undesirable event, such as conflict? The significance test has three limitations. First, relying on statistical significance misses the fact that uncertainty is a continuous scale. Second, focusing on a standard point estimate overlooks the variation in plausible effect sizes. Third, the criterion of substantive significance is rarely explained or justified. A new Bayesian decision-theoretic model, "causal binary loss function model," overcomes these issues. It compares the expected loss under a policy intervention with the one under no intervention. These losses are computed based on a particular range of the effect sizes of a policy, the probability mass of this effect size range, the cost of the policy, and the cost of the undesirable event the policy intends to address. The model is more applicable than common statistical decision-theoretic models using the standard loss functions or capturing costs in terms of false positives and false negatives. I exemplify the model's use through three applications and provide an R package.

</details>

<details>

<summary>2022-05-02 13:04:25 - Modeling and mitigation of occupational safety risks in dynamic industrial environments</summary>

- *Ashutosh Tewari, Antonio R. Paiva*

- `2205.00894v1` - [abs](http://arxiv.org/abs/2205.00894v1) - [pdf](http://arxiv.org/pdf/2205.00894v1)

> Identifying and mitigating safety risks is paramount in a number of industries. In addition to guidelines and best practices, many industries already have safety management systems (SMSs) designed to monitor and reinforce good safety behaviors. The analytic capabilities to analyze the data acquired through such systems, however, are still lacking in terms of their ability to robustly quantify risks posed by various occupational hazards. Moreover, best practices and modern SMSs are unable to account for dynamically evolving environments/behavioral characteristics commonly found in many industrial settings. This article proposes a method to address these issues by enabling continuous and quantitative assessment of safety risks in a data-driven manner. The backbone of our method is an intuitive hierarchical probabilistic model that explains sparse and noisy safety data collected by a typical SMS. A fully Bayesian approach is developed to calibrate this model from safety data in an online fashion. Thereafter, the calibrated model holds necessary information that serves to characterize risk posed by different safety hazards. Additionally, the proposed model can be leveraged for automated decision making, for instance solving resource allocation problems -- targeted towards risk mitigation -- that are often encountered in resource-constrained industrial environments. The methodology is rigorously validated on a simulated test-bed and its scalability is demonstrated on real data from large maintenance projects at a petrochemical plant.

</details>

<details>

<summary>2022-05-02 14:19:52 - Multiple hypothesis screening using mixtures of non-local distributions</summary>

- *Francesco Denti, Stefano Peluso, Michele Guindani, Antonietta Mira*

- `2205.00930v1` - [abs](http://arxiv.org/abs/2205.00930v1) - [pdf](http://arxiv.org/pdf/2205.00930v1)

> The analysis of large-scale datasets, especially in biomedical contexts, frequently involves a principled screening of multiple hypotheses. The celebrated two-group model jointly models the distribution of the test statistics with mixtures of two competing densities, the null and the alternative distributions. We investigate the use of weighted densities and, in particular, non-local densities as working alternative distributions, to enforce separation from the null and thus refine the screening procedure. We show how these weighted alternatives improve various operating characteristics, such as the Bayesian False Discovery rate, of the resulting tests for a fixed mixture proportion with respect to a local, unweighted likelihood approach. Parametric and nonparametric model specifications are proposed, along with efficient samplers for posterior inference. By means of a simulation study, we exhibit how our model outperforms both well-established and state-of-the-art alternatives in terms of various operating characteristics. Finally, to illustrate the versatility of our method, we conduct three differential expression analyses with publicly-available datasets from genomic studies of heterogeneous nature.

</details>

<details>

<summary>2022-05-02 18:17:20 - Incorporating Measurement Error in Astronomical Object Classification</summary>

- *Sarah Shy, Hyungsuk Tak, Eric D. Feigelson, John D. Timlin, G. Jogesh Babu*

- `2112.06831v2` - [abs](http://arxiv.org/abs/2112.06831v2) - [pdf](http://arxiv.org/pdf/2112.06831v2)

> Most general-purpose classification methods, such as support-vector machine (SVM) and random forest (RF), fail to account for an unusual characteristic of astronomical data: known measurement error uncertainties. In astronomical data, this information is often given in the data but discarded because popular machine learning classifiers cannot incorporate it. We propose a simulation-based approach that incorporates heteroscedastic measurement error into existing classification method to better quantify uncertainty in classification. The proposed method first simulates perturbed realizations of the data from a Bayesian posterior predictive distribution of a Gaussian measurement error model. Then, a chosen classifier is fit to each simulation. The variation across the simulations naturally reflects the uncertainty propagated from the measurement errors in both labeled and unlabeled data sets. We demonstrate the use of this approach via two numerical studies. The first is a thorough simulation study applying the proposed procedure to SVM and RF, which are well-known hard and soft classifiers, respectively. The second study is a realistic classification problem of identifying high-$z$ $(2.9 \leq z \leq 5.1)$ quasar candidates from photometric data. The data are from merged catalogs of the Sloan Digital Sky Survey, the $Spitzer$ IRAC Equatorial Survey, and the $Spitzer$-HETDEX Exploratory Large-Area Survey. The proposed approach reveals that out of 11,847 high-$z$ quasar candidates identified by a random forest without incorporating measurement error, 3,146 are potential misclassifications with measurement error. Additionally, out of $1.85$ million objects not identified as high-$z$ quasars without measurement error, 936 can be considered new candidates with measurement error.

</details>

<details>

<summary>2022-05-02 21:37:54 - COMET Flows: Towards Generative Modeling of Multivariate Extremes and Tail Dependence</summary>

- *Andrew McDonald, Pang-Ning Tan, Lifeng Luo*

- `2205.01224v1` - [abs](http://arxiv.org/abs/2205.01224v1) - [pdf](http://arxiv.org/pdf/2205.01224v1)

> Normalizing flows, a popular class of deep generative models, often fail to represent extreme phenomena observed in real-world processes. In particular, existing normalizing flow architectures struggle to model multivariate extremes, characterized by heavy-tailed marginal distributions and asymmetric tail dependence among variables. In light of this shortcoming, we propose COMET (COpula Multivariate ExTreme) Flows, which decompose the process of modeling a joint distribution into two parts: (i) modeling its marginal distributions, and (ii) modeling its copula distribution. COMET Flows capture heavy-tailed marginal distributions by combining a parametric tail belief at extreme quantiles of the marginals with an empirical kernel density function at mid-quantiles. In addition, COMET Flows capture asymmetric tail dependence among multivariate extremes by viewing such dependence as inducing a low-dimensional manifold structure in feature space. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of COMET Flows in capturing both heavy-tailed marginals and asymmetric tail dependence compared to other state-of-the-art baseline architectures. All code is available on GitHub at https://github.com/andrewmcdonald27/COMETFlows.

</details>

<details>

<summary>2022-05-02 23:39:12 - Heterogeneous Treatment Effects for Networks, Panels, and other Outcome Matrices</summary>

- *Eric Auerbach, Yong Cai*

- `2205.01246v1` - [abs](http://arxiv.org/abs/2205.01246v1) - [pdf](http://arxiv.org/pdf/2205.01246v1)

> We are interested in the distribution of treatment effects for an experiment where units are randomized to treatment but outcomes are measured for pairs of units. For example, we might measure risk sharing links between households enrolled in a microfinance program, employment relationships between workers and firms exposed to a trade shock, or bids from bidders to items assigned to an auction format. Such a double randomized experimental design may be appropriate when there are social interactions, market externalities, or other spillovers across units assigned to the same treatment. Or it may describe a natural or quasi experiment given to the researcher. In this paper, we propose a new empirical strategy based on comparing the eigenvalues of the outcome matrices associated with each treatment. Our proposal is based on a new matrix analog of the Fr\'echet-Hoeffding bounds that play a key role in the standard theory. We first use this result to bound the distribution of treatment effects. We then propose a new matrix analog of quantile treatment effects based on the difference in the eigenvalues. We call this analog spectral treatment effects.

</details>

<details>

<summary>2022-05-03 02:56:42 - Complementary Goodness of Fit Procedure for Crash Frequency Models</summary>

- *Mohammadreza Hashemi, Adrian Ricardo Archilla*

- `2205.01279v1` - [abs](http://arxiv.org/abs/2205.01279v1) - [pdf](http://arxiv.org/pdf/2205.01279v1)

> This paper presents a new procedure for evaluating the goodness of fit of Generalized Linear Models (GLM) estimated with Roadway Departure (RwD) crash frequency data for the State of Hawaii on two-lane two-way (TLTW) state roads. The procedure is analyzed using ten years of RwD crash data (including all severity levels) and roadway characteristics (e.g., traffic, geometry, and inventory databases) that can be aggregated at the section level. The three estimation methods evaluated using the proposed procedure include: Negative Binomial (NB), Zero-Inflated Negative Binomial (ZINB), and Generalized Linear Mixed Model-Negative Binomial (GLMM-NB). The procedure shows that the three methodologies can provide very good fits in terms of the distributions of crashes within narrow ranges of the predicted mean frequency of crashes and in terms of observed vs. predicted average crash frequencies for those data segments. The proposed procedure complements other statistics such as Akaike Information Criterion, Bayesian Information Criterion, and Log-likelihood used for model selection. It is consistent with those statistics for models without random effects, but it diverges for GLMM-NB models. The procedure can aid model selection by providing a clear visualization of the fit of crash frequency models and allowing the computation of a pseudo R2 similar the one used in linear regression. It is recommended to evaluate its use for evaluating the trade-off between the number of random effects in GLMM-NB models and their goodness of fit using more appropriate datasets that do not lead to convergence problems.

</details>

<details>

<summary>2022-05-03 09:29:36 - Integration of Behavioral Economic Models to Optimize ML performance and interpretability: a sandbox example</summary>

- *Emilio Soria-Olivas, Jos E. Vila Gisbert, Regino Barranquero Cardeosa, Yolanda Gomez*

- `2205.01387v1` - [abs](http://arxiv.org/abs/2205.01387v1) - [pdf](http://arxiv.org/pdf/2205.01387v1)

> This paper presents a sandbox example of how the integration of models borrowed from Behavioral Economic (specifically Protection-Motivation Theory) into ML algorithms (specifically Bayesian Networks) can improve the performance and interpretability of ML algorithms when applied to Behavioral Data. The integration of Behavioral Economics knowledge to define the architecture of the Bayesian Network increases the accuracy of the predictions in 11 percentage points. Moreover, it simplifies the training process, making unnecessary training computational efforts to identify the optimal structure of the Bayesian Network. Finally, it improves the explicability of the algorithm, avoiding illogical relations among variables that are not supported by previous behavioral cybersecurity literature. Although preliminary and limited to 0ne simple model trained with a small dataset, our results suggest that the integration of behavioral economics and complex ML models may open a promising strategy to improve the predictive power, training costs and explicability of complex ML models. This integration will contribute to solve the scientific issue of ML exhaustion problem and to create a new ML technology with relevant scientific, technological and market implications.

</details>

<details>

<summary>2022-05-03 10:06:37 - A robust Bayesian analysis of variable selection under prior ignorance</summary>

- *Tathagata Basu, Matthias C. M. Troffaes, Jochen Einbeck*

- `2204.13341v2` - [abs](http://arxiv.org/abs/2204.13341v2) - [pdf](http://arxiv.org/pdf/2204.13341v2)

> We propose a cautious Bayesian variable selection routine by investigating the sensitivity of a hierarchical model, where the regression coefficients are specified by spike and slab priors. We exploit the use of latent variables to understand the importance of the co-variates. These latent variables also allow us to obtain the size of the model space which is an important aspect of high dimensional problems. In our approach, instead of fixing a single prior, we adopt a specific type of robust Bayesian analysis, where we consider a set of priors within the same parametric family to specify the selection probabilities of these latent variables. We achieve that by considering a set of expected prior selection probabilities, which allows us to perform a sensitivity analysis to understand the effect of prior elicitation on the variable selection. The sensitivity analysis provides us sets of posteriors for the regression coefficients as well as the selection indicators and we show that the posterior odds of the model selection probabilities are monotone with respect to the prior expectations of the selection probabilities. We also analyse synthetic and real life datasets to illustrate our cautious variable selection method and compare it with other well known methods.

</details>

<details>

<summary>2022-05-03 11:56:58 - spOccupancy: An R package for single-species, multi-species, and integrated spatial occupancy models</summary>

- *Jeffrey W. Doser, Andrew O. Finley, Marc Kry, Elise F. Zipkin*

- `2111.12163v2` - [abs](http://arxiv.org/abs/2111.12163v2) - [pdf](http://arxiv.org/pdf/2111.12163v2)

> Occupancy modeling is a common approach to assess spatial and temporal species distribution patterns, while explicitly accounting for measurement errors common in detection-nondetection data. Numerous extensions of the basic single species occupancy model exist to address dynamics, multiple species or states, interactions, false positive errors, autocorrelation, and to integrate multiple data sources. However, development of specialized and computationally efficient software to fit spatial models to large data sets is scarce or absent. We introduce the spOccupancy R package designed to fit single-species, multi-species, and integrated spatially-explicit occupancy models. Using a Bayesian framework, we leverage P\'olya-Gamma data augmentation and Nearest Neighbor Gaussian Processes to ensure models are computationally efficient for potentially massive data sets. spOccupancy provides user-friendly functions for data simulation, model fitting, model validation (by posterior predictive checks), model comparison (using information criteria and k-fold cross-validation), and out-of-sample prediction. We illustrate the package's functionality via a vignette, simulated data analysis, and two bird case studies, in which we estimate occurrence of the Black-throated Green Warbler (Setophaga virens) across the eastern USA and species richness of a foliage-gleaning bird community in the Hubbard Brook Experimental Forest in New Hampshire, USA. The spOccupancy package provides a user-friendly approach to fit a variety of single and multi-species occupancy models, making it straightforward to address detection biases and spatial autocorrelation in species distribution models even for large data sets.

</details>

<details>

<summary>2022-05-03 12:04:18 - A comparison of mixed-variables Bayesian optimization approaches</summary>

- *Jhouben Cuesta-Ramirez, Rodolphe Le Riche, Olivier Roustant, Guillaume Perrin, Cedric Durantin, Alain Gliere*

- `2111.01533v3` - [abs](http://arxiv.org/abs/2111.01533v3) - [pdf](http://arxiv.org/pdf/2111.01533v3)

> Most real optimization problems are defined over a mixed search space where the variables are both discrete and continuous. In engineering applications, the objective function is typically calculated with a numerically costly black-box simulation.General mixed and costly optimization problems are therefore of a great practical interest, yet their resolution remains in a large part an open scientific question. In this article, costly mixed problems are approached through Gaussian processes where the discrete variables are relaxed into continuous latent variables. The continuous space is more easily harvested by classical Bayesian optimization techniques than a mixed space would. Discrete variables are recovered either subsequently to the continuous optimization, or simultaneously with an additional continuous-discrete compatibility constraint that is handled with augmented Lagrangians. Several possible implementations of such Bayesian mixed optimizers are compared. In particular, the reformulation of the problem with continuous latent variables is put in competition with searches working directly in the mixed space. Among the algorithms involving latent variables and an augmented Lagrangian, a particular attention is devoted to the Lagrange multipliers for which a local and a global estimation techniques are studied. The comparisons are based on the repeated optimization of three analytical functions and a beam design problem.

</details>

<details>

<summary>2022-05-03 13:57:40 - A hierarchical Bayesian non-asymptotic extreme value model for spatial data</summary>

- *Federica Stolf, Antonio Canale*

- `2205.01499v1` - [abs](http://arxiv.org/abs/2205.01499v1) - [pdf](http://arxiv.org/pdf/2205.01499v1)

> Spatial maps of extreme precipitation are crucial in flood protection. With the aim of producing maps of precipitation return levels, we propose a novel approach to model a collection of spatially distributed time series where the asymptotic assumption, typical of the traditional extreme value theory, is relaxed. We introduce a Bayesian hierarchical model that accounts for the possible underlying variability in the distribution of event magnitudes and occurrences, which are described through latent temporal and spatial processes. Spatial dependence is characterized by geographical covariates and effects not fully described by the covariates are captured by spatial structure in the hierarchies. The performance of the approach is illustrated through simulation studies and an application to daily rainfall extremes across North Carolina (USA). The results show that we significantly reduce the estimation uncertainty with respect to state of the art techniques.

</details>

<details>

<summary>2022-05-03 16:48:16 - Efficiently Generating Independent Samples Directly from the Posterior Distribution for a Large Class of Bayesian Generalized Linear Mixed Effects Models</summary>

- *Jonathan R. Bradley*

- `2203.10028v2` - [abs](http://arxiv.org/abs/2203.10028v2) - [pdf](http://arxiv.org/pdf/2203.10028v2)

> Markov chain Monte Carlo (MCMC) is an all-purpose tool that allows one to generate dependent replicates from a posterior distribution for effectively any Bayesian hierarchical model. As such, MCMC has become a standard in Bayesian statistics. However, convergence issues, tuning, and the effective sample size of the MCMC are nontrivial considerations that are often overlooked or can be difficult to assess. Moreover, these practical issues can produce a significant computational burden. This motivates us to consider finding closed-form expressions of the posterior distribution that are computationally straightforward to sample from directly. We focus on a broad class of Bayesian generalized linear mixed-effects models (GLMM) that allows one to jointly model data of different types (e.g., Gaussian, Poisson, and binomial distributed observations). Exact sampling from the posterior distribution for Bayesian GLMMs is such a difficult problem that it is now arguably overlooked as a possible problem to solve. To solve this problem, we derive a new class of distributions that gives one the flexibility to specify the prior on fixed and random effects to be any conjugate multivariate distribution. We refer to this new distribution as the generalized conjugate multivariate (GCM) distribution, and several technical results are provided. The expression of the exact posterior distribution is given along with the steps to obtain direct independent simulations from the posterior distribution. These direct simulations have an efficient projection/regression form, and hence, we refer to our method as Exact Posterior Regression (EPR). Several theoretical results are developed that create the foundation for EPR. Illustrative examples are provided including a simulation study and an analysis of estimates from the U.S. Census Bureau's American Community Survey (ACS).

</details>

<details>

<summary>2022-05-03 19:27:37 - Station-wise statistical joint assessment of wind speed and direction under future climates across the United States</summary>

- *Qiuyi Wu, Julie Bessac, Whitney Huang, Jiali Wang*

- `2205.02936v1` - [abs](http://arxiv.org/abs/2205.02936v1) - [pdf](http://arxiv.org/pdf/2205.02936v1)

> This study develops a statistical conditional approach to evaluate climate model performance in wind speed and direction and to project their future changes under the representative concentration pathway 8.5 scenario over inland and offshore locations across the Continental United States. The proposed conditional approach extends the scope of existing studies by characterizing the changes of the full range of the joint wind speed and direction distribution. Directional wind speed distributions are estimated using two statistical methods: a Weibull distributional regression model and a quantile regression model, both of which enforce the circular constraint to their resulting estimates of directional distributions. Projected uncertainties associated with different climate models and model internal variability are investigated and compared with the climate change signal to quantify the statistical significance of the future projections. In particular this work extends the concept of internal variability to the standard deviation and high quantiles to assess the relative magnitudes to their projected changes. The evaluation results show that the studied climate model capture both historical wind speed, wind direction, and their dependencies reasonably well over both inland and offshore locations. In the future, most of the locations show no significant changes in mean wind speeds in both winter and summer, although the changes in standard deviation and 95th-quantile show some robust changes over certain locations in winter. The proposed conditional approach enables the characterization of the directional wind speed distributions, which offers additional insights for the joint assessment of speed and direction.

</details>

<details>

<summary>2022-05-03 19:49:57 - Bzier Curve Gaussian Processes</summary>

- *Ronny Hug, Stefan Becker, Wolfgang Hbner, Michael Arens, Jrgen Beyerer*

- `2205.01754v1` - [abs](http://arxiv.org/abs/2205.01754v1) - [pdf](http://arxiv.org/pdf/2205.01754v1)

> Probabilistic models for sequential data are the basis for a variety of applications concerned with processing timely ordered information. The predominant approach in this domain is given by neural networks, which incorporate either stochastic units or components. This paper proposes a new probabilistic sequence model building on probabilistic B\'ezier curves. Using Gaussian distributed control points, these parametric curves pose a special case for Gaussian processes (GP). Combined with a Mixture Density network, Bayesian conditional inference can be performed without the need for mean field variational approximation or Monte Carlo simulation, which is a requirement of common approaches. For assessing this hybrid model's viability, it is applied to an exemplary sequence prediction task. In this case the model is used for pedestrian trajectory prediction, where a generated prediction also serves as a GP prior. Following this, the initial prediction can be refined using the GP framework by calculating different posterior distributions, in order to adapt more towards a given observed trajectory segment.

</details>

<details>

<summary>2022-05-03 21:40:36 - Bayesian index models for heterogeneous treatment effects</summary>

- *Hyung Park, Danni Wu, Eva Petkova, Thaddeus Tarpey, R. Todd Ogden*

- `2205.01795v1` - [abs](http://arxiv.org/abs/2205.01795v1) - [pdf](http://arxiv.org/pdf/2205.01795v1)

> The general idea of this article is to develop a Bayesian model with a flexible link function connecting an exponential family treatment response to a linear combination of covariates and a treatment indicator and the interaction between the two. Generalized linear models allowing data-driven link functions are often called "single-index models," and among popular semi-parametric modeling methods. In this article, we will focus on modeling heterogeneous treatment effects, with the goal of developing a treatment benefit index (TBI) incorporating prior information from historical data. This treatment benefit index can be useful for stratifying patients according to their predicted treatment benefit levels and can be especially useful for precision health applications. The proposed method is applied to a COVID-19 treatment study.

</details>

<details>

<summary>2022-05-04 03:35:12 - Machine Learning based Framework for Robust Price-Sensitivity Estimation with Application to Airline Pricing</summary>

- *Ravi Kumar, Shahin Boluki, Karl Isler, Jonas Rauch, Darius Walczak*

- `2205.01875v1` - [abs](http://arxiv.org/abs/2205.01875v1) - [pdf](http://arxiv.org/pdf/2205.01875v1)

> We consider the problem of dynamic pricing of a product in the presence of feature-dependent price sensitivity. Based on the Poisson semi-parametric approach, we construct a flexible yet interpretable demand model where the price related part is parametric while the remaining (nuisance) part of the model is non-parametric and can be modeled via sophisticated ML techniques. The estimation of price-sensitivity parameters of this model via direct one-stage regression techniques may lead to biased estimates. We propose a two-stage estimation methodology which makes the estimation of the price-sensitivity parameters robust to biases in the nuisance parameters of the model. In the first-stage we construct the estimators of observed purchases and price given the feature vector using sophisticated ML estimators like deep neural networks. Utilizing the estimators from the first-stage, in the second-stage we leverage a Bayesian dynamic generalized linear model to estimate the price-sensitivity parameters. We test the performance of the proposed estimation schemes on simulated and real sales transaction data from Airline industry. Our numerical studies demonstrate that the two-stage approach provides more accurate estimates of price-sensitivity parameters as compared to direct one-stage approach.

</details>

<details>

<summary>2022-05-04 03:43:21 - A Change Dynamic Model for the Online Detection of Gradual Change</summary>

- *Chris Browne*

- `2205.01054v3` - [abs](http://arxiv.org/abs/2205.01054v3) - [pdf](http://arxiv.org/pdf/2205.01054v3)

> Changes in the statistical properties of a stochastic process are typically assumed to occur via change-points, which demark instantaneous moments of complete and total change in process behavior. In cases where these transitions occur gradually, this assumption can result in a reduced ability to properly identify and respond to process change. With this observation in mind, we introduce a novel change-dynamic model for the online detection of gradual change in a Bayesian framework, in which change-points are used within a hierarchical model to indicate moments of gradual change onset or termination. We apply this model to synthetic data and EEG readings drawn during epileptic seizure, where we find our change-dynamic model can enable faster and more accurate identification of gradual change than traditional change-point models allow.

</details>

<details>

<summary>2022-05-04 18:12:17 - DeepBayes -- an estimator for parameter estimation in stochastic nonlinear dynamical models</summary>

- *Anubhab Ghosh, Mohamed Abdalmoaty, Saikat Chatterjee, Hkan Hjalmarsson*

- `2205.02264v1` - [abs](http://arxiv.org/abs/2205.02264v1) - [pdf](http://arxiv.org/pdf/2205.02264v1)

> Stochastic nonlinear dynamical systems are ubiquitous in modern, real-world applications. Yet, estimating the unknown parameters of stochastic, nonlinear dynamical models remains a challenging problem. The majority of existing methods employ maximum likelihood or Bayesian estimation. However, these methods suffer from some limitations, most notably the substantial computational time for inference coupled with limited flexibility in application. In this work, we propose DeepBayes estimators that leverage the power of deep recurrent neural networks in learning an estimator. The method consists of first training a recurrent neural network to minimize the mean-squared estimation error over a set of synthetically generated data using models drawn from the model set of interest. The a priori trained estimator can then be used directly for inference by evaluating the network with the estimation data. The deep recurrent neural network architectures can be trained offline and ensure significant time savings during inference. We experiment with two popular recurrent neural networks -- long short term memory network (LSTM) and gated recurrent unit (GRU). We demonstrate the applicability of our proposed method on different example models and perform detailed comparisons with state-of-the-art approaches. We also provide a study on a real-world nonlinear benchmark problem. The experimental evaluations show that the proposed approach is asymptotically as good as the Bayes estimator.

</details>

<details>

<summary>2022-05-04 18:47:17 - Choosing Exogeneity Assumptions in Potential Outcome Models</summary>

- *Matthew A. Masten, Alexandre Poirier*

- `2205.02288v1` - [abs](http://arxiv.org/abs/2205.02288v1) - [pdf](http://arxiv.org/pdf/2205.02288v1)

> There are many kinds of exogeneity assumptions. How should researchers choose among them? When exogeneity is imposed on an unobservable like a potential outcome, we argue that the form of exogeneity should be chosen based on the kind of selection on unobservables it allows. Consequently, researchers can assess the plausibility of any exogeneity assumption by studying the distributions of treatment given the unobservables that are consistent with that assumption. We use this approach to study two common exogeneity assumptions: quantile and mean independence. We show that both assumptions require a kind of non-monotonic relationship between treatment and the potential outcomes. We discuss how to assess the plausibility of this kind of treatment selection. We also show how to define a new and weaker version of quantile independence that allows for monotonic treatment selection. We then show the implications of the choice of exogeneity assumption for identification. We apply these results in an empirical illustration of the effect of child soldiering on wages.

</details>

<details>

<summary>2022-05-04 19:06:39 - Population Predictive Checks</summary>

- *Gemma E. Moran, David M. Blei, Rajesh Ranganath*

- `1908.00882v4` - [abs](http://arxiv.org/abs/1908.00882v4) - [pdf](http://arxiv.org/pdf/1908.00882v4)

> Bayesian modeling has become a staple for researchers to articulate assumptions and develop methods tailored for specific data applications. Thanks to recent developments in approximate posterior inference, researchers can easily build, use, and revise complicated Bayesian models for large and rich data. These new abilities, however, bring into focus the problem of model criticism. Researchers need tools to diagnose the fitness of their models, to understand where they fall short, and to guide their revision. In this paper we develop a new method for Bayesian model criticism, the population predictive check (POP-PC). POP-PCs are built on posterior predictive checks (PPCs), a seminal method that checks a model by assessing the posterior predictive distribution on the observed data. However, PPCs use the data twice -- both to calculate the posterior predictive and to evaluate it -- which can lead to overconfident assessments of the quality of a model. POP-PCs, in contrast, compare the posterior predictive distribution to a draw from the population distribution, which in practice is a heldout dataset. We prove this strategy, which blends Bayesian modeling with frequentist assessment, is calibrated, unlike the PPC. Moreover, we demonstrate that calibrating PPC p-values post-hoc does not resolve the "double use of the data" problem. Finally, we study POP-PCs on classical regression and a hierarchical model of text data.

</details>

<details>

<summary>2022-05-04 22:44:31 - Polarization and Media Bias</summary>

- *A. Arda Gitmez, Pooya Molavi*

- `2203.12698v2` - [abs](http://arxiv.org/abs/2203.12698v2) - [pdf](http://arxiv.org/pdf/2203.12698v2)

> This paper presents a model of partisan media trying to persuade a sophisticated and heterogeneous audience. We base our analysis on a Bayesian persuasion framework where receivers have heterogeneous preferences and beliefs. We identify an intensive-versus-extensive-margin trade-off that drives the media's choice of slant: Biasing the news garners more support from the audience who follows the media but reduces the size of the audience. The media's slant and target audience are qualitatively different in polarized and unimodal (or non-polarized) societies. When the media's agenda becomes more popular, the media become more biased. When society becomes more polarized, the media become less biased. Thus, polarization may have an unexpected consequence: It may compel partisan media to be less biased and more informative.

</details>

<details>

<summary>2022-05-05 04:25:02 - A Unified Algorithm for Penalized Convolution Smoothed Quantile Regression</summary>

- *Rebeka Man, Xiaoou Pan, Kean Ming Tan, Wen-Xin Zhou*

- `2205.02432v1` - [abs](http://arxiv.org/abs/2205.02432v1) - [pdf](http://arxiv.org/pdf/2205.02432v1)

> Penalized quantile regression (QR) is widely used for studying the relationship between a response variable and a set of predictors under data heterogeneity in high-dimensional settings. Compared to penalized least squares, scalable algorithms for fitting penalized QR are lacking due to the non-differentiable piecewise linear loss function. To overcome the lack of smoothness, a recently proposed convolution-type smoothed method brings an interesting tradeoff between statistical accuracy and computational efficiency for both standard and penalized quantile regressions. In this paper, we propose a unified algorithm for fitting penalized convolution smoothed quantile regression with various commonly used convex penalties, accompanied by an R-language package conquer available from the Comprehensive R Archive Network. We perform extensive numerical studies to demonstrate the superior performance of the proposed algorithm over existing methods in both statistical and computational aspects. We further exemplify the proposed algorithm by fitting a fused lasso additive QR model on the world happiness data.

</details>

<details>

<summary>2022-05-05 07:09:15 - Markovian Persuasion with Stochastic Revelations</summary>

- *Ehud Lehrer, Dimitry Shaiderman*

- `2204.08659v2` - [abs](http://arxiv.org/abs/2204.08659v2) - [pdf](http://arxiv.org/pdf/2204.08659v2)

> In the classical Bayesian persuasion model an informed player and an uninformed one engage in a static interaction. The informed player, the sender, knows the state of nature, while the uninformed one, the receiver, does not. The informed player partially shares his private information with the receiver and the latter then, based on her belief about the state, takes an action. This action, together with the state of nature, determines the utility of both players. This paper analyzes a dynamic Bayesian persuasion model where the state of nature evolves according to a Markovian law. Here, the sender always knows the realized state, while the receiver randomly gets to know it. We discuss the value of the sender when he becomes more and more patient and its relation to the revelation rate, namely the probability at which the true state is revealed to the receiver at any stage.

</details>

<details>

<summary>2022-05-05 08:52:51 - Resilience of Bayesian Layer-Wise Explanations under Adversarial Attacks</summary>

- *Ginevra Carbone, Guido Sanguinetti, Luca Bortolussi*

- `2102.11010v3` - [abs](http://arxiv.org/abs/2102.11010v3) - [pdf](http://arxiv.org/pdf/2102.11010v3)

> We consider the problem of the stability of saliency-based explanations of Neural Network predictions under adversarial attacks in a classification task. Saliency interpretations of deterministic Neural Networks are remarkably brittle even when the attacks fail, i.e. for attacks that do not change the classification label. We empirically show that interpretations provided by Bayesian Neural Networks are considerably more stable under adversarial perturbations of the inputs and even under direct attacks to the explanations. By leveraging recent results, we also provide a theoretical explanation of this result in terms of the geometry of the data manifold. Additionally, we discuss the stability of the interpretations of high level representations of the inputs in the internal layers of a Network. Our results demonstrate that Bayesian methods, in addition to being more robust to adversarial attacks, have the potential to provide more stable and interpretable assessments of Neural Network predictions.

</details>

<details>

<summary>2022-05-05 09:59:35 - Efficient posterior sampling for Bayesian Poisson regression</summary>

- *Laura D'Angelo, Antonio Canale*

- `2109.09520v2` - [abs](http://arxiv.org/abs/2109.09520v2) - [pdf](http://arxiv.org/pdf/2109.09520v2)

> Poisson log-linear models are ubiquitous in many applications, and one of the most popular approaches for parametric count regression. In the Bayesian context, however, there are no sufficient specific computational tools for efficient sampling from the posterior distribution of parameters, and standard algorithms, such as random walk Metropolis-Hastings or Hamiltonian Monte Carlo algorithms, are typically used. Herein, we developed an efficient Metropolis-Hastings algorithm and importance sampler to simulate from the posterior distribution of the parameters of Poisson log-linear models under conditional Gaussian priors with superior performance with respect to the state-of-the-art alternatives. The key for both algorithms is the introduction of a proposal density based on a Gaussian approximation of the posterior distribution of parameters. Specifically, our result leverages the negative binomial approximation of the Poisson likelihood and the successful P\'olya-gamma data augmentation scheme. Via simulation, we obtained that the time per independent sample of the proposed samplers is competitive with that obtained using the successful Hamiltonian Monte Carlo sampling, with the Metropolis-Hastings showing superior performance in all scenarios considered.

</details>

<details>

<summary>2022-05-05 10:41:01 - Bivariate vine copula based quantile regression</summary>

- *Marija Tepegjozova, Claudia Czado*

- `2205.02557v1` - [abs](http://arxiv.org/abs/2205.02557v1) - [pdf](http://arxiv.org/pdf/2205.02557v1)

> The statistical analysis of univariate quantiles is a well developed research topic. However, there is a profound need for research in multivariate quantiles. We tackle the topic of bivariate quantiles and bivariate quantile regression using vine copulas. They are graph theoretical models identified by a sequence of linked trees, which allow for separate modelling of marginal distributions and the dependence structure. We introduce a novel graph structure model (given by a tree sequence) specifically designed for a symmetric treatment of two responses in a predictive regression setting. We establish computational tractability of the model and a straight forward way of obtaining different conditional distributions. Using vine copulas the typical shortfalls of regression, as the need for transformations or interactions of predictors, collinearity or quantile crossings are avoided. We illustrate the copula based bivariate quantiles for different copula distributions and provide a data set example. Further, the data example emphasizes the benefits of the joint bivariate response modelling in contrast to two separate univariate regressions or by assuming conditional independence, for bivariate response data set in the presence of conditional dependence.

</details>

<details>

<summary>2022-05-05 12:32:08 - The interventional Bayesian Gaussian equivalent score for Bayesian causal inference with unknown soft interventions</summary>

- *Jack Kuipers, Giusi Moffa*

- `2205.02602v1` - [abs](http://arxiv.org/abs/2205.02602v1) - [pdf](http://arxiv.org/pdf/2205.02602v1)

> Describing the causal relations governing a system is a fundamental task in many scientific fields, ideally addressed by experimental studies. However, obtaining data under intervention scenarios may not always be feasible, while discovering causal relations from purely observational data is notoriously challenging. In certain settings, such as genomics, we may have data from heterogeneous study conditions, with soft (partial) interventions only pertaining to a subset of the study variables, whose effects and targets are possibly unknown. Combining data from experimental and observational studies offers the opportunity to leverage both domains and improve on the identifiability of causal structures. To this end, we define the interventional BGe score for a mixture of observational and interventional data, where the targets and effects of intervention may be unknown. To demonstrate the approach we compare its performance to other state-of-the-art algorithms, both in simulations and data analysis applications. Prerogative of our method is that it takes a Bayesian perspective leading to a full characterisation of the posterior distribution of the DAG structures. Given a sample of DAGs one can also automatically derive full posterior distributions of the intervention effects. Consequently the method effectively captures the uncertainty both in the structure and the parameter estimates. Codes to reproduce the simulations and analyses are publicly available at github.com/jackkuipers/iBGe

</details>

<details>

<summary>2022-05-05 14:19:08 - A Market for Trading Forecasts: A Wagering Mechanism</summary>

- *Aitazaz Ali Raja, Pierre Pinson, Jalal Kazempour, Sergio Grammatico*

- `2205.02668v1` - [abs](http://arxiv.org/abs/2205.02668v1) - [pdf](http://arxiv.org/pdf/2205.02668v1)

> The ever-increasing interest in the collection of data by advancing technical and social sectors makes it distributed in terms of ownership. Also, the diverse expertise of these owners results in the extraction of varying quality of predictive information. Thus, the platforms for pooling forecasts based on distributed data and heterogeneous predictive skills allow gaining a collective value for a prediction task. For this purpose, we design a wagering-based forecast elicitation market platform, where a buyer intending to improve their forecasts posts a prediction task, and sellers respond to it with their forecast reports and wagers. This market delivers an aggregated forecast to the buyer (pre-event) and allocates a payoff to the sellers (post-event) for their contribution. We propose a payoff mechanism and prove that it satisfies several desirable economic properties, including those specific to electronic platforms. Furthermore, we discuss the properties of the scoring rules that allow us to provide ex-ante theoretical support for showing that the quantile averaging produces higher-quality aggregate forecasts in terms of scores, compared to the widely used linear pooling methods. Finally, we provide numerical examples to illustrate the structure and properties of the proposed market platform.

</details>

<details>

<summary>2022-05-05 15:46:31 - Optimal subsampling for functional quantile regression</summary>

- *Qian Yan, Hanyu Li, Chengmei Niu*

- `2205.02718v1` - [abs](http://arxiv.org/abs/2205.02718v1) - [pdf](http://arxiv.org/pdf/2205.02718v1)

> Subsampling is an efficient method to deal with massive data. In this paper, we investigate the optimal subsampling for linear quantile regression when the covariates are functions. The asymptotic distribution of the subsampling estimator is first derived. Then, we obtain the optimal subsampling probabilities based on the A-optimality criterion. Furthermore, the modified subsampling probabilities without estimating the densities of the response variables given the covariates are also proposed, which are easier to implement in practise. Numerical experiments on synthetic and real data show that the proposed methods always outperform the one with uniform sampling and can approximate the results based on full data well with less computational efforts.

</details>

<details>

<summary>2022-05-05 16:58:13 - Embedded Multilevel Regression and Poststratification: Model-based Inference with Incomplete Auxiliary Information</summary>

- *Katherine Li, Yajuan Si*

- `2205.02775v1` - [abs](http://arxiv.org/abs/2205.02775v1) - [pdf](http://arxiv.org/pdf/2205.02775v1)

> Health disparity research often evaluates health outcomes across subgroups. Multilevel regression and poststratification (MRP) is a popular approach for small subgroup estimation due to its ability to stabilize estimates by fitting multilevel models and to adjust for selection bias by poststratifying on auxiliary variables, which are population characteristics predictive of the analytic outcome. However, the granularity and quality of the estimates produced by MRP are limited by the availability of the auxiliary variables' joint distribution; data analysts often only have access to the marginal distributions. To overcome this limitation, we develop an integrative inference framework that embeds the estimation of population cell counts needed for poststratification into the MRP workflow: embedded MRP (EMRP). Under EMRP, we generate synthetic populations of the auxiliary variables before implementing MRP. All sources of estimation uncertainty are propagated with a fully Bayesian framework. Through simulation studies, we compare different methods and demonstrate EMRP's improvements over classical MRP on the bias-variance tradeoff to yield valid subpopulation inferences of interest. As an illustration, we estimate food insecurity prevalence among vulnerable groups in New York City by applying EMRP to the Longitudinal Survey of Wellbeing. We find that the improvement is primarily on subgroup estimation with efficiency gains.

</details>

<details>

<summary>2022-05-05 17:00:56 - Detecting and diagnosing prior and likelihood sensitivity with power-scaling</summary>

- *Noa Kallioinen, Topi Paananen, Paul-Christian Brkner, Aki Vehtari*

- `2107.14054v2` - [abs](http://arxiv.org/abs/2107.14054v2) - [pdf](http://arxiv.org/pdf/2107.14054v2)

> Determining the sensitivity of the posterior to perturbations of the prior and likelihood is an important part of the Bayesian workflow. We introduce a practical and computationally efficient sensitivity analysis approach using importance sampling to estimate properties of posteriors resulting from power-scaling the prior or likelihood. On this basis, we suggest a diagnostic that can indicate the presence of prior-data conflict or likelihood noninformativity and discuss limitations to the power-scaling approach. The approach can be easily included in Bayesian workflows with minimal effort by the model builder and we present an implementation in our new R package \texttt{priorsense}. We further demonstrate the workflow on case studies of real data using models varying in complexity from simple linear models to Gaussian process models.

</details>

<details>

<summary>2022-05-05 18:03:00 - Uniformly consistent proportion estimation for composite hypotheses via integral equations</summary>

- *Xiongzhi Chen*

- `1906.10246v2` - [abs](http://arxiv.org/abs/1906.10246v2) - [pdf](http://arxiv.org/pdf/1906.10246v2)

> We consider estimating the proportion of random variables for two types of composite null hypotheses: (i) the means or medians of the random variables belonging to a non-empty, bounded interval; (ii) the means or medians of the random variables belonging to an unbounded interval that is not the whole real line. For each type of composite null hypotheses, uniform consistent estimators of the proportion of false null hypotheses are constructed respectively for random variables whose distributions are members of a Type I location-shift family or are members of the Gamma family. Further, uniformly consistent estimators of certain functions of a bounded null on the means or medians are provided for the two types of random variables mentioned earlier. These functions are continuous and of bounded variation. The estimators are constructed via solutions to Lebesgue-Stieltjes integral equations and harmonic analysis, do not rely on a concept of p-value, can be used to construct adaptive false discovery rate procedures and adaptive false nondiscovery rate procedures for multiple hypothesis testing, can be used in Bayesian inference via mixture models, and may be used to estimate the sparsity level in high-dimensional Gaussian linear models.

</details>

<details>

<summary>2022-05-06 07:32:57 - baymedr: An R Package and Web Application for the Calculation of Bayes Factors for Superiority, Equivalence, and Non-Inferiority Designs</summary>

- *Maximilian Linde, Don van Ravenzwaaij*

- `1910.11616v3` - [abs](http://arxiv.org/abs/1910.11616v3) - [pdf](http://arxiv.org/pdf/1910.11616v3)

> Clinical trials often seek to determine the superiority, equivalence, or non-inferiority of an experimental condition (e.g., a new drug) compared to a control condition (e.g., a placebo or an already existing drug). The use of frequentist statistical methods to analyze data for these types of designs is ubiquitous even though they have several limitations. Bayesian inference remedies many of these shortcomings and allows for intuitive interpretations. In this article, we outline the frequentist conceptualization of superiority, equivalence, and non-inferiority designs and discuss its disadvantages. Subsequently, we explain how Bayes factors can be used to compare the relative plausibility of competing hypotheses. We present baymedr, an R package and web application, that provides user-friendly tools for the computation of Bayes factors for superiority, equivalence, and non-inferiority designs. Instructions on how to use baymedr are provided and an example illustrates how already existing results can be reanalyzed with baymedr.

</details>

<details>

<summary>2022-05-06 10:04:59 - Specification analysis for technology use and teenager well-being: statistical validity and a Bayesian proposal</summary>

- *Christoph Semken, David Rossell*

- `2201.05381v2` - [abs](http://arxiv.org/abs/2201.05381v2) - [pdf](http://arxiv.org/pdf/2201.05381v2)

> A key issue in science is assessing robustness to data analysis choices, while avoiding selective reporting and providing valid inference. Specification Curve Analysis is a tool intended to prevent selective reporting. Alas, when used for inference it can create severe biases and false positives, due to wrongly adjusting for covariates, and mask important treatment effect heterogeneity. As our motivating application, it led an influential study to conclude there is no relevant association between technology use and teenager mental well-being. We discuss these issues and propose a strategy for valid inference. Bayesian Specification Curve Analysis (BSCA) uses Bayesian Model Averaging to incorporate covariates and heterogeneous effects across treatments, outcomes and sub-populations. BSCA gives significantly different insights into teenager well-being, revealing that the association with technology differs by device, gender and who assesses well-being (teenagers or their parents).

</details>

<details>

<summary>2022-05-06 10:46:40 - Exploiting network information to disentangle spillover effects in a field experiment on teens' museum attendance</summary>

- *Silvia Noirjean, Marco Mariani, Alessandra Mattei, Fabrizia Mealli*

- `2011.11023v2` - [abs](http://arxiv.org/abs/2011.11023v2) - [pdf](http://arxiv.org/pdf/2011.11023v2)

> A key element in the education of youths is their sensitization to historical and artistic heritage. We analyze a field experiment conducted in Florence (Italy) to assess how appropriate incentives assigned to high-school classes may induce teens to visit museums in their free time. Non-compliance and spillover effects make the impact evaluation of this clustered encouragement design challenging. We propose to blend principal stratification and causal mediation, by defining sub-populations of units according to their compliance behavior and using the information on their friendship networks as mediator. We formally define principal natural direct and indirect effects and principal controlled direct and spillover effects, and use them to disentangle spillovers from other causal channels. We adopt a Bayesian approach for inference.

</details>

<details>

<summary>2022-05-06 11:03:26 - Active Offline Policy Selection</summary>

- *Ksenia Konyushkova, Yutian Chen, Tom Le Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz, Misha Denil, Nando de Freitas*

- `2106.10251v4` - [abs](http://arxiv.org/abs/2106.10251v4) - [pdf](http://arxiv.org/pdf/2106.10251v4)

> This paper addresses the problem of policy selection in domains with abundant logged data, but with a restricted interaction budget. Solving this problem would enable safe evaluation and deployment of offline reinforcement learning policies in industry, robotics, and recommendation domains among others. Several off-policy evaluation (OPE) techniques have been proposed to assess the value of policies using only logged data. However, there is still a big gap between the evaluation by OPE and the full online evaluation. Yet, large amounts of online interactions are often not possible in practice. To overcome this problem, we introduce active offline policy selection - a novel sequential decision approach that combines logged data with online interaction to identify the best policy. We use OPE estimates to warm start the online evaluation. Then, in order to utilize the limited environment interactions wisely we decide which policy to evaluate next based on a Bayesian optimization method with a kernel that represents policy similarity. We use multiple benchmarks, including real-world robotics, with a large number of candidate policies to show that the proposed approach improves upon state-of-the-art OPE estimates and pure online policy evaluation.

</details>

<details>

<summary>2022-05-06 13:18:31 - Scalable computation of prediction intervals for neural networks via matrix sketching</summary>

- *Alexander Fishkov, Maxim Panov*

- `2205.03194v1` - [abs](http://arxiv.org/abs/2205.03194v1) - [pdf](http://arxiv.org/pdf/2205.03194v1)

> Accounting for the uncertainty in the predictions of modern neural networks is a challenging and important task in many domains. Existing algorithms for uncertainty estimation require modifying the model architecture and training procedure (e.g., Bayesian neural networks) or dramatically increase the computational cost of predictions such as approaches based on ensembling. This work proposes a new algorithm that can be applied to a given trained neural network and produces approximate prediction intervals. The method is based on the classical delta method in statistics but achieves computational efficiency by using matrix sketching to approximate the Jacobian matrix. The resulting algorithm is competitive with state-of-the-art approaches for constructing predictive intervals on various regression datasets from the UCI repository.

</details>

<details>

<summary>2022-05-06 14:17:23 - Estimation and Inference by Stochastic Optimization</summary>

- *Jean-Jacques Forneron*

- `2205.03254v1` - [abs](http://arxiv.org/abs/2205.03254v1) - [pdf](http://arxiv.org/pdf/2205.03254v1)

> In non-linear estimations, it is common to assess sampling uncertainty by bootstrap inference. For complex models, this can be computationally intensive. This paper combines optimization with resampling: turning stochastic optimization into a fast resampling device. Two methods are introduced: a resampled Newton-Raphson (rNR) and a resampled quasi-Newton (rqN) algorithm. Both produce draws that can be used to compute consistent estimates, confidence intervals, and standard errors in a single run. The draws are generated by a gradient and Hessian (or an approximation) computed from batches of data that are resampled at each iteration. The proposed methods transition quickly from optimization to resampling when the objective is smooth and strictly convex. Simulated and empirical applications illustrate the properties of the methods on large scale and computationally intensive problems. Comparisons with frequentist and Bayesian methods highlight the features of the algorithms.

</details>

<details>

<summary>2022-05-06 15:51:31 - Benchmarking Econometric and Machine Learning Methodologies in Nowcasting</summary>

- *Daniel Hopp*

- `2205.03318v1` - [abs](http://arxiv.org/abs/2205.03318v1) - [pdf](http://arxiv.org/pdf/2205.03318v1)

> Nowcasting can play a key role in giving policymakers timelier insight to data published with a significant time lag, such as final GDP figures. Currently, there are a plethora of methodologies and approaches for practitioners to choose from. However, there lacks a comprehensive comparison of these disparate approaches in terms of predictive performance and characteristics. This paper addresses that deficiency by examining the performance of 12 different methodologies in nowcasting US quarterly GDP growth, including all the methods most commonly employed in nowcasting, as well as some of the most popular traditional machine learning approaches. Performance was assessed on three different tumultuous periods in US economic history: the early 1980s recession, the 2008 financial crisis, and the COVID crisis. The two best performing methodologies in the analysis were long short-term memory artificial neural networks (LSTM) and Bayesian vector autoregression (BVAR). To facilitate further application and testing of each of the examined methodologies, an open-source repository containing boilerplate code that can be applied to different datasets is published alongside the paper, available at: github.com/dhopp1/nowcasting_benchmark.

</details>

<details>

<summary>2022-05-06 16:23:12 - Far from Asymptopia</summary>

- *Michael C. Abbott, Benjamin B. Machta*

- `2205.03343v1` - [abs](http://arxiv.org/abs/2205.03343v1) - [pdf](http://arxiv.org/pdf/2205.03343v1)

> Inference from limited data requires a notion of measure on parameter space, which is most explicit in the Bayesian framework as a prior distribution. Jeffreys prior is the best-known uninformative choice, the invariant volume element from information geometry, but we demonstrate here that this leads to enormous bias in typical high-dimensional models. This is because models found in science typically have an effective dimensionality of accessible behaviours much smaller than the number of microscopic parameters. Any measure which treats all of these parameters equally is far from uniform when projected onto the sub-space of relevant parameters, due to variations in the local co-volume of irrelevant directions. We present results on a principled choice of measure which avoids this issue, and leads to unbiased posteriors, by focusing on relevant parameters. This optimal prior depends on the quantity of data to be gathered, and approaches Jeffreys prior in the asymptotic limit. But for typical models this limit cannot be justified without an impossibly large increase in the quantity of data, exponential in the number of microscopic parameters.

</details>

<details>

<summary>2022-05-06 17:39:20 - Consistency of mixture models with a prior on the number of components</summary>

- *Jeffrey W. Miller*

- `2205.03384v1` - [abs](http://arxiv.org/abs/2205.03384v1) - [pdf](http://arxiv.org/pdf/2205.03384v1)

> This article establishes general conditions for posterior consistency of Bayesian finite mixture models with a prior on the number of components. That is, we provide sufficient conditions under which the posterior concentrates on neighborhoods of the true parameter values when the data are generated from a finite mixture over the assumed family of component distributions. Specifically, we establish almost sure consistency for the number of components, the mixture weights, and the component parameters, up to a permutation of the component labels. The approach taken here is based on Doob's theorem, which has the advantage of holding under extraordinarily general conditions, and the disadvantage of only guaranteeing consistency at a set of parameter values that has probability one under the prior. However, we show that in fact, for commonly used choices of prior, this yields consistency at Lebesgue-almost all parameter values -- which is satisfactory for most practical purposes. We aim to formulate the results in a way that maximizes clarity, generality, and ease of use.

</details>

<details>

<summary>2022-05-06 17:44:18 - Hypothesis Tests with Functional Data for Surface Quality Change Detection in Surface Finishing Processes</summary>

- *Shilan Jin, Rui Tuo, Akash Tiwari, Satish Bukkapatnam, Chantel Aracne-Ruddle, Ariel Lighty, Haley Hamza, Yu Ding*

- `2205.04431v1` - [abs](http://arxiv.org/abs/2205.04431v1) - [pdf](http://arxiv.org/pdf/2205.04431v1)

> This work is concerned with providing a principled decision process for stopping or tool-changing in a surface finishing process. The decision process is supposed to work for products of non-flat geometry. The solution is based on conducting hypothesis testing on the bearing area curves from two consecutive stages of a surface finishing process. In each stage, the bearing area curves, which are in fact the nonparametric quantile curves representing the surface roughness, are extracted from surface profile measurements at a number of sampling locations on the surface of the products. The hypothesis test of these curves informs the decision makers whether there is a change in surface quality induced by the current finishing action. When such change is detected, the current action is deemed effective and should thus continue, while when no change is detected, the effectiveness of the current action is then called into question, signaling possibly some change in the course of action. Application of the hypothesis testing-based decision procedure to both spherical and flat surfaces demonstrates the effectiveness and benefit of the proposed method and confirms its geometry-agnostic nature.

</details>

<details>

<summary>2022-05-06 19:51:52 - Consistent Second-Order Conic Integer Programming for Learning Bayesian Networks</summary>

- *Simge Kucukyavuz, Ali Shojaie, Hasan Manzour, Linchuan Wei, Hao-Hsiang Wu*

- `2005.14346v3` - [abs](http://arxiv.org/abs/2005.14346v3) - [pdf](http://arxiv.org/pdf/2005.14346v3)

> Bayesian Networks (BNs) represent conditional probability relations among a set of random variables (nodes) in the form of a directed acyclic graph (DAG), and have found diverse applications in knowledge discovery. We study the problem of learning the sparse DAG structure of a BN from continuous observational data. The central problem can be modeled as a mixed-integer program with an objective function composed of a convex quadratic loss function and a regularization penalty subject to linear constraints. The optimal solution to this mathematical program is known to have desirable statistical properties under certain conditions. However, the state-of-the-art optimization solvers are not able to obtain provably optimal solutions to the existing mathematical formulations for medium-size problems within reasonable computational times. To address this difficulty, we tackle the problem from both computational and statistical perspectives. On the one hand, we propose a concrete early stopping criterion to terminate the branch-and-bound process in order to obtain a near-optimal solution to the mixed-integer program, and establish the consistency of this approximate solution. On the other hand, we improve the existing formulations by replacing the linear "big-$M$" constraints that represent the relationship between the continuous and binary indicator variables with second-order conic constraints. Our numerical results demonstrate the effectiveness of the proposed approaches.

</details>

<details>

<summary>2022-05-06 21:22:01 - On off-line and on-line Bayesian filtering for uncertainty quantification of structural deterioration</summary>

- *Antonios Kamariotis, Luca Sardi, Iason Papaioannou, Eleni Chatzi, Daniel Straub*

- `2205.03478v1` - [abs](http://arxiv.org/abs/2205.03478v1) - [pdf](http://arxiv.org/pdf/2205.03478v1)

> Predictive maintenance planning in the presence of structural deterioration largely relies on stochastic deterioration models, which typically contain time-invariant uncertain parameters. Monitoring information obtained sequentially at different points in time can be utilized to update prior knowledge on the time invariant parameters within the Bayesian framework. In sequential settings, Bayesian parameter estimation can be performed either in an off-line (batch) or an on-line (recursive) framework. With a focus on the quantification of the full parameter uncertainty, we review, discuss and investigate selected methods for Bayesian inference: an on-line particle filter, an online iterated batch importance sampling filter, which performs Markov chain Monte Carlo (MCMC) move steps, and an off-line MCMC-based sequential Monte Carlo filter. A Gaussian mixture model is used to approximate the posterior distribution within the resampling process in all three filters. Two numerical examples serve as the basis for a comparative assessment of off-line and on-line Bayesian estimation of time-invariant deterioration model parameters. The first case study considers a low-dimensional probabilistic fatigue crack growth model that is updated with sequential crack monitoring measurements. The second high-dimensional case study employs a random field to model the spatially and temporally varying corrosion deterioration across a beam, which is updated with sequential measurements from sensors. The numerical investigations provide insights into the performance of off-line and on-line filters in terms of the accuracy of posterior estimates and the computational cost. The investigated on-line particle filter proves competitive with MCMC-based filters. The effects of increasing problem dimensionality and sensor information amount on posterior estimates are demonstrated.

</details>

<details>

<summary>2022-05-06 22:16:06 - Unbiased Self-Play</summary>

- *Shohei Ohsawa*

- `2106.03007v2` - [abs](http://arxiv.org/abs/2106.03007v2) - [pdf](http://arxiv.org/pdf/2106.03007v2)

> We present a general optimization framework for emergent belief-state representation without any supervision. We employed the common configuration of multiagent reinforcement learning and communication to improve exploration coverage over an environment by leveraging the knowledge of each agent. In this paper, we obtained that recurrent neural nets (RNNs) with shared weights are highly biased in partially observable environments because of their noncooperativity. To address this, we designated an unbiased version of self-play via mechanism design, also known as reverse game theory, to clarify unbiased knowledge at the Bayesian Nash equilibrium. The key idea is to add imaginary rewards using the peer prediction mechanism, i.e., a mechanism for mutually criticizing information in a decentralized environment. Numerical analyses, including StarCraft exploration tasks with up to 20 agents and off-the-shelf RNNs, demonstrate the state-of-the-art performance.

</details>

<details>

<summary>2022-05-06 22:51:25 - Credible Persuasion</summary>

- *Xiao Lin, Ce Liu*

- `2205.03495v1` - [abs](http://arxiv.org/abs/2205.03495v1) - [pdf](http://arxiv.org/pdf/2205.03495v1)

> We propose a new notion of credibility for Bayesian persuasion problems. A disclosure policy is credible if the sender cannot profit from tampering with her messages while keeping the message distribution unchanged. We show that the credibility of a disclosure policy is equivalent to a cyclical monotonicity condition on its induced distribution over states and actions. We also characterize how credibility restricts the Sender's ability to persuade under different payoff structures. In particular, when the sender's payoff is state-independent, all disclosure policies are credible. We apply our results to the market for lemons, and show that no useful information can be credibly disclosed by the seller, even though a seller who can commit to her disclosure policy would perfectly reveal her private information to maximize profit.

</details>

<details>

<summary>2022-05-07 04:48:36 - Order Restricted Inference for Adaptive Progressively Censored Competing Risks Data</summary>

- *Ayon Ganguly, Debanjan Mitra, Debasis Kundu*

- `2205.03550v1` - [abs](http://arxiv.org/abs/2205.03550v1) - [pdf](http://arxiv.org/pdf/2205.03550v1)

> Under adaptive progressive Type-II censoring schemes, order restricted inference based on competing risks data is discussed in this article. The latent failure lifetimes for the competing causes are assumed to follow Weibull distributions, with an order restriction on the scale parameters of the distributions. The practical implication of this order restriction is that one of the risk factors is dominant, as often observed in competing risks scenarios. In this setting, likelihood estimation for the model parameters, along with bootstrap based techniques for constructing asymptotic confidence intervals are presented. Bayesian inferential methods for obtaining point estimates and credible intervals for the model parameters are also discussed. Through a detailed Monte Carlo simulation study, the performance of order restricted inferential methods are assessed. In addition, the results are also compared with the case when no order restriction is imposed on the estimation approach. The simulation study shows that order restricted inference is more efficient between the two, when this additional information is taken into consideration. A numerical example is provided for illustrative purpose.

</details>

<details>

<summary>2022-05-07 11:53:41 - Practical strategies for GEV-based regression models for extremes</summary>

- *Daniela Castro-Camilo, Raphal Huser, Hvard Rue*

- `2106.13110v2` - [abs](http://arxiv.org/abs/2106.13110v2) - [pdf](http://arxiv.org/pdf/2106.13110v2)

> The generalised extreme value (GEV) distribution is a three parameter family that describes the asymptotic behaviour of properly renormalised maxima of a sequence of independent and identically distributed random variables. If the shape parameter $\xi$ is zero, the GEV distribution has unbounded support, whereas if $\xi$ is positive, the limiting distribution is heavy-tailed with infinite upper endpoint but finite lower endpoint. In practical applications, we assume that the GEV family is a reasonable approximation for the distribution of maxima over blocks, and we fit it accordingly. This implies that GEV properties, such as finite lower endpoint in the case $\xi>0$, are inherited by the finite-sample maxima, which might not have bounded support. This is particularly problematic when predicting extreme observations based on multiple and interacting covariates. To tackle this usually overlooked issue, we propose a blended GEV distribution, which smoothly combines the left tail of a Gumbel distribution (GEV with $\xi=0$) with the right tail of a Fr\'echet distribution (GEV with $\xi>0$) and, therefore, has unbounded support. Using a Bayesian framework, we reparametrise the GEV distribution to offer a more natural interpretation of the (possibly covariate-dependent) model parameters. Independent priors over the new location and spread parameters induce a joint prior distribution for the original location and scale parameters. We introduce the concept of property-preserving penalised complexity (P$^3$C) priors and apply it to the shape parameter to preserve first and second moments. We illustrate our methods with an application to NO$_2$ pollution levels in California, which reveals the robustness of the bGEV distribution, as well as the suitability of the new parametrisation and the P$^3$C prior framework.

</details>

<details>

<summary>2022-05-07 14:08:37 - Statistical methods for Mendelian models with multiple genes and cancers</summary>

- *Jane W. Liang, Gregory E. Idos, Christine Hong, Stephen B. Gruber, Giovanni Parmigiani, Danielle Braun*

- `2108.12504v4` - [abs](http://arxiv.org/abs/2108.12504v4) - [pdf](http://arxiv.org/pdf/2108.12504v4)

> Risk evaluation to identify individuals who are at greater risk of cancer as a result of heritable pathogenic variants is a valuable component of individualized clinical management. Using principles of Mendelian genetics, Bayesian probability theory, and variant-specific knowledge, Mendelian models derive the probability of carrying a pathogenic variant and developing cancer in the future, based on family history. Existing Mendelian models are widely employed, but are generally limited to specific genes and syndromes. However, the upsurge of multi-gene panel germline testing has spurred the discovery of many new gene-cancer associations that are not presently accounted for in these models. We have developed PanelPRO, a flexible, efficient Mendelian risk prediction framework that can incorporate an arbitrary number of genes and cancers, overcoming the computational challenges that arise because of the increased model complexity. We implement an eleven-gene, eleven-cancer model, the largest Mendelian model created thus far, based on this framework. Using simulations and a clinical cohort with germline panel testing data, we evaluate model performance, validate the reverse-compatibility of our approach with existing Mendelian models, and illustrate its usage. Our implementation is freely available for research use in the PanelPRO R package.

</details>

<details>

<summary>2022-05-07 16:30:01 - Variational Inference for Nonlinear Inverse Problems via Neural Net Kernels: Comparison to Bayesian Neural Networks, Application to Topology Optimization</summary>

- *Vahid Keshavarzzadeh, Robert M. Kirby, Akil Narayan*

- `2205.03681v1` - [abs](http://arxiv.org/abs/2205.03681v1) - [pdf](http://arxiv.org/pdf/2205.03681v1)

> Inverse problems and, in particular, inferring unknown or latent parameters from data are ubiquitous in engineering simulations. A predominant viewpoint in identifying unknown parameters is Bayesian inference where both prior information about the parameters and the information from the observations via likelihood evaluations are incorporated into the inference process. In this paper, we adopt a similar viewpoint with a slightly different numerical procedure from standard inference approaches to provide insight about the localized behavior of unknown underlying parameters. We present a variational inference approach which mainly incorporates the observation data in a point-wise manner, i.e. we invert a limited number of observation data leveraging the gradient information of the forward map with respect to parameters, and find true individual samples of the latent parameters when the forward map is noise-free and one-to-one. For statistical calculations (as the ultimate goal in simulations), a large number of samples are generated from a trained neural network which serves as a transport map from the prior to posterior latent parameters. Our neural network machinery, developed as part of the inference framework and referred to as Neural Net Kernels (NNK), is based on hierarchical (deep) kernels which provide greater flexibility for training compared to standard neural networks. We showcase the effectiveness of our inference procedure in identifying bimodal and irregular distributions compared to a number of approaches including Markov Chain Monte Carlo sampling approaches and a Bayesian neural network approach.

</details>

<details>

<summary>2022-05-08 12:34:42 - Bayesian Estimation of Multinomial Cell Probabilities Incorporating Information from Aggregated Observations</summary>

- *Yasuyuki Hamura*

- `2205.03851v1` - [abs](http://arxiv.org/abs/2205.03851v1) - [pdf](http://arxiv.org/pdf/2205.03851v1)

> In this note, we consider the problem of estimating multinomial cell probabilities under the entropy loss when side information in aggregated data is available. We use the Jeffreys prior to obtain Bayes estimators. It is shown that by incorporating the side information, we can construct an improved estimator.

</details>

<details>

<summary>2022-05-08 17:59:05 - Unifying Classical and Bayesian Revealed Preference</summary>

- *Kunal Pattanayak, Vikram Krishnamurthy*

- `2106.14486v3` - [abs](http://arxiv.org/abs/2106.14486v3) - [pdf](http://arxiv.org/pdf/2106.14486v3)

> This paper establishes the equivalence between Bayesian revealed preference (Caplin and Dean, 2015) and classical revealed preference with non-linear budget constraints (Forges and Minelli, 2009). Classical revealed preference tests for utility maximization given known budget constraints. Bayesian revealed preference tests for costly information acquisition given a utility function. Our main result shows that the key theorem in Caplin and Dean (2015) on Bayesian revealed preference is equivalent to Afriat-type feasibility inequalities (Afriat, 1967) for general (non-linear) budget sets. Our second result exploits this connection between classical and Bayesian revealed preference to construct a monotone convex information acquisition cost from decision maker's utilities and decisions in Bayesian revealed preference. Keywords: Afriat's Theorem, Revealed preference, Bayesian revealed preference, Costly Information acquisition, Rational Inattention, Blackwell ordering, Utility Theory.

</details>

<details>

<summary>2022-05-08 22:04:25 - Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series</summary>

- *Enyan Dai, Jie Chen*

- `2202.07857v2` - [abs](http://arxiv.org/abs/2202.07857v2) - [pdf](http://arxiv.org/pdf/2202.07857v2)

> Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear frequently in applications, including for example, power grids and traffic networks. Detecting anomalies for multiple time series, however, is a challenging subject, owing to the intricate interdependencies among the constituent series. We hypothesize that anomalies occur in low density regions of a distribution and explore the use of normalizing flows for unsupervised anomaly detection, because of their superior quality in density estimation. Moreover, we propose a novel flow model by imposing a Bayesian network among constituent series. A Bayesian network is a directed acyclic graph (DAG) that models causal relationships; it factorizes the joint probability of the series into the product of easy-to-evaluate conditional probabilities. We call such a graph-augmented normalizing flow approach GANF and propose joint estimation of the DAG with flow parameters. We conduct extensive experiments on real-world datasets and demonstrate the effectiveness of GANF for density estimation, anomaly detection, and identification of time series distribution drift.

</details>

<details>

<summary>2022-05-09 02:30:52 - Posterior Collapse of a Linear Latent Variable Model</summary>

- *Zihao Wang, Liu Ziyin*

- `2205.04009v1` - [abs](http://arxiv.org/abs/2205.04009v1) - [pdf](http://arxiv.org/pdf/2205.04009v1)

> This work identifies the existence and cause of a type of posterior collapse that frequently occurs in the Bayesian deep learning practice. For a general linear latent variable model that includes linear variational autoencoders as a special case, we precisely identify the nature of posterior collapse to be the competition between the likelihood and the regularization of the mean due to the prior. Our result also suggests that posterior collapse may be a general problem of learning for deeper architectures and deepens our understanding of Bayesian deep learning.

</details>

<details>

<summary>2022-05-09 03:51:46 - Wild Bootstrap Inference for Penalized Quantile Regression for Longitudinal Data</summary>

- *Carlos Lamarche, Thomas Parker*

- `2004.05127v3` - [abs](http://arxiv.org/abs/2004.05127v3) - [pdf](http://arxiv.org/pdf/2004.05127v3)

> The existing theory of penalized quantile regression for longitudinal data has focused primarily on point estimation. In this work, we investigate statistical inference. We propose a wild residual bootstrap procedure and show that it is asymptotically valid for approximating the distribution of the penalized estimator. The model puts no restrictions on individual effects, and the estimator achieves consistency by letting the shrinkage decay in importance asymptotically. The new method is easy to implement and simulation studies show that it has accurate small sample behavior in comparison with existing procedures. Finally, we illustrate the new approach using U.S. Census data to estimate a model that includes more than eighty thousand parameters.

</details>

<details>

<summary>2022-05-09 04:08:16 - A Comparative Tutorial of Bayesian Sequential Design and Reinforcement Learning</summary>

- *Mauricio Tec, Yunshan Duan, Peter Mller*

- `2205.04023v1` - [abs](http://arxiv.org/abs/2205.04023v1) - [pdf](http://arxiv.org/pdf/2205.04023v1)

> Reinforcement Learning (RL) is a computational approach to reward-driven learning in sequential decision problems. It implements the discovery of optimal actions by learning from an agent interacting with an environment rather than from supervised data. We contrast and compare RL with traditional sequential design, focusing on simulation-based Bayesian sequential design (BSD). Recently, there has been an increasing interest in RL techniques for healthcare applications. We introduce two related applications as motivating examples. In both applications, the sequential nature of the decisions is restricted to sequential stopping. Rather than a comprehensive survey, the focus of the discussion is on solutions using standard tools for these two relatively simple sequential stopping problems. Both problems are inspired by adaptive clinical trial design. We use examples to explain the terminology and mathematical background that underlie each framework and map one to the other. The implementations and results illustrate the many similarities between RL and BSD. The results motivate the discussion of the potential strengths and limitations of each approach.

</details>

<details>

<summary>2022-05-09 09:22:40 - Hierarchical Bayesian Uncertainty Quantification of Finite Element Models using Modal Statistical Information</summary>

- *Omid Sedehi, Costas Papadimitriou, Lambros S. Katafygiotis*

- `2205.04136v1` - [abs](http://arxiv.org/abs/2205.04136v1) - [pdf](http://arxiv.org/pdf/2205.04136v1)

> This paper develops a Hierarchical Bayesian Modeling (HBM) framework for uncertainty quantification of Finite Element (FE) models based on modal information. This framework uses an existing Fast Fourier Transform (FFT) approach to identify experimental modal parameters from time-history data and employs a class of maximum-entropy probability distributions to account for the mismatch between the modal parameters. It also considers a parameterized probability distribution for capturing the variability of structural parameters across multiple data sets. In this framework, the computation is addressed through Expectation-Maximization (EM) strategies, empowered by Laplace approximations. As a result, a new rationale is introduced for assigning optimal weights to the modal properties when updating structural parameters. According to this framework, the modal features weights are equal to the inverse of the aggregate uncertainty, comprised of the identification and prediction uncertainties. The proposed framework is coherent in modeling the entire process of inferring structural parameters from response-only measurements and is comprehensive in accounting for different sources of uncertainty, including the variability of both modal and structural parameters over multiple data sets, as well as their identification uncertainties. Numerical and experimental examples are employed to demonstrate the HBM framework, wherein the environmental and operational conditions are almost constant. It is observed that the variability of parameters across data sets remains the dominant source of uncertainty while being much larger than the identification uncertainties.

</details>

<details>

<summary>2022-05-09 14:27:35 - On a wider class of prior distributions for graphical models</summary>

- *Abhinav Natarajan, Willem van den Boom, Kristoforus Bryant Odang, Maria De Iorio*

- `2205.04324v1` - [abs](http://arxiv.org/abs/2205.04324v1) - [pdf](http://arxiv.org/pdf/2205.04324v1)

> Gaussian graphical models are useful tools for conditional independence structure inference of multivariate random variables. Unfortunately, Bayesian inference of latent graph structures is challenging due to exponential growth of $\mathcal{G}_n$, the set of all graphs in $n$ vertices. One approach that has been proposed to tackle this problem is to limit search to subsets of $\mathcal{G}_n$. In this paper, we study subsets that are vector subspaces with the cycle space $\mathcal{C}_n$ as main example. We propose a novel prior on $\mathcal{C}_n$ based on linear combinations of cycle basis elements and present its theoretical properties. Using this prior, we implemented a Markov chain Monte Carlo algorithm and show that (i) posterior edge inclusion estimates compared to the standard technique are comparable despite searching a smaller graph space and (ii) the vector space perspective enables straightforward MCMC algorithms.

</details>

<details>

<summary>2022-05-09 15:33:48 - Direct Gibbs posterior inference on risk minimizers: construction, concentration, and calibration</summary>

- *Ryan Martin, Nicholas Syring*

- `2203.09381v2` - [abs](http://arxiv.org/abs/2203.09381v2) - [pdf](http://arxiv.org/pdf/2203.09381v2)

> Real-world problems, often couched as machine learning applications, involve quantities of interest that have real-world meaning, independent of any statistical model. To avoid potential model misspecification bias or over-complicating the problem formulation, a direct, model-free approach is desired. The traditional Bayesian framework relies on a model for the data-generating process so, apparently, the desired direct, model-free, posterior-probabilistic inference is out of reach. Fortunately, likelihood functions are not the only means of linking data and quantities of interest. Loss functions provide an alternative link, where the quantity of interest is defined, or at least could be defined, as a minimizer of the corresponding risk, or expected loss. In this case, one can obtain what is commonly referred to as a Gibbs posterior distribution by using the empirical risk function directly. This manuscript explores the Gibbs posterior construction, its asymptotic concentration properties, and the frequentist calibration of its credible regions. By being free from the constraints of model specification, Gibbs posteriors create new opportunities for probabilistic inference in modern statistical learning problems.

</details>

<details>

<summary>2022-05-09 17:54:50 - Bayesian Capture-Recapture Models that Facilitate Recursive Computing</summary>

- *Mevin B Hooten, Michael R Schwob, Devin S Johnson, Jacob S. Ivan*

- `2205.04453v1` - [abs](http://arxiv.org/abs/2205.04453v1) - [pdf](http://arxiv.org/pdf/2205.04453v1)

> Ecologists increasingly rely on Bayesian capture-recapture models to estimate abundance of wildlife populations. Capture-recapture models account for imperfect detectability in individual-level presence data. A variety of approaches have been used to implement such models, including integrated likelihood, parameter-expanded data augmentation, and combinations of those. Recently proposed conditional specifications have improved the stability of algorithms for fitting capture-recapture models. We arrive at similar conditional specifications of capture-recapture models by considering recursive implementation strategies that facilitate fitting models to large data sets. Our approach enjoys the same computational stability but also allows us to fit the desired model in stages and leverage parallel computing resources. Our model specification includes a component for the capture history of detected individuals and another component for the sample size which is random before observed. We demonstrate this approach using three examples including simulation and two data sets resulting from capture-recapture studies of different species.

</details>

<details>

<summary>2022-05-09 18:45:49 - A Unified Bayesian Framework for Pricing Catastrophe Bond Derivatives</summary>

- *Dixon Domfeh, Arpita Chatterjee, Matthew Dixon*

- `2205.04520v1` - [abs](http://arxiv.org/abs/2205.04520v1) - [pdf](http://arxiv.org/pdf/2205.04520v1)

> Catastrophe (CAT) bond markets are incomplete and hence carry uncertainty in instrument pricing. As such various pricing approaches have been proposed, but none treat the uncertainty in catastrophe occurrences and interest rates in a sufficiently flexible and statistically reliable way within a unifying asset pricing framework. Consequently, little is known empirically about the expected risk-premia of CAT bonds. The primary contribution of this paper is to present a unified Bayesian CAT bond pricing framework based on uncertainty quantification of catastrophes and interest rates. Our framework allows for complex beliefs about catastrophe risks to capture the distinct and common patterns in catastrophe occurrences, and when combined with stochastic interest rates, yields a unified asset pricing approach with informative expected risk premia. Specifically, using a modified collective risk model -- Dirichlet Prior-Hierarchical Bayesian Collective Risk Model (DP-HBCRM) framework -- we model catastrophe risk via a model-based clustering approach. Interest rate risk is modeled as a CIR process under the Bayesian approach. As a consequence of casting CAT pricing models into our framework, we evaluate the price and expected risk premia of various CAT bond contracts corresponding to clustering of catastrophe risk profiles. Numerical experiments show how these clusters reveal how CAT bond prices and expected risk premia relate to claim frequency and loss severity.

</details>

<details>

<summary>2022-05-09 21:12:17 - Differentially Private Learning with Adaptive Clipping</summary>

- *Galen Andrew, Om Thakkar, H. Brendan McMahan, Swaroop Ramaswamy*

- `1905.03871v5` - [abs](http://arxiv.org/abs/1905.03871v5) - [pdf](http://arxiv.org/pdf/1905.03871v5)

> Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user's model update by clipping it to some constant value. However there is no good a priori setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a fixed clipping norm, one clips to a value at a specified quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy. The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg. Experiments demonstrate that adaptive clipping to the median update norm works well across a range of realistic federated learning tasks, sometimes outperforming even the best fixed clip chosen in hindsight, and without the need to tune any clipping hyperparameter.

</details>

<details>

<summary>2022-05-09 21:36:20 - Robust Data-Driven Decisions Under Model Uncertainty</summary>

- *Xiaoyu Cheng*

- `2205.04573v1` - [abs](http://arxiv.org/abs/2205.04573v1) - [pdf](http://arxiv.org/pdf/2205.04573v1)

> When sample data are governed by an unknown sequence of independent but possibly non-identical distributions, the data-generating process (DGP) in general cannot be perfectly identified from the data. For making decisions facing such uncertainty, this paper presents a novel approach by studying how the data can best be used to robustly improve decisions. That is, no matter which DGP governs the uncertainty, one can make a better decision than without using the data. I show that common inference methods, e.g., maximum likelihood and Bayesian updating cannot achieve this goal. To address, I develop new updating rules that lead to robustly better decisions either asymptotically almost surely or in finite sample with a pre-specified probability. Especially, they are easy to implement as are given by simple extensions of the standard statistical procedures in the case where the possible DGPs are all independent and identically distributed. Finally, I show that the new updating rules also lead to more intuitive conclusions in existing economic models such as asset pricing under ambiguity.

</details>

<details>

<summary>2022-05-09 21:52:51 - Nested conformal prediction and quantile out-of-bag ensemble methods</summary>

- *Chirag Gupta, Arun K. Kuchibhotla, Aaditya K. Ramdas*

- `1910.10562v4` - [abs](http://arxiv.org/abs/1910.10562v4) - [pdf](http://arxiv.org/pdf/1910.10562v4)

> Conformal prediction is a popular tool for providing valid prediction sets for classification and regression problems, without relying on any distributional assumptions on the data. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to find a valid prediction set. The nested framework subsumes all nonconformity scores, including recent proposals based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to other aggregation schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. We develop a computationally efficient implementation of cross-conformal, that is also used by QOOB. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets. Code for QOOB is available at https://github.com/aigen/QOOB.

</details>

<details>

<summary>2022-05-10 05:13:35 - Bayesian Reasoning and Evidence Communication</summary>

- *Steven Lund, Hari Iyer*

- `2205.04677v1` - [abs](http://arxiv.org/abs/2205.04677v1) - [pdf](http://arxiv.org/pdf/2205.04677v1)

> Many resources for forensic scholars and practitioners, such as journal articles, guidance documents, and textbooks, address how to make a value of evidence assessment in the form of a likelihood ratio (LR) when deciding between two competing propositions. These texts often describe experts presenting their LR values to other parties in the judicial system, such as lawyers, judges, and potentially jurors, but few texts explicitly address how a recipient is expected to utilize the provided LR value. Those that do often imply, or directly suggest, a hybrid modification of Bayes' rule in which a decision maker multiplies their prior odds with another person's assessment of LR to obtain their posterior odds. In this paper, we illustrate how someone adhering to Bayesian reasoning would update their personal uncertainty in response to someone else presenting a personal LR value (or any other form of an opinion) and emphasize that the hybrid approach is a departure from Bayesian reasoning. We further consider implications of recipients adhering to Bayesian reasoning on the role and ideal content of expert's reports and testimony and address published responses to our 2017 paper (Lund and Iyer, 2017), where we previously argued that the hybrid equation is not supported by Bayesian reasoning.

</details>

<details>

<summary>2022-05-10 08:52:36 - Adaptation Strategies for Automated Machine Learning on Evolving Data</summary>

- *Bilge Celik, Joaquin Vanschoren*

- `2006.06480v3` - [abs](http://arxiv.org/abs/2006.06480v3) - [pdf](http://arxiv.org/pdf/2006.06480v3)

> Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of data stream challenges such as concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on different AutoML approaches. We do this for a variety of AutoML approaches for building machine learning pipelines, including those that leverage Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.

</details>

<details>

<summary>2022-05-10 13:55:05 - Parametric G-computation for Compatible Indirect Treatment Comparisons with Limited Individual Patient Data</summary>

- *Antonio Remiro-Azcar, Anna Heath, Gianluca Baio*

- `2108.12208v4` - [abs](http://arxiv.org/abs/2108.12208v4) - [pdf](http://arxiv.org/pdf/2108.12208v4)

> Population adjustment methods such as matching-adjusted indirect comparison (MAIC) are increasingly used to compare marginal treatment effects when there are cross-trial differences in effect modifiers and limited patient-level data. MAIC is based on propensity score weighting, which is sensitive to poor covariate overlap and cannot extrapolate beyond the observed covariate space. Current outcome regression-based alternatives can extrapolate but target a conditional treatment effect that is incompatible in the indirect comparison. When adjusting for covariates, one must integrate or average the conditional estimate over the relevant population to recover a compatible marginal treatment effect. We propose a marginalization method based on parametric G-computation that can be easily applied where the outcome regression is a generalized linear model or a Cox model. The approach views the covariate adjustment regression as a nuisance model and separates its estimation from the evaluation of the marginal treatment effect of interest. The method can accommodate a Bayesian statistical framework, which naturally integrates the analysis into a probabilistic framework. A simulation study provides proof-of-principle and benchmarks the method's performance against MAIC and the conventional outcome regression. Parametric G-computation achieves more precise and more accurate estimates than MAIC, particularly when covariate overlap is poor, and yields unbiased marginal treatment effect estimates under no failures of assumptions. Furthermore, the marginalized regression-adjusted estimates provide greater precision and accuracy than the conditional estimates produced by the conventional outcome regression, which are systematically biased because the measure of effect is non-collapsible.

</details>

<details>

<summary>2022-05-10 14:24:26 - Bayes factors and posterior estimation: Two sides of the very same coin</summary>

- *Harlan Campbell, Paul Gustafson*

- `2204.06054v2` - [abs](http://arxiv.org/abs/2204.06054v2) - [pdf](http://arxiv.org/pdf/2204.06054v2)

> Recently, several researchers have claimed that conclusions obtained from a Bayes factor (or the posterior odds) may contradict those obtained from Bayesian posterior estimation. In this short paper, we wish to point out that no such "incompatibility" exists if one is willing to consistently define one's priors and posteriors. The key for compatibility is that the (implied) prior model odds used for testing are the same as those used for estimation. Our recommendation is simple: If one reports a Bayes factor comparing two models, then one should also report posterior estimates which appropriately acknowledge the uncertainty with regards to which of the two models is correct.

</details>

<details>

<summary>2022-05-10 16:18:05 - Mechanisms for Global Differential Privacy under Bayesian Data Synthesis</summary>

- *Jingchen Hu, Matthew R. Williams, Terrance D. Savitsky*

- `2205.05003v1` - [abs](http://arxiv.org/abs/2205.05003v1) - [pdf](http://arxiv.org/pdf/2205.05003v1)

> This paper introduces a new method that embeds any Bayesian model used to generate synthetic data and converts it into a differentially private (DP) mechanism. We propose an alteration of the model synthesizer to utilize a censored likelihood that induces upper and lower bounds of [$\exp(-\epsilon / 2), \exp(\epsilon / 2)$], where $\epsilon$ denotes the level of the DP guarantee. This censoring mechanism equipped with an $\epsilon-$DP guarantee will induce distortion into the joint parameter posterior distribution by flattening or shifting the distribution towards a weakly informative prior. To minimize the distortion in the posterior distribution induced by likelihood censoring, we embed a vector-weighted pseudo posterior mechanism within the censoring mechanism. The pseudo posterior is formulated by selectively downweighting each likelihood contribution proportionally to its disclosure risk. On its own, the pseudo posterior mechanism produces a weaker asymptotic differential privacy (aDP) guarantee. After embedding in the censoring mechanism, the DP guarantee becomes strict such that it does not rely on asymptotics. We demonstrate that the pseudo posterior mechanism creates synthetic data with the highest utility at the price of a weaker, aDP guarantee, while embedding the pseudo posterior mechanism in the proposed censoring mechanism produces synthetic data with a stronger, non-asymptotic DP guarantee at the cost of slightly reduced utility. The perturbed histogram mechanism is included for comparison.

</details>

<details>

<summary>2022-05-10 20:07:13 - Evaluating Pest Management Strategies: A Robust Method and its Application to Strawberry Disease Management</summary>

- *Ariel Soto-Caro, Feng Wu, Zhengfei Guan, Natalia Peres*

- `1908.01808v2` - [abs](http://arxiv.org/abs/1908.01808v2) - [pdf](http://arxiv.org/pdf/1908.01808v2)

> Farmers use pesticides to reduce yield losses. The efficacies of pesticide treatments are often evaluated by analyzing the average treatment effects and risks. The stochastic efficiency with respect to a function is often employed in such evaluations through ranking the certainty equivalents of each treatment. The main challenge of using this method is gathering an adequate number of observations to produce results with statistical power. However, in many cases, only a limited number of trials are replicated in field experiments, leaving an inadequate number of observations. In addition, this method focuses only on the farmer's profit without incorporating the impact of disease pressure on yield and profit. The objective of our study is to propose a methodology to address the issue of an insufficient number of observations using simulations and take into account the effect of disease pressure on yield through a quantile regression model. We apply this method to the case of strawberry disease management in Florida.

</details>

<details>

<summary>2022-05-10 20:28:52 - Estimating functional parameters for understanding the impact of weather and government interventions on COVID-19 outbreak</summary>

- *Chih-Li Sung*

- `2101.05350v2` - [abs](http://arxiv.org/abs/2101.05350v2) - [pdf](http://arxiv.org/pdf/2101.05350v2)

> As the coronavirus disease 2019 (COVID-19) has shown profound effects on public health and the economy worldwide, it becomes crucial to assess the impact on the virus transmission and develop effective strategies to address the challenge. A new statistical model derived from the SIR epidemic model with functional parameters is proposed to understand the impact of weather and government interventions on the virus spread in the presence of asymptomatic infections among eight metropolitan areas in the United States. The model uses Bayesian inference with Gaussian process priors to study the functional parameters nonparametrically, and sensitivity analysis is adopted to investigate the main and interaction effects of these factors. This analysis reveals several important results including the potential interaction effects between weather and government interventions, which shed new light on the effective strategies for policymakers to mitigate the COVID-19 outbreak.

</details>

<details>

<summary>2022-05-11 10:42:52 - Bayesian clustering of multiple zero-inflated outcomes</summary>

- *Beatrice Franzolini, Andrea Cremaschi, Willem van den Boom, Maria De Iorio*

- `2205.05054v2` - [abs](http://arxiv.org/abs/2205.05054v2) - [pdf](http://arxiv.org/pdf/2205.05054v2)

> Several applications involving counts present a large proportion of zeros (excess-of-zeros data). A popular model for such data is the Hurdle model, which explicitly models the probability of a zero count, while assuming a sampling distribution on the positive integers. We consider data from multiple count processes. In this context, it is of interest to study the patterns of counts and cluster the subjects accordingly. We introduce a novel Bayesian nonparametric approach to cluster multiple, possibly related, zero-inflated processes. We propose a joint model for zero-inflated counts, specifying a Hurdle model for each process with a shifted Negative Binomial sampling distribution. Conditionally on the model parameters, the different processes are assumed independent, leading to a substantial reduction in the number of parameters as compared to traditional multivariate approaches. The subject-specific probabilities of zero-inflation and the parameters of the sampling distribution are flexibly modelled via an enriched finite mixture with random number of components. This induces a two-level clustering of the subjects based on the zero/non-zero patterns (outer clustering) and on the sampling distribution (inner clustering). Posterior inference is performed through tailored MCMC schemes. We demonstrate the proposed approach on an application involving the use of the messaging service WhatsApp.

</details>

<details>

<summary>2022-05-11 10:43:23 - Stochastic Variational Smoothed Model Checking</summary>

- *Luca Bortolussi, Francesca Cairoli, Ginevra Carbone, Paolo Pulcini*

- `2205.05398v1` - [abs](http://arxiv.org/abs/2205.05398v1) - [pdf](http://arxiv.org/pdf/2205.05398v1)

> Model-checking for parametric stochastic models can be expressed as checking the satisfaction probability of a certain property as a function of the parameters of the model. Smoothed model checking (smMC) leverages Gaussian Processes (GP) to infer the satisfaction function over the entire parameter space from a limited set of observations obtained via simulation. This approach provides accurate reconstructions with statistically sound quantification of the uncertainty. However, it inherits the scalability issues of GP. In this paper, we exploit recent advances in probabilistic machine learning to push this limitation forward, making Bayesian inference of smMC scalable to larger datasets, enabling its application to larger models in terms of the dimension of the parameter set. We propose Stochastic Variational Smoothed Model Checking (SV-smMC), a solution that exploits stochastic variational inference (SVI) to approximate the posterior distribution of the smMC problem. The strength and flexibility of SVI make SV-smMC applicable to two alternative probabilistic models: Gaussian Processes (GP) and Bayesian Neural Networks (BNN). Moreover, SVI makes inference easily parallelizable and it enables GPU acceleration. In this paper, we compare the performances of smMC against those of SV-smMC by looking at the scalability, the computational efficiency and at the accuracy of the reconstructed satisfaction function.

</details>

<details>

<summary>2022-05-11 13:36:36 - Forecasting foreign exchange rates with regression networks tuned by Bayesian optimization</summary>

- *Linwei Li, Paul-Amaury Matt, Christian Heumann*

- `2204.12914v3` - [abs](http://arxiv.org/abs/2204.12914v3) - [pdf](http://arxiv.org/pdf/2204.12914v3)

> The article is concerned with the problem of multi-step financial time series forecasting of Foreign Exchange (FX) rates. To address this problem, we introduce a regression network termed RegPred Net. The exchange rate to forecast is treated as a stochastic process. It is assumed to follow a generalization of Brownian motion and the mean-reverting process referred to as the generalized Ornstein-Uhlenbeck (OU) process, with time-dependent coefficients. Using past observed values of the input time series, these coefficients can be regressed online by the cells of the first half of the network (Reg). The regressed coefficients depend only on - but are very sensitive to - a small number of hyperparameters required to be set by a global optimization procedure for which, Bayesian optimization is an adequate heuristic. Thanks to its multi-layered architecture, the second half of the regression network (Pred) can project time-dependent values for the OU process coefficients and generate realistic trajectories of the time series. Predictions can be easily derived in the form of expected values estimated by averaging values obtained by Monte Carlo simulation. The forecasting accuracy on a 100 days horizon is evaluated for several of the most important FX rates such as EUR/USD, EUR/CNY, and EUR/GBP. Our experimental results show that the RegPred Net significantly outperforms ARMA, ARIMA, LSTMs, and Autoencoder-LSTM models in terms of metrics measuring the absolute error (RMSE) and correlation between predicted and actual values (Pearson R, R-squared, MDA). Compared to black-box deep learning models such as LSTM, RegPred Net has better interpretability, simpler structure, and fewer parameters.

</details>

<details>

<summary>2022-05-11 15:18:03 - Marginalization of Regression-Adjusted Treatment Effects in Indirect Comparisons with Limited Patient-Level Data</summary>

- *Antonio Remiro-Azcar, Anna Heath, Gianluca Baio*

- `2008.05951v8` - [abs](http://arxiv.org/abs/2008.05951v8) - [pdf](http://arxiv.org/pdf/2008.05951v8)

> Population adjustment methods such as matching-adjusted indirect comparison (MAIC) are increasingly used to compare marginal treatment effects when there are cross-trial differences in effect modifiers and limited patient-level data. MAIC is sensitive to poor covariate overlap and cannot extrapolate beyond the observed covariate space. Current outcome regression-based alternatives can extrapolate but target a conditional treatment effect that is incompatible in the indirect comparison. When adjusting for covariates, one must integrate or average the conditional estimate over the population of interest to recover a compatible marginal treatment effect. We propose a marginalization method based on parametric G-computation that can be easily applied where the outcome regression is a generalized linear model or a Cox model. In addition, we introduce a novel general-purpose method based on multiple imputation, which we term multiple imputation marginalization (MIM) and is applicable to a wide range of models. Both methods can accommodate a Bayesian statistical framework, which naturally integrates the analysis into a probabilistic framework. A simulation study provides proof-of-principle for the methods and benchmarks their performance against MAIC and the conventional outcome regression. The marginalized outcome regression approaches achieve more precise and more accurate estimates than MAIC, particularly when covariate overlap is poor, and yield unbiased marginal treatment effect estimates under no failures of assumptions. Furthermore, the marginalized regression-adjusted estimates provide greater precision and accuracy than the conditional estimates produced by the conventional outcome regression, which are systematically biased because the measure of effect is non-collapsible.

</details>

<details>

<summary>2022-05-12 07:04:14 - Comments on: "Hybrid Semiparametric Bayesian Networks"</summary>

- *Marco Scutari*

- `2205.05910v1` - [abs](http://arxiv.org/abs/2205.05910v1) - [pdf](http://arxiv.org/pdf/2205.05910v1)

> Invited discussion on the paper "Hybrid Semiparametric Bayesian Networks" by David Atienza, Pedro Larranaga and Concha Bielza (TEST, 2022).

</details>

<details>

<summary>2022-05-12 08:49:14 - Bayesian inference of scaled versus fractional Brownian motion</summary>

- *Samudrajit Thapa, Seongyu Park, Yeongjin Kim, Jae-Hyung Jeon, Ralf Metzler, Michael A. Lomholt*

- `2201.00654v3` - [abs](http://arxiv.org/abs/2201.00654v3) - [pdf](http://arxiv.org/pdf/2201.00654v3)

> We present a Bayesian inference scheme for scaled Brownian motion, and investigate its performance on synthetic data for parameter estimation and model selection in a combined inference with fractional Brownian motion. We include the possibility of measurement noise in both models. We find that for trajectories of a few hundred time points the procedure is able to resolve well the true model and parameters. Using the prior of the synthetic data generation process also for the inference, the approach is optimal based on decision theory. We include a comparison with inference using a prior different from the data generating one.

</details>

<details>

<summary>2022-05-12 08:50:11 - Bayesian inference for stochastic oscillatory systems using the phase-corrected Linear Noise Approximation</summary>

- *Ben Swallow, David A. Rand, Giorgos Minas*

- `2205.05955v1` - [abs](http://arxiv.org/abs/2205.05955v1) - [pdf](http://arxiv.org/pdf/2205.05955v1)

> Likelihood-based inference in stochastic non-linear dynamical systems, such as those found in chemical reaction networks and biological clock systems, is inherently complex and has largely been limited to small and unrealistically simple systems. Recent advances in analytically tractable approximations to the underlying conditional probability distributions enable long-term dynamics to be accurately modelled, and make the large number of model evaluations required for exact Bayesian inference much more feasible. We propose a new methodology for inference in stochastic non-linear dynamical systems exhibiting oscillatory behaviour and show the parameters in these models can be realistically estimated from simulated data. Preliminary analyses based on the Fisher Information Matrix of the model can guide the implementation of Bayesian inference. We show that this parameter sensitivity analysis can predict which parameters are practically identifiable. Several Markov chain Monte Carlo algorithms are compared, with our results suggesting a parallel tempering algorithm consistently gives the best approach for these systems, which are shown to frequently exhibit multi-modal posterior distributions.

</details>

<details>

<summary>2022-05-12 09:21:26 - Probability Distribution of Hypervolume Improvement in Bi-objective Bayesian Optimization</summary>

- *Hao Wang, Kaifeng Yang, Michael Affenzeller, Michael Emmerich*

- `2205.05505v2` - [abs](http://arxiv.org/abs/2205.05505v2) - [pdf](http://arxiv.org/pdf/2205.05505v2)

> This work provides the exact expression of the probability distribution of the hypervolume improvement (HVI) for bi-objective generalization of Bayesian optimization. Here, instead of a single-objective improvement, we consider the improvement of the hypervolume indicator concerning the current best approximation of the Pareto front. Gaussian process regression models are trained independently on both objective functions, resulting in a bi-variate separated Gaussian distribution serving as a predictive model for the vector-valued objective function. Some commonly HVI-based acquisition functions (probability of improvement and upper confidence bound) are also leveraged with the help of the exact distribution of HVI. In addition, we show the superior numerical accuracy and efficiency of the exact distribution compared to the commonly used approximation by Monte-Carlo sampling. Finally, we benchmark distribution-leveraged acquisition functions on the widely applied ZDT problem set, demonstrating a significant advantage of using the exact distribution of HVI in multi-objective Bayesian optimization.

</details>

<details>

<summary>2022-05-12 11:31:30 - Loss-Based Variational Bayes Prediction</summary>

- *David T. Frazier, Ruben Loaiza-Maya, Gael M. Martin, Bonsoo Koo*

- `2104.14054v2` - [abs](http://arxiv.org/abs/2104.14054v2) - [pdf](http://arxiv.org/pdf/2104.14054v2)

> We propose a new approach to Bayesian prediction that caters for models with a large number of parameters and is robust to model misspecification. Given a class of high-dimensional (but parametric) predictive models, this new approach constructs a posterior predictive using a variational approximation to a generalized posterior that is directly focused on predictive accuracy. The theoretical behavior of the new prediction approach is analyzed and a form of optimality demonstrated. Applications to both simulated and empirical data using high-dimensional Bayesian neural network and autoregressive mixture models demonstrate that the approach provides more accurate results than various alternatives, including misspecified likelihood-based predictions.

</details>

<details>

<summary>2022-05-12 14:15:17 - Social learning via actions in bandit environments</summary>

- *Aroon Narayanan*

- `2205.06107v1` - [abs](http://arxiv.org/abs/2205.06107v1) - [pdf](http://arxiv.org/pdf/2205.06107v1)

> I study a game of strategic exploration with private payoffs and public actions in a Bayesian bandit setting. In particular, I look at cascade equilibria, in which agents switch over time from the risky action to the riskless action only when they become sufficiently pessimistic. I show that these equilibria exist under some conditions and establish their salient properties. Individual exploration in these equilibria can be more or less than the single-agent level depending on whether the agents start out with a common prior or not, but the most optimistic agent always underexplores. I also show that allowing the agents to write enforceable ex-ante contracts will lead to the most ex-ante optimistic agent to buy all payoff streams, providing an explanation to the buying out of smaller start-ups by more established firms.

</details>

<details>

<summary>2022-05-12 14:41:45 - Addressing Census data problems in race imputation via fully Bayesian Improved Surname Geocoding and name supplements</summary>

- *Kosuke Imai, Santiago Olivella, Evan T. R. Rosenman*

- `2205.06129v1` - [abs](http://arxiv.org/abs/2205.06129v1) - [pdf](http://arxiv.org/pdf/2205.06129v1)

> Prediction of an individual's race and ethnicity plays an important role in social science and public health research. Examples include studies of racial disparity in health and voting. Recently, Bayesian Improved Surname Geocoding (BISG), which uses Bayes' rule to combine information from Census surname files with the geocoding of an individual's residence, has emerged as a leading methodology for this prediction task. Unfortunately, BISG suffers from two Census data problems that contribute to unsatisfactory predictive performance for minorities. First, the decennial Census often contains zero counts for minority racial groups in the Census blocks where some members of those groups reside. Second, because the Census surname files only include frequent names, many surnames -- especially those of minorities -- are missing from the list. To address the zero counts problem, we introduce a fully Bayesian Improved Surname Geocoding (fBISG) methodology that accounts for potential measurement error in Census counts by extending the na\"ive Bayesian inference of the BISG methodology to full posterior inference. To address the missing surname problem, we supplement the Census surname data with additional data on last, first, and middle names taken from the voter files of six Southern states where self-reported race is available. Our empirical validation shows that the fBISG methodology and name supplements significantly improve the accuracy of race imputation across all racial groups, and especially for Asians. The proposed methodology, together with additional name data, is available via the open-source software package wru.

</details>

<details>

<summary>2022-05-12 19:13:56 - Modelling spatially autocorrelated detection probabilities in spatial capture-recapture using random effects</summary>

- *Soumen Dey, Ehsan M. Moqanaki, Cyril Milleret, Pierre Dupont, Mahdieh Tourani, Richard Bischof*

- `2205.06320v1` - [abs](http://arxiv.org/abs/2205.06320v1) - [pdf](http://arxiv.org/pdf/2205.06320v1)

> Spatial capture-recapture (SCR) models are now widely used for estimating density from repeated individual spatial encounters. SCR accounts for the inherent spatial autocorrelation in individual detections by modelling detection probabilities as a function of distance between the detectors and individual activity centres. However, additional spatial heterogeneity in detection probability may still creep in due to environmental or sampling characteristics. if unaccounted for, such variation can lead to pronounced bias in population size estimates. Using simulations, we describe and test three Bayesian SCR models that use generalized linear mixed models (GLMM) to account for latent heterogeneity in baseline detection probability across detectors using: independent random effects (RE), spatially autocorrelated random effects (SARE), and a two-group finite mixture model (FM). Overall, SARE provided the least biased population size estimates (median RB: -9 -- 6%). When spatial autocorrelation was high, SARE also performed best at predicting the spatial pattern of heterogeneity in detection probability. At intermediate levels of autocorrelation, spatially-explicit estimates of detection probability obtained with FM where more accurate than those generated by SARE and RE. In cases where the number of detections per detector is realistically low (at most 1), all GLMMs considered here may require dimension reduction of the random effects by pooling baseline detection probability parameters across neighboring detectors ("aggregation") to avoid over-parameterization. The added complexity and computational overhead associated with SCR-GLMMs may only be justified in extreme cases of spatial heterogeneity. However, even in less extreme cases, detecting and estimating spatially heterogeneous detection probability may assist in planning or adjusting monitoring schemes.

</details>

<details>

<summary>2022-05-12 20:10:31 - Generalized Variational Inference in Function Spaces: Gaussian Measures meet Bayesian Deep Learning</summary>

- *Veit D. Wild, Robert Hu, Dino Sejdinovic*

- `2205.06342v1` - [abs](http://arxiv.org/abs/2205.06342v1) - [pdf](http://arxiv.org/pdf/2205.06342v1)

> We develop a framework for generalized variational inference in infinite-dimensional function spaces and use it to construct a method termed Gaussian Wasserstein inference (GWI). GWI leverages the Wasserstein distance between Gaussian measures on the Hilbert space of square-integrable functions in order to determine a variational posterior using a tractable optimisation criterion and avoids pathologies arising in standard variational function space inference. An exciting application of GWI is the ability to use deep neural networks in the variational parametrisation of GWI, combining their superior predictive performance with the principled uncertainty quantification analogous to that of Gaussian processes. The proposed method obtains state-of-the-art performance on several benchmark datasets.

</details>

<details>

<summary>2022-05-13 00:22:44 - Interpretable Model Summaries Using the Wasserstein Distance</summary>

- *Eric Dunipace, Lorenzo Trippa*

- `2012.09999v3` - [abs](http://arxiv.org/abs/2012.09999v3) - [pdf](http://arxiv.org/pdf/2012.09999v3)

> Statistical models often include thousands of parameters. However, large models decrease the investigator's ability to interpret and communicate the estimated parameters. Reducing the dimensionality of the parameter space in the estimation phase is a commonly used approach, but less work has focused on selecting subsets of the parameters for interpreting the estimated model -- especially in settings such as Bayesian inference and model averaging. Importantly, many models do not have straightforward interpretations and create another layer of obfuscation. To solve this gap, we introduce a new method that uses the Wasserstein distance to identify a low-dimensional interpretable model projection. After the estimation of complex models, users can budget how many parameters they wish to interpret and the proposed generates a simplified model of the desired dimension minimizing the distance to the full model. We provide simulation results to illustrate the method and apply it to cancer datasets.

</details>

<details>

<summary>2022-05-13 00:28:35 - Fast Conditional Network Compression Using Bayesian HyperNetworks</summary>

- *Phuoc Nguyen, Truyen Tran, Ky Le, Sunil Gupta, Santu Rana, Dang Nguyen, Trong Nguyen, Shannon Ryan, Svetha Venkatesh*

- `2205.06404v1` - [abs](http://arxiv.org/abs/2205.06404v1) - [pdf](http://arxiv.org/pdf/2205.06404v1)

> We introduce a conditional compression problem and propose a fast framework for tackling it. The problem is how to quickly compress a pretrained large neural network into optimal smaller networks given target contexts, e.g. a context involving only a subset of classes or a context where only limited compute resource is available. To solve this, we propose an efficient Bayesian framework to compress a given large network into much smaller size tailored to meet each contextual requirement. We employ a hypernetwork to parameterize the posterior distribution of weights given conditional inputs and minimize a variational objective of this Bayesian neural network. To further reduce the network sizes, we propose a new input-output group sparsity factorization of weights to encourage more sparseness in the generated weights. Our methods can quickly generate compressed networks with significantly smaller sizes than baseline methods.

</details>

<details>

<summary>2022-05-13 08:17:47 - Hypothesis testing for varying coefficient models in tail index regression</summary>

- *Koki Momoki, Takuma Yoshida*

- `2205.04176v2` - [abs](http://arxiv.org/abs/2205.04176v2) - [pdf](http://arxiv.org/pdf/2205.04176v2)

> This study examines the varying coefficient model in tail index regression. The varying coefficient model is an efficient semiparametric model that avoids the curse of dimensionality when including large covariates in the model. In fact, the varying coefficient model is useful in mean, quantile, and other regressions. The tail index regression is not an exception. However, the varying coefficient model is flexible, but leaner and simpler models are preferred for applications. Therefore, it is important to evaluate whether the estimated coefficient function varies significantly with covariates. If the effect of the non-linearity is weak, the varying coefficient structure is reduced to a simpler model, such as a constant or zero. Accordingly, the hypothesis test for model assessment in the varying coefficient model has been discussed in mean and quantile regression. However, there are no results in tail index regression. In this study, we investigate asymptotic properties of an estimator and provide a hypothesis testing method for varying coefficient models for tail index regression.n.

</details>

<details>

<summary>2022-05-13 09:41:47 - High-dimensional modeling of spatial and spatio-temporal conditional extremes using INLA and Gaussian Markov random fields</summary>

- *Emma S. Simpson, Thomas Opitz, Jennifer L. Wadsworth*

- `2011.04486v3` - [abs](http://arxiv.org/abs/2011.04486v3) - [pdf](http://arxiv.org/pdf/2011.04486v3)

> The conditional extremes framework allows for event-based stochastic modeling of dependent extremes, and has recently been extended to spatial and spatio-temporal settings. After standardizing the marginal distributions and applying an appropriate linear normalization, certain non-stationary Gaussian processes can be used as asymptotically-motivated models for the process conditioned on threshold exceedances at a fixed reference location and time. In this work, we adapt existing conditional extremes models to allow for the handling of large spatial datasets. This involves specifying the model for spatial observations at $d$ locations in terms of a latent $m\ll d$ dimensional Gaussian model, whose structure is specified by a Gaussian Markov random field. We perform Bayesian inference for such models for datasets containing thousands of observation locations using the integrated nested Laplace approximation, or INLA. We explain how constraints on the spatial and spatio-temporal Gaussian processes, arising from the conditioning mechanism, can be implemented through the latent variable approach without losing the computationally convenient Markov property. We discuss tools for the comparison of models via their posterior distributions, and illustrate the flexibility of the approach with gridded Red Sea surface temperature data at over $6,000$ observed locations. Posterior sampling is exploited to study the probability distribution of cluster functionals of spatial and spatio-temporal extreme episodes.

</details>

<details>

<summary>2022-05-13 10:32:33 - Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees</summary>

- *Valentinian Lungu, Ioannis Papageorgiou, Ioannis Kontoyiannis*

- `2203.04341v2` - [abs](http://arxiv.org/abs/2203.04341v2) - [pdf](http://arxiv.org/pdf/2203.04341v2)

> A new Bayesian modelling framework is introduced for piece-wise homogeneous variable-memory Markov chains, along with a collection of effective algorithmic tools for change-point detection and segmentation of discrete time series. Building on the recently introduced Bayesian Context Trees (BCT) framework, the distributions of different segments in a discrete time series are described as variable-memory Markov chains. Inference for the presence and location of change-points is then performed via Markov chain Monte Carlo sampling. The key observation that facilitates effective sampling is that, using one of the BCT algorithms, the prior predictive likelihood of the data can be computed exactly, integrating out all the models and parameters in each segment. This makes it possible to sample directly from the posterior distribution of the number and location of the change-points, leading to accurate estimates and providing a natural quantitative measure of uncertainty in the results. Estimates of the actual model in each segment can also be obtained, at essentially no additional computational cost. Results on both simulated and real-world data indicate that the proposed methodology performs better than or as well as state-of-the-art techniques.

</details>

<details>

<summary>2022-05-13 13:20:25 - Modelling stellar activity with Gaussian process regression networks</summary>

- *J. D. Camacho, J. P. Faria, P. T. P. Viana*

- `2205.06627v1` - [abs](http://arxiv.org/abs/2205.06627v1) - [pdf](http://arxiv.org/pdf/2205.06627v1)

> Stellar photospheric activity is known to limit the detection and characterisation of extra-solar planets. In particular, the study of Earth-like planets around Sun-like stars requires data analysis methods that can accurately model the stellar activity phenomena affecting radial velocity (RV) measurements. Gaussian Process Regression Networks (GPRNs) offer a principled approach to the analysis of simultaneous time-series, combining the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian Processes. Using HARPS-N solar spectroscopic observations encompassing three years, we demonstrate that this framework is capable of jointly modelling RV data and traditional stellar activity indicators. Although we consider only the simplest GPRN configuration, we are able to describe the behaviour of solar RV data at least as accurately as previously published methods. We confirm the correlation between the RV and stellar activity time series reaches a maximum at separations of a few days, and find evidence of non-stationary behaviour in the time series, associated with an approaching solar activity minimum.

</details>

<details>

<summary>2022-05-13 15:03:38 - On the use of a local R-hat to improve MCMC convergence diagnostic</summary>

- *Tho Moins, Julyan Arbel, Anne Dutfoy, Stphane Girard*

- `2205.06694v1` - [abs](http://arxiv.org/abs/2205.06694v1) - [pdf](http://arxiv.org/pdf/2205.06694v1)

> Diagnosing convergence of Markov chain Monte Carlo is crucial and remains an essentially unsolved problem. Among the most popular methods, the potential scale reduction factor, commonly named $\hat{R}$, is an indicator that monitors the convergence of output chains to a target distribution, based on a comparison of the between- and within-variances. Several improvements have been suggested since its introduction in the 90s. Here, we aim at better understanding the $\hat{R}$ behavior by proposing a localized version that focuses on quantiles of the target distribution. This new version relies on key theoretical properties of the associated population value. It naturally leads to proposing a new indicator $\hat{R}_\infty$, which is shown to allow both for localizing the Markov chain Monte Carlo convergence in different quantiles of the target distribution, and at the same time for handling some convergence issues not detected by other $\hat{R}$ versions.

</details>

<details>

<summary>2022-05-13 18:47:50 - Calibrated Model Criticism Using Split Predictive Checks</summary>

- *Jiawei Li, Jonathan H. Huggins*

- `2203.15897v2` - [abs](http://arxiv.org/abs/2203.15897v2) - [pdf](http://arxiv.org/pdf/2203.15897v2)

> Checking how well a fitted model explains the data is one of the most fundamental parts of a Bayesian data analysis. However, existing model checking methods suffer from trade-offs between being well-calibrated, automated, and computationally efficient. To overcome these limitations, we propose split predictive checks (SPCs), which combine the ease-of-use and speed of posterior predictive checks with the good calibration properties of predictive checks that rely on model-specific derivations or inference schemes. We develop an asymptotic theory for two types of SPCs: single SPCs and the divided SPC. Our results demonstrate that they offer complementary strengths: single SPCs provide superior power in the small-data regime or when the misspecification is significant and divided SPCs provide superior power as the dataset size increases or when the form of misspecification is more subtle. We validate the finite-sample utility of SPCs through extensive simulation experiments in exponential family and hierarchical models, and provide four real-data examples where SPCs offer novel insights and additional flexibility beyond what is available when using posterior predictive checks.

</details>

<details>

<summary>2022-05-13 19:22:05 - A Probabilistic Generative Model of Free Categories</summary>

- *Eli Sennesh, Tom Xu, Yoshihiro Maruyama*

- `2205.04545v2` - [abs](http://arxiv.org/abs/2205.04545v2) - [pdf](http://arxiv.org/pdf/2205.04545v2)

> Applied category theory has recently developed libraries for computing with morphisms in interesting categories, while machine learning has developed ways of learning programs in interesting languages. Taking the analogy between categories and languages seriously, this paper defines a probabilistic generative model of morphisms in free monoidal categories over domain-specific generating objects and morphisms. The paper shows how acyclic directed wiring diagrams can model specifications for morphisms, which the model can use to generate morphisms. Amortized variational inference in the generative model then enables learning of parameters (by maximum likelihood) and inference of latent variables (by Bayesian inversion). A concrete experiment shows that the free category prior achieves competitive reconstruction performance on the Omniglot dataset.

</details>

<details>

<summary>2022-05-14 16:11:10 - Evaluating Forecasts with scoringutils in R</summary>

- *Nikos I. Bosse, Hugo Gruson, Anne Cori, Edwin van Leeuwen, Sebastian Funk, Sam Abbott*

- `2205.07090v1` - [abs](http://arxiv.org/abs/2205.07090v1) - [pdf](http://arxiv.org/pdf/2205.07090v1)

> Evaluating forecasts is essential in order to understand and improve forecasting and make forecasts useful to decision-makers. Much theoretical work has been done on the development of proper scoring rules and other scoring metrics that can help evaluate forecasts. In practice, however, conducting a forecast evaluation and comparison of different forecasters remains challenging. In this paper we introduce scoringutils, an R package that aims to greatly facilitate this process. It is especially geared towards comparing multiple forecasters, regardless of how forecasts were created, and visualising results. The package is able to handle missing forecasts and is the first R package to offer extensive support for forecasts represented through predictive quantiles, a format used by several collaborative ensemble forecasting efforts. The paper gives a short introduction to forecast evaluation, discusses the metrics implemented in scoringutils and gives guidance on when they are appropriate to use, and illustrates the application of the package using example data of forecasts for COVID-19 cases and deaths submitted to the European Forecast Hub between May and September 2021

</details>

<details>

<summary>2022-05-15 03:47:55 - Covariance Model with General Linear Structure and Divergent Parameters</summary>

- *Xinyan Fan, Wei Lan, Tao Zou, Chih-Ling Tsai*

- `2205.07174v1` - [abs](http://arxiv.org/abs/2205.07174v1) - [pdf](http://arxiv.org/pdf/2205.07174v1)

> For estimating the large covariance matrix with a limited sample size, we propose the covariance model with general linear structure (CMGL) by employing the general link function to connect the covariance of the continuous response vector to a linear combination of weight matrices. Without assuming the distribution of responses, and allowing the number of parameters associated with weight matrices to diverge, we obtain the quasi-maximum likelihood estimators (QMLE) of parameters and show their asymptotic properties. In addition, an extended Bayesian information criteria (EBIC) is proposed to select relevant weight matrices, and the consistency of EBIC is demonstrated. Under the identity link function, we introduce the ordinary least squares estimator (OLS) that has the closed form. Hence, its computational burden is reduced compared to QMLE, and the theoretical properties of OLS are also investigated. To assess the adequacy of the link function, we further propose the quasi-likelihood ratio test and obtain its limiting distribution. Simulation studies are presented to assess the performance of the proposed methods, and the usefulness of generalized covariance models is illustrated by an analysis of the US stock market.

</details>

<details>

<summary>2022-05-15 11:22:48 - Large Data and (Not Even Very) Complex Ecological Models: When Worlds Collide</summary>

- *Ruth King, Blanca Sarzo, Vctor Elvira*

- `2205.07261v1` - [abs](http://arxiv.org/abs/2205.07261v1) - [pdf](http://arxiv.org/pdf/2205.07261v1)

> We consider the challenges that arise when fitting complex ecological models to 'large' data sets. In particular, we focus on random effect models which are commonly used to describe individual heterogeneity, often present in ecological populations under study. In general, these models lead to a likelihood that is expressible only as an analytically intractable integral. Common techniques for fitting such models to data include, for example, the use of numerical approximations for the integral, or a Bayesian data augmentation approach. However, as the size of the data set increases (i.e. the number of individuals increases), these computational tools may become computationally infeasible. We present an efficient Bayesian model-fitting approach, whereby we initially sample from the posterior distribution of a smaller subsample of the data, before correcting this sample to obtain estimates of the posterior distribution of the full dataset, using an importance sampling approach. We consider several practical issues, including the subsampling mechanism, computational efficiencies (including the ability to parallelise the algorithm) and combining subsampling estimates using multiple subsampled datasets. We demonstrate the approach in relation to individual heterogeneity capture-recapture models. We initially demonstrate the feasibility of the approach via simulated data before considering a challenging real dataset of approximately 30,000 guillemots, and obtain posterior estimates in substantially reduced computational time.

</details>

<details>

<summary>2022-05-15 14:35:43 - Inward and Outward Network Influence Analysis</summary>

- *Yujia Wu, Wei Lan, Tao Zou, Chih-Ling Tsai*

- `2205.07297v1` - [abs](http://arxiv.org/abs/2205.07297v1) - [pdf](http://arxiv.org/pdf/2205.07297v1)

> Measuring heterogeneous influence across nodes in a network is critical in network analysis. This paper proposes an Inward and Outward Network Influence (IONI) model to assess nodal heterogeneity. Specifically, we allow for two types of influence parameters; one measures the magnitude of influence that each node exerts on others (outward influence), while we introduce a new parameter to quantify the receptivity of each node to being influenced by others (inward influence). Accordingly, these two types of influence measures naturally classify all nodes into four quadrants (high inward and high outward, low inward and high outward, low inward and low outward, high inward and low outward). To demonstrate our four-quadrant clustering method in practice, we apply the quasi-maximum likelihood approach to estimate the influence parameters, and we show the asymptotic properties of the resulting estimators. In addition, score tests are proposed to examine the homogeneity of the two types of influence parameters. To improve the accuracy of inferences about nodal influences, we introduce a Bayesian information criterion that selects the optimal influence model. The usefulness of the IONI model and the four-quadrant clustering method is illustrated via simulation studies and an empirical example involving customer segmentation.

</details>

<details>

<summary>2022-05-15 17:47:05 - Nowcasting Growth using Google Trends Data: A Bayesian Structural Time Series Model</summary>

- *David Kohns, Arnab Bhattacharjee*

- `2011.00938v2` - [abs](http://arxiv.org/abs/2011.00938v2) - [pdf](http://arxiv.org/pdf/2011.00938v2)

> This paper investigates the benefits of internet search data in the form of Google Trends for nowcasting real U.S. GDP growth in real time through the lens of mixed frequency Bayesian Structural Time Series (BSTS) models. We augment and enhance both model and methodology to make these better amenable to nowcasting with large number of potential covariates. Specifically, we allow shrinking state variances towards zero to avoid overfitting, extend the SSVS (spike and slab variable selection) prior to the more flexible normal-inverse-gamma prior which stays agnostic about the underlying model size, as well as adapt the horseshoe prior to the BSTS. The application to nowcasting GDP growth as well as a simulation study demonstrate that the horseshoe prior BSTS improves markedly upon the SSVS and the original BSTS model with the largest gains in dense data-generating-processes. Our application also shows that a large dimensional set of search terms is able to improve nowcasts early in a specific quarter before other macroeconomic data become available. Search terms with high inclusion probability have good economic interpretation, reflecting leading signals of economic anxiety and wealth effects.

</details>

<details>

<summary>2022-05-15 18:45:21 - Inference of Stochastic Disease Transmission Models Using Particle-MCMC and a Gradient Based Proposal</summary>

- *Conor Rosato, John Harris, Jasmina Panovska-Griffiths, Simon Maskell*

- `2205.07356v1` - [abs](http://arxiv.org/abs/2205.07356v1) - [pdf](http://arxiv.org/pdf/2205.07356v1)

> State-space models have been widely used to model the dynamics of communicable diseases in populations of interest by fitting to time-series data. Particle filters have enabled these models to incorporate stochasticity and so can better reflect the true nature of population behaviours. Relevant parameters such as the spread of the disease, $R_t$, and recovery rates can be inferred using Particle MCMC. The standard method uses a Metropolis-Hastings random-walk proposal which can struggle to reach the stationary distribution in a reasonable time when there are multiple parameters.   In this paper we obtain full Bayesian parameter estimations using gradient information and the No U-Turn Sampler (NUTS) when proposing new parameters of stochastic non-linear Susceptible-Exposed-Infected-Recovered (SEIR) and SIR models. Although NUTS makes more than one target evaluation per iteration, we show that it can provide more accurate estimates in a shorter run time than Metropolis-Hastings.

</details>

<details>

<summary>2022-05-15 21:10:13 - Proximal MCMC for Bayesian Inference of Constrained and Regularized Estimation</summary>

- *Xinkai Zhou, Eric C. Chi, Hua Zhou*

- `2205.07378v1` - [abs](http://arxiv.org/abs/2205.07378v1) - [pdf](http://arxiv.org/pdf/2205.07378v1)

> This paper advocates proximal Markov Chain Monte Carlo (ProxMCMC) as a flexible and general Bayesian inference framework for constrained or regularized estimation. Originally introduced in the Bayesian imaging literature, ProxMCMC employs the Moreau-Yosida envelope for a smooth approximation of the total-variation regularization term, fixes nuisance and regularization strength parameters as constants, and relies on the Langevin algorithm for the posterior sampling. We extend ProxMCMC to the full Bayesian framework with modeling and data adaptive estimation of all parameters including the regularization strength parameter. More efficient sampling algorithms such as the Hamiltonian Monte Carlo are employed to scale ProxMCMC to high-dimensional problems. Analogous to the proximal algorithms in optimization, ProxMCMC offers a versatile and modularized procedure for the inference of constrained and non-smooth problems. The power of ProxMCMC is illustrated on various statistical estimation and machine learning tasks. The inference in these problems is traditionally considered difficult from both frequentist and Bayesian perspectives.

</details>

<details>

<summary>2022-05-15 22:03:45 - Inference with Imputed Data: The Allure of Making Stuff Up</summary>

- *Charles F. Manski*

- `2205.07388v1` - [abs](http://arxiv.org/abs/2205.07388v1) - [pdf](http://arxiv.org/pdf/2205.07388v1)

> Incomplete observability of data generates an identification problem. There is no panacea for missing data. What one can learn about a population parameter depends on the assumptions one finds credible to maintain. The credibility of assumptions varies with the empirical setting. No specific assumptions can provide a realistic general solution to the problem of inference with missing data. Yet Rubin has promoted random multiple imputation (RMI) as a general way to deal with missing values in public-use data. This recommendation has been influential to empirical researchers who seek a simple fix to the nuisance of missing data. This paper adds to my earlier critiques of imputation. It provides a transparent assessment of the mix of Bayesian and frequentist thinking used by Rubin to argue for RMI. It evaluates random imputation to replace missing outcome or covariate data when the objective is to learn a conditional expectation. It considers steps that might help combat the allure of making stuff up.

</details>

<details>

<summary>2022-05-15 22:11:20 - Reinforcing RCTs with Multiple Priors while Learning about External Validity</summary>

- *Frederico Finan, Demian Pouzo*

- `2112.09170v3` - [abs](http://arxiv.org/abs/2112.09170v3) - [pdf](http://arxiv.org/pdf/2112.09170v3)

> This paper presents a framework for how to incorporate prior sources of information into the design of a sequential experiment. These sources can include previous experiments, expert opinions, or the experimenter's own introspection. We formalize this problem using a multi-prior Bayesian approach that maps each source to a Bayesian model. These models are aggregated according to their associated posterior probabilities. We evaluate a broad of policy rules according to three criteria: whether the experimenter learns the parameters of the payoff distributions, the probability that the experimenter chooses the wrong treatment when deciding to stop the experiment, and the average rewards. We show that our framework exhibits several nice finite sample properties, including robustness to any source that is not externally valid.

</details>

<details>

<summary>2022-05-16 06:56:42 - Generalization Error Bounds on Deep Learning with Markov Datasets</summary>

- *Lan V. Truong*

- `2201.11059v3` - [abs](http://arxiv.org/abs/2201.11059v3) - [pdf](http://arxiv.org/pdf/2201.11059v3)

> In this paper, we derive upper bounds on generalization errors for deep neural networks with Markov datasets. These bounds are developed based on Koltchinskii and Panchenko's approach for bounding the generalization error of combined classifiers with i.i.d. datasets. The development of new symmetrization inequalities in high-dimensional probability for Markov chains is a key element in our extension, where the spectral gap of the infinitesimal generator of the Markov chain plays a key parameter in these inequalities. We also propose a simple method to convert these bounds and other similar ones in traditional deep learning and machine learning to Bayesian counterparts for both i.i.d. and Markov datasets. Extensions to $m$-order homogeneous Markov chains such as AR and ARMA models and mixtures of several Markov data services are given.

</details>

<details>

<summary>2022-05-16 10:13:09 - Model selection in the space of Gaussian models invariant by symmetry</summary>

- *Piotr Graczyk, Hideyuki Ishi, Bartosz Koodziejek, Hlne Massam*

- `2004.03503v3` - [abs](http://arxiv.org/abs/2004.03503v3) - [pdf](http://arxiv.org/pdf/2004.03503v3)

> We consider multivariate centered Gaussian models for the random variable $Z=(Z_1,\ldots, Z_p)$, invariant under the action of a subgroup of the group of permutations on $\{1,\ldots, p\}$. Using the representation theory of the symmetric group on the field of reals, we derive the distribution of the maximum likelihood estimate of the covariance parameter $\Sigma$ and also the analytic expression of the normalizing constant of the Diaconis-Ylvisaker conjugate prior for the precision parameter $K=\Sigma^{-1}$. We can thus perform Bayesian model selection in the class of complete Gaussian models invariant by the action of a subgroup of the symmetric group, which we could also call complete RCOP models. We illustrate our results with a toy example of dimension $4$ and several examples for selection within cyclic groups, including a high dimensional example with $p=100$.

</details>

<details>

<summary>2022-05-16 10:14:47 - Appropriate reduction of the posterior distribution in fully Bayesian inversions</summary>

- *Dye SK Sato, Yukitoshi Fukahata, Yohei Nozue*

- `2205.07559v1` - [abs](http://arxiv.org/abs/2205.07559v1) - [pdf](http://arxiv.org/pdf/2205.07559v1)

> Bayesian inversion generates a posterior distribution of model parameters from an observation equation and prior information both weighted by hyperparameters. The prior is also introduced for the hyperparameters in fully Bayesian inversions and enables us to evaluate both the model parameters and hyperparameters probabilistically by the joint posterior. However, even in a linear inverse problem, it is unsolved how we should extract useful information on the model parameters from the joint posterior. This study presents a theoretical exploration into the appropriate dimensionality reduction of the joint posterior in the fully Bayesian inversion. We classify the ways of probability reduction into the following three categories focused on the marginalisation of the joint posterior: (1) using the joint posterior without marginalisation, (2) using the marginal posterior of the model parameters and (3) using the marginal posterior of the hyperparameters. First, we derive several analytical results that characterise these categories. One is a suite of semianalytic representations of the probability maximisation estimators for respective categories in the linear inverse problem. The mode estimators of categories (1) and (2) are found asymptotically identical for a large number of data and model parameters. We also prove the asymptotic distributions of categories (2) and (3) delta-functionally concentrate on their probability peaks, which predicts two distinct optimal estimates of the model parameters. Second, we conduct a synthetic test and find an appropriate reduction is realised by category (3), typified by Akaike's Bayesian information criterion (ABIC). The other reduction categories are shown inappropriate for the case of many model parameters, where the probability concentration of the marginal posterior of the model parameters is found no longer to mean the central limit theorem...

</details>

<details>

<summary>2022-05-16 12:49:08 - A bootstrap approach for validating the number of groups identified by latent class growth models</summary>

- *Miceline Msidor, Caroline Sirois, Marc Simard, Denis Talbot*

- `2205.07631v1` - [abs](http://arxiv.org/abs/2205.07631v1) - [pdf](http://arxiv.org/pdf/2205.07631v1)

> The use of longitudinal finite mixture models such as group-based trajectory modeling has seen a sharp increase during the last decades in the medical literature. However, these methods have been criticized especially because of the data-driven modelling process which involves statistical decision-making. In this paper, we propose an approach that uses bootstrap to sample observations with replacement from the original data to validate the number of groups identified and to quantify the uncertainty in the number of groups. The method allows investigating the statistical validity and the uncertainty of the groups identified in the original data by checking if the same solution is also found across the bootstrap samples. In a simulation study, we examined whether the bootstrap-estimated variability in the number of groups reflected the replication-wise variability. We also compared the replication-wise variability to the Bayesian posterior probability. We evaluated the ability of three commonly used adequacy criteria (average posterior probability, odds of correct classification and relative entropy) to identify uncertainty in the number of groups. Finally, we illustrated the proposed approach using data from the Quebec Integrated Chronic Disease Surveillance System to identify longitudinal medication patterns between 2015 and 2018 in older adults with diabetes.

</details>

<details>

<summary>2022-05-16 13:56:15 - Gaussian mixture modeling of nodes in Bayesian network according to maximal parental cliques</summary>

- *Yiran Dong, Chuanhou Gao*

- `2204.09532v3` - [abs](http://arxiv.org/abs/2204.09532v3) - [pdf](http://arxiv.org/pdf/2204.09532v3)

> This paper uses Gaussian mixture model instead of linear Gaussian model to fit the distribution of every node in Bayesian network. We will explain why and how we use Gaussian mixture models in Bayesian network. Meanwhile we propose a new method, called double iteration algorithm, to optimize the mixture model, the double iteration algorithm combines the expectation maximization algorithm and gradient descent algorithm, and it performs perfectly on the Bayesian network with mixture models. In experiments we test the Gaussian mixture model and the optimization algorithm on different graphs which is generated by different structure learning algorithm on real data sets, and give the details of every experiment.

</details>

<details>

<summary>2022-05-16 14:29:53 - A modeler's guide to extreme value software</summary>

- *Lo R. Belzile, Christophe Dutang, Paul J. Northrop, Thomas Opitz*

- `2205.07714v1` - [abs](http://arxiv.org/abs/2205.07714v1) - [pdf](http://arxiv.org/pdf/2205.07714v1)

> This review paper surveys recent development in software implementations for extreme value analyses since the publication of Stephenson and Gilleland (2006) and Gilleland et al. (2013), here with a focus on numerical challenges. We provide a comparative review by topic and highlight differences in existing routines, along with listing areas where software development is lacking. The online supplement contains two vignettes providing a comparison of implementations of frequentist and Bayesian estimation of univariate extreme value models.

</details>

<details>

<summary>2022-05-16 15:08:33 - Spike-and-Slab LASSO Generalized Additive Models and Scalable Algorithms for High-Dimensional Data Analysis</summary>

- *Boyi Guo, Byron C. Jaeger, A. K. M. Fazlur Rahman, D. Leann Long, Nengjun Yi*

- `2110.14449v3` - [abs](http://arxiv.org/abs/2110.14449v3) - [pdf](http://arxiv.org/pdf/2110.14449v3)

> There are proposals that extend the classical generalized additive models (GAMs) to accommodate high-dimensional data ($p>>n$) using group sparse regularization. However, the sparse regularization may induce excess shrinkage when estimating smooth functions, damaging predictive performance. Moreover, most of these GAMs consider an "all-in-all-out" approach for functional selection, rendering them difficult to answer if nonlinear effects are necessary. While some Bayesian models can address these shortcomings, using Markov chain Monte Carlo algorithms for model fitting creates a new challenge, scalability. Hence, we propose Bayesian hierarchical generalized additive models as a solution: we consider the smoothing penalty for proper shrinkage of curve interpolation via reparameterization. A novel two-part spike-and-slab LASSO prior for smooth functions is developed to address the sparsity of signals while providing extra flexibility to select the linear or nonlinear components of smooth functions. A scalable and deterministic algorithm, EM-Coordinate Descent, is implemented in an open-source R package BHAM. Simulation studies and metabolomics data analyses demonstrate improved predictive and computational performance against state-of-the-art models. Functional selection performance suggests trade-offs exist regarding the effect hierarchy assumption.

</details>

<details>

<summary>2022-05-16 15:30:04 - Doubly Robust Calibration of Prediction Sets under Covariate Shift</summary>

- *Yachong Yang, Arun Kumar Kuchibhotla, Eric Tchetgen Tchetgen*

- `2203.01761v2` - [abs](http://arxiv.org/abs/2203.01761v2) - [pdf](http://arxiv.org/pdf/2203.01761v2)

> Conformal prediction has received tremendous attention in recent years and has offered new solutions to problems in missing data and causal inference; yet these advances have not leveraged modern semiparametric efficiency theory for more robust and efficient uncertainty quantification. In this paper, we consider the problem of obtaining distribution-free prediction regions accounting for a shift in the distribution of the covariates between the training and test data. Under an explainable covariate shift assumption analogous to the standard missing at random assumption, we propose three variants of a general framework to construct well-calibrated prediction regions for the unobserved outcome in the test sample. Our approach is based on the efficient influence function for the quantile of the unobserved outcome in the test population combined with an arbitrary machine learning prediction algorithm, without compromising asymptotic coverage. Next, we extend our approach to account for departure from the explainable covariate shift assumption in a semiparametric sensitivity analysis for potential latent covariate shift. In all cases, we establish that the resulting prediction sets eventually attain nominal average coverage in large samples. This guarantee is a consequence of the product bias form of our proposal which implies correct coverage if either the propensity score or the conditional distribution of the response is estimated sufficiently well. Our results also provide a framework for construction of doubly robust prediction sets of individual treatment effects, under both unconfoundedness and allowing for some degree of unmeasured confounding. Finally, we discuss aggregation of prediction sets from different machine learning algorithms for optimal prediction and illustrate the performance of our methods in both synthetic and real data.

</details>

<details>

<summary>2022-05-16 19:07:53 - binspp: An R Package for Bayesian Inference for Neyman-Scott Point Processes with Complex Inhomogeneity Structure</summary>

- *Ji Dvok, Radim Reme, Ladislav Bernek, Tom Mrkvika*

- `2205.07946v1` - [abs](http://arxiv.org/abs/2205.07946v1) - [pdf](http://arxiv.org/pdf/2205.07946v1)

> The Neyman-Scott point process is a widely used point process model which is easily interpretable and easily extendable to include various types of inhomogeneity. The inference for such complex models is then complicated and fast methods, such as minimum contrast method or composite likelihood approach do not provide accurate estimates or fail completely. Therefore, we introduce Bayesian MCMC approach for the inference of Neymann-Scott point process models with inhomogeneity in any or all of the following model components: process of cluster centers, mean number of points in a cluster, spread of the clusters. We also extend the Neyman-Scott point process to the case of overdispersed or underdispersed cluster sizes and provide a Bayesian MCMC algorithm for its inference. The R package binspp provides these estimation methods in an easy to handle implementation, with detailed graphical output including traceplots for all model parameters and further diagnostic plots. All inhomogeneities are modelled by spatial covariates and the Bayesian inference for the corresponding regression parameters is provided.

</details>

<details>

<summary>2022-05-16 21:25:01 - Semiparametric Functional Factor Models with Bayesian Rank Selection</summary>

- *Daniel R. Kowal, Antonio Canale*

- `2108.02151v3` - [abs](http://arxiv.org/abs/2108.02151v3) - [pdf](http://arxiv.org/pdf/2108.02151v3)

> Functional data are frequently accompanied by a parametric template that describes the typical shapes of the functions. However, these parametric templates can incur significant bias, which undermines both utility and interpretability. To correct for model misspecification, we augment the parametric template with an infinite-dimensional nonparametric functional basis. The nonparametric basis functions are learned from the data and constrained to be orthogonal to the parametric template, which preserves distinctness between the parametric and nonparametric terms. This distinctness is essential to prevent functional confounding, which otherwise induces severe bias for the parametric terms. The nonparametric factors are regularized with an ordered spike-and-slab prior that provides consistent rank selection and satisfies several appealing theoretical properties. The versatility of the proposed approach is illustrated through applications to synthetic data, human motor control data, and dynamic yield curve data. Relative to parametric and semiparametric alternatives, the proposed semiparametric functional factor model eliminates bias, reduces excessive posterior and predictive uncertainty, and provides reliable inference on the effective number of nonparametric terms--all with minimal additional computational costs.

</details>

<details>

<summary>2022-05-16 21:25:33 - PriorVAE: Encoding spatial priors with VAEs for small-area estimation</summary>

- *Elizaveta Semenova, Yidan Xu, Adam Howes, Theo Rashid, Samir Bhatt, Swapnil Mishra, Seth Flaxman*

- `2110.10422v3` - [abs](http://arxiv.org/abs/2110.10422v3) - [pdf](http://arxiv.org/pdf/2110.10422v3)

> Gaussian processes (GPs), implemented through multivariate Gaussian distributions for a finite collection of data, are the most popular approach in small-area spatial statistical modelling. In this context they are used to encode correlation structures over space and can generalise well in interpolation tasks. Despite their flexibility, off-the-shelf GPs present serious computational challenges which limit their scalability and practical usefulness in applied settings. Here, we propose a novel, deep generative modelling approach to tackle this challenge, termed PriorVAE: for a particular spatial setting, we approximate a class of GP priors through prior sampling and subsequent fitting of a variational autoencoder (VAE). Given a trained VAE, the resultant decoder allows spatial inference to become incredibly efficient due to the low dimensional, independently distributed latent Gaussian space representation of the VAE. Once trained, inference using the VAE decoder replaces the GP within a Bayesian sampling framework. This approach provides tractable and easy-to-implement means of approximately encoding spatial priors and facilitates efficient statistical inference. We demonstrate the utility of our VAE two stage approach on Bayesian, small-area estimation tasks.

</details>

<details>

<summary>2022-05-16 22:43:31 - The e-value and the Full Bayesian Significance Test: Logical Properties and Philosophical Consequences</summary>

- *Julio Michael Stern, Carlos Alberto de Braganca Pereira, Marcelo de Souza Lauretto, Luis Gustavo Esteves, Rafael Izbicki, Rafael Bassi Stern, Marcio Alves Diniz*

- `2205.08010v1` - [abs](http://arxiv.org/abs/2205.08010v1) - [pdf](http://arxiv.org/pdf/2205.08010v1)

> This article gives a conceptual review of the e-value, ev(H|X) -- the epistemic value of hypothesis H given observations X. This statistical significance measure was developed in order to allow logically coherent and consistent tests of hypotheses, including sharp or precise hypotheses, via the Full Bayesian Significance Test (FBST). Arguments of analysis allow a full characterization of this statistical test by its logical or compositional properties, showing a mutual complementarity between results of mathematical statistics and the logical desiderata lying at the foundations of this theory.

</details>

<details>

<summary>2022-05-17 00:02:06 - Microbiome subcommunity learning with logistic-tree normal latent Dirichlet allocation</summary>

- *Patrick LeBlanc, Li Ma*

- `2109.05386v3` - [abs](http://arxiv.org/abs/2109.05386v3) - [pdf](http://arxiv.org/pdf/2109.05386v3)

> Mixed-membership (MM) models such as Latent Dirichlet Allocation (LDA) have been applied to microbiome compositional data to identify latent subcommunities of microbial species. These subcommunities are informative for understanding the biological interplay of microbes and for predicting health outcomes. However, microbiome compositions typically display substantial cross-sample heterogeneities in subcommunity compositions -- that is, the variability in the proportions of microbes in shared subcommunities across samples -- which is not accounted for in prior analyses. As a result, LDA can produce inference which is highly sensitive to the specification of the number of subcommunities and often divides a single subcommunity into multiple artificial ones. To address this limitation, we incorporate the logistic-tree normal (LTN) model into LDA to form a new MM model. This model allows cross-sample variation in the composition of each subcommunity around some "centroid" composition that defines the subcommunity. Incorporation of auxiliary P\'olya-Gamma variables enables a computationally efficient collapsed blocked Gibbs sampler to carry out Bayesian inference under this model. By accounting for such heterogeneity, our new model restores the robustness of the inference in the specification of the number of subcommunities and allows meaningful subcommunities to be identified.

</details>

<details>

<summary>2022-05-17 00:45:35 - Bayesian Semiparametric Longitudinal Inverse-Probit Mixed Models for Category Learning</summary>

- *Minerva Mukhopadhyay, Jacie R. McHaney, Bharath Chandrasekaran, Abhra Sarkar*

- `2112.04626v2` - [abs](http://arxiv.org/abs/2112.04626v2) - [pdf](http://arxiv.org/pdf/2112.04626v2)

> Understanding how adult humans learn to categorize can shed novel insights into the mechanisms underlying experience-dependent brain plasticity. Drift-diffusion processes are popular in such contexts for their ability to mimic underlying neural mechanisms but require data on both category responses and associated response times for inference. Category response accuracies are, however, often the only reliable measure recorded by behavioral scientists to describe human learning. Building carefully on drift-diffusion models with latent response times, we derive a biologically interpretable inverse-probit categorical probability model for such data. The model, however, presents significant identifiability and inference challenges. We address these challenges via a novel projection-based approach with a symmetry preserving identifiability constraint that allows us to work with conjugate priors in an unconstrained space. We adapt the model for group and individual level inference in longitudinal settings. Building again on the model's latent variable representation, we design an efficient Markov chain Monte Carlo algorithm for posterior computation. We evaluate the method's empirical performances through simulation experiments. The method's practical efficacy is illustrated in applications to longitudinal tone learning studies.

</details>

<details>

<summary>2022-05-17 00:46:05 - Bayesian Semiparametric Covariate Informed Multivariate Density Deconvolution</summary>

- *Abhra Sarkar*

- `2204.01686v2` - [abs](http://arxiv.org/abs/2204.01686v2) - [pdf](http://arxiv.org/pdf/2204.01686v2)

> Estimating the marginal and joint densities of the long-term average intakes of different dietary components is an important problem in nutritional epidemiology. Since these variables cannot be directly measured, data are usually collected in the form of 24-hour recalls of the intakes. The problem of estimating the density of the latent long-term average intakes from their observed but error contaminated recalls then becomes a problem of multivariate deconvolution of densities. The underlying densities could potentially vary with the subjects' demographic characteristics such as sex, ethnicity, age, etc. The problem of density deconvolution in the presence of associated precisely measured covariates has, however, never been considered before, not even in the univariate setting. We present a flexible Bayesian semiparametric approach to covariate informed multivariate deconvolution. Building on recent advances in copula deconvolution and conditional tensor factorization techniques, our proposed method not only allows the joint and the marginal densities to vary flexibly with the associated predictors but also allows automatic selection of the most influential predictors. Importantly, the method also allows the density of interest and the density of the measurement errors to vary with potentially different sets of predictors. We design Markov chain Monte Carlo algorithms that enable efficient posterior inference, appropriately accommodating uncertainty in all aspects of our analysis. The empirical efficacy of the proposed method is illustrated through simulation experiments. Its practical utility is demonstrated in the afore-described nutritional epidemiology applications in estimating covariate-adjusted long term intakes of different dietary components. Supplementary materials include substantive additional details and R codes are also available online.

</details>

<details>

<summary>2022-05-17 05:59:52 - Sequential Elimination Contests with All-Pay Auctions</summary>

- *Fupeng Sun, Yanwei Sun, Chiwei Yan, Li Jin*

- `2205.08104v1` - [abs](http://arxiv.org/abs/2205.08104v1) - [pdf](http://arxiv.org/pdf/2205.08104v1)

> By modeling contests as all-pay auctions, we study two-stage sequential elimination contests (SEC) under incomplete information, where only the players with top efforts in the first stage can proceed to the second and final stage to compete for prizes. Players have privately held type/ability information that impacts their costs of exerting efforts. We characterize players' Perfect Bayesian Equilibrium strategies and discover a somewhat surprising result: all players exert weakly lower efforts in the final stage of the SEC compared to those under a one-round contest, regardless of the number of players admitted to the final stage. This result holds under any multi-prize reward structure, any type distribution and cost function. As a consequence, in terms of the expected highest effort or total efforts of the final stage, the optimal SEC is equivalent to a one-round contest by letting all players proceed to the final stage.

</details>

<details>

<summary>2022-05-17 06:26:05 - Bayesian Uncertainty Quantification of Local Volatility Model</summary>

- *Kai Yin, Anirban Mondal*

- `2105.10210v2` - [abs](http://arxiv.org/abs/2105.10210v2) - [pdf](http://arxiv.org/pdf/2105.10210v2)

> Local volatility is an important quantity in option pricing, portfolio hedging, and risk management. It is not directly observable from the market; hence calibrations of local volatility models are necessary using observable market data. Unlike most existing point-estimate methods, we cast the large-scale nonlinear inverse problem into the Bayesian framework, yielding a posterior distribution of the local volatility, which naturally quantifies its uncertainty. This extra uncertainty information enables traders and risk managers to make better decisions. To alleviate the computational cost, we apply Karhunen--L\`oeve expansion to reduce the dimensionality of the Gaussian Process prior for local volatility. A modified two-stage adaptive Metropolis algorithm is applied to sample the posterior probability distribution, which further reduces computational burdens caused by repetitive numerical forward option pricing model solver and time of heuristic tuning. We demonstrate our methodology with both synthetic and market data.

</details>

<details>

<summary>2022-05-17 07:13:13 - Uncertainty Quantification and Experimental Design for Large-Scale Linear Inverse Problems under Gaussian Process Priors</summary>

- *Cdric Travelletti, David Ginsbourger, Niklas Linde*

- `2109.03457v3` - [abs](http://arxiv.org/abs/2109.03457v3) - [pdf](http://arxiv.org/pdf/2109.03457v3)

> We consider the use of Gaussian process (GP) priors for solving inverse problems in a Bayesian framework. As is well known, the computational complexity of GPs scales cubically in the number of datapoints. We here show that in the context of inverse problems involving integral operators, one faces additional difficulties that hinder inversion on large grids. Furthermore, in that context, covariance matrices can become too large to be stored. By leveraging results about sequential disintegrations of Gaussian measures, we are able to introduce an implicit representation of posterior covariance matrices that reduces the memory footprint by only storing low rank intermediate matrices, while allowing individual elements to be accessed on-the-fly without needing to build full posterior covariance matrices. Moreover, it allows for fast sequential inclusion of new observations. These features are crucial when considering sequential experimental design tasks. We demonstrate our approach by computing sequential data collection plans for excursion set recovery for a gravimetric inverse problem, where the goal is to provide fine resolution estimates of high density regions inside the Stromboli volcano, Italy. Sequential data collection plans are computed by extending the weighted integrated variance reduction (wIVR) criterion to inverse problems. Our results show that this criterion is able to significantly reduce the uncertainty on the excursion volume, reaching close to minimal levels of residual uncertainty. Overall, our techniques allow the advantages of probabilistic models to be brought to bear on large-scale inverse problems arising in the natural sciences.

</details>

<details>

<summary>2022-05-17 07:23:45 - BayesMix: Bayesian Mixture Models in C++</summary>

- *Mario Beraha, Bruno Guindani, Matteo Gianella, Alessandra Guglielmi*

- `2205.08144v1` - [abs](http://arxiv.org/abs/2205.08144v1) - [pdf](http://arxiv.org/pdf/2205.08144v1)

> We describe BayesMix, a C++ library for MCMC posterior simulation for general Bayesian mixture models. The goal of BayesMix is to provide a self-contained ecosystem to perform inference for mixture models to computer scientists, statisticians and practitioners. The key idea of this library is extensibility, as we wish the users to easily adapt our software to their specific Bayesian mixture models. In addition to the several models and MCMC algorithms for posterior inference included in the library, new users with little familiarity on mixture models and the related MCMC algorithms can extend our library with minimal coding effort. Our library is computationally very efficient when compared to competitor software. Examples show that the typical code runtimes are from two to 25 times faster than competitors for data dimension from one to ten. Our library is publicly available on Github at https://github.com/bayesmix-dev/bayesmix/.

</details>

<details>

<summary>2022-05-17 08:28:52 - High-Dimensional Sparse Multivariate Stochastic Volatility Models</summary>

- *Benjamin Poignard, Manabu Asai*

- `2201.08584v2` - [abs](http://arxiv.org/abs/2201.08584v2) - [pdf](http://arxiv.org/pdf/2201.08584v2)

> Although multivariate stochastic volatility models usually produce more accurate forecasts compared to the MGARCH models, their estimation techniques such as Bayesian MCMC typically suffer from the curse of dimensionality. We propose a fast and efficient estimation approach for MSV based on a penalized OLS framework. Specifying the MSV model as a multivariate state space model, we carry out a two-step penalized procedure. We provide the asymptotic properties of the two-step estimator and the oracle property of the first-step estimator when the number of parameters diverges. The performances of our method are illustrated through simulations and financial data.

</details>

<details>

<summary>2022-05-17 08:53:53 - Estimation of the covariance structure from SNP allele frequencies</summary>

- *Jan van Waaij, Zilong Li, Carsten Wiuf*

- `2201.09098v2` - [abs](http://arxiv.org/abs/2201.09098v2) - [pdf](http://arxiv.org/pdf/2201.09098v2)

> We propose two new statistics, V and S, to disentangle the population history of related populations from SNP frequency data. If the populations are related by a tree, we show by theoretical means as well as by simulation that the new statistics are able to identify the root of a tree correctly, in contrast to standard statistics, such as the observed matrix of F2-statistics (distances between pairs of populations). The statistic V is obtained by averaging over all SNPs (similar to standard statistics). Its expectation is the true covariance matrix of the observed population SNP frequencies, offset by a matrix with identical entries. In contrast, the statistic S is put in a Bayesian context and is obtained by averaging over pairs of SNPs, such that each SNP is only used once. It thus makes use of the joint distribution of pairs of SNPs.   In addition, we provide a number of novel mathematical results about old and new statistics, and their mutual relationship.

</details>

<details>

<summary>2022-05-17 08:56:34 - Active learning of causal probability trees</summary>

- *Tue Herlau*

- `2205.08178v1` - [abs](http://arxiv.org/abs/2205.08178v1) - [pdf](http://arxiv.org/pdf/2205.08178v1)

> The past two decades have seen a growing interest in combining causal information, commonly represented using causal graphs, with machine learning models. Probability trees provide a simple yet powerful alternative representation of causal information. They enable both computation of intervention and counterfactuals, and are strictly more general, since they allow context-dependent causal dependencies. Here we present a Bayesian method for learning probability trees from a combination of interventional and observational data. The method quantifies the expected information gain from an intervention, and selects the interventions with the largest gain. We demonstrate the efficiency of the method on simulated and real data. An effective method for learning probability trees on a limited interventional budget will greatly expand their applicability.

</details>

<details>

<summary>2022-05-17 09:48:54 - Bayesian Dropout</summary>

- *Tue Herlau, Morten Mrup, Mikkel N. Schmidt*

- `1508.02905v2` - [abs](http://arxiv.org/abs/1508.02905v2) - [pdf](http://arxiv.org/pdf/1508.02905v2)

> Dropout has recently emerged as a powerful and simple method for training neural networks preventing co-adaptation by stochastically omitting neurons. Dropout is currently not grounded in explicit modelling assumptions which so far has precluded its adoption in Bayesian modelling. Using Bayesian entropic reasoning we show that dropout can be interpreted as optimal inference under constraints. We demonstrate this on an analytically tractable regression model providing a Bayesian interpretation of its mechanism for regularizing and preventing co-adaptation as well as its connection to other Bayesian techniques. We also discuss two general approximate techniques for applying Bayesian dropout for general models, one based on an analytical approximation and the other on stochastic variational techniques. These techniques are then applied to a Baysian logistic regression problem and are shown to improve performance as the model become more misspecified. Our framework roots dropout as a theoretically justified and practical tool for statistical modelling allowing Bayesians to tap into the benefits of dropout training.

</details>

<details>

<summary>2022-05-17 10:39:19 - Bayesian inference in Epidemics: linear noise analysis</summary>

- *Samuel Bronstein, Stefan Engblom, Robin Marin*

- `2203.10906v2` - [abs](http://arxiv.org/abs/2203.10906v2) - [pdf](http://arxiv.org/pdf/2203.10906v2)

> This paper offers a qualitative insight into the convergence of Bayesian parameter inference in a setup which mimics the modeling of the spread of a disease with associated disease measurements. Specifically, we are interested in the Bayesian model's convergence with increasing amounts of data under measurement limitations. Depending on how weakly informative the disease measurements are, we offer a kind of `best case' as well as a `worst case' analysis where, in the former case, we assume that the prevalence is directly accessible, while in the latter that only a binary signal corresponding to a prevalence detection threshold is available. Both cases are studied under an assumed so-called linear noise approximation as to the true dynamics. Numerical experiments test the sharpness of our results when confronted with more realistic situations for which analytical results are unavailable.

</details>

<details>

<summary>2022-05-17 11:40:46 - Bayesian Inference for Non-Parametric Extreme Value Theory</summary>

- *Tobias Kallehauge*

- `2205.08245v1` - [abs](http://arxiv.org/abs/2205.08245v1) - [pdf](http://arxiv.org/pdf/2205.08245v1)

> Statistical inference for extreme values of random events is difficult in practice due to low sample sizes and inaccurate models for the studied rare events. If prior knowledge for extreme values is available, Bayesian statistics can be applied to reduce the sample complexity, but this requires a known probability distribution. By working with the quantiles for extremely low probabilities (in the order of $10^{-2}$ or lower) and relying on their asymptotic normality, inference can be carried out without assuming any distributions. Despite relying on asymptotic results, it is shown that a Bayesian framework that incorporates prior information can reduce the number of observations required to estimate a particular quantile to some level of accuracy.

</details>

<details>

<summary>2022-05-17 12:53:57 - Explaining Recruitment to Extremism: A Bayesian Case-Control Approach</summary>

- *Roberto Cerina, Christopher Barrie, Neil Ketchley, Aaron Zelin*

- `2106.01814v3` - [abs](http://arxiv.org/abs/2106.01814v3) - [pdf](http://arxiv.org/pdf/2106.01814v3)

> Who joins extremist movements? Answering this question poses considerable methodological challenges. Survey techniques are infeasible and selective samples provide no counterfactual. Recruits can be assigned to contextual units, but this is vulnerable to problems of ecological inference. In this article, we take inspiration from epidemiology and elaborate a technique that combines survey and ecological approaches. The multilevel Bayesian case-control design that we propose allows us to identify individual-level and contextual factors patterning the incidence of recruitment, while accounting for rare events, contamination, and spatial autocorrelation. We validate our approach by matching a sample of Islamic State (ISIS) fighters from nine MENA countries with representative population surveys enumerated shortly before recruits joined the movement. High status individuals in their early twenties with university education were more likely to join ISIS. There is more mixed evidence for relative deprivation. We provide software for applied researchers to implement our method.

</details>

<details>

<summary>2022-05-17 13:43:17 - Stochastic orders and measures of skewness and dispersion based on expectiles</summary>

- *Andreas Eberl, Bernhard Klar*

- `2108.06138v4` - [abs](http://arxiv.org/abs/2108.06138v4) - [pdf](http://arxiv.org/pdf/2108.06138v4)

> Recently, expectile-based measures of skewness akin to well-known quantile-based skewness measures have been introduced, and it has been shown that these measures possess quite promising properties (Eberl and Klar, 2021, 2020). However, it remained unanswered whether they preserve the convex transformation order of van Zwet, which is sometimes seen as a basic requirement for a measure of skewness. It is one of the aims of the present work to answer this question in the affirmative. These measures of skewness are scaled using interexpectile distances. We introduce orders of variability based on these quantities and show that the so-called weak expectile dispersive order is equivalent to the dilation order. Further, we analyze the statistical properties of empirical interexpectile ranges in some detail.

</details>

<details>

<summary>2022-05-17 14:35:23 - Variance partitioning in spatio-temporal disease mapping models</summary>

- *Maria Franco-Villoria, Massimo Ventrucci, Hvard Rue*

- `2109.13374v2` - [abs](http://arxiv.org/abs/2109.13374v2) - [pdf](http://arxiv.org/pdf/2109.13374v2)

> Bayesian disease mapping, yet if undeniably useful to describe variation in risk over time and space, comes with the hurdle of prior elicitation on hard-to-interpret random effect precision parameters. We introduce a reparametrized version of the popular spatio-temporal interaction models, based on Kronecker product intrinsic Gaussian Markov Random Fields, that we name the variance partitioning (VP) model. The VP model includes a mixing parameter that balances the contribution of the main and interaction effects to the total (generalized) variance and enhances interpretability. The use of a penalized complexity prior on the mixing parameter aids in coding prior information in a intuitive way. We illustrate the advantages of the VP model using two case studies.

</details>

<details>

<summary>2022-05-17 19:26:43 - Bayesian Discrete Conditional Transformation Models</summary>

- *Manuel Carlan, Thomas Kneib*

- `2205.08594v1` - [abs](http://arxiv.org/abs/2205.08594v1) - [pdf](http://arxiv.org/pdf/2205.08594v1)

> We propose a novel Bayesian model framework for discrete ordinal and count data based on conditional transformations of the responses. The conditional transformation function is estimated from the data in conjunction with an a priori chosen reference distribution. For count responses, the resulting transformation model is novel in the sense that it is a Bayesian fully parametric yet distribution-free approach that can additionally account for excess zeros with additive transformation function specifications. For ordinal categoric responses, our cumulative link transformation model allows the inclusion of linear and nonlinear covariate effects that can additionally be made category-specific, resulting in (non-)proportional odds or hazards models and more, depending on the choice of the reference distribution. Inference is conducted by a generic modular Markov chain Monte Carlo algorithm where multivariate Gaussian priors enforce specific properties such as smoothness on the functional effects. To illustrate the versatility of Bayesian discrete conditional transformation models, applications to counts of patent citations in the presence of excess zeros and on treating forest health categories in a discrete partial proportional odds model are presented.

</details>

<details>

<summary>2022-05-17 22:39:20 - Individualized conditional independence testing under model-X with heterogeneous samples and interactions</summary>

- *Matteo Sesia, Tianshu Sun*

- `2205.08653v1` - [abs](http://arxiv.org/abs/2205.08653v1) - [pdf](http://arxiv.org/pdf/2205.08653v1)

> Model-X knockoffs and the conditional randomization test are methods that search for conditional associations in large data sets, controlling the type-I errors if the joint distribution of the predictors is known. However, they cannot test for interactions nor find whether an association is only significant within a latent subset of a heterogeneous population. We address this limitation by developing an extension of the knockoff filter that tests conditional associations within automatically detected subsets of individuals, provably controlling the false discovery rate for the selected hypotheses. Then, under the additional assumption of a partially linear model with a binary predictor, we extend the conditional randomization test as to make inferences about quantiles of individual effects that are robust to sample heterogeneity and interactions. The performances of these methods are investigated through simulations and with the analysis of data from a randomized blood donation experiment with several treatments.

</details>

<details>

<summary>2022-05-18 01:23:05 - Dependent Latent Class Models</summary>

- *Jesse Bowers, Steve Culpepper*

- `2205.08677v1` - [abs](http://arxiv.org/abs/2205.08677v1) - [pdf](http://arxiv.org/pdf/2205.08677v1)

> Latent Class Models (LCMs) are used to cluster multivariate categorical data (e.g. group participants based on survey responses). Traditional LCMs assume a property called conditional independence. This assumption can be restrictive, leading to model misspecification and overparameterization. To combat this problem, we developed a novel Bayesian model called a Dependent Latent Class Model (DLCM), which permits conditional dependence. We verify identifiability of DLCMs. We also demonstrate the effectiveness of DLCMs in both simulations and real-world applications. Compared to traditional LCMs, DLCMs are effective in applications with time series, overlapping items, and structural zeroes.

</details>

<details>

<summary>2022-05-18 02:55:18 - Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning</summary>

- *Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai*

- `2205.08698v1` - [abs](http://arxiv.org/abs/2205.08698v1) - [pdf](http://arxiv.org/pdf/2205.08698v1)

> Prediction intervals offer an effective tool for quantifying the uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to unforeseen changes in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles. It relies on the online learning ability of reinforcement learning to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve the quality of PIs. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.

</details>

<details>

<summary>2022-05-18 05:50:30 - Lower-bounds on the Bayesian Risk in Estimation Procedures via $f$-Divergences</summary>

- *Adrien Vandenbroucque, Amedeo Roberto Esposito, Michael Gastpar*

- `2202.02557v3` - [abs](http://arxiv.org/abs/2202.02557v3) - [pdf](http://arxiv.org/pdf/2202.02557v3)

> We consider the problem of parameter estimation in a Bayesian setting and propose a general lower-bound that includes part of the family of $f$-Divergences. The results are then applied to specific settings of interest and compared to other notable results in the literature. In particular, we show that the known bounds using Mutual Information can be improved by using, for example, Maximal Leakage, Hellinger divergence, or generalizations of the Hockey-Stick divergence.

</details>

<details>

<summary>2022-05-18 07:24:49 - Marginal and Joint Cross-Entropies & Predictives for Online Bayesian Inference, Active Learning, and Active Sampling</summary>

- *Andreas Kirsch, Jannik Kossen, Yarin Gal*

- `2205.08766v1` - [abs](http://arxiv.org/abs/2205.08766v1) - [pdf](http://arxiv.org/pdf/2205.08766v1)

> Principled Bayesian deep learning (BDL) does not live up to its potential when we only focus on marginal predictive distributions (marginal predictives). Recent works have highlighted the importance of joint predictives for (Bayesian) sequential decision making from a theoretical and synthetic perspective. We provide additional practical arguments grounded in real-world applications for focusing on joint predictives: we discuss online Bayesian inference, which would allow us to make predictions while taking into account additional data without retraining, and we propose new challenging evaluation settings using active learning and active sampling. These settings are motivated by an examination of marginal and joint predictives, their respective cross-entropies, and their place in offline and online learning. They are more realistic than previously suggested ones, building on work by Wen et al. (2021) and Osband et al. (2022), and focus on evaluating the performance of approximate BNNs in an online supervised setting. Initial experiments, however, raise questions on the feasibility of these ideas in high-dimensional parameter spaces with current BDL inference techniques, and we suggest experiments that might help shed further light on the practicality of current research for these problems. Importantly, our work highlights previously unidentified gaps in current research and the need for better approximate joint predictives.

</details>

<details>

<summary>2022-05-18 08:01:33 - Probability trees and the value of a single intervention</summary>

- *Tue Herlau*

- `2205.08779v1` - [abs](http://arxiv.org/abs/2205.08779v1) - [pdf](http://arxiv.org/pdf/2205.08779v1)

> The most fundamental problem in statistical causality is determining causal relationships from limited data. Probability trees, which combine prior causal structures with Bayesian updates, have been suggested as a possible solution. In this work, we quantify the information gain from a single intervention and show that both the anticipated information gain, prior to making an intervention, and the expected gain from an intervention have simple expressions. This results in an active-learning method that simply selects the intervention with the highest anticipated gain, which we illustrate through several examples. Our work demonstrates how probability trees, and Bayesian estimation of their parameters, offer a simple yet viable approach to fast causal induction.

</details>

<details>

<summary>2022-05-18 10:07:21 - Fair and Green Hyperparameter Optimization via Multi-objective and Multiple Information Source Bayesian Optimization</summary>

- *Antonio Candelieri, Andrea Ponti, Francesco Archetti*

- `2205.08835v1` - [abs](http://arxiv.org/abs/2205.08835v1) - [pdf](http://arxiv.org/pdf/2205.08835v1)

> There is a consensus that focusing only on accuracy in searching for optimal machine learning models amplifies biases contained in the data, leading to unfair predictions and decision supports. Recently, multi-objective hyperparameter optimization has been proposed to search for machine learning models which offer equally Pareto-efficient trade-offs between accuracy and fairness. Although these approaches proved to be more versatile than fairness-aware machine learning algorithms -- which optimize accuracy constrained to some threshold on fairness -- they could drastically increase the energy consumption in the case of large datasets. In this paper we propose FanG-HPO, a Fair and Green Hyperparameter Optimization (HPO) approach based on both multi-objective and multiple information source Bayesian optimization. FanG-HPO uses subsets of the large dataset (aka information sources) to obtain cheap approximations of both accuracy and fairness, and multi-objective Bayesian Optimization to efficiently identify Pareto-efficient machine learning models. Experiments consider two benchmark (fairness) datasets and two machine learning algorithms (XGBoost and Multi-Layer Perceptron), and provide an assessment of FanG-HPO against both fairness-aware machine learning algorithms and hyperparameter optimization via a multi-objective single-source optimization algorithm in BoTorch, a state-of-the-art platform for Bayesian Optimization.

</details>

<details>

<summary>2022-05-18 10:43:25 - Bayesian Inference with Nonlinear Generative Models: Comments on Secure Learning</summary>

- *Ali Bereyhi, Bruno Loureiro, Florent Krzakala, Ralf R. Mller, Hermann Schulz-Baldes*

- `2201.09986v2` - [abs](http://arxiv.org/abs/2201.09986v2) - [pdf](http://arxiv.org/pdf/2201.09986v2)

> Unlike the classical linear model, nonlinear generative models have been addressed sparsely in the literature. This work aims to bring attention to these models and their secrecy potential. To this end, we invoke the replica method to derive the asymptotic normalized cross entropy in an inverse probability problem whose generative model is described by a Gaussian random field with a generic covariance function. Our derivations further demonstrate the asymptotic statistical decoupling of Bayesian inference algorithms and specify the decoupled setting for a given nonlinear model.   The replica solution depicts that strictly nonlinear models establish an all-or-nothing phase transition: There exists a critical load at which the optimal Bayesian inference changes from being perfect to an uncorrelated learning. This finding leads to design of a new secure coding scheme which achieves the secrecy capacity of the wiretap channel. This interesting result implies that strictly nonlinear generative models are perfectly secured without any secure coding. We justify this latter statement through the analysis of an illustrative model for perfectly secure and reliable inference.

</details>

<details>

<summary>2022-05-18 16:06:03 - SoQal: Selective Oracle Questioning for Consistency Based Active Learning of Cardiac Signals</summary>

- *Dani Kiyasseh, Tingting Zhu, David A. Clifton*

- `2004.09557v3` - [abs](http://arxiv.org/abs/2004.09557v3) - [pdf](http://arxiv.org/pdf/2004.09557v3)

> Clinical settings are often characterized by abundant unlabelled data and limited labelled data. This is typically driven by the high burden placed on oracles (e.g., physicians) to provide annotations. One way to mitigate this burden is via active learning (AL) which involves the (a) acquisition and (b) annotation of informative unlabelled instances. Whereas previous work addresses either one of these elements independently, we propose an AL framework that addresses both. For acquisition, we propose Bayesian Active Learning by Consistency (BALC), a sub-framework which perturbs both instances and network parameters and quantifies changes in the network output probability distribution. For annotation, we propose SoQal, a sub-framework that dynamically determines whether, for each acquired unlabelled instance, to request a label from an oracle or to pseudo-label it instead. We show that BALC can outperform start-of-the-art acquisition functions such as BALD, and SoQal outperforms baseline methods even in the presence of a noisy oracle.

</details>

<details>

<summary>2022-05-18 16:41:37 - An importance sampling approach for reliable and efficient inference in Bayesian ordinary differential equation models</summary>

- *Juho Timonen, Nikolas Siccha, Ben Bales, Harri Lhdesmki, Aki Vehtari*

- `2205.09059v1` - [abs](http://arxiv.org/abs/2205.09059v1) - [pdf](http://arxiv.org/pdf/2205.09059v1)

> Statistical models can involve implicitly defined quantities, such as solutions to nonlinear ordinary differential equations (ODEs), that unavoidably need to be numerically approximated in order to evaluate the model. The approximation error inherently biases statistical inference results, but the amount of this bias is generally unknown and often ignored in Bayesian parameter inference. We propose a computationally efficient method for verifying the reliability of posterior inference for such models, when the inference is performed using Markov chain Monte Carlo methods. We validate the efficiency and reliability of our workflow in experiments using simulated and real data, and different ODE solvers. We highlight problems that arise with commonly used adaptive ODE solvers, and propose robust and effective alternatives which, accompanied by our workflow, can now be taken into use without losing reliability of the inferences.

</details>

<details>

<summary>2022-05-18 20:15:25 - Hybrid Machine Learning Modeling of Engineering Systems -- A Probabilistic Perspective Tested on a Multiphase Flow Modeling Case Study</summary>

- *Timur Bikmukhametov, Johannes Jschke*

- `2205.09196v1` - [abs](http://arxiv.org/abs/2205.09196v1) - [pdf](http://arxiv.org/pdf/2205.09196v1)

> To operate process engineering systems in a safe and reliable manner, predictive models are often used in decision making. In many cases, these are mechanistic first principles models which aim to accurately describe the process. In practice, the parameters of these models need to be tuned to the process conditions at hand. If the conditions change, which is common in practice, the model becomes inaccurate and needs to be re-tuned. In this paper, we propose a hybrid modeling machine learning framework that allows tuning first principles models to process conditions using two different types of Bayesian Neural Networks. Our approach not only estimates the expected values of the first principles model parameters but also quantifies the uncertainty of these estimates. Such an approach of hybrid machine learning modeling is not yet well described in the literature, so we believe this paper will provide an additional angle at which hybrid machine learning modeling of physical systems can be considered. As an example, we choose a multiphase pipe flow process for which we constructed a three-phase steady state model based on the drift-flux approach which can be used for modeling of pipe and well flow behavior in oil and gas production systems with or without the neural network tuning. In the simulation results, we show how uncertainty estimates of the resulting hybrid models can be used to make better operation decisions.

</details>

<details>

<summary>2022-05-18 21:00:19 - Power Transformations of Relative Count Data as a Shrinkage Problem</summary>

- *Ionas Erb*

- `2205.09215v1` - [abs](http://arxiv.org/abs/2205.09215v1) - [pdf](http://arxiv.org/pdf/2205.09215v1)

> Here we show an application of our recently proposed information-geometric approach to compositional data analysis (CoDA). This application regards relative count data, which are, e.g., obtained from sequencing experiments. First we review in some detail a variety of necessary concepts ranging from basic count distributions and their information-geometric description over the link between Bayesian statistics and shrinkage to the use of power transformations in CoDA. We then show that powering, i.e., the equivalent to scalar multiplication on the simplex, can be understood as a shrinkage problem on the tangent space of the simplex. In information-geometric terms, traditional shrinkage corresponds to an optimization along a mixture (or m-) geodesic, while powering (or 'exponential' shrinkage) can be optimized along an exponential (or e-) geodesic. While the m-geodesic corresponds to the posterior mean of the multinomial counts using a conjugate prior, the e-geodesic corresponds to an alternative parametrization of the posterior where prior and data contributions are weighted by geometric rather than arithmetic means. To optimize the exponential shrinkage parameter, we use mean-squared error as a cost function on the tangent space. This is just the expected squared Aitchison distance from the true parameter. We derive an analytic solution for its minimum based on the delta method and test it via simulations. We also discuss exponential shrinkage as an alternative to zero imputation for dimension reduction and data normalization.

</details>

<details>

<summary>2022-05-18 22:27:15 - Use of copula to model within-study association in bivariate meta-analysis of binomial data at the aggregate level a Bayesian approach and application to surrogate endpoint evaluation</summary>

- *Tasos Papanikos, John R Thompson, Keith R Abrams, Sylwia Bujkiewicz*

- `2004.02007v2` - [abs](http://arxiv.org/abs/2004.02007v2) - [pdf](http://arxiv.org/pdf/2004.02007v2)

> Bivariate meta-analysis provides a useful framework for combining information across related studies and has been utilised to combine evidence from clinical studies to evaluate treatment efficacy on two outcomes. It has also been used to investigate surrogacy patterns between treatment effects on the surrogate endpoint and the final outcome. Surrogate endpoints play an important role in drug development when they can be used to measure treatment effect early compared to the final outcome and to predict clinical benefit or harm. The standard bivariate meta-analytic approach models the observed treatment effects on the surrogate and the final outcome outcomes jointly, at both the within-study and between-studies levels, using a bivariate normal distribution. For binomial data, a normal approximation on log odds ratio scale can be used. However, this method may lead to biased results when the proportions of events are close to one or zero, affecting the validation of surrogate endpoints. In this paper, we explore modelling the two outcomes on the original binomial scale. Firstly, we present a method that uses independent binomial likelihoods to model the within-study variability avoiding to approximate the observed treatment effects. However, the method ignores the within-study association. To overcome this issue, we propose a method using a bivariate copula with binomial marginals, which allows the model to account for the within-study association. We applied the methods to an illustrative example in chronic myeloid leukemia to investigate the surrogate relationship between complete cytogenetic response (CCyR) and event-free-survival (EFS).

</details>

<details>

<summary>2022-05-19 05:12:56 - Hierarchical Ensemble Kalman Methods with Sparsity-Promoting Generalized Gamma Hyperpriors</summary>

- *Hwanwoo Kim, Daniel Sanz-Alonso, Alexander Strang*

- `2205.09322v1` - [abs](http://arxiv.org/abs/2205.09322v1) - [pdf](http://arxiv.org/pdf/2205.09322v1)

> This paper introduces a computational framework to incorporate flexible regularization techniques in ensemble Kalman methods for nonlinear inverse problems. The proposed methodology approximates the maximum a posteriori (MAP) estimate of a hierarchical Bayesian model characterized by a conditionally Gaussian prior and generalized gamma hyperpriors. Suitable choices of hyperparameters yield sparsity-promoting regularization. We propose an iterative algorithm for MAP estimation, which alternates between updating the unknown with an ensemble Kalman method and updating the hyperparameters in the regularization to promote sparsity. The effectiveness of our methodology is demonstrated in several computed examples, including compressed sensing and subsurface flow inverse problems.

</details>

<details>

<summary>2022-05-19 10:00:12 - Enhanced monitoring of atmospheric methane from space over the Permian basin with hierarchical Bayesian inference</summary>

- *Clayton Roberts, Oliver Shorttle, Kaisey Mandel, Matthew Jones, Rutger Ijzermans, Bill Hirst, Philip Jonathan*

- `2111.12486v5` - [abs](http://arxiv.org/abs/2111.12486v5) - [pdf](http://arxiv.org/pdf/2111.12486v5)

> Methane is a strong greenhouse gas, with a higher radiative forcing per unit mass and shorter atmospheric lifetime than carbon dioxide. The remote sensing of methane in regions of industrial activity is a key step toward the accurate monitoring of emissions that drive climate change. Whilst the TROPOspheric Monitoring Instrument (TROPOMI) on board the Sentinal-5P satellite is capable of providing daily global measurement of methane columns, data are often compromised by cloud cover. Here, we develop a statistical model which uses nitrogen dioxide concentration data from TROPOMI to efficiently predict values of methane columns, expanding the average daily spatial coverage of observations of the Permian basin from 16% to 88% in the year 2019. The addition of predicted methane abundances at locations where direct observations are not available will support inversion methods for estimating methane emission rates at shorter timescales than is currently possible.

</details>

<details>

<summary>2022-05-19 12:29:09 - Variational Inference for Bayesian Bridge Regression</summary>

- *Carlos Tadeu Pagani Zanini, Helio dos Santos Migon, Ronaldo Dias*

- `2205.09515v1` - [abs](http://arxiv.org/abs/2205.09515v1) - [pdf](http://arxiv.org/pdf/2205.09515v1)

> We study the implementation of Automatic Differentiation Variational inference (ADVI) for Bayesian inference on regression models with bridge penalization. The bridge approach uses $\ell_{\alpha}$ norm, with $\alpha \in (0, +\infty)$ to define a penalization on large values of the regression coefficients, which includes the Lasso ($\alpha = 1$) and ridge $(\alpha = 2)$ penalizations as special cases. Full Bayesian inference seamlessly provides joint uncertainty estimates for all model parameters. Although MCMC aproaches are available for bridge regression, it can be slow for large dataset, specially in high dimensions. The ADVI implementation allows the use of small batches of data at each iteration (due to stochastic gradient based algorithms), therefore speeding up computational time in comparison with MCMC. We illustrate the approach on non-parametric regression models with B-splines, although the method works seamlessly for other choices of basis functions. A simulation study shows the main properties of the proposed method.

</details>

<details>

<summary>2022-05-19 16:20:44 - Bayesian Network Structure Learning using Digital Annealer</summary>

- *Yuta Shikuri*

- `2006.06926v3` - [abs](http://arxiv.org/abs/2006.06926v3) - [pdf](http://arxiv.org/pdf/2006.06926v3)

> Annealing processors, which solve a quadratic unconstrained binary optimization (QUBO), are a potential breakthrough in improving the accuracy of score-based Bayesian network structure learning. However, currently, the bit capacity of an annealing processor is very limited. To utilize the power of annealing processors, it is necessary to encode score-based learning problems into QUBO within the upper bound of bits. In this paper, we propose a novel approach with the decomposition of candidate parent sets. Experimental results on benchmark networks with $37$ to $223$ variables show that our approach requires lesser bits than the bit capacity of the fourth-generation Fujitsu Digital Annealer, a fully coupled annealing processor developed with semiconductor technology. Moreover, we demonstrate that the Digital Annealer with our conversion method outperforms existing algorithms on some benchmark networks. It is expected that our approach promotes the utility of annealing processors in learning the Bayesian network.

</details>

<details>

<summary>2022-05-19 22:01:26 - Prediction for Distributional Outcomes in High-Performance Computing I/O Variability</summary>

- *Li Xu, Yili Hong, Max D. Morris, Kirk W. Cameron*

- `2205.09879v1` - [abs](http://arxiv.org/abs/2205.09879v1) - [pdf](http://arxiv.org/pdf/2205.09879v1)

> Although high-performance computing (HPC) systems have been scaled to meet the exponentially-growing demand for scientific computing, HPC performance variability remains a major challenge and has become a critical research topic in computer science. Statistically, performance variability can be characterized by a distribution. Predicting performance variability is a critical step in HPC performance variability management and is nontrivial because one needs to predict a distribution function based on system factors. In this paper, we propose a new framework to predict performance distributions. The proposed model is a modified Gaussian process that can predict the distribution function of the input/output (I/O) throughput under a specific HPC system configuration. We also impose a monotonic constraint so that the predicted function is nondecreasing, which is a property of the cumulative distribution function. Additionally, the proposed model can incorporate both quantitative and qualitative input variables. We evaluate the performance of the proposed method by using the IOzone variability data based on various prediction tasks. Results show that the proposed method can generate accurate predictions, and outperform existing methods. We also show how the predicted functional output can be used to generate predictions for a scalar summary of the performance distribution, such as the mean, standard deviation, and quantiles. Our methods can be further used as a surrogate model for HPC system variability monitoring and optimization.

</details>

<details>

<summary>2022-05-20 00:29:20 - Joint modeling of landslide counts and sizes using spatial marked point processes with sub-asymptotic mark distributions</summary>

- *Rishikesh Yadav, Raphal Huser, Thomas Opitz, Luigi Lombardo*

- `2205.09908v1` - [abs](http://arxiv.org/abs/2205.09908v1) - [pdf](http://arxiv.org/pdf/2205.09908v1)

> To accurately quantify landslide hazard in a region of Turkey, we develop new marked point process models within a Bayesian hierarchical framework for the joint prediction of landslide counts and sizes. To accommodate for the dominant role of the few largest landslides in aggregated sizes, we leverage mark distributions with strong justification from extreme-value theory, thus bridging the two broad areas of statistics of extremes and marked point patterns. At the data level, we assume a Poisson distribution for landslide counts, while we compare different "sub-asymptotic" distributions for landslide sizes to flexibly model their upper and lower tails. At the latent level, Poisson intensities and the median of the size distribution vary spatially in terms of fixed and random effects, with shared spatial components capturing cross-correlation between landslide counts and sizes. We robustly model spatial dependence using intrinsic conditional autoregressive priors. Our novel models are fitted efficiently using a customized adaptive Markov chain Monte Carlo algorithm. We show that, for our dataset, sub-asymptotic mark distributions provide improved predictions of large landslide sizes compared to more traditional choices. To showcase the benefits of joint occurrence-size models and illustrate their usefulness for risk assessment, we map landslide hazard along major roads.

</details>

<details>

<summary>2022-05-20 01:07:41 - Robust Expected Information Gain for Optimal Bayesian Experimental Design Using Ambiguity Sets</summary>

- *Jinwoo Go, Tobin Isaac*

- `2205.09914v1` - [abs](http://arxiv.org/abs/2205.09914v1) - [pdf](http://arxiv.org/pdf/2205.09914v1)

> The ranking of experiments by expected information gain (EIG) in Bayesian experimental design is sensitive to changes in the model's prior distribution, and the approximation of EIG yielded by sampling will have errors similar to the use of a perturbed prior. We define and analyze \emph{robust expected information gain} (REIG), a modification of the objective in EIG maximization by minimizing an affine relaxation of EIG over an ambiguity set of distributions that are close to the original prior in KL-divergence. We show that, when combined with a sampling-based approach to estimating EIG, REIG corresponds to a `log-sum-exp' stabilization of the samples used to estimate EIG, meaning that it can be efficiently implemented in practice. Numerical tests combining REIG with variational nested Monte Carlo (VNMC), adaptive contrastive estimation (ACE) and mutual information neural estimation (MINE) suggest that in practice REIG also compensates for the variability of under-sampled estimators.

</details>

<details>

<summary>2022-05-20 01:17:03 - Multidimensional heterogeneity learning for count value tensor data with applications to field goal attempt analysis of NBA players</summary>

- *Guanyu Hu, Yishu Xue, Weining Shen*

- `2205.09918v1` - [abs](http://arxiv.org/abs/2205.09918v1) - [pdf](http://arxiv.org/pdf/2205.09918v1)

> We propose a multidimensional tensor clustering approach for studying how professional basketball players' shooting patterns vary over court locations and game time. Unlike most existing methods that only study continuous-valued tensors or have to assume the same cluster structure along different tensor directions, we propose a Bayesian nonparametric model that deals with count-valued tensors and projects the heterogeneity among players onto tensor dimensions while allowing cluster structures to be different over directions. Our method is fully probabilistic; hence allows simultaneous inference on both the number of clusters and the cluster configurations. We present an efficient posterior sampling method and establish the large-sample convergence properties for the posterior distribution. Simulation studies have demonstrated an excellent empirical performance of the proposed method. Finally, an application to shot chart data collected from 191 NBA players during the 2017-2018 regular season is conducted and reveals several interesting insights for basketball analytics.

</details>

<details>

<summary>2022-05-20 05:25:40 - A General Framework for quantifying Aleatoric and Epistemic uncertainty in Graph Neural Networks</summary>

- *Sai Munikoti, Deepesh Agarwal, Laya Das, Balasubramaniam Natarajan*

- `2205.09968v1` - [abs](http://arxiv.org/abs/2205.09968v1) - [pdf](http://arxiv.org/pdf/2205.09968v1)

> Graph Neural Networks (GNN) provide a powerful framework that elegantly integrates Graph theory with Machine learning for modeling and analysis of networked data. We consider the problem of quantifying the uncertainty in predictions of GNN stemming from modeling errors and measurement uncertainty. We consider aleatoric uncertainty in the form of probabilistic links and noise in feature vector of nodes, while epistemic uncertainty is incorporated via a probability distribution over the model parameters. We propose a unified approach to treat both sources of uncertainty in a Bayesian framework, where Assumed Density Filtering is used to quantify aleatoric uncertainty and Monte Carlo dropout captures uncertainty in model parameters. Finally, the two sources of uncertainty are aggregated to estimate the total uncertainty in predictions of a GNN. Results in the real-world datasets demonstrate that the Bayesian model performs at par with a frequentist model and provides additional information about predictions uncertainty that are sensitive to uncertainties in the data and model.

</details>

<details>

<summary>2022-05-20 09:24:39 - Posterior Refinement Improves Sample Efficiency in Bayesian Neural Networks</summary>

- *Agustinus Kristiadi, Runa Eschenhagen, Philipp Hennig*

- `2205.10041v1` - [abs](http://arxiv.org/abs/2205.10041v1) - [pdf](http://arxiv.org/pdf/2205.10041v1)

> Monte Carlo (MC) integration is the de facto method for approximating the predictive distribution of Bayesian neural networks (BNNs). But, even with many MC samples, Gaussian-based BNNs could still yield bad predictive performance due to the posterior approximation's error. Meanwhile, alternatives to MC integration tend to be more expensive or biased. In this work, we experimentally show that the key to good MC-approximated predictive distributions is the quality of the approximate posterior itself. However, previous methods for obtaining accurate posterior approximations are expensive and non-trivial to implement. We, therefore, propose to refine Gaussian approximate posteriors with normalizing flows. When applied to last-layer BNNs, it yields a simple \emph{post hoc} method for improving pre-existing parametric approximations. We show that the resulting posterior approximation is competitive with even the gold-standard full-batch Hamiltonian Monte Carlo.

</details>

<details>

<summary>2022-05-20 10:10:32 - The Unreasonable Effectiveness of Deep Evidential Regression</summary>

- *Nis Meinert, Jakob Gawlikowski, Alexander Lavin*

- `2205.10060v1` - [abs](http://arxiv.org/abs/2205.10060v1) - [pdf](http://arxiv.org/pdf/2205.10060v1)

> There is a significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware regression-based neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, notably with the capabilities to disentangle aleatoric and epistemic uncertainties. Despite some empirical success of Deep Evidential Regression (DER), there are important gaps in the mathematical foundation that raise the question of why the proposed technique seemingly works. We detail the theoretical shortcomings and analyze the performance on synthetic and real-world data sets, showing that Deep Evidential Regression is a heuristic rather than an exact uncertainty quantification. We go on to propose corrections and redefinitions of how aleatoric and epistemic uncertainties should be extracted from NNs.

</details>

<details>

<summary>2022-05-20 12:46:13 - Zero-inflated Beta distribution regression modeling</summary>

- *Becky Tang, Henry A Frye, Alan E. Gelfand, John A Silander Jr*

- `2112.07249v2` - [abs](http://arxiv.org/abs/2112.07249v2) - [pdf](http://arxiv.org/pdf/2112.07249v2)

> A frequent challenge encountered with ecological data is how to interpret, analyze, or model data having a high proportion of zeros. Much attention has been given to zero-inflated count data, whereas models for non-negative continuous data with an abundance of 0s are lacking. We consider zero-inflated data on the unit interval and provide modeling to capture two types of 0s in the context of the Beta regression model. We model 0s due to missing by chance through left censoring of a latent regression, and 0s due to unsuitability using an independent Bernoulli specification to create a point mass at 0. We first develop the model as a spatial regression in environmental features and then extend to introduce spatial random effects. We specify models hierarchically, employing latent variables, fit them within a Bayesian framework, and present new model comparison tools. Our motivating dataset consists of percent cover abundance of two plant species at a collection of sites in the Cape Floristic Region of South Africa. We find that environmental features enable learning about the incidence of both types of 0s as well as the positive percent covers. We also show that the spatial random effects model improves predictive performance. The proposed modeling enables ecologists, using environmental regressors, to extract a better understanding of the presence/absence of species in terms of absence due to unsuitability vs. missingness by chance, as well as abundance when present.

</details>

<details>

<summary>2022-05-20 14:01:59 - Adaptive Bayesian Inference of Markov Transition Rates</summary>

- *Nicholas W. Barendregt, Emily G. Webb, Zachary P. Kilpatrick*

- `2205.10371v1` - [abs](http://arxiv.org/abs/2205.10371v1) - [pdf](http://arxiv.org/pdf/2205.10371v1)

> Optimal designs minimize the number of experimental runs (samples) needed to accurately estimate model parameters, resulting in algorithms that, for instance, efficiently minimize parameter estimate variance. Governed by knowledge of past observations, adaptive approaches adjust sampling constraints online as model parameter estimates are refined, continually maximizing expected information gained or variance reduced. We apply adaptive Bayesian inference to estimate transition rates of Markov chains, a common class of models for stochastic processes in nature. Unlike most previous studies, our sequential Bayesian optimal design is updated with each observation, and can be simply extended beyond two-state models to birth-death processes and multistate models. By iteratively finding the best time to obtain each sample, our adaptive algorithm maximally reduces variance, resulting in lower overall error in ground truth parameter estimates across a wide range of Markov chain parameterizations and conformations.

</details>

<details>

<summary>2022-05-20 14:28:35 - Estimation of binary time-frequency masks from ambient noise</summary>

- *Jos Luis Romero, Michael Speckbacher*

- `2205.10205v1` - [abs](http://arxiv.org/abs/2205.10205v1) - [pdf](http://arxiv.org/pdf/2205.10205v1)

> We investigate the retrieval of a binary time-frequency mask from a few observations of filtered white ambient noise. Confirming household wisdom in acoustic modeling, we show that this is possible by inspecting the average spectrogram of ambient noise. Specifically, we show that the lower quantile of the average of $\mathcal{O}(\log(|\Omega|/\varepsilon))$ masked spectrograms is enough to identify a rather general mask $\Omega$ with confidence at least $\varepsilon$, up to shape details concentrated near the boundary of $\Omega$. As an application, the expected measure of the estimation error is dominated by the perimeter of the time-frequency mask. The estimator requires no knowledge of the noise variance, and only a very qualitative profile of the filtering window, but no exact knowledge of it.

</details>

<details>

<summary>2022-05-20 15:40:15 - Triangulation candidates for Bayesian optimization</summary>

- *Robert B. Gramacy, Annie Sauer, Nathan Wycoff*

- `2112.07457v2` - [abs](http://arxiv.org/abs/2112.07457v2) - [pdf](http://arxiv.org/pdf/2112.07457v2)

> Bayesian optimization involves "inner optimization" over a new-data acquisition criterion which is non-convex/highly multi-modal, may be non-differentiable, or may otherwise thwart local numerical optimizers. In such cases it is common to replace continuous search with a discrete one over random candidates. Here we propose using candidates based on a Delaunay triangulation of the existing input design. We detail the construction of these "tricands" and demonstrate empirically how they outperform both numerically optimized acquisitions and random candidate-based alternatives, and are well-suited for hybrid schemes, on benchmark synthetic and real simulation experiments.

</details>

<details>

<summary>2022-05-20 15:55:04 - The Bayesian Context Trees State Space Model: Interpretable mixture models for time series</summary>

- *Ioannis Papageorgiou, Ioannis Kontoyiannis*

- `2106.03023v3` - [abs](http://arxiv.org/abs/2106.03023v3) - [pdf](http://arxiv.org/pdf/2106.03023v3)

> A general hierarchical Bayesian framework is introduced for mixture modelling of real-valued time series, including a collection of effective tools for learning and inference. At the top level, a discrete context (or `state') is extracted for each sample, consisting of a discretised version of some of the most recent observations preceding it. The set of all relevant contexts are represented as a discrete context tree. At the bottom level, a different real-valued time series model is associated with each context (i.e., with each state). This defines a very general framework that can be used in conjunction with any existing model class to build flexible and interpretable mixture models. We introduce algorithms that allow for efficient, exact Bayesian inference; in particular, the maximum a posteriori probability (MAP) model, including the relevant MAP context tree, can be identified exactly. These algorithms can be updated sequentially, facilitating efficient online forecasting. The utility of the general framework is illustrated in detail when autoregressive (AR) models are used at the bottom level, resulting in a nonlinear AR mixture model. Our methods are found to outperform several state-of-the-art techniques on both simulated and real-world data from economics and finance, both in terms of forecasting accuracy and computational requirements.

</details>

<details>

<summary>2022-05-20 16:39:39 - Bayesian Approximations to Hidden Semi-Markov Models for Telemetric Monitoring of Physical Activity</summary>

- *Beniamino Hadj-Amar, Jack Jewson, Mark Fiecas*

- `2006.09061v2` - [abs](http://arxiv.org/abs/2006.09061v2) - [pdf](http://arxiv.org/pdf/2006.09061v2)

> We propose a Bayesian hidden Markov model for analyzing time series and sequential data where a special structure of the transition probability matrix is embedded to model explicit-duration semi-Markovian dynamics. Our formulation allows for the development of highly flexible and interpretable models that can integrate available prior information on state durations while keeping a moderate computational cost to perform efficient posterior inference. We show the benefits of choosing a Bayesian approach for HSMM estimation over its frequentist counterpart, in terms of model selection and out-of-sample forecasting, also highlighting the computational feasibility of our inference procedure whilst incurring negligible statistical error. The use of our methodology is illustrated in an application relevant to e-Health, where we investigate rest-activity rhythms using telemetric activity data collected via a wearable sensing device. This analysis considers for the first time Bayesian model selection for the form of the explicit state dwell distribution. We further investigate the inclusion of a circadian covariate into the emission density and estimate this in a data-driven manner.

</details>

<details>

<summary>2022-05-20 16:57:45 - Scalable Bayesian Approach for the DINA Q-matrix Estimation Combining Stochastic Optimization and Variational Inference</summary>

- *Motonori Oka, Kensuke Okada*

- `2105.09495v3` - [abs](http://arxiv.org/abs/2105.09495v3) - [pdf](http://arxiv.org/pdf/2105.09495v3)

> Diagnostic classification models (DCMs) offer statistical tools to inspect the fined-grained attribute of respondents' strengths and weaknesses. However, the diagnosis accuracy deteriorates when misspecification occurs in the predefined item-attribute relationship, which is encoded into a Q-matrix. To prevent such misspecification, methodologists have recently developed several Bayesian Q-matrix estimation methods for greater estimation flexibility. However, these methods become infeasible in the case of large-scale assessments with a large number of attributes and items. In this study, we focused on the deterministic inputs, noisy "and" gate (DINA) model and proposed a new framework for the Q-matrix estimation to find the Q-matrix with the maximum marginal likelihood. Based on this framework, we developed a scalable estimation algorithm for the DINA Q-matrix by constructing an iteration algorithm that utilizes stochastic optimization and variational inference. The simulation and empirical studies reveal that the proposed method achieves high-speed computation, good accuracy, and robustness to potential misspecifications, such as initial value's choices and hyperparameter settings. Thus, the proposed method can be a useful tool for estimating a Q-matrix in large-scale settings.

</details>

<details>

<summary>2022-05-20 17:28:43 - Interpretable Personalization via Policy Learning with Linear Decision Boundaries</summary>

- *Zhaonan Qu, Isabella Qian, Zhengyuan Zhou*

- `2003.07545v3` - [abs](http://arxiv.org/abs/2003.07545v3) - [pdf](http://arxiv.org/pdf/2003.07545v3)

> With the rise of the digital economy and an explosion of available information on consumers, effective personalization of offers, goods, and services has become a core business focus for companies to improve revenues and maintain competitive edge. This paper studies the personalization problem through the lens of policy learning, where the goal is to learn a decision-making rule (a policy) that maps from consumer and product characteristics (features) to recommendations (actions) in order to optimize outcomes (rewards). We focus on using available historical data for offline learning with unknown data collection procedure. Importantly, in many business and medical settings, interpretability of a policy is essential. To address these challenges, we study the class of policies with linear decision boundaries and propose learning algorithms using tools from causal inference. We propose several optimization schemes to solve the associated non-convex, non-smooth optimization problem, and find that an adapted Bayesian optimization algorithm is fast and effective. We test our algorithm with extensive simulation studies and apply it to an online marketplace customer purchase dataset, where the learned policy outputs a personalized discount recommendation based on customer and product features in order to maximize gross merchandise value (GMV) for sellers. Our learned policy improves upon the platform's baseline by 88.2\% in net sales revenue, while also providing informative insights on which features are important for the decision-making process, e.g. when "Attribute 2" is large, marginal increase in GMV is low for discounts higher than 10\%. Our findings suggest that the proposed policy learning algorithm provides a promising practical approach for interpretable personalization across a wide range of applications.

</details>

<details>

<summary>2022-05-20 19:11:15 - Modeling Random Directions in 2D Simplex Data</summary>

- *Rayleigh Lei, XuanLong Nguyen*

- `2103.12214v2` - [abs](http://arxiv.org/abs/2103.12214v2) - [pdf](http://arxiv.org/pdf/2103.12214v2)

> We propose models and algorithms for learning about random directions in two-dimensional simplex data, and apply our methods to the study of income level proportions and their changes over time in a geostatistical area. There are several notable challenges in the analysis of simplex-valued data: the measurements must respect the simplex constraint and the changes exhibit spatiotemporal smoothness while allowing for possible heterogeneous behaviors. To that end, we propose Bayesian models that rely on and expand upon building blocks in circular and spatial statistics by exploiting suitable transformation based on the polar coordinates for circular data. Our models also account for spatial correlation across locations in the simplex and the heterogeneous patterns via mixture modeling. We describe some properties of the models and model fitting via MCMC techniques. Our models and methods are illustrated via a thorough simulation study, and applied to an analysis of movements and trends of income categories using the Home Mortgage Disclosure Act data.

</details>

<details>

<summary>2022-05-20 20:35:10 - Exact Bayesian inference for level-set Cox processes with piecewise constant intensity function</summary>

- *Flavio B. Gonalves, Barbara C. C. Dias*

- `2012.05764v4` - [abs](http://arxiv.org/abs/2012.05764v4) - [pdf](http://arxiv.org/pdf/2012.05764v4)

> This paper proposes a new methodology to perform Bayesian inference for a class of multidimensional Cox processes in which the intensity function is piecewise constant. Poisson processes with piecewise constant intensity functions are believed to be suitable to model a variety of point process phenomena and, given its simpler structure, are expected to provide more precise inference when compared to processes with non-parametric and continuously varying intensity functions. The partition of the space domain is flexibly determined by a level-set function of a latent Gaussian process. Despite the intractability of the likelihood function and the infinite dimensionality of the parameter space, inference is performed exactly, in the sense that no space discretization approximation is used and MCMC error is the only source of inaccuracy. That is achieved by using retrospective sampling techniques and devising a pseudo-marginal infinite-dimensional MCMC algorithm that converges to the exact target posterior distribution. Computational efficiency is favored by considering a nearest neighbor Gaussian process, allowing for the analysis of large datasets. An extension to consider spatiotemporal models is also proposed. The efficiency of the proposed methodology is investigated in simulated examples and its applicability is illustrated in the analysis of some real point process datasets.

</details>

<details>

<summary>2022-05-20 20:43:47 - Flexible Bayesian modelling in dichotomous item response theory using mixtures of skewed item curves</summary>

- *Flvio B. Gonalves, Juliane Venturelli, Rosangela H. Loschi*

- `1910.10233v4` - [abs](http://arxiv.org/abs/1910.10233v4) - [pdf](http://arxiv.org/pdf/1910.10233v4)

> Most Item Response Theory (IRT) models for dichotomous responses are based on probit or logit link functions which assume a symmetric relationship between the probability of a correct response and the latent traits of individuals submitted to a test. This assumption restricts the use of those models to the case in which all items have a symmetric behaviour. On the other hand, asymmetric models proposed in the literature impose that all the items in a test have an asymmetric behaviour. This assumption is inappropriate for great part of the tests which are, in general, composed by both symmetric and asymmetric items. Furthermore, a straightforward extension of the existing models in the literature would require a prior selection of the items' symmetry/asymmetry status. This paper proposes a Bayesian IRT model that accounts for symmetric and asymmetric items in a flexible though parsimonious way. That is achieved by assigning a finite mixture prior to the skewness parameter, with one of the mixture components being a point-mass at zero. This allows for analyses under both model selection and model averaging approaches. Asymmetric item curves are designed through the centred skew normal distribution, which has a particularly appealing parametrisation in terms of parameter interpretation and computational efficiency. An efficient MCMC algorithm is proposed to perform Bayesian inference and its performance is investigated in some simulated examples. Finally, the proposed methodology is applied to a data set from a large scale educational exam in Brazil.

</details>

<details>

<summary>2022-05-20 20:44:48 - Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation</summary>

- *Ryan-Rhys Griffiths, Alexander A. Aldrick, Miguel Garcia-Ortegon, Vidhi R. Lalchand, Alpha A. Lee*

- `1910.07779v3` - [abs](http://arxiv.org/abs/1910.07779v3) - [pdf](http://arxiv.org/pdf/1910.07779v3)

> Bayesian optimisation is a sample-efficient search methodology that holds great promise for accelerating drug and materials discovery programs. A frequently-overlooked modelling consideration in Bayesian optimisation strategies however, is the representation of heteroscedastic aleatoric uncertainty. In many practical applications it is desirable to identify inputs with low aleatoric noise, an example of which might be a material composition which consistently displays robust properties in response to a noisy fabrication process. In this paper, we propose a heteroscedastic Bayesian optimisation scheme capable of representing and minimising aleatoric noise across the input space. Our scheme employs a heteroscedastic Gaussian process (GP) surrogate model in conjunction with two straightforward adaptations of existing acquisition functions. First, we extend the augmented expected improvement (AEI) heuristic to the heteroscedastic setting and second, we introduce the aleatoric noise-penalised expected improvement (ANPEI) heuristic. Both methodologies are capable of penalising aleatoric noise in the suggestions and yield improved performance relative to homoscedastic Bayesian optimisation and random sampling on toy problems as well as on two real-world scientific datasets. Code is available at: \url{https://github.com/Ryan-Rhys/Heteroscedastic-BO}

</details>

<details>

<summary>2022-05-20 20:57:37 - Microbiome compositional analysis with logistic-tree normal models</summary>

- *Zhuoqun Wang, Jialiang Mao, Li Ma*

- `2106.15051v3` - [abs](http://arxiv.org/abs/2106.15051v3) - [pdf](http://arxiv.org/pdf/2106.15051v3)

> Modern microbiome compositional data are often high-dimensional and exhibit complex dependency among microbial taxa. However, existing approaches to analyzing microbiome compositional data either do not adequately account for the complex dependency or lack scalability to high-dimensionality, which presents challenges in appropriately incorporating the "random effects" in microbiome compositions in the resulting statistical analysis. We introduce a generative model called the "logistic-tree normal" (LTN) model to address this need. The LTN marries two popular classes of models -- the log-ratio normal (LN) and the Dirichlet-tree (DT) -- and inherits key benefits of each. LN models are flexible in characterizing covariance among taxa but lacks scalability to higher dimensions; DT avoids this issue through a tree-based binomial decomposition but incurs restrictive covariance. The LTN incorporates the tree-based decomposition as the DT does, but it jointly models the corresponding binomial probabilities using a (multivariate) logistic-normal distribution as in LN models. It therefore allows rich covariance structures as LN, along with computational efficiency realized through a Polya-Gamma augmentation on the binomial models at the tree nodes. Accordingly, Bayesian inference on LTN can readily proceed by Gibbs sampling. The LTN also allows common techniques for effective inference on high-dimensional data -- such as those based on sparsity and low-rank assumptions in the covariance structure -- to be readily incorporated. Depending on the goal of the analysis, LTN can be used either as a standalone model or embedded into more sophisticated hierarchical models. We demonstrate its use in estimating taxa covariance and in mixed-effects modeling. Finally, we carry out an extensive case study using an LTN-based mixed-effects model to analyze a longitudinal dataset from the DIABIMMUNE project.

</details>

<details>

<summary>2022-05-21 06:13:23 - Bayesian Conditional Transformation Models</summary>

- *Manuel Carlan, Thomas Kneib, Nadja Klein*

- `2012.11016v3` - [abs](http://arxiv.org/abs/2012.11016v3) - [pdf](http://arxiv.org/pdf/2012.11016v3)

> Recent developments in statistical regression methodology shift away from pure mean regression towards distributional regression models. One important strand thereof is that of conditional transformation models (CTMs). CTMs infer the entire conditional distribution directly by applying a transformation function to the response conditionally on a set of covariates towards a simple log-concave reference distribution. Thereby, CTMs allow not only variance, kurtosis or skewness but the complete conditional distribution to depend on the explanatory variables. We propose a Bayesian notion of conditional transformation models (BCTMs) focusing on exactly observed continuous responses, but also incorporating extensions to randomly censored and discrete responses. Rather than relying on Bernstein polynomials that have been considered in likelihood-based CTMs, we implement a spline-based parametrization for monotonic effects that are supplemented with smoothness priors. Furthermore, we are able to benefit from the Bayesian paradigm via easily obtainable credible intervals and other quantities without relying on large sample approximations. A simulation study demonstrates the competitiveness of our approach against its likelihood-based counterpart but also Bayesian additive models of location, scale and shape and Bayesian quantile regression. Two applications illustrate the versatility of BCTMs in problems involving real world data, again including the comparison with various types of competitors.

</details>

<details>

<summary>2022-05-21 13:36:22 - Modelling sub-daily precipitation extremes with the blended generalised extreme value distribution</summary>

- *Silius M. Vandeskog, Sara Martino, Daniela Castro-Camilo, Hvard Rue*

- `2105.09062v5` - [abs](http://arxiv.org/abs/2105.09062v5) - [pdf](http://arxiv.org/pdf/2105.09062v5)

> A new method is proposed for modelling the yearly maxima of sub-daily precipitation, with the aim of producing spatial maps of return level estimates. Yearly precipitation maxima are modelled using a Bayesian hierarchical model with a latent Gaussian field, with the blended generalised extreme value (bGEV) distribution used as a substitute for the more standard generalised extreme value (GEV) distribution. Inference is made less wasteful with a novel two-step procedure that performs separate modelling of the scale parameter of the bGEV distribution using peaks over threshold data. Fast inference is performed using integrated nested Laplace approximations (INLA) together with the stochastic partial differential equation (SPDE) approach, both implemented in R-INLA. Heuristics for improving the numerical stability of R-INLA with the GEV and bGEV distributions are also presented. The model is fitted to yearly maxima of sub-daily precipitation from the south of Norway, and is able to quickly produce high-resolution return level maps with uncertainty. The proposed two-step procedure provides an improved model fit over standard inference techniques when modelling the yearly maxima of sub-daily precipitation with the bGEV distribution.

</details>

<details>

<summary>2022-05-21 14:54:29 - Bayesian Multi Scale Neural Network for Crowd Counting</summary>

- *Abhinav Sagar*

- `2007.14245v3` - [abs](http://arxiv.org/abs/2007.14245v3) - [pdf](http://arxiv.org/pdf/2007.14245v3)

> Crowd Counting is a difficult but important problem in computer vision. Convolutional Neural Networks based on estimating the density map over the image has been highly successful in this domain. However dense crowd counting remains an open problem because of severe occlusion and perspective view in which people can be present at various sizes. In this work, we propose a new network which uses a ResNet based feature extractor, downsampling block which uses dilated convolutions and upsampling block using transposed convolutions. We present a novel aggregation module which makes our network robust to the perspective view problem. We present the optimization details, loss functions and the algorithm used in our work. On evaluating on ShanghaiTech, UCF-CC-50 and UCF-QNRF datasets using MSE and MAE as evaluation metrics, our network outperforms previous state of the art approaches while giving uncertainty estimates in a principled bayesian manner.

</details>

<details>

<summary>2022-05-21 16:45:09 - Bayesian Clustering of Neural Activity with a Mixture of Dynamic Poisson Factor Analyzers</summary>

- *Ganchao Wei, Ian H. Stevenson, Xiaojing Wang*

- `2205.10639v1` - [abs](http://arxiv.org/abs/2205.10639v1) - [pdf](http://arxiv.org/pdf/2205.10639v1)

> Modern neural recording techniques allow neuroscientists to observe the spiking activity of many neurons simultaneously. Although previous work has illustrated how activity within and between known populations of neurons can be summarized by low-dimensional latent vectors, in many cases what determines a unique population may be unclear. Neurons differ in their anatomical location, but also, in their cell types and response properties. Moreover, multiple distinct populations may not be well described by a single low-dimensional, linear representation. To tackle these challenges, we develop a clustering method based on a mixture of dynamic Poisson factor analyzers (DPFA) model, with the number of clusters treated as an unknown parameter. To do the analysis of DPFA model, we propose a novel Markov chain Monte Carlo (MCMC) algorithm to efficiently sample its posterior distribution. Validating our proposed MCMC algorithm with simulations, we find that it can accurately recover the true clustering and latent states and is insensitive to the initial cluster assignments. We then apply the proposed mixture of DPFA model to multi-region experimental recordings, where we find that the proposed method can identify novel, reliable clusters of neurons based on their activity, and may, thus, be a useful tool for neural data analysis.

</details>

<details>

<summary>2022-05-21 23:41:11 - Non-asymptotic confidence bands on the probability an individual benefits from treatment (PIBT)</summary>

- *Gabriel Ruiz, Oscar Hernan Madrid Padilla*

- `2205.09094v2` - [abs](http://arxiv.org/abs/2205.09094v2) - [pdf](http://arxiv.org/pdf/2205.09094v2)

> The premise of this work, in a vein similar to predictive inference with quantile regression, is that observations may lie far away from their conditional expectation. In the context of causal inference, due to the missing-ness of one outcome, it is difficult to check whether an individual's treatment effect lies close to its prediction given by the estimated Average Treatment Effect (ATE) or Conditional Average Treatment Effect (CATE). With the aim of augmenting the inference with these estimands in practice, we further study an existing distribution-free framework for the plug-in estimation of bounds on the probability an individual benefits from treatment (PIBT), a generally inestimable quantity that would concisely summarize an intervention's efficacy if it could be known. Given the innate uncertainty in the target population-level bounds on PIBT, we seek to better understand the margin of error for the estimation of these target parameters in order to help discern whether estimated bounds on treatment efficacy are tight (or wide) due to random chance or not. In particular, we present non-asymptotic guarantees to the estimation of bounds on marginal PIBT for a randomized controlled trial (RCT) setting. We also derive new non-asymptotic results for the case where we would like to understand heterogeneity in PIBT across strata of pre-treatment covariates, with one of our main results in this setting making strategic use of regression residuals. These results, especially those in the RCT case, can be used to help with formal statistical power analyses and frequentist confidence statements for settings where we are interested in inferring PIBT through the target bounds under minimal parametric assumptions.

</details>

<details>

<summary>2022-05-22 05:18:01 - Athlete rating in multi-competitor games with scored outcomes via monotone transformations</summary>

- *Jonathan Che, Mark Glickman*

- `2205.10746v1` - [abs](http://arxiv.org/abs/2205.10746v1) - [pdf](http://arxiv.org/pdf/2205.10746v1)

> Sports organizations often want to estimate athlete strengths. For games with scored outcomes, a common approach is to assume observed game scores follow a normal distribution conditional on athletes' latent abilities, which may change over time. In many games, however, this assumption of conditional normality does not hold. To estimate athletes' time-varying latent abilities using non-normal game score data, we propose a Bayesian dynamic linear model with flexible monotone response transformations. Our model learns nonlinear monotone transformations to address non-normality in athlete scores and can be easily fit using standard regression and optimization routines. We demonstrate our method on data from several Olympic sports, including biathlon, diving, rugby, and fencing.

</details>

<details>

<summary>2022-05-22 08:06:54 - Fast Instrument Learning with Faster Rates</summary>

- *Ziyu Wang, Yuhao Zhou, Jun Zhu*

- `2205.10772v1` - [abs](http://arxiv.org/abs/2205.10772v1) - [pdf](http://arxiv.org/pdf/2205.10772v1)

> We investigate nonlinear instrumental variable (IV) regression given high-dimensional instruments. We propose a simple algorithm which combines kernelized IV methods and an arbitrary, adaptive regression algorithm, accessed as a black box. Our algorithm enjoys faster-rate convergence and adapts to the dimensionality of informative latent features, while avoiding an expensive minimax optimization procedure, which has been necessary to establish similar guarantees. It further brings the benefit of flexible machine learning models to quasi-Bayesian uncertainty quantification, likelihood-based model selection, and model averaging. Simulation studies demonstrate the competitive performance of our method.

</details>

<details>

<summary>2022-05-22 14:23:48 - Privacy Protection for Youth Risk Behavior Using Bayesian Data Synthesis: A Case Study to the YRBS</summary>

- *Yixiao Cao, Jingchen Hu*

- `2205.10833v1` - [abs](http://arxiv.org/abs/2205.10833v1) - [pdf](http://arxiv.org/pdf/2205.10833v1)

> The large number of publicly available survey datasets of wide variety, albeit useful, raise respondent-level privacy concerns. The synthetic data approach to data privacy and confidentiality has been shown useful in terms of privacy protection and utility preservation. This paper aims at illustrating how synthetic data can facilitate the dissemination of highly sensitive information about youth risk behavior by presenting a case study of synthetic data for a sample of the Youth Risk Behavior Survey (YRBS). Given the categorical nature of almost all variables in YRBS, the Dirichlet Process mixture of products of multinomials (DPMPM) synthesizer is adopted to partially synthesize the YRBS sample. Detailed evaluations of utility and disclosure risks demonstrate that the generated synthetic data are able to significantly reduce the disclosure risks compared to the confidential YRSB sample while maintaining a high level of utility.

</details>

<details>

<summary>2022-05-22 17:38:36 - Fast Gaussian Process Posterior Mean Prediction via Local Cross Validation and Precomputation</summary>

- *Alec M. Dunton, Benjamin W. Priest, Amanda Muyskens*

- `2205.10879v1` - [abs](http://arxiv.org/abs/2205.10879v1) - [pdf](http://arxiv.org/pdf/2205.10879v1)

> Gaussian processes (GPs) are Bayesian non-parametric models useful in a myriad of applications. Despite their popularity, the cost of GP predictions (quadratic storage and cubic complexity with respect to the number of training points) remains a hurdle in applying GPs to large data. We present a fast posterior mean prediction algorithm called FastMuyGPs to address this shortcoming. FastMuyGPs is based upon the MuyGPs hyperparameter estimation algorithm and utilizes a combination of leave-one-out cross-validation, batching, nearest neighbors sparsification, and precomputation to provide scalable, fast GP prediction. We demonstrate several benchmarks wherein FastMuyGPs prediction attains superior accuracy and competitive or superior runtime to both deep neural networks and state-of-the-art scalable GP algorithms.

</details>

<details>

<summary>2022-05-22 19:34:07 - Thompson Sampling with Unrestricted Delays</summary>

- *Han Wu, Stefan Wager*

- `2202.12431v2` - [abs](http://arxiv.org/abs/2202.12431v2) - [pdf](http://arxiv.org/pdf/2202.12431v2)

> We investigate properties of Thompson Sampling in the stochastic multi-armed bandit problem with delayed feedback. In a setting with i.i.d delays, we establish to our knowledge the first regret bounds for Thompson Sampling with arbitrary delay distributions, including ones with unbounded expectation. Our bounds are qualitatively comparable to the best available bounds derived via ad-hoc algorithms, and only depend on delays via selected quantiles of the delay distributions. Furthermore, in extensive simulation experiments, we find that Thompson Sampling outperforms a number of alternative proposals, including methods specifically designed for settings with delayed feedback.

</details>

<details>

<summary>2022-05-23 06:51:34 - Nonparametric Difference-in-Differences in Repeated Cross-Sections with Continuous Treatments</summary>

- *Xavier D'Haultfoeuille, Stefan Hoderlein, Yuya Sasaki*

- `2104.14458v2` - [abs](http://arxiv.org/abs/2104.14458v2) - [pdf](http://arxiv.org/pdf/2104.14458v2)

> This paper studies the identification of causal effects of a continuous treatment using a new difference-in-difference strategy. Our approach allows for endogeneity of the treatment, and employs repeated cross-sections. It requires an exogenous change over time which affects the treatment in a heterogeneous way, stationarity of the distribution of unobservables and a rank invariance condition on the time trend. On the other hand, we do not impose any functional form restrictions or an additive time trend, and we are invariant to the scaling of the dependent variable. Under our conditions, the time trend can be identified using a control group, as in the binary difference-in-differences literature. In our scenario, however, this control group is defined by the data. We then identify average and quantile treatment effect parameters. We develop corresponding nonparametric estimators and study their asymptotic properties. Finally, we apply our results to the effect of disposable income on consumption.

</details>

<details>

<summary>2022-05-23 07:29:51 - Conformal Prediction with Temporal Quantile Adjustments</summary>

- *Zhen Lin, Shubhendu Trivedi, Jimeng Sun*

- `2205.09940v2` - [abs](http://arxiv.org/abs/2205.09940v2) - [pdf](http://arxiv.org/pdf/2205.09940v2)

> We develop Temporal Quantile Adjustment (TQA), a general method to construct efficient and valid prediction intervals (PIs) for regression on cross-sectional time series data. Such data is common in many domains, including econometrics and healthcare. A canonical example in healthcare is predicting patient outcomes using physiological time-series data, where a population of patients composes a cross-section. Reliable PI estimators in this setting must address two distinct notions of coverage: cross-sectional coverage across a cross-sectional slice, and longitudinal coverage along the temporal dimension for each time series. Recent works have explored adapting Conformal Prediction (CP) to obtain PIs in the time series context. However, none handles both notions of coverage simultaneously. CP methods typically query a pre-specified quantile from the distribution of nonconformity scores on a calibration set. TQA adjusts the quantile to query in CP at each time $t$, accounting for both cross-sectional and longitudinal coverage in a theoretically-grounded manner. The post-hoc nature of TQA facilitates its use as a general wrapper around any time series regression model. We validate TQA's performance through extensive experimentation: TQA generally obtains efficient PIs and improves longitudinal coverage while preserving cross-sectional coverage.

</details>

<details>

<summary>2022-05-23 08:43:04 - AODisaggregation: toward global aerosol vertical profiles</summary>

- *Shahine Bouabid, Duncan Watson-Parris, Sofija Stefanovi, Athanasios Nenes, Dino Sejdinovic*

- `2205.04296v2` - [abs](http://arxiv.org/abs/2205.04296v2) - [pdf](http://arxiv.org/pdf/2205.04296v2)

> Aerosol-cloud interactions constitute the largest source of uncertainty in assessments of the anthropogenic climate change. This uncertainty arises in part from the difficulty in measuring the vertical distributions of aerosols, and only sporadic vertically resolved observations are available. We often have to settle for less informative vertically aggregated proxies such as aerosol optical depth (AOD). In this work, we develop a framework for the vertical disaggregation of AOD into extinction profiles, i.e. the measure of light extinction throughout an atmospheric column, using readily available vertically resolved meteorological predictors such as temperature, pressure or relative humidity. Using Bayesian nonparametric modelling, we devise a simple Gaussian process prior over aerosol vertical profiles and update it with AOD observations to infer a distribution over vertical extinction profiles. To validate our approach, we use ECHAM-HAM aerosol-climate model data which offers self-consistent simulations of meteorological covariates, AOD and extinction profiles. Our results show that, while very simple, our model is able to reconstruct realistic extinction profiles with well-calibrated uncertainty, outperforming by an order of magnitude the idealized baseline which is typically used in satellite AOD retrieval algorithms. In particular, the model demonstrates a faithful reconstruction of extinction patterns arising from aerosol water uptake in the boundary layer. Observations however suggest that other extinction patterns, due to aerosol mass concentration, particle size and radiative properties, might be more challenging to capture and require additional vertically resolved predictors.

</details>

<details>

<summary>2022-05-23 09:23:27 - Learning Summary Statistics for Bayesian Inference with Autoencoders</summary>

- *Carlo Albert, Simone Ulzega, Firat Ozdemir, Fernando Perez-Cruz, Antonietta Mira*

- `2201.12059v2` - [abs](http://arxiv.org/abs/2201.12059v2) - [pdf](http://arxiv.org/pdf/2201.12059v2)

> For stochastic models with intractable likelihood functions, approximate Bayesian computation offers a way of approximating the true posterior through repeated comparisons of observations with simulated model outputs in terms of a small set of summary statistics. These statistics need to retain the information that is relevant for constraining the parameters but cancel out the noise. They can thus be seen as thermodynamic state variables, for general stochastic models. For many scientific applications, we need strictly more summary statistics than model parameters to reach a satisfactory approximation of the posterior. Therefore, we propose to use the inner dimension of deep neural network based Autoencoders as summary statistics. To create an incentive for the encoder to encode all the parameter-related information but not the noise, we give the decoder access to explicit or implicit information on the noise that has been used to generate the training data. We validate the approach empirically on two types of stochastic models.

</details>

<details>

<summary>2022-05-23 09:24:37 - Split personalities in Bayesian Neural Networks: the case for full marginalisation</summary>

- *David Yallup, Will Handley, Mike Hobson, Anthony Lasenby, Pablo Lemos*

- `2205.11151v1` - [abs](http://arxiv.org/abs/2205.11151v1) - [pdf](http://arxiv.org/pdf/2205.11151v1)

> The true posterior distribution of a Bayesian neural network is massively multimodal. Whilst most of these modes are functionally equivalent, we demonstrate that there remains a level of real multimodality that manifests in even the simplest neural network setups. It is only by fully marginalising over all posterior modes, using appropriate Bayesian sampling tools, that we can capture the split personalities of the network. The ability of a network trained in this manner to reason between multiple candidate solutions dramatically improves the generalisability of the model, a feature we contend is not consistently captured by alternative approaches to the training of Bayesian neural networks. We provide a concise minimal example of this, which can provide lessons and a future path forward for correctly utilising the explainability and interpretability of Bayesian neural networks.

</details>

<details>

<summary>2022-05-23 12:47:13 - RL with KL penalties is better viewed as Bayesian inference</summary>

- *Tomasz Korbak, Ethan Perez, Christopher L Buckley*

- `2205.11275v1` - [abs](http://arxiv.org/abs/2205.11275v1) - [pdf](http://arxiv.org/pdf/2205.11275v1)

> Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.

</details>

<details>

<summary>2022-05-23 13:27:58 - A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions</summary>

- *Wei Deng, Guang Lin, Faming Liang*

- `2010.09800v2` - [abs](http://arxiv.org/abs/2010.09800v2) - [pdf](http://arxiv.org/pdf/2010.09800v2)

> We propose an adaptively weighted stochastic gradient Langevin dynamics algorithm (SGLD), so-called contour stochastic gradient Langevin dynamics (CSGLD), for Bayesian learning in big data statistics. The proposed algorithm is essentially a \emph{scalable dynamic importance sampler}, which automatically \emph{flattens} the target distribution such that the simulation for a multi-modal distribution can be greatly facilitated. Theoretically, we prove a stability condition and establish the asymptotic convergence of the self-adapting parameter to a {\it unique fixed-point}, regardless of the non-convexity of the original energy function; we also present an error analysis for the weighted averaging estimators. Empirically, the CSGLD algorithm is tested on multiple benchmark datasets including CIFAR10 and CIFAR100. The numerical results indicate its superiority to avoid the local trap problem in training deep neural networks.

</details>

<details>

<summary>2022-05-23 15:58:20 - Bayesian Active Meta-Learning for Black-Box Optimization</summary>

- *Ivana Nikoloska, Osvaldo Simeone*

- `2110.09943v2` - [abs](http://arxiv.org/abs/2110.09943v2) - [pdf](http://arxiv.org/pdf/2110.09943v2)

> Data-efficient learning algorithms are essential in many practical applications for which data collection is expensive, e.g., for the optimal deployment of wireless systems in unknown propagation scenarios. Meta-learning can address this problem by leveraging data from a set of related learning tasks, e.g., from similar deployment settings. In practice, one may have available only unlabeled data sets from the related tasks, requiring a costly labeling procedure to be carried out before use in meta-learning. For instance, one may know the possible positions of base stations in a given area, but not the performance indicators achievable with each deployment. To decrease the number of labeling steps required for meta-learning, this paper introduces an information-theoretic active task selection mechanism, and evaluates an instantiation of the approach for Bayesian optimization of black-box models.

</details>

<details>

<summary>2022-05-23 17:05:06 - PAC$^m$-Bayes: Narrowing the Empirical Risk Gap in the Misspecified Bayesian Regime</summary>

- *Warren R. Morningstar, Alexander A. Alemi, Joshua V. Dillon*

- `2010.09629v3` - [abs](http://arxiv.org/abs/2010.09629v3) - [pdf](http://arxiv.org/pdf/2010.09629v3)

> The Bayesian posterior minimizes the "inferential risk" which itself bounds the "predictive risk". This bound is tight when the likelihood and prior are well-specified. However since misspecification induces a gap, the Bayesian posterior predictive distribution may have poor generalization performance. This work develops a multi-sample loss (PAC$^m$) which can close the gap by spanning a trade-off between the two risks. The loss is computationally favorable and offers PAC generalization guarantees. Empirical study demonstrates improvement to the predictive distribution.

</details>

<details>

<summary>2022-05-23 17:40:31 - Robust and Agnostic Learning of Conditional Distributional Treatment Effects</summary>

- *Nathan Kallus, Miruna Oprescu*

- `2205.11486v1` - [abs](http://arxiv.org/abs/2205.11486v1) - [pdf](http://arxiv.org/pdf/2205.11486v1)

> The conditional average treatment effect (CATE) is the best point prediction of individual causal effects given individual baseline covariates and can help personalize treatments. However, as CATE only reflects the (conditional) average, it can wash out potential risks and tail events, which are crucially relevant to treatment choice. In aggregate analyses, this is usually addressed by measuring distributional treatment effect (DTE), such as differences in quantiles or tail expectations between treatment groups. Hypothetically, one can similarly fit covariate-conditional quantile regressions in each treatment group and take their difference, but this would not be robust to misspecification or provide agnostic best-in-class predictions. We provide a new robust and model-agnostic methodology for learning the conditional DTE (CDTE) for a wide class of problems that includes conditional quantile treatment effects, conditional super-quantile treatment effects, and conditional treatment effects on coherent risk measures given by $f$-divergences. Our method is based on constructing a special pseudo-outcome and regressing it on baseline covariates using any given regression learner. Our method is model-agnostic in the sense that it can provide the best projection of CDTE onto the regression model class. Our method is robust in the sense that even if we learn these nuisances nonparametrically at very slow rates, we can still learn CDTEs at rates that depend on the class complexity and even conduct inferences on linear projections of CDTEs. We investigate the performance of our proposal in simulation studies, and we demonstrate its use in a case study of 401(k) eligibility effects on wealth.

</details>

<details>

<summary>2022-05-23 18:21:07 - Agreement and Statistical Efficiency in Bayesian Perception Models</summary>

- *Yash Deshpande, Elchanan Mossel, Youngtak Sohn*

- `2205.11561v1` - [abs](http://arxiv.org/abs/2205.11561v1) - [pdf](http://arxiv.org/pdf/2205.11561v1)

> Bayesian models of group learning are studied in Economics since the 1970s and more recently in computational linguistics. The models from Economics postulate that agents maximize utility in their communication and actions. The Economics models do not explain the "probability matching" phenomena that are observed in many experimental studies. To address these observations, Bayesian models that do not formally fit into the economic utility maximization framework were introduced. In these models individuals sample from their posteriors in communication.   In this work, we study the asymptotic behavior of such models on connected networks with repeated communication. Perhaps surprisingly, despite the fact that individual agents are not utility maximizers in the classical sense, we establish that the individuals ultimately agree and furthermore show that the limiting posterior is Bayes optimal.

</details>

<details>

<summary>2022-05-23 18:26:51 - Risk-Sensitive Reinforcement Learning via Policy Gradient Search</summary>

- *Prashanth L. A., Michael Fu*

- `1810.09126v3` - [abs](http://arxiv.org/abs/1810.09126v3) - [pdf](http://arxiv.org/pdf/1810.09126v3)

> The objective in a traditional reinforcement learning (RL) problem is to find a policy that optimizes the expected value of a performance metric such as the infinite-horizon cumulative discounted or long-run average cost/reward. In practice, optimizing the expected value alone may not be satisfactory, in that it may be desirable to incorporate the notion of risk into the optimization problem formulation, either in the objective or as a constraint. Various risk measures have been proposed in the literature, e.g., exponential utility, variance, percentile performance, chance constraints, value at risk (quantile), conditional value-at-risk, prospect theory and its later enhancement, cumulative prospect theory. In this book, we consider risk-sensitive RL in two settings: one where the goal is to find a policy that optimizes the usual expected value objective while ensuring that a risk constraint is satisfied, and the other where the risk measure is the objective. We survey some of the recent work in this area specifically where policy gradient search is the solution approach. In the first risk-sensitive RL setting, we cover popular risk measures based on variance, conditional value-at-risk, and chance constraints, and present a template for policy gradient-based risk-sensitive RL algorithms using a Lagrangian formulation. For the setting where risk is incorporated directly into the objective function, we consider an exponential utility formulation, cumulative prospect theory, and coherent risk measures. This non-exhaustive survey aims to give a flavor of the challenges involved in solving risk-sensitive RL problems using policy gradient methods, as well as outlining some potential future research directions.

</details>

<details>

<summary>2022-05-23 18:54:27 - Quasi Black-Box Variational Inference with Natural Gradients for Bayesian Learning</summary>

- *Martin Magris, Mostafa Shabani, Alexandros Iosifidis*

- `2205.11568v1` - [abs](http://arxiv.org/abs/2205.11568v1) - [pdf](http://arxiv.org/pdf/2205.11568v1)

> We develop an optimization algorithm suitable for Bayesian learning in complex models. Our approach relies on natural gradient updates within a general black-box framework for efficient training with limited model-specific derivations. It applies within the class of exponential-family variational posterior distributions, for which we extensively discuss the Gaussian case for which the updates have a rather simple form. Our Quasi Black-box Variational Inference (QBVI) framework is readily applicable to a wide class of Bayesian inference problems and is of simple implementation as the updates of the variational posterior do not involve gradients with respect to the model parameters, nor the prescription of the Fisher information matrix. We develop QBVI under different hypotheses for the posterior covariance matrix, discuss details about its robust and feasible implementation, and provide a number of real-world applications to demonstrate its effectiveness.

</details>

<details>

<summary>2022-05-24 01:55:54 - Refined normal approximations for the Student distribution</summary>

- *Frdric Ouimet*

- `2201.05950v2` - [abs](http://arxiv.org/abs/2201.05950v2) - [pdf](http://arxiv.org/pdf/2201.05950v2)

> In this paper, we develop a local limit theorem for the Student distribution. We use it to improve the normal approximation of the Student survival function given in Shafiei & Saberali (2015) and to derive asymptotic bounds for the corresponding maximal errors at four levels of approximation. As a corollary, approximations for the percentage points (or quantiles) of the Student distribution are obtained in terms of the percentage points of the standard normal distribution.

</details>

<details>

<summary>2022-05-24 04:46:54 - Stochastic Neural Networks with Infinite Width are Deterministic</summary>

- *Liu Ziyin, Hanlin Zhang, Xiangming Meng, Yuting Lu, Eric Xing, Masahito Ueda*

- `2201.12724v2` - [abs](http://arxiv.org/abs/2201.12724v2) - [pdf](http://arxiv.org/pdf/2201.12724v2)

> This work theoretically studies stochastic neural networks, a main type of neural network in use. We prove that as the width of an optimized stochastic neural network tends to infinity, its predictive variance on the training set decreases to zero. Our theory justifies the common intuition that adding stochasticity to the model can help regularize the model by introducing an averaging effect. Two common examples that our theory can be relevant to are neural networks with dropout and Bayesian latent variable models in a special limit. Our result thus helps better understand how stochasticity affects the learning of neural networks and potentially design better architectures for practical problems.

</details>

<details>

<summary>2022-05-24 06:45:06 - Advanced Manufacturing Configuration by Sample-efficient Batch Bayesian Optimization</summary>

- *Xavier Guidetti, Alisa Rupenyan, Lutz Fassl, Majid Nabavi, John Lygeros*

- `2205.11827v1` - [abs](http://arxiv.org/abs/2205.11827v1) - [pdf](http://arxiv.org/pdf/2205.11827v1)

> We propose a framework for the configuration and operation of expensive-to-evaluate advanced manufacturing methods, based on Bayesian optimization. The framework unifies a tailored acquisition function, a parallel acquisition procedure, and the integration of process information providing context to the optimization procedure. The novel acquisition function is demonstrated and analyzed on benchmark illustrative problems. We apply the optimization approach to atmospheric plasma spraying in simulation and experiments. Our results demonstrate that the proposed framework can efficiently find input parameters that produce the desired outcome and minimize the process cost.

</details>

<details>

<summary>2022-05-24 06:55:28 - Adjusted Expected Improvement for Cumulative Regret Minimization in Noisy Bayesian Optimization</summary>

- *Shouri Hu, Haowei Wang, Zhongxiang Dai, Bryan Kian Hsiang Low, Szu Hui Ng*

- `2205.04901v2` - [abs](http://arxiv.org/abs/2205.04901v2) - [pdf](http://arxiv.org/pdf/2205.04901v2)

> The expected improvement (EI) is one of the most popular acquisition functions for Bayesian optimization (BO) and has demonstrated good empirical performances in many applications for the minimization of simple regret. However, under the evaluation metric of cumulative regret, the performance of EI may not be competitive, and its existing theoretical regret upper bound still has room for improvement. To adapt the EI for better performance under cumulative regret, we introduce a novel quantity called the evaluation cost which is compared against the acquisition function, and with this, develop the expected improvement-cost (EIC) algorithm. In each iteration of EIC, a new point with the largest acquisition function value is sampled, only if that value exceeds its evaluation cost. If none meets this criteria, the current best point is resampled. This evaluation cost quantifies the potential downside of sampling a point, which is important under the cumulative regret metric as the objective function value in every iteration affects the performance measure. We further establish in theory a tight regret upper bound of EIC for the squared-exponential covariance kernel under mild regularity conditions, and perform experiments to illustrate the improvement of EIC over several popular BO algorithms.

</details>

<details>

<summary>2022-05-24 08:10:47 - Partial frontiers are not quantiles</summary>

- *Sheng Dai, Timo Kuosmanen, Xun Zhou*

- `2205.11885v1` - [abs](http://arxiv.org/abs/2205.11885v1) - [pdf](http://arxiv.org/pdf/2205.11885v1)

> Quantile regression and partial frontier are two distinct approaches to nonparametric quantile frontier estimation. In this article, we demonstrate that partial frontiers are not quantiles. Both convex and nonconvex technologies are considered. To this end, we propose convexified order-$\alpha$ as an alternative to convex quantile regression (CQR) and convex expectile regression (CER), and two new nonconvex estimators: isotonic CQR and isotonic CER as alternatives to order-$\alpha$. A Monte Carlo study shows that the partial frontier estimators perform relatively poorly and even can violate the quantile property, particularly at low quantiles. In addition, the simulation evidence shows that the indirect expectile approach to estimating quantiles generally outperforms the direct quantile estimations. We further find that the convex estimators outperform their nonconvex counterparts owing to their global shape constraints. An illustration of those estimators is provided using a real-world dataset of U.S. electric power plants.

</details>

<details>

<summary>2022-05-24 08:20:13 - Bayesian Target-Vector Optimization for Efficient Parameter Reconstruction</summary>

- *Matthias Plock, Kas Andrle, Sven Burger, Philipp-Immanuel Schneider*

- `2202.11559v2` - [abs](http://arxiv.org/abs/2202.11559v2) - [pdf](http://arxiv.org/pdf/2202.11559v2)

> Parameter reconstructions are indispensable in metrology. Here, the objective is to to explain $K$ experimental measurements by fitting to them a parameterized model of the measurement process. The model parameters are regularly determined by least-square methods, i.e., by minimizing the sum of the squared residuals between the $K$ model predictions and the $K$ experimental observations, $\chi^2$. The model functions often involve computationally demanding numerical simulations. Bayesian optimization methods are specifically suited for minimizing expensive model functions. However, in contrast to least-square methods such as the Levenberg-Marquardt algorithm, they only take the value of $\chi^2$ into account, and neglect the $K$ individual model outputs. We present a Bayesian target-vector optimization scheme with improved performance over previous developments, that considers all $K$ contributions of the model function and that is specifically suited for parameter reconstruction problems which are often based on hundreds of observations. Its performance is compared to established methods for an optical metrology reconstruction problem and two synthetic least-squares problems. The proposed method outperforms established optimization methods. It also enables to determine accurate uncertainty estimates with very few observations of the actual model function by using Markov chain Monte Carlo sampling on a trained surrogate model.

</details>

<details>

<summary>2022-05-24 08:21:45 - A Quadrature Rule combining Control Variates and Adaptive Importance Sampling</summary>

- *Rmi Leluc, Franois Portier, Johan Segers, Aigerim Zhuman*

- `2205.11890v1` - [abs](http://arxiv.org/abs/2205.11890v1) - [pdf](http://arxiv.org/pdf/2205.11890v1)

> Driven by several successful applications such as in stochastic gradient descent or in Bayesian computation, control variates have become a major tool for Monte Carlo integration. However, standard methods do not allow the distribution of the particles to evolve during the algorithm, as is the case in sequential simulation methods. Within the standard adaptive importance sampling framework, a simple weighted least squares approach is proposed to improve the procedure with control variates. The procedure takes the form of a quadrature rule with adapted quadrature weights to reflect the information brought in by the control variates. The quadrature points and weights do not depend on the integrand, a computational advantage in case of multiple integrands. Moreover, the target density needs to be known only up to a multiplicative constant. Our main result is a non-asymptotic bound on the probabilistic error of the procedure. The bound proves that for improving the estimate's accuracy, the benefits from adaptive importance sampling and control variates can be combined. The good behavior of the method is illustrated empirically on synthetic examples and real-world data for Bayesian linear regression.

</details>

<details>

<summary>2022-05-24 08:30:00 - All that Glitters is not Gold: Relational Events Models with Spurious Events</summary>

- *Cornelius Fritz, Marius Mehrl, Paul W. Thurner, Gran Kauermann*

- `2109.10348v2` - [abs](http://arxiv.org/abs/2109.10348v2) - [pdf](http://arxiv.org/pdf/2109.10348v2)

> As relational event models are an increasingly popular model for studying relational structures, the reliability of large-scale event data collection becomes more and more important. Automated or human-coded events often suffer from non-negligible false-discovery rates in event identification. And most sensor data is primarily based on actors' spatial proximity for predefined time windows; hence, the observed events could relate either to a social relationship or random co-location. Both examples imply spurious events that may bias estimates and inference. We propose the Relational Event Model for Spurious Events (REMSE), an extension to existing approaches for interaction data. The model provides a flexible solution for modeling data while controlling for spurious events. Estimation of our model is carried out in an empirical Bayesian approach via data augmentation. Based on a simulation study, we investigate the properties of the estimation procedure. To demonstrate its usefulness in two distinct applications, we employ this model to combat events from the Syrian civil war and student co-location data. Results from the simulation and the applications identify the REMSE as a suitable approach to modeling relational event data in the presence of spurious events.

</details>

<details>

<summary>2022-05-24 14:07:13 - Optimality Conditions and Algorithms for Top-K Arm Identification</summary>

- *Zihao Wang, Shuoguang Yang, Wei You*

- `2205.12086v1` - [abs](http://arxiv.org/abs/2205.12086v1) - [pdf](http://arxiv.org/pdf/2205.12086v1)

> We consider the top-k arm identification problem for multi-armed bandits with rewards belonging to a one-parameter canonical exponential family. The objective is to select the set of k arms with the highest mean rewards by sequential allocation of sampling efforts. We propose a unified optimal allocation problem that identifies the complexity measures of this problem under the fixed-confidence, fixed-budget settings, and the posterior convergence rate from the Bayesian perspective. We provide the first characterization of its optimality. We provide the first provably optimal algorithm in the fixed-confidence setting for k>1. We also propose an efficient heuristic algorithm for the top-k arm identification problem. Extensive numerical experiments demonstrate superior performance compare to existing methods in all three settings.

</details>

<details>

<summary>2022-05-24 15:18:59 - Flexible Bayesian Support Vector Machines for Brain Network-based Classification</summary>

- *Jin Ming, Suprateek Kundu*

- `2205.12143v1` - [abs](http://arxiv.org/abs/2205.12143v1) - [pdf](http://arxiv.org/pdf/2205.12143v1)

> Objective: Brain networks have gained increasing recognition as potential biomarkers in mental health studies, but there are limited approaches that can leverage complex brain networks for accurate classification. Our goal is to develop a novel Bayesian Support Vector Machine (SVM) approach that incorporates high-dimensional networks as covariates and is able to overcome limitations of existing penalized methods. Methods: We develop a novel Dirichlet process mixture of double exponential priors on the coefficients in the Bayesian SVM model that is able to perform feature selection and uncertainty quantification, by pooling information across edges to determine differential sparsity levels in an unsupervised manner. We develop different versions of the model that incorporates static and dynamic connectivity features, as well as an integrative analysis that jointly includes features from multiple scanning sessions. We perform classification of intelligence levels using resting state fMRI data from the Human Connectome Project (HCP), and a second Attention Deficiency Hyperactivity Disorder (ADHD) classification task. Results: Our results clearly reveal the considerable greater classification accuracy under the proposed approach over state-of-the-art methods. The multi-session analysis results in the highest classification accuracy in the HCP data analysis. Conclusion: We provide concrete evidence that the novel Bayesian SVMs provides an unsupervised and automated approach for network-based classification, that results in considerable improvements over penalized methods and parametric Bayesian approaches. Significance: Our work is one of the first to conclusively demonstrate the advantages of a Bayesian SVM in network-based classification of mental health outcomes, and the importance of multi-session network analysis.

</details>

<details>

<summary>2022-05-24 15:48:29 - Informative Bayesian Tools for Damage Localisation by Decomposition of Lamb Wave Signals</summary>

- *Marcus Haywood-Alexander, Nikolaos Dervilis, Keith Worden, Gordon Dobie, Timothy J. Rogers*

- `2205.12161v1` - [abs](http://arxiv.org/abs/2205.12161v1) - [pdf](http://arxiv.org/pdf/2205.12161v1)

> Ultrasonic guided waves offer a convenient and practical approach to structural health monitoring and non-destructive evaluation, thanks to some distinct advantages. Guided waves, in particular Lamb waves, can be used to localise damage by utilising prior knowledge of propagation and reflection characteristics. Typical localisation methods make use of the time of arrival of waves emitted or reflected from the damage, the simplest of which involves triangulation. It is useful to decompose the measured signal into the expected waves propagating directly from the actuation source in the absence of damage, and for this paper referred to as nominal waves. This decomposition allows for determination of waves reflected from damage, boundaries or other local inhomogeneities. Previous decomposition methods make use of accurate analytical models, but there is a gap in methods of decomposition for complex materials and structures. A new method is shown here which uses a Bayesian approach to decompose single-source signals, which has the advantage of quantification of the uncertainty of the expected signal. Furthermore, the approach produces inherent parametric features which correlate to known physics of guided waves. In this paper, the decomposition method is demonstrated on data from a simulation of guided wave propagation in a small aluminium plate, using the local interaction simulation approach, for a damaged and undamaged case. Analysis of the decomposition method is done in three ways; inspect individual decomposed signals, track the inherently produced parametric features along propagation distance, and use method in a localisation strategy. The Bayesian decomposition was found to work well for the assessment criteria mentioned above. The use of these waves in the localisation method returned estimates accurate to within 1mm in many sensor configurations.

</details>

<details>

<summary>2022-05-24 17:35:00 - Bayesian sample size determination in basket trials borrowing information between subsets</summary>

- *Haiyan Zheng, Michael J. Grayling, Pavel Mozgunov, Thomas Jaki, James M. S. Wason*

- `2205.12227v1` - [abs](http://arxiv.org/abs/2205.12227v1) - [pdf](http://arxiv.org/pdf/2205.12227v1)

> Basket trials are increasingly used for the simultaneous evaluation of a new treatment in various patient subgroups under one overarching protocol. We propose a Bayesian approach to sample size determination in basket trials that permit borrowing of information between commensurate subsets. Specifically, we consider a randomised basket trial design where patients are randomly assigned to the new treatment or a control within each trial subset (`subtrial' for short). Closed-form sample size formulae are derived to ensure each subtrial has a specified chance of correctly deciding whether the new treatment is superior to or not better than the control by some clinically relevant difference. Given pre-specified levels of pairwise (in)commensurability, the subtrial sample sizes are solved simultaneously. The proposed Bayesian approach resembles the frequentist formulation of the problem in yielding comparable sample sizes for circumstances of no borrowing. When borrowing is enabled between commensurate subtrials, a considerably smaller trial sample size is required compared to the widely implemented approach of no borrowing. We illustrate the use of our sample size formulae with two examples based on real basket trials. A comprehensive simulation study further shows that the proposed methodology can maintain the true positive and false positive rates at desired levels.

</details>

<details>

<summary>2022-05-24 19:23:05 - Bayesian model-based clustering for multiple network data</summary>

- *Anastasia Mantziou, Simon Lunagomez, Robin Mitra*

- `2107.03431v2` - [abs](http://arxiv.org/abs/2107.03431v2) - [pdf](http://arxiv.org/pdf/2107.03431v2)

> There is increasing appetite for analysing multiple network data. This is due to the fast-growing body of applications that demand such methods. These include: the study of connectomes in neuroscience and the study of human mobility with respect to intelligent displays in computer science. Recent technological advancements have allowed the collection of this type of data. Both applications entail the analysis of a heterogeneous population of networks. In this paper we focus on the problem of clustering the elements of a network population, here each cluster will be characterised by a network representative. We take advantage of the Bayesian machinery to simultaneously infer the cluster membership, the representatives and the community structure of the representatives. Extensive simulation studies show our model performs well in both clustering multiple network data and inferring the model parameters.

</details>

<details>

<summary>2022-05-24 20:45:44 - Bayesian Functional Principal Components Analysis using Relaxed Mutually Orthogonal Processes</summary>

- *James Matuk, Amy H. Herring, David B. Dunson*

- `2205.12361v1` - [abs](http://arxiv.org/abs/2205.12361v1) - [pdf](http://arxiv.org/pdf/2205.12361v1)

> Functional Principal Component Analysis (FPCA) is a prominent tool to characterize variability and reduce dimension of longitudinal and functional datasets. Bayesian implementations of FPCA are advantageous because of their ability to propagate uncertainty in subsequent modeling. To ease computation, many modeling approaches rely on the restrictive assumption that functional principal components can be represented through a pre-specified basis. Under this assumption, inference is sensitive to the basis, and misspecification can lead to erroneous results. Alternatively, we develop a flexible Bayesian FPCA model using Relaxed Mutually Orthogonal (ReMO) processes. We define ReMO processes to enforce mutual orthogonality between principal components to ensure identifiability of model parameters. The joint distribution of ReMO processes is governed by a penalty parameter that determines the degree to which the processes are mutually orthogonal and is related to ease of posterior computation. In comparison to other methods, FPCA using ReMO processes provides a more flexible, computationally convenient approach that facilitates accurate propagation of uncertainty. We demonstrate our proposed model using extensive simulation experiments and in an application to study the effects of breastfeeding status, illness, and demographic factors on weight dynamics in early childhood. Code is available on GitHub at https://github.com/jamesmatuk/ReMO-FPCA

</details>

<details>

<summary>2022-05-25 05:33:44 - The Fake News Effect: Experimentally Identifying Motivated Reasoning Using Trust in News</summary>

- *Michael Thaler*

- `2012.01663v4` - [abs](http://arxiv.org/abs/2012.01663v4) - [pdf](http://arxiv.org/pdf/2012.01663v4)

> Motivated reasoning posits that people distort how they process information in the direction of beliefs they find attractive. This paper creates a novel experimental design to identify motivated reasoning from Bayesian updating when people have preconceived beliefs. It analyzes how subjects assess the veracity of information sources that tell them the median of their belief distribution is too high or too low. Bayesians infer nothing about the source veracity, but motivated beliefs are evoked. Evidence supports politically-motivated reasoning about immigration, income mobility, crime, racial discrimination, gender, climate change, and gun laws. Motivated reasoning helps explain belief biases, polarization, and overconfidence.

</details>

<details>

<summary>2022-05-25 07:56:29 - Asymptotic Analysis of Statistical Estimators related to MultiGraphex Processes under Misspecification</summary>

- *Zacharie Naulet, Judith Rousseau, Franois Caron*

- `2107.01120v2` - [abs](http://arxiv.org/abs/2107.01120v2) - [pdf](http://arxiv.org/pdf/2107.01120v2)

> This article studies the asymptotic properties of Bayesian or frequentist estimators of a vector of parameters related to structural properties of sequences of graphs. The estimators studied originate from a particular class of graphex model introduced by Caron and Fox. The analysis is however performed here under very weak assumptions on the underlying data generating process, which may be different from the model of Caron and Fox or from a graphex model. In particular, we consider generic sparse graph models, with unbounded degree, whose degree distribution satisfies some assumptions. We show that one can relate the limit of the estimator of one of the parameters to the sparsity constant of the true graph generating process. When taking a Bayesian approach, we also show that the posterior distribution is asymptotically normal. We discuss situations where classical random graphs models such as configuration models, sparse graphon models, edge exchangeable models or graphon processes satisfy our assumptions.

</details>

<details>

<summary>2022-05-25 11:44:48 - Probabilistic model-error assessment of deep learning proxies: an application to real-time inversion of borehole electromagnetic measurements</summary>

- *Muzammil Hussain Rammay, Sergey Alyaev, Ahmed H Elsheikh*

- `2205.12684v1` - [abs](http://arxiv.org/abs/2205.12684v1) - [pdf](http://arxiv.org/pdf/2205.12684v1)

> The advent of fast sensing technologies allows for real-time model updates in many applications where the model parameters are uncertain. Bayesian algorithms, such as ensemble smoothers, offer a real-time probabilistic inversion accounting for uncertainties. However, they rely on the repeated evaluation of the computational models, and deep neural network (DNN) based proxies can be useful to address this computational bottleneck. This paper studies the effects of the approximate nature of the deep learned models and associated model errors during the inversion of extra-deep borehole electromagnetic (EM) measurements, which are critical for geosteering. Using a deep neural network (DNN) as a forward model allows us to perform thousands of model evaluations within seconds, which is very useful for quantifying uncertainties and non-uniqueness in real-time. While significant efforts are usually made to ensure the accuracy of the DNN models, it is known that they contain unknown model errors in the regions not covered by the training data. When DNNs are utilized during inversion of EM measurements, the effects of the model errors could manifest themselves as a bias in the estimated input parameters and, consequently, might result in a low-quality geosteering decision. We present numerical results highlighting the challenges associated with the inversion of EM measurements while neglecting model error. We further demonstrate the utility of a recently proposed flexible iterative ensemble smoother in reducing the effect of model bias by capturing the unknown model errors, thus improving the quality of the estimated subsurface properties for geosteering operation. Moreover, we describe a procedure for identifying inversion multimodality and propose possible solutions to alleviate it in real-time.

</details>

<details>

<summary>2022-05-25 17:21:42 - Clustering consistency with Dirichlet process mixtures</summary>

- *Filippo Ascolani, Antonio Lijoi, Giovanni Rebaudo, Giacomo Zanella*

- `2205.12924v1` - [abs](http://arxiv.org/abs/2205.12924v1) - [pdf](http://arxiv.org/pdf/2205.12924v1)

> Dirichlet process mixtures are flexible non-parametric models, particularly suited to density estimation and probabilistic clustering. In this work we study the posterior distribution induced by Dirichlet process mixtures as the sample size increases, and more specifically focus on consistency for the unknown number of clusters when the observed data are generated from a finite mixture. Crucially, we consider the situation where a prior is placed on the concentration parameter of the underlying Dirichlet process. Previous findings in the literature suggest that Dirichlet process mixtures are typically not consistent for the number of clusters if the concentration parameter is held fixed and data come from a finite mixture. Here we show that consistency for the number of clusters can be achieved if the concentration parameter is adapted in a fully Bayesian way, as commonly done in practice. Our results are derived for data coming from a class of finite mixtures, with mild assumptions on the prior for the concentration parameter and for a variety of choices of likelihood kernels for the mixture.

</details>

<details>

<summary>2022-05-25 19:22:24 - Spatial Cluster-based Copula Model to Interpolate Skewed Conditional Spatial Random Field</summary>

- *Debjoy Thakur, Ishapathik Das, Shubhashree Chakravarty*

- `2205.13024v1` - [abs](http://arxiv.org/abs/2205.13024v1) - [pdf](http://arxiv.org/pdf/2205.13024v1)

> Interpolating a skewed conditional spatial random field with missing data is cumbersome in the absence of Gaussianity assumptions. Maintaining spatial homogeneity and continuity around the observed random spatial point is also challenging, especially when interpolating along a spatial surface, focusing on the boundary points as a neighborhood. Otherwise, the point far away from one may appear the closest to another. As a result, importing the hierarchical clustering concept on the spatial random field is as convenient as developing the copula with the interface of the Expectation-Maximization algorithm and concurrently utilizing the idea of the Bayesian framework. This paper introduces a spatial cluster-based C-vine copula and a modified Gaussian kernel to derive a novel spatial probability distribution. Another investigation in this paper uses an algorithm in conjunction with a different parameter estimation technique to make spatial-based copula interpolation more compatible and efficient. We apply the proposed spatial interpolation approach to the air pollution of Delhi as a crucial circumstantial study to demonstrate this newly developed novel spatial estimation technique.

</details>

<details>

<summary>2022-05-25 22:22:52 - The Neural Testbed: Evaluating Joint Predictions</summary>

- *Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Botao Hao, Morteza Ibrahimi, Dieterich Lawson, Xiuyuan Lu, Brendan O'Donoghue, Benjamin Van Roy*

- `2110.04629v3` - [abs](http://arxiv.org/abs/2110.04629v3) - [pdf](http://arxiv.org/pdf/2110.04629v3)

> Predictive distributions quantify uncertainties ignored by point estimates. This paper introduces \textit{The Neural Testbed}: an open-source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process. Our results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community.

</details>

<details>

<summary>2022-05-26 00:51:12 - Optimal Neural Network Approximation of Wasserstein Gradient Direction via Convex Optimization</summary>

- *Yifei Wang, Peng Chen, Mert Pilanci, Wuchen Li*

- `2205.13098v1` - [abs](http://arxiv.org/abs/2205.13098v1) - [pdf](http://arxiv.org/pdf/2205.13098v1)

> The computation of Wasserstein gradient direction is essential for posterior sampling problems and scientific computing. The approximation of the Wasserstein gradient with finite samples requires solving a variational problem. We study the variational problem in the family of two-layer networks with squared-ReLU activations, towards which we derive a semi-definite programming (SDP) relaxation. This SDP can be viewed as an approximation of the Wasserstein gradient in a broader function family including two-layer networks. By solving the convex SDP, we obtain the optimal approximation of the Wasserstein gradient direction in this class of functions. Numerical experiments including PDE-constrained Bayesian inference and parameter estimation in COVID-19 modeling demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2022-05-26 12:11:15 - Gaussian Process Sampling and Optimization with Approximate Upper and Lower Bounds</summary>

- *Vu Nguyen, Marc Peter Deisenroth, Michael A. Osborne*

- `2110.12087v3` - [abs](http://arxiv.org/abs/2110.12087v3) - [pdf](http://arxiv.org/pdf/2110.12087v3)

> Many functions have approximately-known upper and/or lower bounds, potentially aiding the modeling of such functions. In this paper, we introduce Gaussian process models for functions where such bounds are (approximately) known. More specifically, we propose the first use of such bounds to improve Gaussian process (GP) posterior sampling and Bayesian optimization (BO). That is, we transform a GP model satisfying the given bounds, and then sample and weight functions from its posterior. To further exploit these bounds in BO settings, we present bounded entropy search (BES) to select the point gaining the most information about the underlying function, estimated by the GP samples, while satisfying the output constraints. We characterize the sample variance bounds and show that the decision made by BES is explainable. Our proposed approach is conceptually straightforward and can be used as a plug in extension to existing methods for GP posterior sampling and Bayesian optimization.

</details>

<details>

<summary>2022-05-26 14:24:23 - Bayesian Adaptive Selection of Basis Functions for Functional Data Representation</summary>

- *Pedro Henrique T. O. Sousa, Camila P. E. de Souza, Ronaldo Dias*

- `2204.03115v2` - [abs](http://arxiv.org/abs/2204.03115v2) - [pdf](http://arxiv.org/pdf/2204.03115v2)

> Considering the context of functional data analysis, we developed and applied a new Bayesian approach via Gibbs sampler to select basis functions for a finite representation of functional data. The proposed methodology uses Bernoulli latent variables to assign zero to some of the basis function coefficients with a positive probability. This procedure allows for an adaptive basis selection since it can determine the number of bases and which should be selected to represent functional data. Moreover, the proposed procedure measures the uncertainty of the selection process and can be applied to multiple curves simultaneously. The methodology developed can deal with observed curves that may differ due to experimental error and random individual differences between subjects, which one can observe in a real dataset application involving daily numbers of COVID-19 cases in Brazil. Simulation studies show the main properties of the proposed method, such as its accuracy in estimating the coefficients and the strength of the procedure to find the true set of basis functions. Despite having been developed in the context of functional data analysis, we also compared the proposed model via simulation with the well-established LASSO and Bayesian LASSO, which are methods developed for non-functional data.

</details>

<details>

<summary>2022-05-26 16:07:45 - Communicating with Anecdotes</summary>

- *Nika Haghtalab, Nicole Immorlica, Brendan Lucier, Markus Mobius, Divyarthi Mohan*

- `2205.13461v1` - [abs](http://arxiv.org/abs/2205.13461v1) - [pdf](http://arxiv.org/pdf/2205.13461v1)

> We study a communication game between a sender and receiver where the sender has access to a set of informative signals about a state of the world. The sender chooses one of her signals, called an ``anecdote'' and communicates it to the receiver. The receiver takes an action, yielding a utility for both players. Sender and receiver both care about the state of the world but are also influenced by a personal preference so that their ideal actions differ. We characterize perfect Bayesian equilibria when the sender cannot commit to a particular communication scheme. In this setting the sender faces ``persuasion temptation'': she is tempted to select a more biased anecdote to influence the receiver's action. Anecdotes are still informative to the receiver but persuasion comes at the cost of precision. This gives rise to ``informational homophily'' where the receiver prefers to listen to like-minded senders because they provide higher-precision signals. In particular, we show that a sender with access to many anecdotes will essentially send the minimum or maximum anecdote even though with high probability she has access to an anecdote close to the state of the world that would almost perfectly reveal it to the receiver. In contrast to the classic Crawford-Sobel model, full revelation is a knife-edge equilibrium and even small differences in personal preferences will induce highly polarized communication and a loss in utility for any equilibrium. We show that for fat-tailed anecdote distributions the receiver might even prefer to talk to poorly informed senders with aligned preferences rather than a knowledgeable expert whose preferences may differ from her own. We also show that under commitment differences in personal preferences no longer affect communication and the sender will generally report the most representative anecdote closest to the posterior mean for common distributions.

</details>

<details>

<summary>2022-05-26 17:10:28 - Censored Quantile Regression Neural Networks</summary>

- *Tim Pearce, Jong-Hyeon Jeong, Yichen Jia, Jun Zhu*

- `2205.13496v1` - [abs](http://arxiv.org/abs/2205.13496v1) - [pdf](http://arxiv.org/pdf/2205.13496v1)

> This paper considers doing quantile regression on censored data using neural networks (NNs). This adds to the survival analysis toolkit by allowing direct prediction of the target variable, along with a distribution-free characterisation of uncertainty, using a flexible function approximator. We begin by showing how an algorithm popular in linear models can be applied to NNs. However, the resulting procedure is inefficient, requiring sequential optimisation of an individual NN at each desired quantile. Our major contribution is a novel algorithm that simultaneously optimises a grid of quantiles output by a single NN. To offer theoretical insight into our algorithm, we show firstly that it can be interpreted as a form of expectation-maximisation, and secondly that it exhibits a desirable `self-correcting' property. Experimentally, the algorithm produces quantiles that are better calibrated than existing methods on 10 out of 12 real datasets.

</details>

<details>

<summary>2022-05-26 20:19:28 - Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods</summary>

- *Michael Whitehouse, Nick Whiteley, Lorenzo Rimella*

- `2205.13602v1` - [abs](http://arxiv.org/abs/2205.13602v1) - [pdf](http://arxiv.org/pdf/2205.13602v1)

> Addressing the challenge of scaling-up epidemiological inference to complex and heterogeneous models, we introduce Poisson Approximate Likelihood (PAL) methods. In contrast to the popular ODE approach to compartmental modelling, in which a large population limit is used to motivate a deterministic model, PALs are derived from approximate filtering equations for finite-population, stochastic compartmental models, and the large population limit drives the consistency of maximum PAL estimators. Our theoretical results appear to be the first likelihood-based parameter estimation consistency results applicable across a broad class of partially observed stochastic compartmental models. Compared to simulation-based methods such as Approximate Bayesian Computation and Sequential Monte Carlo, PALs are simple to implement, involving only elementary arithmetic operations and no tuning parameters; and fast to evaluate, requiring no simulation from the model and having computational cost independent of population size. Through examples, we demonstrate how PALs can be: embedded within Delayed Acceptance Particle Markov Chain Monte Carlo to facilitate Bayesian inference; used to fit an age-structured model of influenza, taking advantage of automatic differentiation in Stan; and applied to calibrate a spatial meta-population model of measles.

</details>

<details>

<summary>2022-05-26 20:35:41 - A Bayesian Model for Online Activity Sample Sizes</summary>

- *Thomas Richardson, Yu Liu, James McQueen, Doug Hains*

- `2111.12157v3` - [abs](http://arxiv.org/abs/2111.12157v3) - [pdf](http://arxiv.org/pdf/2111.12157v3)

> In many contexts it is useful to predict the number of individuals in some population who will initiate a particular activity during a given period. For example, the number of users who will install a software update, the number of customers who will use a new feature on a website or who will participate in an A/B test. In practical settings, there is heterogeneity amongst individuals with regard to the distribution of time until they will initiate. For these reasons it is inappropriate to assume that the number of new individuals observed on successive days will be identically distributed. Given observations on the number of unique users participating in an initial period, we present a simple but novel Bayesian method for predicting the number of additional individuals who will participate during a subsequent period. We illustrate the performance of the method in predicting sample size in online experimentation.

</details>

<details>

<summary>2022-05-26 21:28:25 - A Model Predictive Control Functional Continuous Time Bayesian Network for Self-Management of Multiple Chronic Conditions</summary>

- *Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, Susan P Fisher-Hoch, Joseph B Mccormick, Julian Carvajal Rico*

- `2205.13639v1` - [abs](http://arxiv.org/abs/2205.13639v1) - [pdf](http://arxiv.org/pdf/2205.13639v1)

> Multiple chronic conditions (MCC) are one of the biggest challenges of modern times. The evolution of MCC follows a complex stochastic process that is influenced by a variety of risk factors, ranging from pre-existing conditions to modifiable lifestyle behavioral factors (e.g. diet, exercise habits, tobacco use, alcohol use, etc.) to non-modifiable socio-demographic factors (e.g., age, gender, education, marital status, etc.). People with MCC are at an increased risk of new chronic conditions and mortality. This paper proposes a model predictive control functional continuous time Bayesian network, an online recursive method to examine the impact of various lifestyle behavioral changes on the emergence trajectories of MCC and generate strategies to minimize the risk of progression of chronic conditions in individual patients. The proposed method is validated based on the Cameron county Hispanic cohort (CCHC) dataset, which has a total of 385 patients. The dataset examines the emergence of 5 chronic conditions (diabetes, obesity, cognitive impairment, hyperlipidemia, and hypertension) based on four modifiable risk factors representing lifestyle behaviors (diet, exercise habits, tobacco use, alcohol use) and four non-modifiable risk factors, including socio-demographic information (age, gender, education, marital status). The proposed method is tested under different scenarios (e.g., age group, the prior existence of MCC), demonstrating the effective intervention strategies for improving the lifestyle behavioral risk factors to offset MCC evolution.

</details>

<details>

<summary>2022-05-26 21:50:46 - Comparing two samples through stochastic dominance: a graphical approach</summary>

- *Etor Arza, Josu Ceberio, Ekhie Irurozki, Aritz Prez*

- `2203.07889v2` - [abs](http://arxiv.org/abs/2203.07889v2) - [pdf](http://arxiv.org/pdf/2203.07889v2)

> Non-deterministic measurements are common in real-world scenarios: the performance of a stochastic optimization algorithm or the total reward of a reinforcement learning agent in a chaotic environment are just two examples in which unpredictable outcomes are common. These measures can be modeled as random variables and compared among each other via their expected values or more sophisticated tools such as null hypothesis statistical tests. In this paper, we propose an alternative framework to visually compare two samples according to their estimated cumulative distribution functions. First, we introduce a dominance measure for two random variables that quantifies the proportion in which the cumulative distribution function of one of the random variables scholastically dominates the other one. Then, we present a graphical method that decomposes in quantiles i) the proposed dominance measure and ii) the probability that one of the random variables takes lower values than the other. With illustrative purposes, we re-evaluate the experimentation of an already published work with the proposed methodology and we show that additional conclusions (missed by the rest of the methods) can be inferred. Additionally, the software package RVCompare was created as a convenient way of applying and experimenting with the proposed framework.

</details>

<details>

<summary>2022-05-26 23:41:43 - Fast variable selection makes scalable Gaussian process BSS-ANOVA a speedy and accurate choice for tabular and time series regression</summary>

- *David S. Mebane, Kyle Hayes, Ali Baheri*

- `2205.13676v1` - [abs](http://arxiv.org/abs/2205.13676v1) - [pdf](http://arxiv.org/pdf/2205.13676v1)

> Gaussian processes (GPs) are non-parametric regression engines with a long history. They are often overlooked in modern machine learning contexts because of scalability issues: regression for traditional GP kernels are $\mathcal{O}(N^3)$ where $N$ is the size of the dataset. One of a number of scalable GP approaches is the Karhunen-Lo\'eve (KL) decomposed kernel BSS-ANOVA, developed in 2009. It is $\mathcal{O}(NP)$ in training and $\mathcal{O}(P)$ per point in prediction, where $P$ is the number of terms in the ANOVA / KL expansion. A new method of forward variable selection, quickly and effectively limits the number of terms, yielding a method with competitive accuracies, training and inference times for large tabular datasets. The new algorithm balances model fidelity with model complexity using Bayesian and Akaike information criteria (BIC/AIC). The inference speed and accuracy makes the method especially useful for modeling dynamic systems in a model-free manner, by modeling the derivative in a dynamic system as a static problem, then integrating the learned dynamics using a high-order scheme. The methods are demonstrated on a `Susceptible, Infected, Recovered' (SIR) toy problem, with the transmissibility used as forcing function, along with the `Cascaded Tanks' benchmark dataset. Comparisons on the static prediction of derivatives are made with a Random Forest and Residual Neural Network, while for the timeseries prediction comparisons are made with LSTM and GRU recurrent neural networks. The GP outperforms the other methods in all modeling tasks on accuracy, while (in the case of the neural networks) performing many orders of magnitude fewer calculations. For the SIR test, which involved prediction for a set of forcing functions qualitatively different from those appearing in the training set, the GP captured the correct dynamics while the neural networks failed to do so.

</details>

<details>

<summary>2022-05-27 00:47:29 - Sparse Infinite Random Feature Latent Variable Modeling</summary>

- *Michael Minyi Zhang*

- `2205.09909v2` - [abs](http://arxiv.org/abs/2205.09909v2) - [pdf](http://arxiv.org/pdf/2205.09909v2)

> We propose a non-linear, Bayesian non-parametric latent variable model where the latent space is assumed to be sparse and infinite dimensional a priori using an Indian buffet process prior. A posteriori, the number of instantiated dimensions in the latent space is guaranteed to be finite. The purpose of placing the Indian buffet process on the latent variables is to: 1.) Automatically and probabilistically select the number of latent dimensions. 2.) Impose sparsity in the latent space, where the Indian buffet process will select which elements are exactly zero. Our proposed model allows for sparse, non-linear latent variable modeling where the number of latent dimensions is selected automatically. Inference is made tractable using the random Fourier approximation and we can easily implement posterior inference through Markov chain Monte Carlo sampling. This approach is amenable to many observation models beyond the Gaussian setting. We demonstrate the utility of our method on a variety of synthetic, biological and text datasets and show that we can obtain superior test set performance compared to previous latent variable models.

</details>

<details>

<summary>2022-05-27 01:23:11 - Characterizing the robustness of Bayesian adaptive experimental designs to active learning bias</summary>

- *Sabina J. Sloman, Daniel M. Oppenheimer, Stephen B. Broomell, Cosma Rohilla Shalizi*

- `2205.13698v1` - [abs](http://arxiv.org/abs/2205.13698v1) - [pdf](http://arxiv.org/pdf/2205.13698v1)

> Bayesian adaptive experimental design is a form of active learning, which chooses samples to maximize the information they give about uncertain parameters. Prior work has shown that other forms of active learning can suffer from active learning bias, where unrepresentative sampling leads to inconsistent parameter estimates. We show that active learning bias can also afflict Bayesian adaptive experimental design, depending on model misspecification. We develop an information-theoretic measure of misspecification, and show that worse misspecification implies more severe active learning bias. At the same time, model classes incorporating more "noise" - i.e., specifying higher inherent variance in observations - suffer less from active learning bias, because their predictive distributions are likely to overlap more with the true distribution. Finally, we show how these insights apply to a (simulated) preference learning experiment.

</details>

<details>

<summary>2022-05-27 05:58:54 - Small domain estimation of census coverage: A case study in Bayesian analysis of complex survey data</summary>

- *Joane S. Elleouet, Patrick Graham, Nikolai Kondratev, Abby K. Morgan, Rebecca M. Green*

- `2205.12769v2` - [abs](http://arxiv.org/abs/2205.12769v2) - [pdf](http://arxiv.org/pdf/2205.12769v2)

> Many countries conduct a full census survey to report official population statistics. As no census survey ever achieves 100 per cent response rate, a post-enumeration survey (PES) is usually conducted and analysed to assess census coverage and produce official population estimates by geographic area and demographic attributes. Considering the usually small size of PES, direct estimation at the desired level of disaggregation is not feasible. Design-based estimation with sampling weight adjustment is a commonly used method but is difficult to implement when survey non-response patterns cannot be fully documented and population benchmarks are not available. We overcome these limitations with a fully model-based Bayesian approach applied to the New Zealand PES. Although theory for the Bayesian treatment of complex surveys has been described, published applications of individual level Bayesian models for complex survey data remain scarce. We provide such an application through a case study of the 2018 census and PES surveys. We implement a multilevel model that accounts for the complex design of PES. We then illustrate how mixed posterior predictive checking and cross-validation can assist with model building and model selection. Finally, we discuss potential methodological improvements to the model and potential solutions to mitigate dependence between the two surveys.

</details>

<details>

<summary>2022-05-27 09:04:59 - A nonparametric Bayesian approach to the rare type match problem</summary>

- *Giulia Cereda, Richard D. Gill*

- `1908.02954v4` - [abs](http://arxiv.org/abs/1908.02954v4) - [pdf](http://arxiv.org/pdf/1908.02954v4)

> The "rare type match problem" is the situation in which the suspect's DNA profile, matching the DNA profile of the crime stain, is not in the database of reference. The evaluation of this match in the light of the two competing hypotheses (the crime stain has been left by the suspect or by another person) is based on the calculation of the likelihood ratio and depends on the population proportions of the DNA profiles, that are unknown. We propose a Bayesian nonparametric method that uses a two-parameter Poisson Dirichlet distribution as a prior over the ranked population proportions, and discards the information about the names of the different DNA profiles. This fits very well the data coming from European Y-STR DNA profiles, and the calculation of the likelihood ratio becomes quite simple thanks to a justified Empirical Bayes approach.

</details>

<details>

<summary>2022-05-27 11:06:56 - How Tempering Fixes Data Augmentation in Bayesian Neural Networks</summary>

- *Gregor Bachmann, Lorenzo Noci, Thomas Hofmann*

- `2205.13900v1` - [abs](http://arxiv.org/abs/2205.13900v1) - [pdf](http://arxiv.org/pdf/2205.13900v1)

> While Bayesian neural networks (BNNs) provide a sound and principled alternative to standard neural networks, an artificial sharpening of the posterior usually needs to be applied to reach comparable performance. This is in stark contrast to theory, dictating that given an adequate prior and a well-specified model, the untempered Bayesian posterior should achieve optimal performance. Despite the community's extensive efforts, the observed gains in performance still remain disputed with several plausible causes pointing at its origin. While data augmentation has been empirically recognized as one of the main drivers of this effect, a theoretical account of its role, on the other hand, is largely missing. In this work we identify two interlaced factors concurrently influencing the strength of the cold posterior effect, namely the correlated nature of augmentations and the degree of invariance of the employed model to such transformations. By theoretically analyzing simplified settings, we prove that tempering implicitly reduces the misspecification arising from modeling augmentations as i.i.d. data. The temperature mimics the role of the effective sample size, reflecting the gain in information provided by the augmentations. We corroborate our theoretical findings with extensive empirical evaluations, scaling to realistic BNNs. By relying on the framework of group convolutions, we experiment with models of varying inherent degree of invariance, confirming its hypothesized relationship with the optimal temperature.

</details>

<details>

<summary>2022-05-27 11:25:14 - Pseudo-Mallows for Efficient Probabilistic Preference Learning</summary>

- *Qinghua Liu, Valeria Vitelli, Carlo Mannino, Arnoldo Frigessi, Ida Scheel*

- `2205.13911v1` - [abs](http://arxiv.org/abs/2205.13911v1) - [pdf](http://arxiv.org/pdf/2205.13911v1)

> We propose the Pseudo-Mallows distribution over the set of all permutations of $n$ items, to approximate the posterior distribution with a Mallows likelihood. The Mallows model has been proven to be useful for recommender systems where it can be used to learn personal preferences from highly incomplete data provided by the users. Inference based on MCMC is however slow, preventing its use in real time applications. The Pseudo-Mallows distribution is a product of univariate discrete Mallows-like distributions, constrained to remain in the space of permutations. The quality of the approximation depends on the order of the $n$ items used to determine the factorization sequence. In a variational setting, we optimise the variational order parameter by minimising a marginalized KL-divergence. We propose an approximate algorithm for this discrete optimization, and conjecture a certain form of the optimal variational order that depends on the data. Empirical evidence and some theory support our conjecture. Sampling from the Pseudo-Mallows distribution allows fast preference learning, compared to alternative MCMC based options, when the data exists in form of partial rankings of the items or of clicking on some items. Through simulations and a real life data case study, we demonstrate that the Pseudo-Mallows model learns personal preferences very well and makes recommendations much more efficiently, while maintaining similar accuracy compared to the exact Bayesian Mallows model.

</details>

<details>

<summary>2022-05-27 11:41:17 - Distributionally Robust Bayesian Optimization with $$-divergences</summary>

- *Hisham Husain, Vu Nguyen, Anton van den Hengel*

- `2203.02128v2` - [abs](http://arxiv.org/abs/2203.02128v2) - [pdf](http://arxiv.org/pdf/2203.02128v2)

> The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. (2020), which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question Can one devise a computationally tractable algorithm for solving this DRO-BO problem? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\phi$-divergences, which subsumes many popular choices, such as the $\chi^2$-divergence, Total Variation, and the extant Kullback-Leibler (KL) divergence. We show that the DRO-BO problem in this setting is equivalent to a finite-dimensional optimization problem which, even in the continuous context setting, can be easily implemented with provable sublinear regret bounds. We then show experimentally that our method surpasses existing methods, attesting to the theoretical results

</details>

<details>

<summary>2022-05-27 15:39:29 - Stratified stochastic variational inference for high-dimensional network factor model</summary>

- *Emanuele Aliverti, Massimiliano Russo*

- `2006.14217v4` - [abs](http://arxiv.org/abs/2006.14217v4) - [pdf](http://arxiv.org/pdf/2006.14217v4)

> There has been considerable recent interest in Bayesian modeling of high-dimensional networks via latent space approaches. When the number of nodes increases, estimation based on Markov Chain Monte Carlo can be extremely slow and show poor mixing, thereby motivating research on alternative algorithms that scale well in high-dimensional settings. In this article, we focus on the latent factor model, a widely used approach for latent space modeling of network data. We develop scalable algorithms to conduct approximate Bayesian inference via stochastic optimization. Leveraging sparse representations of network data, the proposed algorithms show massive computational and storage benefits, and allow to conduct inference in settings with thousands of nodes.

</details>

<details>

<summary>2022-05-27 16:05:47 - Benchpress: A Scalable and Versatile Workflow for Benchmarking Structure Learning Algorithms</summary>

- *Felix L. Rios, Giusi Moffa, Jack Kuipers*

- `2107.03863v3` - [abs](http://arxiv.org/abs/2107.03863v3) - [pdf](http://arxiv.org/pdf/2107.03863v3)

> Describing the relationship between the variables in a study domain and modelling the data generating mechanism is a fundamental problem in many empirical sciences. Probabilistic graphical models are one common approach to tackle the problem. Learning the graphical structure for such models is computationally challenging and a fervent area of current research with a plethora of algorithms being developed. To facilitate the benchmarking of different methods, we present a novel Snakemake workflow, called Benchpress for producing scalable, reproducible, and platform-independent benchmarks of structure learning algorithms for probabilistic graphical models. Benchpress is interfaced via a simple JSON-file, which makes it accessible for all users, while the code is designed in a fully modular fashion to enable researchers to contribute additional methodologies. Benchpress currently provides an interface to a large number of state-of-the-art algorithms from libraries such as BDgraph, BiDAG, bnlearn, gCastle, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as a variety of methods for data generating models and performance evaluation. Alongside user-defined models and randomly generated datasets, the workflow also includes a number of standard datasets and graphical models from the literature, which may be included in a benchmarking study. We demonstrate the applicability of this workflow for learning Bayesian networks in five typical data scenarios. The source code and documentation is publicly available from http://github.com/felixleopoldo/benchpress.

</details>

<details>

<summary>2022-05-27 16:29:32 - Towards a Unified Framework for Uncertainty-aware Nonlinear Variable Selection with Theoretical Guarantees</summary>

- *Wenying Deng, Beau Coker, Rajarshi Mukherjee, Jeremiah Zhe Liu, Brent A. Coull*

- `2204.07293v2` - [abs](http://arxiv.org/abs/2204.07293v2) - [pdf](http://arxiv.org/pdf/2204.07293v2)

> We develop a simple and unified framework for nonlinear variable selection that incorporates uncertainty in the prediction function and is compatible with a wide range of machine learning models (e.g., tree ensembles, kernel methods, neural networks, etc). In particular, for a learned nonlinear model $f(\mathbf{x})$, we consider quantifying the importance of an input variable $\mathbf{x}^j$ using the integrated partial derivative $\Psi_j = \Vert \frac{\partial}{\partial \mathbf{x}^j} f(\mathbf{x})\Vert^2_{P_\mathcal{X}}$. We then (1) provide a principled approach for quantifying variable selection uncertainty by deriving its posterior distribution, and (2) show that the approach is generalizable even to non-differentiable models such as tree ensembles. Rigorous Bayesian nonparametric theorems are derived to guarantee the posterior consistency and asymptotic uncertainty of the proposed approach. Extensive simulations and experiments on healthcare benchmark datasets confirm that the proposed algorithm outperforms existing classic and recent variable selection methods.

</details>

<details>

<summary>2022-05-27 16:43:10 - Surrogate modeling for Bayesian optimization beyond a single Gaussian process</summary>

- *Qin Lu, Konstantinos D. Polyzos, Bingcong Li, Georgios B. Giannakis*

- `2205.14090v1` - [abs](http://arxiv.org/abs/2205.14090v1) - [pdf](http://arxiv.org/pdf/2205.14090v1)

> Bayesian optimization (BO) has well-documented merits for optimizing black-box functions with an expensive evaluation cost. Such functions emerge in applications as diverse as hyperparameter tuning, drug discovery, and robotics. BO hinges on a Bayesian surrogate model to sequentially select query points so as to balance exploration with exploitation of the search space. Most existing works rely on a single Gaussian process (GP) based surrogate model, where the kernel function form is typically preselected using domain knowledge. To bypass such a design process, this paper leverages an ensemble (E) of GPs to adaptively select the surrogate model fit on-the-fly, yielding a GP mixture posterior with enhanced expressiveness for the sought function. Acquisition of the next evaluation input using this EGP-based function posterior is then enabled by Thompson sampling (TS) that requires no additional design parameters. To endow function sampling with scalability, random feature-based kernel approximation is leveraged per GP model. The novel EGP-TS readily accommodates parallel operation. To further establish convergence of the proposed EGP-TS to the global optimum, analysis is conducted based on the notion of Bayesian regret for both sequential and parallel settings. Tests on synthetic functions and real-world applications showcase the merits of the proposed method.

</details>

<details>

<summary>2022-05-27 19:29:24 - Targeted Adaptive Design</summary>

- *Carlo Graziani, Marieme Ngom*

- `2205.14208v1` - [abs](http://arxiv.org/abs/2205.14208v1) - [pdf](http://arxiv.org/pdf/2205.14208v1)

> Modern advanced manufacturing and advanced materials design often require searches of relatively high-dimensional process control parameter spaces for settings that result in optimal structure, property, and performance parameters. The mapping from the former to the latter must be determined from noisy experiments or from expensive simulations. We abstract this problem to a mathematical framework in which an unknown function from a control space to a design space must be ascertained by means of expensive noisy measurements, which locate optimal control settings generating desired design features within specified tolerances, with quantified uncertainty. We describe targeted adaptive design (TAD), a new algorithm that performs this optimal sampling task. TAD creates a Gaussian process surrogate model of the unknown mapping at each iterative stage, proposing a new batch of control settings to sample experimentally and optimizing the updated log-predictive likelihood of the target design. TAD either stops upon locating a solution with uncertainties that fit inside the tolerance box or uses a measure of expected future information to determine that the search space has been exhausted with no solution. TAD thus embodies the exploration-exploitation tension in a manner that recalls, but is essentially different from, Bayesian optimization and optimal experimental design.

</details>

<details>

<summary>2022-05-27 21:21:03 - Deterministic Langevin Monte Carlo with Normalizing Flows for Bayesian Inference</summary>

- *Uros Seljak, Richard D. P. Grumitt, Biwei Dai*

- `2205.14240v1` - [abs](http://arxiv.org/abs/2205.14240v1) - [pdf](http://arxiv.org/pdf/2205.14240v1)

> We propose a general purpose Bayesian inference algorithm for expensive likelihoods, replacing the stochastic term in the Langevin equation with a deterministic density gradient term. The particle density is evaluated from the current particle positions using a Normalizing Flow (NF), which is differentiable and has good generalization properties in high dimensions. We take advantage of NF preconditioning and NF based Metropolis-Hastings updates for a faster and unbiased convergence. We show on various examples that the method is competitive against state of the art sampling methods.

</details>

<details>

<summary>2022-05-28 00:43:52 - Rethinking Bayesian Learning for Data Analysis: The Art of Prior and Inference in Sparsity-Aware Modeling</summary>

- *Lei Cheng, Feng Yin, Sergios Theodoridis, Sotirios Chatzis, Tsung-Hui Chang*

- `2205.14283v1` - [abs](http://arxiv.org/abs/2205.14283v1) - [pdf](http://arxiv.org/pdf/2205.14283v1)

> Sparse modeling for signal processing and machine learning has been at the focus of scientific research for over two decades. Among others, supervised sparsity-aware learning comprises two major paths paved by: a) discriminative methods and b) generative methods. The latter, more widely known as Bayesian methods, enable uncertainty evaluation w.r.t. the performed predictions. Furthermore, they can better exploit related prior information and naturally introduce robustness into the model, due to their unique capacity to marginalize out uncertainties related to the parameter estimates. Moreover, hyper-parameters associated with the adopted priors can be learnt via the training data. To implement sparsity-aware learning, the crucial point lies in the choice of the function regularizer for discriminative methods and the choice of the prior distribution for Bayesian learning. Over the last decade or so, due to the intense research on deep learning, emphasis has been put on discriminative techniques. However, a come back of Bayesian methods is taking place that sheds new light on the design of deep neural networks, which also establish firm links with Bayesian models and inspire new paths for unsupervised learning, such as Bayesian tensor decomposition.   The goal of this article is two-fold. First, to review, in a unified way, some recent advances in incorporating sparsity-promoting priors into three highly popular data modeling tools, namely deep neural networks, Gaussian processes, and tensor decomposition. Second, to review their associated inference techniques from different aspects, including: evidence maximization via optimization and variational inference methods. Challenges such as small data dilemma, automatic model structure search, and natural prediction uncertainty evaluation are also discussed. Typical signal processing and machine learning tasks are demonstrated.

</details>

<details>

<summary>2022-05-28 16:59:46 - Noise-Aware Statistical Inference with Differentially Private Synthetic Data</summary>

- *Ossi Ris, Joonas Jlk, Samuel Kaski, Antti Honkela*

- `2205.14485v1` - [abs](http://arxiv.org/abs/2205.14485v1) - [pdf](http://arxiv.org/pdf/2205.14485v1)

> While generation of synthetic data under differential privacy (DP) has received a lot of attention in the data privacy community, analysis of synthetic data has received much less. Existing work has shown that simply analysing DP synthetic data as if it were real does not produce valid inferences of population-level quantities. For example, confidence intervals become too narrow, which we demonstrate with a simple experiment. We tackle this problem by combining synthetic data analysis techniques from the field of multiple imputation, and synthetic data generation using noise-aware Bayesian modeling into a pipeline NA+MI that allows computing accurate uncertainty estimates for population-level quantities from DP synthetic data. To implement NA+MI for discrete data generation from marginal queries, we develop a novel noise-aware synthetic data generation algorithm NAPSU-MQ using the principle of maximum entropy. Our experiments demonstrate that the pipeline is able to produce accurate confidence intervals from DP synthetic data. The intervals become wider with tighter privacy to accurately capture the additional uncertainty stemming from DP noise.

</details>

<details>

<summary>2022-05-28 17:30:38 - Bayesian Graph Contrastive Learning</summary>

- *Arman Hasanzadeh, Mohammadreza Armandpour, Ehsan Hajiramezanali, Mingyuan Zhou, Nick Duffield, Krishna Narayanan*

- `2112.07823v3` - [abs](http://arxiv.org/abs/2112.07823v3) - [pdf](http://arxiv.org/pdf/2112.07823v3)

> Contrastive learning has become a key component of self-supervised learning approaches for graph-structured data. Despite their success, existing graph contrastive learning methods are incapable of uncertainty quantification for node representations or their downstream tasks, limiting their application in high-stakes domains. In this paper, we propose a novel Bayesian perspective of graph contrastive learning methods showing random augmentations leads to stochastic encoders. As a result, our proposed method represents each node by a distribution in the latent space in contrast to existing techniques which embed each node to a deterministic vector. By learning distributional representations, we provide uncertainty estimates in downstream graph analytics tasks and increase the expressive power of the predictive model. In addition, we propose a Bayesian framework to infer the probability of perturbations in each view of the contrastive model, eliminating the need for a computationally expensive search for hyperparameter tuning. We empirically show a considerable improvement in performance compared to existing state-of-the-art methods on several benchmark datasets.

</details>

<details>

<summary>2022-05-29 03:52:44 - Calibrated Predictive Distributions via Diagnostics for Conditional Coverage</summary>

- *Biprateep Dey, David Zhao, Jeffrey A. Newman, Brett H. Andrews, Rafael Izbicki, Ann B. Lee*

- `2205.14568v1` - [abs](http://arxiv.org/abs/2205.14568v1) - [pdf](http://arxiv.org/pdf/2205.14568v1)

> Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. A large body of work (including normalizing flows and Bayesian neural networks) has been devoted to describing the entire predictive distribution (PD) of a target variable Y given input features $\mathbf{X}$. However, off-the-shelf PDs are usually far from being conditionally calibrated; i.e., the probability of occurrence of an event given input $\mathbf{X}$ can be significantly different from the predicted probability. Most current research on predictive inference (such as conformal prediction) concerns constructing prediction sets, that do not only provide correct uncertainties on average over the entire population (that is, averaging over $\mathbf{X}$), but that are also approximately conditionally calibrated with accurate uncertainties for individual instances. It is often believed that the problem of obtaining and assessing entire conditionally calibrated PDs is too challenging to approach. In this work, we show that recalibration as well as validation are indeed attainable goals in practice. Our proposed method relies on the idea of regressing probability integral transform (PIT) scores against $\mathbf{X}$. This regression gives full diagnostics of conditional coverage across the entire feature space and can be used to recalibrate misspecified PDs. We benchmark our corrected prediction bands against oracle bands and state-of-the-art predictive inference algorithms for synthetic data, including settings with distributional shift and dependent high-dimensional sequence data. Finally, we demonstrate an application to the physical sciences in which we assess and produce calibrated PDs for measurements of galaxy distances using imaging data (i.e., photometric redshifts).

</details>

<details>

<summary>2022-05-29 15:29:06 - Quantum Bayesian Statistical Inference</summary>

- *Huayu Liu*

- `2204.08845v2` - [abs](http://arxiv.org/abs/2204.08845v2) - [pdf](http://arxiv.org/pdf/2204.08845v2)

> In this work a quantum analogue of Bayesian statistical inference is considered. Based on the notion of instrument, we propose a sequential measurement scheme from which observations needed for statistical inference are obtained. We further put forward a quantum analogue of Bayes rule, which states how the prior normal state of a quantum system updates under those observations. We next generalize the fundamental notions and results of Bayesian statistics according to the quantum Bayes rule. It is also note that our theory retains the classical one as its special case. Finally, we investigate the limit of posterior normal state as the number of observations tends to infinity.

</details>

<details>

<summary>2022-05-29 17:13:17 - Transfer Learning in Information Criteria-based Feature Selection</summary>

- *Shaohan Chen, Nikolaos V. Sahinidis, Chuanhou Gao*

- `2107.02847v2` - [abs](http://arxiv.org/abs/2107.02847v2) - [pdf](http://arxiv.org/pdf/2107.02847v2)

> This paper investigates the effectiveness of transfer learning based on Mallows' Cp. We propose a procedure that combines transfer learning with Mallows' Cp (TLCp) and prove that it outperforms the conventional Mallows' Cp criterion in terms of accuracy and stability. Our theoretical results indicate that, for any sample size in the target domain, the proposed TLCp estimator performs better than the Cp estimator by the mean squared error (MSE) metric in the case of orthogonal predictors, provided that i) the dissimilarity between the tasks from source domain and target domain is small, and ii) the procedure parameters (complexity penalties) are tuned according to certain explicit rules. Moreover, we show that our transfer learning framework can be extended to other feature selection criteria, such as the Bayesian information criterion. By analyzing the solution of the orthogonalized Cp, we identify an estimator that asymptotically approximates the solution of the Cp criterion in the case of non-orthogonal predictors. Similar results are obtained for the non-orthogonal TLCp. Finally, simulation studies and applications with real data demonstrate the usefulness of the TLCp scheme.

</details>

<details>

<summary>2022-05-30 06:59:06 - Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles</summary>

- *Jung-hun Kim, Se-Young Yun, Minchan Jeong, Jun Hyun Nam, Jinwoo Shin, Richard Combes*

- `1703.01347v2` - [abs](http://arxiv.org/abs/1703.01347v2) - [pdf](http://arxiv.org/pdf/1703.01347v2)

> We study contextual linear bandit problems under uncertainty on features; they are noisy with missing entries. To address the challenges from the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on noise characteristics, which is highly non-intuitive and does not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. We thus propose an algorithm aiming at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound with respect to feature dimension $d$ and time horizon $T$. We demonstrate the proposed algorithm using synthetic and real-world datasets.

</details>

<details>

<summary>2022-05-30 08:24:42 - VICE: Variational Interpretable Concept Embeddings</summary>

- *Lukas Muttenthaler, Charles Y. Zheng, Patrick McClure, Robert A. Vandermeulen, Martin N. Hebart, Francisco Pereira*

- `2205.00756v7` - [abs](http://arxiv.org/abs/2205.00756v7) - [pdf](http://arxiv.org/pdf/2205.00756v7)

> A central goal in the cognitive sciences is the development of numerical models for mental representations of object concepts. This paper introduces Variational Interpretable Concept Embeddings (VICE), an approximate Bayesian method for embedding object concepts in a vector space using data collected from humans in an odd-one-out triplet task. VICE uses variational inference to obtain sparse, non-negative representations of object concepts with uncertainty estimates for the embedding values. These estimates are used to automatically select the dimensions that best explain the data. We derive a PAC learning bound for VICE that can be used to estimate generalization performance or determine sufficient sample size in experimental design. VICE rivals or outperforms its predecessor, SPoSE, at predicting human behavior in the odd-one-out triplet task. Furthermore, VICE's object representations are more reproducible and consistent across random initializations.

</details>

<details>

<summary>2022-05-30 09:40:19 - Combining chains of Bayesian models with Markov melding</summary>

- *Andrew A. Manderson, Robert J. B. Goudie*

- `2111.11566v2` - [abs](http://arxiv.org/abs/2111.11566v2) - [pdf](http://arxiv.org/pdf/2111.11566v2)

> A challenge for practitioners of Bayesian inference is specifying a model that incorporates multiple relevant, heterogeneous data sets. It may be easier to instead specify distinct submodels for each source of data, then join the submodels together. We consider chains of submodels, where submodels directly relate to their neighbours via common quantities which may be parameters or deterministic functions thereof. We propose chained Markov melding, an extension of Markov melding, a generic method to combine chains of submodels into a joint model. One challenge we address is appropriately capturing the prior dependence between common quantities within a submodel, whilst also reconciling differences in priors for the same common quantity between two adjacent submodels. Estimating the posterior of the resulting overall joint model is also challenging, so we describe a sampler that uses the chain structure to incorporate information contained in the submodels in multiple stages, possibly in parallel. We demonstrate our methodology using two examples. The first example considers an ecological integrated population model, where multiple data sets are required to accurately estimate population immigration and reproduction rates. We also consider a joint longitudinal and time-to-event model with uncertain, submodel-derived event times. Chained Markov melding is a conceptually appealing approach to integrating submodels in these settings.

</details>

<details>

<summary>2022-05-30 10:25:34 - Fast Nonlinear Vector Quantile Regression</summary>

- *Aviv A. Rosenberg, Sanketh Vedula, Yaniv Romano, Alex M. Bronstein*

- `2205.14977v1` - [abs](http://arxiv.org/abs/2205.14977v1) - [pdf](http://arxiv.org/pdf/2205.14977v1)

> Quantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\mathrm{Y}$ given explanatory features $\boldsymbol{\mathrm{X}}$. A limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions. Recently, vector quantile regression (VQR) was proposed as an extension of QR for high-dimensional target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions. Despite its elegance, VQR is arguably not applicable in practice due to several limitations: (i) it assumes a linear model for the quantiles of the target $\mathrm{Y}$ given the features $\boldsymbol{\mathrm{X}}$; (ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels, or number of features, and its relaxed dual formulation may violate the monotonicity of the estimated quantiles; (iii) no fast or scalable solvers for VQR currently exist. In this work we fully address these limitations, namely: (i) We extend VQR to the non-linear case, showing substantial improvement over linear VQR; (ii) We propose vector monotone rearrangement, a method which ensures the estimates obtained by VQR relaxations are monotone functions; (iii) We provide fast, GPU-accelerated solvers for linear and nonlinear VQR which maintain a fixed memory footprint with number of samples and quantile levels, and demonstrate that they scale to millions of samples and thousands of quantile levels; (iv) We release an optimized python package of our solvers as to widespread the use of VQR in real-world applications.

</details>

<details>

<summary>2022-05-30 14:13:57 - Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks</summary>

- *Marvin Schmitt, Paul-Christian Brkner, Ullrich Kthe, Stefan T. Radev*

- `2112.08866v4` - [abs](http://arxiv.org/abs/2112.08866v4) - [pdf](http://arxiv.org/pdf/2112.08866v4)

> Recent advances in probabilistic deep learning enable amortized Bayesian inference in settings where the likelihood function is implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? In this paper, we conceptualize the types of model misspecification arising in simulation-based inference and systematically investigate the performance of SNPE-C (APT) and the BayesFlow framework under these misspecifications. We propose an augmented optimization objective which imposes a probabilistic structure on the learned latent data summary space and utilize maximum mean discrepancy (MMD) to detect potentially catastrophic misspecifications during inference undermining the validity of the obtained results. We verify our detection criterion on a number of artificial and realistic misspecifications, ranging from toy conjugate models to complex models of decision making and disease outbreak dynamics applied to real data. Further, we show that posterior inference errors increase when the distance between the latent summary distributions of the true data-generating process and the training simulations grows. Thus, we demonstrate the dual utility of MMD as a method for detecting model misspecification and as a proxy for verifying the faithfulness of amortized simulation-based Bayesian inference.

</details>

<details>

<summary>2022-05-30 14:17:56 - Generalizing Hierarchical Bayesian Bandits</summary>

- *Imad Aouali, Branislav Kveton, Sumeet Katariya*

- `2205.15124v1` - [abs](http://arxiv.org/abs/2205.15124v1) - [pdf](http://arxiv.org/pdf/2205.15124v1)

> A contextual bandit is a popular and practical framework for online learning to act under uncertainty. In many problems, the number of actions is huge and their mean rewards are correlated. In this work, we introduce a general framework for capturing such correlations through a two-level graphical model where actions are related through multiple shared latent parameters. We propose a Thompson sampling algorithm G-HierTS that uses this structure to explore efficiently and bound its Bayes regret. The regret has two terms, one for learning action parameters and the other for learning the shared latent parameters. The terms reflect the structure of our model as well as the quality of priors. Our theoretical findings are validated empirically using both synthetic and real-world problems. We also experiment with G-HierTS that maintains a factored posterior over latent parameters. While this approximation does not come with guarantees, it improves computational efficiency with a minimal impact on empirical regret.

</details>

<details>

<summary>2022-05-30 15:34:14 - Rethinking the Funding Line at the Swiss National Science Foundation: Bayesian Ranking and Lottery</summary>

- *Rachel Heyard, Manuela Ott, Georgia Salanti, Matthias Egger*

- `2102.09958v3` - [abs](http://arxiv.org/abs/2102.09958v3) - [pdf](http://arxiv.org/pdf/2102.09958v3)

> Funding agencies rely on peer review and expert panels to select the research deserving funding. Peer review has limitations, including bias against risky proposals or interdisciplinary research. The inter-rater reliability between reviewers and panels is low, particularly for proposals near the funding line. Funding agencies are also increasingly acknowledging the role of chance. The Swiss National Science Foundation (SNSF) introduced a lottery for proposals in the middle group of good but not excellent proposals. In this article, we introduce a Bayesian hierarchical model for the evaluation process. To rank the proposals, we estimate their expected ranks (ER), which incorporates both the magnitude and uncertainty of the estimated differences between proposals. A provisional funding line is defined based on ER and budget. The ER and its credible interval are used to identify proposals with similar quality and credible intervals that overlap with the funding line. These proposals are entered into a lottery. We illustrate the approach for two SNSF grant schemes in career and project funding. We argue that the method could reduce bias in the evaluation process. R code, data and other materials for this article are available online.

</details>

<details>

<summary>2022-05-30 18:22:47 - Infinite-dimensional optimization and Bayesian nonparametric learning of stochastic differential equations</summary>

- *Arnab Ganguly, Riten Mitra, Jinpu Zhou*

- `2205.15368v1` - [abs](http://arxiv.org/abs/2205.15368v1) - [pdf](http://arxiv.org/pdf/2205.15368v1)

> The paper has two major themes. The first part of the paper establishes certain general results for infinite-dimensional optimization problems on Hilbert spaces. These results cover the classical representer theorem and many of its variants as special cases and offer a wider scope of applications. The second part of the paper then develops a systematic approach for learning the drift function of a stochastic differential equation by integrating the results of the first part with Bayesian hierarchical framework. Importantly, our Baysian approach incorporates low-cost sparse learning through proper use of shrinkage priors while allowing proper quantification of uncertainty through posterior distributions. Several examples at the end illustrate the accuracy of our learning scheme.

</details>

<details>

<summary>2022-05-30 18:36:39 - Deep Bootstrap for Bayesian Inference</summary>

- *Lizhen Nie, Veronika Rockova*

- `2205.15374v1` - [abs](http://arxiv.org/abs/2205.15374v1) - [pdf](http://arxiv.org/pdf/2205.15374v1)

> For a Bayesian, the task to define the likelihood can be as perplexing as the task to define the prior. We focus on situations when the parameter of interest has been emancipated from the likelihood and is linked to data directly through a loss function. We survey existing work on both Bayesian parametric inference with Gibbs posteriors as well as Bayesian non-parametric inference. We then highlight recent bootstrap computational approaches to approximating loss-driven posteriors. In particular, we focus on implicit bootstrap distributions defined through an underlying push-forward mapping. We investigate iid samplers from approximate posteriors that pass random bootstrap weights trough a trained generative network. After training the deep-learning mapping, the simulation cost of such iid samplers is negligible. We compare the performance of these deep bootstrap samplers with exact bootstrap as well as MCMC on several examples (including support vector machines or quantile regression). We also provide theoretical insights into bootstrap posteriors by drawing upon connections to model mis-specification.

</details>

<details>

<summary>2022-05-30 20:34:44 - Fast Two-Stage Variational Bayesian Approach to Estimating Panel Spatial Autoregressive Models with Unrestricted Spatial Weights Matrices</summary>

- *Deborah Gefang, Stephen G. Hall, George S. Tavlas*

- `2205.15420v1` - [abs](http://arxiv.org/abs/2205.15420v1) - [pdf](http://arxiv.org/pdf/2205.15420v1)

> This paper proposes a fast two-stage variational Bayesian algorithm to estimating panel spatial autoregressive models with unknown spatial weights matrices. Using Dirichlet-Laplace global-local shrinkage priors, we are able to uncover the spatial impacts between cross-sectional units without imposing any a priori restrictions. Monte Carlo experiments show that our approach works well for both long and short panels. We are also the first in the literature to develop VB methods to estimate large covariance matrices with unrestricted sparsity patterns. The method is important in itself because of its relevance to other popular large data models such as Bayesian vector autoregressions. Matlab code is provided.

</details>

<details>

<summary>2022-05-31 06:50:20 - Fast Predictive Uncertainty for Classification with Bayesian Deep Networks</summary>

- *Marius Hobbhahn, Agustinus Kristiadi, Philipp Hennig*

- `2003.01227v4` - [abs](http://arxiv.org/abs/2003.01227v4) - [pdf](http://arxiv.org/pdf/2003.01227v4)

> In Bayesian Deep Learning, distributions over the output of classification neural networks are often approximated by first constructing a Gaussian distribution over the weights, then sampling from it to receive a distribution over the softmax outputs. This is costly. We reconsider old work (Laplace Bridge) to construct a Dirichlet approximation of this softmax output distribution, which yields an analytic map between Gaussian distributions in logit space and Dirichlet distributions (the conjugate prior to the Categorical distribution) in the output space. Importantly, the vanilla Laplace Bridge comes with certain limitations. We analyze those and suggest a simple solution that compares favorably to other commonly used estimates of the softmax-Gaussian integral. We demonstrate that the resulting Dirichlet distribution has multiple advantages, in particular, more efficient computation of the uncertainty estimate and scaling to large datasets and networks like ImageNet and DenseNet. We further demonstrate the usefulness of this Dirichlet approximation by using it to construct a lightweight uncertainty-aware output ranking for ImageNet.

</details>

<details>

<summary>2022-05-31 07:21:52 - Nested sampling for physical scientists</summary>

- *Greg Ashton, Noam Bernstein, Johannes Buchner, Xi Chen, Gbor Csnyi, Andrew Fowlie, Farhan Feroz, Matthew Griffiths, Will Handley, Michael Habeck, Edward Higson, Michael Hobson, Anthony Lasenby, David Parkinson, Livia B. Prtay, Matthew Pitkin, Doris Schneider, Joshua S. Speagle, Leah South, John Veitch, Philipp Wacker, David J. Wales, David Yallup*

- `2205.15570v1` - [abs](http://arxiv.org/abs/2205.15570v1) - [pdf](http://arxiv.org/pdf/2205.15570v1)

> We review Skilling's nested sampling (NS) algorithm for Bayesian inference and more broadly multi-dimensional integration. After recapitulating the principles of NS, we survey developments in implementing efficient NS algorithms in practice in high-dimensions, including methods for sampling from the so-called constrained prior. We outline the ways in which NS may be applied and describe the application of NS in three scientific fields in which the algorithm has proved to be useful: cosmology, gravitational-wave astronomy, and materials science. We close by making recommendations for best practice when using NS and by summarizing potential limitations and optimizations of NS.

</details>

<details>

<summary>2022-05-31 07:44:22 - Unlabelled landmark matching via Bayesian data selection, and application to cell matching across imaging modalities</summary>

- *Jessica E. Forsyth, Ali H. Al-Anbaki, Berenika Plusa, Simon L. Cotter*

- `2205.15167v2` - [abs](http://arxiv.org/abs/2205.15167v2) - [pdf](http://arxiv.org/pdf/2205.15167v2)

> We consider the problem of landmark matching between two unlabelled point sets, in particular where the number of points in each cloud may differ, and where points in each cloud may not have a corresponding match. We invoke a Bayesian framework to identify the transformation of coordinates that maps one cloud to the other, alongside correspondence of the points. This problem necessitates a novel methodology for Bayesian data selection; simultaneous inference of model parameters, and selection of the data which leads to the best fit of the model to the majority of the data. We apply this to a problem in developmental biology where the landmarks correspond to segmented cell centres, where potential death or division of cells can lead to discrepancies between the point-sets from each image. We validate the efficacy of our approach using in silico tests and a microinjected fluorescent marker experiment. Subsequently we apply our approach to the matching of cells between real time imaging and immunostaining experiments, facilitating the combination of single-cell data between imaging modalities. Furthermore our approach to Bayesian data selection is broadly applicable across data science, and has the potential to change the way we think about fitting models to data.

</details>

<details>

<summary>2022-05-31 09:50:12 - Normalized Latent Measure Factor Models</summary>

- *Mario Beraha, Jim E. Griffin*

- `2205.15654v1` - [abs](http://arxiv.org/abs/2205.15654v1) - [pdf](http://arxiv.org/pdf/2205.15654v1)

> We propose a methodology for modeling and comparing probability distributions within a Bayesian nonparametric framework. Building on dependent normalized random measures, we consider a prior distribution for a collection of discrete random measures where each measure is a linear combination of a set of latent measures, interpretable as characteristic traits shared by different distributions, with positive random weights. The model is non-identified and a method for post-processing posterior samples to achieve identified inference is developed. This uses Riemannian optimization to solve a non-trivial optimization problem over a Lie group of matrices. The effectiveness of our approach is validated on simulated data and in two applications to two real-world data sets: school student test scores and personal incomes in California. Our approach leads to interesting insights for populations and easily interpretable posterior inference

</details>

<details>

<summary>2022-05-31 12:04:05 - Optimally adaptive Bayesian spectral density estimation for stationary and nonstationary processes</summary>

- *Nick James, Max Menzies*

- `2003.02367v3` - [abs](http://arxiv.org/abs/2003.02367v3) - [pdf](http://arxiv.org/pdf/2003.02367v3)

> This article improves on existing methods to estimate the spectral density of stationary and nonstationary time series assuming a Gaussian process prior. By optimising an appropriate eigendecomposition using a smoothing spline covariance structure, our method more appropriately models data with both simple and complex periodic structure. We further justify the utility of this optimal eigendecomposition by investigating the performance of alternative covariance functions other than smoothing splines. We show that the optimal eigendecomposition provides a material improvement, while the other covariance functions under examination do not, all performing comparatively well as the smoothing spline. During our computational investigation, we introduce new validation metrics for the spectral density estimate, inspired from the physical sciences. We validate our models in an extensive simulation study and demonstrate superior performance with real data.

</details>

<details>

<summary>2022-05-31 12:04:44 - Estimating a density near an unknown manifold: a Bayesian nonparametric approach</summary>

- *Clment Berenfeld, Paul Rosa, Judith Rousseau*

- `2205.15717v1` - [abs](http://arxiv.org/abs/2205.15717v1) - [pdf](http://arxiv.org/pdf/2205.15717v1)

> We study the Bayesian density estimation of data living in the offset of an unknown submanifold of the Euclidean space. In this perspective, we introduce a new notion of anisotropic H\"older for the underlying density and obtain posterior rates that are minimax optimal and adaptive to the regularity of the density, to the intrinsic dimension of the manifold, and to the size of the offset, provided that the latter is not too small -- while still allowed to go to zero. Our Bayesian procedure, based on location-scale mixtures of Gaussians, appears to be convenient to implement and yields good practical results, even for quite singular data.

</details>

<details>

<summary>2022-05-31 12:36:18 - Variable importance without impossible data</summary>

- *Masayoshi Mase, Art B. Owen, Benjamin B. Seiler*

- `2205.15750v1` - [abs](http://arxiv.org/abs/2205.15750v1) - [pdf](http://arxiv.org/pdf/2205.15750v1)

> The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. A feature is important if using it to narrow the cohort makes a large difference to the cohort mean. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on. For every subject and every predictor variable, we can compute the importance of that predictor to the subject's predicted response or to their actual response. These values can be aggregated, for example over all Black subjects, and we propose a Bayesian bootstrap to quantify uncertainty in both individual and aggregate Shapley values.

</details>

<details>

<summary>2022-05-31 12:45:10 - QLSD: Quantised Langevin stochastic dynamics for Bayesian federated learning</summary>

- *Maxime Vono, Vincent Plassier, Alain Durmus, Aymeric Dieuleveut, Eric Moulines*

- `2106.00797v3` - [abs](http://arxiv.org/abs/2106.00797v3) - [pdf](http://arxiv.org/pdf/2106.00797v3)

> The objective of Federated Learning (FL) is to perform statistical inference for data which are decentralised and stored locally on networked clients. FL raises many constraints which include privacy and data ownership, communication overhead, statistical heterogeneity, and partial client participation. In this paper, we address these problems in the framework of the Bayesian paradigm. To this end, we propose a novel federated Markov Chain Monte Carlo algorithm, referred to as Quantised Langevin Stochastic Dynamics which may be seen as an extension to the FL setting of Stochastic Gradient Langevin Dynamics, which handles the communication bottleneck using gradient compression. To improve performance, we then introduce variance reduction techniques, which lead to two improved versions coined \texttt{QLSD}$^{\star}$ and \texttt{QLSD}$^{++}$. We give both non-asymptotic and asymptotic convergence guarantees for the proposed algorithms. We illustrate their performances using various Bayesian Federated Learning benchmarks.

</details>

<details>

<summary>2022-05-31 13:32:55 - Likelihood-Free Inference with Generative Neural Networks via Scoring Rule Minimization</summary>

- *Lorenzo Pacchiardi, Ritabrata Dutta*

- `2205.15784v1` - [abs](http://arxiv.org/abs/2205.15784v1) - [pdf](http://arxiv.org/pdf/2205.15784v1)

> Bayesian Likelihood-Free Inference methods yield posterior approximations for simulator models with intractable likelihood. Recently, many works trained neural networks to approximate either the intractable likelihood or the posterior directly. Most proposals use normalizing flows, namely neural networks parametrizing invertible maps used to transform samples from an underlying base measure; the probability density of the transformed samples is then accessible and the normalizing flow can be trained via maximum likelihood on simulated parameter-observation pairs. A recent work [Ramesh et al., 2022] approximated instead the posterior with generative networks, which drop the invertibility requirement and are thus a more flexible class of distributions scaling to high-dimensional and structured data. However, generative networks only allow sampling from the parametrized distribution; for this reason, Ramesh et al. [2022] follows the common solution of adversarial training, where the generative network plays a min-max game against a "critic" network. This procedure is unstable and can lead to a learned distribution underestimating the uncertainty - in extreme cases collapsing to a single point. Here, we propose to approximate the posterior with generative networks trained by Scoring Rule minimization, an overlooked adversarial-free method enabling smooth training and better uncertainty quantification. In simulation studies, the Scoring Rule approach yields better performances with shorter training time with respect to the adversarial framework.

</details>

<details>

<summary>2022-05-31 15:47:51 - Robust prediction of failure time through unified Bayesian analysis of nonparametric transformation models</summary>

- *Chong Zhong, Junshan Shen, Jin Yang, Catherine Liu*

- `2205.14504v2` - [abs](http://arxiv.org/abs/2205.14504v2) - [pdf](http://arxiv.org/pdf/2205.14504v2)

> Nonparametric transformation models (NTMs) have sparked much interest in survival prediction owing to their flexibility with both transformations and error distributions unspecified. However, fitting these models has been hampered because they are unidentified. Existing approaches typically constrain the parameter space to ensure identifiablity, but they incur intractable computation and cannot scale up to complex data; other approaches address the identifiablity issue by making strong \textit{a priori} assumptions on either of the nonparametric components, and thus are subject to misspecifications. Utilizing a Bayesian workflow, we address the challenge by constructing new weakly informative nonparametric priors for infinite-dimensional parameters so as to remedy flat likelihoods associated with unidentified models. To facilitate applicability of these new priors, we subtly impose an exponential transformation on top of NTMs, which compresses the space of infinite-dimensional parameters to positive quadrants while maintaining interpretability. We further develop a cutting-edge posterior modification technique for estimating the fully identified parametric component. Simulations reveal that our method is robust and outperforms the competing methods, and an application to a Veterans lung cancer dataset suggests that our method can predict survival time well and help develop clinically meaningful risk scores, based on patients' demographic and clinical predictors.

</details>

<details>

<summary>2022-05-31 15:53:15 - Variational inference via Wasserstein gradient flows</summary>

- *Marc Lambert, Sinho Chewi, Francis Bach, Silvre Bonnabel, Philippe Rigollet*

- `2205.15902v1` - [abs](http://arxiv.org/abs/2205.15902v1) - [pdf](http://arxiv.org/pdf/2205.15902v1)

> Along with Markov chain Monte Carlo (MCMC) methods, variational inference (VI) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior $\pi$, VI aims at producing a simple but effective approximation $\hat \pi$ to $\pi$ for which summary statistics are easy to compute. However, unlike the well-studied MCMC methodology, VI is still poorly understood and dominated by heuristics. In this work, we propose principled methods for VI, in which $\hat \pi$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures-Wasserstein space of Gaussian measures. Akin to MCMC, it comes with strong theoretical guarantees when $\pi$ is log-concave.

</details>

<details>

<summary>2022-05-31 17:13:31 - Bayesian Modeling of Marketing Attribution</summary>

- *Ritwik Sinha, David Arbour, Aahlad Manas Puli*

- `2205.15965v1` - [abs](http://arxiv.org/abs/2205.15965v1) - [pdf](http://arxiv.org/pdf/2205.15965v1)

> In a multi-channel marketing world, the purchase decision journey encounters many interactions (e.g., email, mobile notifications, display advertising, social media, and so on). These impressions have direct (main effects), as well as interactive influence on the final decision of the customer. To maximize conversions, a marketer needs to understand how each of these marketing efforts individually and collectively affect the customer's final decision. This insight will help her optimize the advertising budget over interacting marketing channels. This problem of interpreting the influence of various marketing channels to the customer's decision process is called marketing attribution. We propose a Bayesian model of marketing attribution that captures established modes of action of advertisements, including the direct effect of the ad, decay of the ad effect, interaction between ads, and customer heterogeneity. Our model allows us to incorporate information from customer's features and provides usable error bounds for parameters of interest, like the ad effect or the half-life of an ad. We apply our model on a real-world dataset and evaluate its performance against alternatives in simulations.

</details>

<details>

<summary>2022-05-31 17:18:20 - A wavelet-based method in aggregated functional data analysis</summary>

- *Alex Rodrigo dos Santos Sousa*

- `2205.15969v1` - [abs](http://arxiv.org/abs/2205.15969v1) - [pdf](http://arxiv.org/pdf/2205.15969v1)

> In this paper we consider aggregated functional data composed by a linear combination of component curves and the problem of estimating these component curves. We propose the application of a bayesian wavelet shrinkage rule based on a mixture of a point mass function at zero and the logistic distribution as prior to wavelet coefficients to estimate mean curves of components. This procedure has the advantage of estimating component functions with important local characteristics such as discontinuities, spikes and oscillations for example, due the features of wavelet basis expansion of functions. Simulation studies were done to evaluate the performance of the proposed method and its results are compared with a spline-based method. An application on the so called tecator dataset is also provided.

</details>

<details>

<summary>2022-05-31 18:00:09 - Online PAC-Bayes Learning</summary>

- *Maxime Haddouche, Benjamin Guedj*

- `2206.00024v1` - [abs](http://arxiv.org/abs/2206.00024v1) - [pdf](http://arxiv.org/pdf/2206.00024v1)

> Most PAC-Bayesian bounds hold in the batch learning setting where data is collected at once, prior to inference or prediction. This somewhat departs from many contemporary learning problems where data streams are collected and the algorithms must dynamically adjust. We prove new PAC-Bayesian bounds in this online learning framework, leveraging an updated definition of regret, and we revisit classical PAC-Bayesian results with a batch-to-online conversion, extending their remit to the case of dependent data. Our results hold for bounded losses, potentially \emph{non-convex}, paving the way to promising developments in online learning.

</details>

<details>

<summary>2022-05-31 19:48:30 - Parallel Tempering With a Variational Reference</summary>

- *Nikola Surjanovic, Saifuddin Syed, Alexandre Bouchard-Ct, Trevor Campbell*

- `2206.00080v1` - [abs](http://arxiv.org/abs/2206.00080v1) - [pdf](http://arxiv.org/pdf/2206.00080v1)

> Sampling from complex target distributions is a challenging task fundamental to Bayesian inference. Parallel tempering (PT) addresses this problem by constructing a Markov chain on the expanded state space of a sequence of distributions interpolating between the posterior distribution and a fixed reference distribution, which is typically chosen to be the prior. However, in the typical case where the prior and posterior are nearly mutually singular, PT methods are computationally prohibitive. In this work we address this challenge by constructing a generalized annealing path connecting the posterior to an adaptively tuned variational reference. The reference distribution is tuned to minimize the forward (inclusive) KL divergence to the posterior distribution using a simple, gradient-free moment-matching procedure. We show that our adaptive procedure converges to the forward KL minimizer, and that the forward KL divergence serves as a good proxy to a previously developed measure of PT performance. We also show that in the large-data limit in typical Bayesian models, the proposed method improves in performance, while traditional PT deteriorates arbitrarily. Finally, we introduce PT with two references -- one fixed, one variational -- with a novel split annealing path that ensures stable variational reference adaptation. The paper concludes with experiments that demonstrate the large empirical gains achieved by our method in a wide range of realistic Bayesian inference scenarios.

</details>

<details>

<summary>2022-05-31 20:06:27 - Easy Variational Inference for Categorical Models via an Independent Binary Approximation</summary>

- *Michael T. Wojnowicz, Shuchin Aeron, Eric L. Miller, Michael C. Hughes*

- `2206.00093v1` - [abs](http://arxiv.org/abs/2206.00093v1) - [pdf](http://arxiv.org/pdf/2206.00093v1)

> We pursue tractable Bayesian analysis of generalized linear models (GLMs) for categorical data. Thus far, GLMs are difficult to scale to more than a few dozen categories due to non-conjugacy or strong posterior dependencies when using conjugate auxiliary variable methods. We define a new class of GLMs for categorical data called categorical-from-binary (CB) models. Each CB model has a likelihood that is bounded by the product of binary likelihoods, suggesting a natural posterior approximation. This approximation makes inference straightforward and fast; using well-known auxiliary variables for probit or logistic regression, the product of binary models admits conjugate closed-form variational inference that is embarrassingly parallel across categories and invariant to category ordering. Moreover, an independent binary model simultaneously approximates multiple CB models. Bayesian model averaging over these can improve the quality of the approximation for any given dataset. We show that our approach scales to thousands of categories, outperforming posterior estimation competitors like Automatic Differentiation Variational Inference (ADVI) and No U-Turn Sampling (NUTS) in the time required to achieve fixed prediction quality.

</details>

<details>

<summary>2022-05-31 21:25:13 - Bayesian Multiscale Analysis of the Cox Model</summary>

- *Bo Y. -C. Ning, Ismal Castillo*

- `2205.12489v2` - [abs](http://arxiv.org/abs/2205.12489v2) - [pdf](http://arxiv.org/pdf/2205.12489v2)

> Piecewise constant priors are routinely used in the Bayesian Cox proportional hazards model for survival analysis. Despite its popularity, large sample properties of this Bayesian method are not yet well understood. This work provides a unified theory for posterior distributions in this setting, not requiring the priors to be conjugate. We first derive contraction rate results for wide classes of histogram priors on the unknown hazard function and prove asymptotic normality of linear functionals of the posterior hazard in the form of Bernstein--von Mises theorems. Second, using recently developed multiscale techniques, we derive functional limiting results for the cumulative hazard and survival function. Frequentist coverage properties of Bayesian credible sets are investigated: we prove that certain easily computable credible bands for the survival function are optimal frequentist confidence bands. We conduct simulation studies that confirm these predictions, with an excellent behavior particularly in finite samples, showing that even simplest possible Bayesian credible bands for the survival function can outperform state-of-the-art frequentist bands in terms of coverage.

</details>

<details>

<summary>2022-05-31 22:28:25 - Parametric quantile autoregressive moving average models with exogenous terms applied to Walmart sales data</summary>

- *Alan Dasilva, Helton Saulo, Roberto Vila, Jose A. Fiorucci, Suvra Pal*

- `2206.00132v1` - [abs](http://arxiv.org/abs/2206.00132v1) - [pdf](http://arxiv.org/pdf/2206.00132v1)

> Parametric autoregressive moving average models with exogenous terms (ARMAX) have been widely used in the literature. Usually, these models consider a conditional mean or median dynamics, which limits the analysis. In this paper, we introduce a class of quantile ARMAX models based on log-symmetric distributions. This class is indexed by quantile and dispersion parameters. It not only accommodates the possibility to model bimodal and/or light/heavy-tailed distributed data but also accommodates heteroscedasticity. We estimate the model parameters by using the conditional maximum likelihood method. Furthermore, we carry out an extensive Monte Carlo simulation study to evaluate the performance of the proposed models and the estimation method in retrieving the true parameter values. Finally, the proposed class of models and the estimation method are applied to a dataset on the competition "M5 Forecasting - Accuracy" that corresponds to the daily sales history of several Walmart products. The results indicate that the proposed log-symmetric quantile ARMAX models have good performance in terms of model fitting and forecasting.

</details>


## 2022-06

<details>

<summary>2022-06-01 00:06:13 - The causal foundations of applied probability and statistics</summary>

- *Sander Greenland*

- `2011.02677v5` - [abs](http://arxiv.org/abs/2011.02677v5) - [pdf](http://arxiv.org/pdf/2011.02677v5)

> Statistical science (as opposed to mathematical statistics) involves far more than probability theory, for it requires realistic causal models of data generators - even for purely descriptive goals. Statistical decision theory requires more causality: Rational decisions are actions taken to minimize costs while maximizing benefits, and thus require explication of causes of loss and gain. Competent statistical practice thus integrates logic, context, and probability into scientific inference and decision using narratives filled with causality. This reality was seen and accounted for intuitively by the founders of modern statistics, but was not well recognized in the ensuing statistical theory (which focused instead on the causally inert properties of probability measures). Nonetheless, both statistical foundations and basic statistics can and should be taught using formal causal models. The causal view of statistical science fits within a broader information-processing framework which illuminates and unifies frequentist, Bayesian, and related probability-based foundations of statistics. Causality theory can thus be seen as a key component connecting computation to contextual information, not extra-statistical but instead essential for sound statistical training and applications.

</details>

<details>

<summary>2022-06-01 01:50:34 - Pseudo Bayesian Estimation of One-way ANOVA Model in Complex Surveys</summary>

- *Terrance D. Savitsky, Matthew R. Williams, Sanvesh Srivastava*

- `2004.06191v4` - [abs](http://arxiv.org/abs/2004.06191v4) - [pdf](http://arxiv.org/pdf/2004.06191v4)

> We devise survey-weighted pseudo posterior distribution estimators under two-stage informative sampling of both primary clusters and secondary nested units for a one-way analysis of variance (ANOVA) population generating model as a simple canonical case where population model random effects are defined to be coincident with the primary clusters, for example student performance based on a survey of schools and students such as the 2000 OECD Programme for International Student Assessment (PISA). We consider estimation on an observed informative sample under both an augmented pseudo likelihood that co-samples the random effects, as well as an integrated likelihood that marginalizes out the random effects from the survey-weighted augmented pseudo likelihood. This paper includes a theoretical exposition that enumerates easily verified conditions for which estimation under the augmented pseudo posterior is guaranteed to be consistent at the true generating parameters. We reveal in simulation that both approaches produce asymptotically unbiased estimation of the generating hyperparameters for the random effects when a key condition on the sum of within cluster weighted residuals is met. We present a comparison with two frequentist alternatives, an expectation-maximization approach and a composite likelihood method that requires pairwise sampling weights.

</details>

<details>

<summary>2022-06-01 05:09:20 - Continuous Prediction with Experts' Advice</summary>

- *Victor Sanches Portella, Christopher Liaw, Nicholas J. A. Harvey*

- `2206.00236v1` - [abs](http://arxiv.org/abs/2206.00236v1) - [pdf](http://arxiv.org/pdf/2206.00236v1)

> Prediction with experts' advice is one of the most fundamental problems in online learning and captures many of its technical challenges. A recent line of work has looked at online learning through the lens of differential equations and continuous-time analysis. This viewpoint has yielded optimal results for several problems in online learning.   In this paper, we employ continuous-time stochastic calculus in order to study the discrete-time experts' problem. We use these tools to design a continuous-time, parameter-free algorithm with improved guarantees for the quantile regret. We then develop an analogous discrete-time algorithm with a very similar analysis and identical quantile regret bounds. Finally, we design an anytime continuous-time algorithm with regret matching the optimal fixed-time rate when the gains are independent Brownian Motions; in many settings, this is the most difficult case. This gives some evidence that, even with adversarial gains, the optimal anytime and fixed-time regrets may coincide.

</details>

<details>

<summary>2022-06-01 05:47:06 - Asymptotic Properties for Bayesian Neural Network in Besov Space</summary>

- *Kyeongwon Lee, Jaeyong Lee*

- `2206.00241v1` - [abs](http://arxiv.org/abs/2206.00241v1) - [pdf](http://arxiv.org/pdf/2206.00241v1)

> Neural networks have shown great predictive power when dealing with various unstructured data such as images and natural languages. The Bayesian neural network captures the uncertainty of prediction by putting a prior distribution for the parameter of the model and computing the posterior distribution. In this paper, we show that the Bayesian neural network using spike-and-slab prior has consistency with nearly minimax convergence rate when the true regression function is in the Besov space. Even when the smoothness of the regression function is unknown the same posterior convergence rate holds and thus the spike and slab prior is adaptive to the smoothness of the regression function. We also consider the shrinkage prior and show that it has the same convergence rate. In other words, we propose a practical Bayesian neural network with guaranteed asymptotic properties.

</details>

<details>

<summary>2022-06-01 06:22:38 - A model aggregation approach for high-dimensional large-scale optimization</summary>

- *Haowei Wang, Ercong Zhang, Szu Hui Ng, Giulia Pedrielli*

- `2205.07525v2` - [abs](http://arxiv.org/abs/2205.07525v2) - [pdf](http://arxiv.org/pdf/2205.07525v2)

> Bayesian optimization (BO) has been widely used in machine learning and simulation optimization. With the increase in computational resources and storage capacities in these fields, high-dimensional and large-scale problems are becoming increasingly common. In this study, we propose a model aggregation method in the Bayesian optimization (MamBO) algorithm for efficiently solving high-dimensional large-scale optimization problems. MamBO uses a combination of subsampling and subspace embeddings to collectively address high dimensionality and large-scale issues; in addition, a model aggregation method is employed to address the surrogate model uncertainty issue that arises when embedding is applied. This surrogate model uncertainty issue is largely ignored in the embedding literature and practice, and it is exacerbated when the problem is high-dimensional and data are limited. Our proposed model aggregation method reduces these lower-dimensional surrogate model risks and improves the robustness of the BO algorithm. We derive an asymptotic bound for the proposed aggregated surrogate model and prove the convergence of MamBO. Benchmark numerical experiments indicate that our algorithm achieves superior or comparable performance to other commonly used high-dimensional BO algorithms. Moreover, we apply MamBO to a cascade classifier of a machine learning algorithm for face detection, and the results reveal that MamBO finds settings that achieve higher classification accuracy than the benchmark settings and is computationally faster than other high-dimensional BO algorithms.

</details>

<details>

<summary>2022-06-01 08:47:15 - Algorithmic Fairness Verification with Graphical Models</summary>

- *Bishwamittra Ghosh, Debabrota Basu, Kuldeep S. Meel*

- `2109.09447v2` - [abs](http://arxiv.org/abs/2109.09447v2) - [pdf](http://arxiv.org/pdf/2109.09447v2)

> In recent years, machine learning (ML) algorithms have been deployed in safety-critical and high-stake decision-making, where the fairness of algorithms is of paramount importance. Fairness in ML centers on detecting bias towards certain demographic populations induced by an ML classifier and proposes algorithmic solutions to mitigate the bias with respect to different fairness definitions. To this end, several fairness verifiers have been proposed that compute the bias in the prediction of an ML classifier--essentially beyond a finite dataset--given the probability distribution of input features. In the context of verifying linear classifiers, existing fairness verifiers are limited by accuracy due to imprecise modeling of correlations among features and scalability due to restrictive formulations of the classifiers as SSAT/SMT formulas or by sampling.   In this paper, we propose an efficient fairness verifier, called FVGM, that encodes the correlations among features as a Bayesian network. In contrast to existing verifiers, FVGM proposes a stochastic subset-sum based approach for verifying linear classifiers. Experimentally, we show that FVGM leads to an accurate and scalable assessment for more diverse families of fairness-enhancing algorithms, fairness attacks, and group/causal fairness metrics than the state-of-the-art fairness verifiers. We also demonstrate that FVGM facilitates the computation of fairness influence functions as a stepping stone to detect the source of bias induced by subsets of features.

</details>

<details>

<summary>2022-06-01 13:41:51 - Bayesian Monitoring of COVID-19 in Sweden</summary>

- *Robin Marin, Hkan Runvik, Alexander Medvedev, Stefan Engblom*

- `2205.00859v3` - [abs](http://arxiv.org/abs/2205.00859v3) - [pdf](http://arxiv.org/pdf/2205.00859v3)

> In an effort to provide regional decision support for the public healthcare, we design a data-driven compartment-based model of COVID-19 in Sweden. From national hospital statistics we derive parameter priors, and we develop linear filtering techniques to drive the simulations given data in the form of daily healthcare demands.   We additionally propose a posterior marginal estimator which enables a refined resolution of the reproduction number estimate, and which also improves substantially on our confidence in the overall results thanks to a parametric bootstrap procedure.   From our computational approach we obtain a Bayesian model of predictive value which provides important insight into the progression of the disease, including estimates of the effective reproduction number, the infection fatality rate, and the regional-level immunity. We successfully validate our posterior model against several different sources, including outputs from extensive screening programs. Since our required data in comparison is easy and non-sensitive to collect, we argue that our approach is particularly promising as a tool to support monitoring and decisions within public health.

</details>

<details>

<summary>2022-06-01 15:18:03 - Drift vs Shift: Decoupling Trends and Changepoint Analysis</summary>

- *Haoxuan Wu, Sean Ryan, David S. Matteson*

- `2201.06606v2` - [abs](http://arxiv.org/abs/2201.06606v2) - [pdf](http://arxiv.org/pdf/2201.06606v2)

> We introduce a new approach for decoupling trends (drift) and changepoints (shifts) in time series. Our locally adaptive model-based approach for robustly decoupling combines Bayesian trend filtering and machine learning based regularization. An over-parameterized Bayesian dynamic linear model (DLM) is first applied to characterize drift. Then a weighted penalized likelihood estimator is paired with the estimated DLM posterior distribution to identify shifts. We show how Bayesian DLMs specified with so-called shrinkage priors can provide smooth estimates of underlying trends in the presence of complex noise components. However, their inability to shrink exactly to zero inhibits direct changepoint detection. In contrast, penalized likelihood methods are highly effective in locating changepoints. However, they require data with simple patterns in both signal and noise. The proposed decoupling approach combines the strengths of both, i.e.\ the flexibility of Bayesian DLMs with the hard thresholding property of penalized likelihood estimators, to provide changepoint analysis in complex, modern settings. The proposed framework is outlier robust and can identify a variety of changes, including in mean and slope. It is also easily extended for analysis of parameter shifts in time-varying parameter models like dynamic regressions. We illustrate the flexibility and contrast the performance and robustness of our approach with several alternative methods across a wide range of simulations and application examples.

</details>

<details>

<summary>2022-06-01 18:44:58 - Data Augmentation MCMC for Bayesian Inference from Privatized Data</summary>

- *Nianqiao Ju, Jordan A. Awan, Ruobin Gong, Vinayak A. Rao*

- `2206.00710v1` - [abs](http://arxiv.org/abs/2206.00710v1) - [pdf](http://arxiv.org/pdf/2206.00710v1)

> Differentially private mechanisms protect privacy by introducing additional randomness into the data. Restricting access to only the privatized data makes it challenging to perform valid statistical inference on parameters underlying the confidential data. Specifically, the likelihood function of the privatized data requires integrating over the large space of confidential databases and is typically intractable. For Bayesian analysis, this results in a posterior distribution that is doubly intractable, rendering traditional MCMC techniques inapplicable. We propose an MCMC framework to perform Bayesian inference from the privatized data, which is applicable to a wide range of statistical models and privacy mechanisms. Our MCMC algorithm augments the model parameters with the unobserved confidential data, and alternately updates each one conditional on the other. For the potentially challenging step of updating the confidential data, we propose a generic approach that exploits the privacy guarantee of the mechanism to ensure efficiency. In particular, we give results on the computational complexity, acceptance rate, and mixing properties of our MCMC. We illustrate the efficacy and applicability of our methods on a na\"ive-Bayes log-linear model as well as on a linear regression model.

</details>

<details>

<summary>2022-06-01 19:10:41 - Bayesian Inference for the Multinomial Probit Model under Gaussian Prior Distribution</summary>

- *Augusto Fasano, Giovanni Rebaudo, Niccol Anceschi*

- `2206.00720v1` - [abs](http://arxiv.org/abs/2206.00720v1) - [pdf](http://arxiv.org/pdf/2206.00720v1)

> Multinomial probit (mnp) models are fundamental and widely-applied regression models for categorical data. Fasano and Durante (2022) proved that the class of unified skew-normal distributions is conjugate to several mnp sampling models. This allows to develop Monte Carlo samplers and accurate variational methods to perform Bayesian inference. In this paper, we adapt the abovementioned results for a popular special case: the discrete-choice mnp model under zero mean and independent Gaussian priors. This allows to obtain simplified expressions for the parameters of the posterior distribution and an alternative derivation for the variational algorithm that gives a novel understanding of the fundamental results in Fasano and Durante (2022) as well as computational advantages in our special settings.

</details>

<details>

<summary>2022-06-01 20:41:10 - Bayesian sample size determination for causal discovery</summary>

- *Federico Castelletti, Guido Consonni*

- `2206.00755v1` - [abs](http://arxiv.org/abs/2206.00755v1) - [pdf](http://arxiv.org/pdf/2206.00755v1)

> Graphical models based on Directed Acyclic Graphs (DAGs) are widely used to answer causal questions across a variety of scientific and social disciplines. However, observational data alone cannot distinguish in general between DAGs representing the same conditional independence assertions (Markov equivalent DAGs); as a consequence the orientation of some edges in the graph remains indeterminate. Interventional data, produced by exogenous manipulations of variables in the network, enhance the process of structure learning because they allow to distinguish among equivalent DAGs, thus sharpening causal inference. Starting from an equivalence class of DAGs, a few procedures have been devised to produce a collection of variables to be manipulated in order to identify a causal DAG. Yet, these algorithmic approaches do not determine the sample size of the interventional data required to obtain a desired level of statistical accuracy. We tackle this problem from a Bayesian experimental design perspective, taking as input a sequence of target variables to be manipulated to identify edge orientation. We then propose a method to determine, at each intervention, the optimal sample size capable of producing a successful experiment based on a pre-experimental evaluation of the overall probability of substantial correct evidence.

</details>

<details>

<summary>2022-06-01 20:57:40 - On Some Properties of the Beta Normal Distribution</summary>

- *L. C. Rgo, R. J. Cintra, G. M. Cordeiro*

- `2206.00762v1` - [abs](http://arxiv.org/abs/2206.00762v1) - [pdf](http://arxiv.org/pdf/2206.00762v1)

> The beta normal distribution is a generalization of both the normal distribution and the normal order statistics. Some of its mathematical properties and a few applications have been studied in the literature. We provide a better foundation for some properties and an analytical study of its bimodality. The hazard rate function and the limiting behavior are examined. We derive explicit expressions for moments, generating function, mean deviations using a power series expansion for the quantile function, and Shannon entropy.

</details>

<details>

<summary>2022-06-01 22:57:52 - Sequential Bayesian Neural Subnetwork Ensembles</summary>

- *Sanket Jantre, Sandeep Madireddy, Shrijita Bhattacharya, Tapabrata Maiti, Prasanna Balaprakash*

- `2206.00794v1` - [abs](http://arxiv.org/abs/2206.00794v1) - [pdf](http://arxiv.org/pdf/2206.00794v1)

> Deep neural network ensembles that appeal to model diversity have been used successfully to improve predictive performance and model robustness in several applications. Whereas, it has recently been shown that sparse subnetworks of dense models can match the performance of their dense counterparts and increase their robustness while effectively decreasing the model complexity. However, most ensembling techniques require multiple parallel and costly evaluations and have been proposed primarily with deterministic models, whereas sparsity induction has been mostly done through ad-hoc pruning. We propose sequential ensembling of dynamic Bayesian neural subnetworks that systematically reduce model complexity through sparsity-inducing priors and generate diverse ensembles in a single forward pass of the model. The ensembling strategy consists of an exploration phase that finds high-performing regions of the parameter space and multiple exploitation phases that effectively exploit the compactness of the sparse model to quickly converge to different minima in the energy landscape corresponding to high-performing subnetworks yielding diverse ensembles. We empirically demonstrate that our proposed approach surpasses the baselines of the dense frequentist and Bayesian ensemble models in prediction accuracy, uncertainty estimation, and out-of-distribution (OoD) robustness on CIFAR10, CIFAR100 datasets, and their out-of-distribution variants: CIFAR10-C, CIFAR100-C induced by corruptions. Furthermore, we found that our approach produced the most diverse ensembles compared to the approaches with a single forward pass and even compared to the approaches with multiple forward passes in some cases.

</details>

<details>

<summary>2022-06-02 00:06:56 - Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective</summary>

- *Keitaro Sakamoto, Issei Sato*

- `2205.07320v2` - [abs](http://arxiv.org/abs/2205.07320v2) - [pdf](http://arxiv.org/pdf/2205.07320v2)

> The lottery ticket hypothesis (LTH) has attracted attention because it can explain why over-parameterized models often show high generalization ability. It is known that when we use iterative magnitude pruning (IMP), which is an algorithm to find sparse networks with high generalization ability that can be trained from the initial weights independently, called winning tickets, the initial large learning rate does not work well in deep neural networks such as ResNet. However, since the initial large learning rate generally helps the optimizer to converge to flatter minima, we hypothesize that the winning tickets have relatively sharp minima, which is considered a disadvantage in terms of generalization ability. In this paper, we confirm this hypothesis and show that the PAC-Bayesian theory can provide an explicit understanding of the relationship between LTH and generalization behavior. On the basis of our experimental findings that flatness is useful for improving accuracy and robustness to label noise and that the distance from the initial weights is deeply involved in winning tickets, we offer the PAC-Bayes bound using a spike-and-slab distribution to analyze winning tickets. Finally, we revisit existing algorithms for finding winning tickets from a PAC-Bayesian perspective and provide new insights into these methods.

</details>

<details>

<summary>2022-06-02 02:59:55 - Masked Bayesian Neural Networks : Computation and Optimality</summary>

- *Insung Kong, Dongyoon Yang, Jongjin Lee, Ilsang Ohn, Yongdai Kim*

- `2206.00853v1` - [abs](http://arxiv.org/abs/2206.00853v1) - [pdf](http://arxiv.org/pdf/2206.00853v1)

> As data size and computing power increase, the architectures of deep neural networks (DNNs) have been getting more complex and huge, and thus there is a growing need to simplify such complex and huge DNNs. In this paper, we propose a novel sparse Bayesian neural network (BNN) which searches a good DNN with an appropriate complexity. We employ the masking variables at each node which can turn off some nodes according to the posterior distribution to yield a nodewise sparse DNN. We devise a prior distribution such that the posterior distribution has theoretical optimalities (i.e. minimax optimality and adaptiveness), and develop an efficient MCMC algorithm. By analyzing several benchmark datasets, we illustrate that the proposed BNN performs well compared to other existing methods in the sense that it discovers well condensed DNN architectures with similar prediction accuracy and uncertainty quantification compared to large DNNs.

</details>

<details>

<summary>2022-06-02 03:24:51 - Multifidelity multilevel Monte Carlo to accelerate approximate Bayesian parameter inference for partially observed stochastic processes</summary>

- *David J. Warne, Thomas P. Prescott, Ruth E. Baker, Matthew J. Simpson*

- `2110.14082v2` - [abs](http://arxiv.org/abs/2110.14082v2) - [pdf](http://arxiv.org/pdf/2110.14082v2)

> Models of stochastic processes are widely used in almost all fields of science. Theory validation, parameter estimation, and prediction all require model calibration and statistical inference using data. However, data are almost always incomplete observations of reality. This leads to a great challenge for statistical inference because the likelihood function will be intractable for almost all partially observed stochastic processes. This renders many statistical methods, especially within a Bayesian framework, impossible to implement. Therefore, computationally expensive likelihood-free approaches are applied that replace likelihood evaluations with realisations of the model and observation process. For accurate inference, however, likelihood-free techniques may require millions of expensive stochastic simulations. To address this challenge, we develop a new method based on recent advances in multilevel and multifidelity. Our approach combines the multilevel Monte Carlo telescoping summation, applied to a sequence of approximate Bayesian posterior targets, with a multifidelity rejection sampler to minimise the number of computationally expensive exact simulations required for accurate inference. We present the derivation of our new algorithm for likelihood-free Bayesian inference, discuss practical implementation details, and demonstrate substantial performance improvements. Using examples from systems biology, we demonstrate improvements of more than two orders of magnitude over standard rejection sampling techniques. Our approach is generally applicable to accelerate other sampling schemes, such as sequential Monte Carlo, to enable feasible Bayesian analysis for realistic practical applications in physics, chemistry, biology, epidemiology, ecology and economics.

</details>

<details>

<summary>2022-06-02 04:35:25 - Spatiotemporal models for Poisson areal data with an application to the AIDS epidemic in Rio de Janeiro</summary>

- *Marco A. R. Ferreira, Juan C. Vivar*

- `2206.00869v1` - [abs](http://arxiv.org/abs/2206.00869v1) - [pdf](http://arxiv.org/pdf/2206.00869v1)

> We present a class of spatiotemporal models for Poisson areal data suitable for the analysis of emerging infectious diseases. These models assume Poisson observations related through a link equation to a latent random field process. This latent random field process evolves through time with proper Gaussian Markov random field convolutions. Our approach naturally accommodates flexible structures such as distinct but interacting temporal trends for each region and across-time contamination among neighboring regions. We develop a Bayesian analysis approach with a simulation-based procedure: specifically, we construct a Markov chain Monte Carlo algorithm based on the generalized extended Kalman filter to obtain samples from an approximate posterior distribution. Finally, for the comparison of Poisson spatiotemporal models, we develop a simulation-based conditional Bayes factor. We illustrate the utility and flexibility of our Poisson spatiotemporal framework with an application to the number of acquired immunodeficiency syndrome (AIDS) cases during the period 1982-2007 in Rio de Janeiro.

</details>

<details>

<summary>2022-06-02 09:16:26 - Feature Space Particle Inference for Neural Network Ensembles</summary>

- *Shingo Yashima, Teppei Suzuki, Kohta Ishikawa, Ikuro Sato, Rei Kawakami*

- `2206.00944v1` - [abs](http://arxiv.org/abs/2206.00944v1) - [pdf](http://arxiv.org/pdf/2206.00944v1)

> Ensembles of deep neural networks demonstrate improved performance over single models. For enhancing the diversity of ensemble members while keeping their performance, particle-based inference methods offer a promising approach from a Bayesian perspective. However, the best way to apply these methods to neural networks is still unclear: seeking samples from the weight-space posterior suffers from inefficiency due to the over-parameterization issues, while seeking samples directly from the function-space posterior often results in serious underfitting. In this study, we propose optimizing particles in the feature space where the activation of a specific intermediate layer lies to address the above-mentioned difficulties. Our method encourages each member to capture distinct features, which is expected to improve ensemble prediction robustness. Extensive evaluation on real-world datasets shows that our model significantly outperforms the gold-standard Deep Ensembles on various metrics, including accuracy, calibration, and robustness. Code is available at https://github.com/DensoITLab/featurePI .

</details>

<details>

<summary>2022-06-02 12:12:24 - Excess risk analysis for epistemic uncertainty with application to variational inference</summary>

- *Futoshi Futami, Tomoharu Iwata, Naonori Ueda, Issei Sato, Masashi Sugiyama*

- `2206.01606v1` - [abs](http://arxiv.org/abs/2206.01606v1) - [pdf](http://arxiv.org/pdf/2206.01606v1)

> We analyze the epistemic uncertainty (EU) of supervised learning in Bayesian inference by focusing on the excess risk. Existing analysis is limited to the Bayesian setting, which assumes a correct model and exact Bayesian posterior distribution. Thus we cannot apply the existing theory to modern Bayesian algorithms, such as variational inference. To address this, we present a novel EU analysis in the frequentist setting, where data is generated from an unknown distribution. We show a relation between the generalization ability and the widely used EU measurements, such as the variance and entropy of the predictive distribution. Then we show their convergence behaviors theoretically. Finally, we propose new variational inference that directly controls the prediction and EU evaluation performances based on the PAC-Bayesian theory. Numerical experiments show that our algorithm significantly improves the EU evaluation over the existing methods.

</details>

<details>

<summary>2022-06-02 12:22:12 - Bayesian high-dimensional covariate selection in non-linear mixed-effects models using the SAEM algorithm</summary>

- *Maud Delattre, Guillaume Kon Kam King, Marion Naveau, Laure Sansonnet*

- `2206.01012v1` - [abs](http://arxiv.org/abs/2206.01012v1) - [pdf](http://arxiv.org/pdf/2206.01012v1)

> High-dimensional data, with many more covariates than observations, such as genomic data for example, are now commonly analysed. In this context, it is often desirable to be able to focus on the few most relevant covariates through a variable selection procedure. High-dimensional variable selection is widely documented in standard regression models, but there are still few tools to address it in the context of non-linear mixed-effects models. In this work, variable selection is approached from a Bayesian perspective and a selection procedure is proposed, combining the use of spike-and-slab priors and the SAEM algorithm. Similarly to LASSO regression, the set of relevant covariates is selected by exploring a grid of values for the penalisation parameter. The proposed approach is much faster than a classical MCMC algorithm and shows very good selection performances on simulated data.

</details>

<details>

<summary>2022-06-02 15:55:50 - Discovery of interpretable structural model errors by combining Bayesian sparse regression and data assimilation: A chaotic Kuramoto-Sivashinsky test case</summary>

- *Rambod Mojgani, Ashesh Chattopadhyay, Pedram Hassanzadeh*

- `2110.00546v2` - [abs](http://arxiv.org/abs/2110.00546v2) - [pdf](http://arxiv.org/pdf/2110.00546v2)

> Models of many engineering and natural systems are imperfect. The discrepancy between the mathematical representations of a true physical system and its imperfect model is called the model error. These model errors can lead to substantial differences between the numerical solutions of the model and the state of the system, particularly in those involving nonlinear, multi-scale phenomena. Thus, there is increasing interest in reducing model errors, particularly by leveraging the rapidly growing observational data to understand their physics and sources. Here, we introduce a framework named MEDIDA: Model Error Discovery with Interpretability and Data Assimilation. MEDIDA only requires a working numerical solver of the model and a small number of noise-free or noisy sporadic observations of the system. In MEDIDA, first the model error is estimated from differences between the observed states and model-predicted states (the latter are obtained from a number of one-time-step numerical integrations from the previous observed states). If observations are noisy, a data assimilation (DA) technique such as ensemble Kalman filter (EnKF) is employed to provide the analysis state of the system, which is then used to estimate the model error. Finally, an equation-discovery technique, here the relevance vector machine (RVM), a sparsity-promoting Bayesian method, is used to identify an interpretable, parsimonious, and closed-form representation of the model error. Using the chaotic Kuramoto-Sivashinsky (KS) system as the test case, we demonstrate the excellent performance of MEDIDA in discovering different types of structural/parametric model errors, representing different types of missing physics, using noise-free and noisy observations.

</details>

<details>

<summary>2022-06-02 17:10:24 - Bayesian Model Selection, the Marginal Likelihood, and Generalization</summary>

- *Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Goldblum, Andrew Gordon Wilson*

- `2202.11678v2` - [abs](http://arxiv.org/abs/2202.11678v2) - [pdf](http://arxiv.org/pdf/2202.11678v2)

> How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.

</details>

<details>

<summary>2022-06-02 18:06:14 - On Some Properties of the Beta Inverse Rayleigh Distribution</summary>

- *J. Leo, H. Saulo, M. Bourguignon, R. J. Cintra, L. C. Rgo, G. M. Cordeiro*

- `2206.01229v1` - [abs](http://arxiv.org/abs/2206.01229v1) - [pdf](http://arxiv.org/pdf/2206.01229v1)

> We study with some details a lifetime model of the class of beta generalized models, called the beta inverse Rayleigh distribution, which is a special case of the Beta Fr\'echet distribution. We provide a better foundation for some properties including quantile function, moments, mean deviations, Bonferroni and Lorenz curves, R\'enyi and Shannon entropies and order statistics. We fit the proposed model using maximum likelihood estimation to a real data set to illustrate its flexibility and potentiality.

</details>

<details>

<summary>2022-06-03 01:18:45 - Structure Learning for Hybrid Bayesian Networks</summary>

- *Wanchuang Zhu, Ngoc Lan Chi Nguyen, Sally Cripps*

- `2206.01356v1` - [abs](http://arxiv.org/abs/2206.01356v1) - [pdf](http://arxiv.org/pdf/2206.01356v1)

> Bayesian networks have been used as a mechanism to represent the joint distribution of multiple random variables in a flexible yet interpretable manner. One major challenge in learning the structure of a network is how to model networks which include a mixture of continuous and discrete random variables, known as hybrid Bayesian networks. This paper reviews the literature on approaches to handle hybrid Bayesian networks. When working with hybrid Bayesian networks, typically one of two approaches is taken: either the data are considered to have a joint multivariate Gaussian distribution, irrespective of the true distribution, or continuous random variables are discretized, resulting in discrete Bayesian networks. In this paper, we show that a strategy to model all random variables as Gaussian outperforms the strategy which converts the continuous random variables to discrete. We demonstrate the superior performance of our strategy over the latter, theoretically and by simulation studies for various settings. Both strategies are also implemented on a childhood obesity data set. The two different strategies give rise to significant differences in the optimal graph structures, with the results of the simulation study suggesting that the inference from the strategy assuming all random variables are Gaussian is more reliable.

</details>

<details>

<summary>2022-06-03 07:49:26 - Segmenting Time Series via Self-Normalization</summary>

- *Zifeng Zhao, Feiyu Jiang, Xiaofeng Shao*

- `2112.05331v2` - [abs](http://arxiv.org/abs/2112.05331v2) - [pdf](http://arxiv.org/pdf/2112.05331v2)

> We propose a novel and unified framework for change-point estimation in multivariate time series. The proposed method is fully nonparametric, enjoys effortless tuning and is robust to temporal dependence. One salient and distinct feature of the proposed method is its versatility, where it allows change-point detection for a broad class of parameters (such as mean, variance, correlation and quantile) in a unified fashion. At the core of our method, we couple the self-normalization (SN) based tests with a novel nested local-window segmentation algorithm, which seems new in the growing literature of change-point analysis. Due to the presence of an inconsistent long-run variance estimator in the SN test, non-standard theoretical arguments are further developed to derive the consistency and convergence rate of the proposed SN-based change-point detection method. Extensive numerical experiments and relevant real data analysis are conducted to illustrate the effectiveness and broad applicability of our proposed method in comparison with state-of-the-art approaches in the literature.

</details>

<details>

<summary>2022-06-03 10:18:17 - A Bayesian Analysis of Two-Stage Randomized Experiments in the Presence of Interference, Treatment Nonadherence, and Missing Outcomes</summary>

- *Yuki Ohnishi, Arman Sabbaghi*

- `2110.10216v3` - [abs](http://arxiv.org/abs/2110.10216v3) - [pdf](http://arxiv.org/pdf/2110.10216v3)

> Three critical issues for causal inference that often occur in modern, complicated experiments are interference, treatment nonadherence, and missing outcomes. A great deal of research efforts has been dedicated to developing causal inferential methodologies that address these issues separately. However, methodologies that can address these issues simultaneously are lacking. We propose a Bayesian causal inference methodology to address this gap. Our methodology extends existing causal frameworks and methods, specifically, two-staged randomized experiments and the principal stratification framework. In contrast to existing methods that invoke strong structural assumptions to identify principal causal effects, our Bayesian approach uses flexible distributional models that can accommodate the complexities of interference and missing outcomes, and that ensure that principal causal effects are weakly identifiable. We illustrate our methodology via simulation studies and a re-analysis of real-life data from an evaluation of India's National Health Insurance Program. Our methodology enables us to identify new significant causal effects that were not identified in past analyses. Ultimately, our simulation studies and case study demonstrate how our methodology can yield more informative analyses in modern experiments with interference, treatment nonadherence, missing outcomes, and complicated outcome generation mechanisms.

</details>

<details>

<summary>2022-06-03 10:56:49 - A Bayesian modelling framework to quantify multiple sources of spatial variation for disease mapping</summary>

- *Sophie A Lee, Theodoros Economou, Rachel Lowe*

- `2206.01500v1` - [abs](http://arxiv.org/abs/2206.01500v1) - [pdf](http://arxiv.org/pdf/2206.01500v1)

> Spatial connectivity is an important consideration when modelling infectious disease data across a geographical region. Connectivity can arise for many reasons, including shared characteristics between regions, and human or vector movement. Bayesian hierarchical models include structured random effects to account for spatial connectivity. However, conventional approaches require the spatial structure to be fully defined prior to model fitting. By applying penalised smoothing splines to coordinates, we create 2-dimensional smooth surfaces describing the spatial structure of the data whilst making minimal assumptions about the structure. The result is a non-stationary surface which is setting specific. These surfaces can be incorporated into a hierarchical modelling framework and interpreted similarly to traditional random effects. Through simulation studies we show that the splines can be applied to any continuous connectivity measure, including measures of human movement, and that the models can be extended to explore multiple sources of spatial structure in the data. Using Bayesian inference and simulation, the relative contribution of each spatial structure can be computed and used to generate hypotheses about the drivers of disease. These models were found to perform at least as well as existing modelling frameworks, whilst allowing for future extensions and multiple sources of spatial connectivity.

</details>

<details>

<summary>2022-06-03 15:05:10 - A note on the normality assumption for Bayesian models of constraint in behavioral individual differences</summary>

- *Thomas J. Faulkenberry*

- `2112.05503v2` - [abs](http://arxiv.org/abs/2112.05503v2) - [pdf](http://arxiv.org/pdf/2112.05503v2)

> To investigate the structure of individual differences in performance on behavioral tasks, Haaf and Rouder (2017) developed a class of hierarchical Bayesian mixed models with varying levels of constraint on the individual effects. The models are then compared via Bayes factors, telling us which model best predicts the observed data. One common criticism of their method is that the observed data are assumed to be drawn from a normal distribution. However, for most cognitive tasks, the primary measure of performance is a response time, the distribution of which is well known to not be normal. In this technical note, I investigate the assumption of normality for two datasets in numerical cognition. Specifically, I show that using a shifted lognormal model for the response times does not change the overall pattern of inference. Further, since the model-estimated effects are now on a logarithmic scale, the interpretation of the modeling becomes more difficult, particularly because the estimated effect is now multiplicative rather than additive. As a result, I recommend that even though response times are not normally distributed in general, the simplification afforded by the Haaf and Rouder (2017) approach provides a pragmatic approach to modeling individual differences in behavioral tasks.

</details>

<details>

<summary>2022-06-03 16:25:48 - BaCaDI: Bayesian Causal Discovery with Unknown Interventions</summary>

- *Alexander Hgele, Jonas Rothfuss, Lars Lorch, Vignesh Ram Somnath, Bernhard Schlkopf, Andreas Krause*

- `2206.01665v1` - [abs](http://arxiv.org/abs/2206.01665v1) - [pdf](http://arxiv.org/pdf/2206.01665v1)

> Learning causal structures from observation and experimentation is a central task in many domains. For example, in biology, recent advances allow us to obtain single-cell expression data under multiple interventions such as drugs or gene knockouts. However, a key challenge is that often the targets of the interventions are uncertain or unknown. Thus, standard causal discovery methods can no longer be used. To fill this gap, we propose a Bayesian framework (BaCaDI) for discovering the causal structure that underlies data generated under various unknown experimental/interventional conditions. BaCaDI is fully differentiable and operates in the continuous space of latent probabilistic representations of both causal structures and interventions. This enables us to approximate complex posteriors via gradient-based variational inference and to reason about the epistemic uncertainty in the predicted structure. In experiments on synthetic causal discovery tasks and simulated gene-expression data, BaCaDI outperforms related methods in identifying causal structures and intervention targets. Finally, we demonstrate that, thanks to its rigorous Bayesian approach, our method provides well-calibrated uncertainty estimates.

</details>

<details>

<summary>2022-06-03 17:38:07 - Approximate confidence distribution computing</summary>

- *Suzanne Thornton, Wentao Li, Minge Xie*

- `2206.01707v1` - [abs](http://arxiv.org/abs/2206.01707v1) - [pdf](http://arxiv.org/pdf/2206.01707v1)

> Approximate confidence distribution computing (ACDC) offers a new take on the rapidly developing field of likelihood-free inference from within a frequentist framework. The appeal of this computational method for statistical inference hinges upon the concept of a confidence distribution, a special type of estimator which is defined with respect to the repeated sampling principle. An ACDC method provides frequentist validation for computational inference in problems with unknown or intractable likelihoods. The main theoretical contribution of this work is the identification of a matching condition necessary for frequentist validity of inference from this method. In addition to providing an example of how a modern understanding of confidence distribution theory can be used to connect Bayesian and frequentist inferential paradigms, we present a case to expand the current scope of so-called approximate Bayesian inference to include non-Bayesian inference by targeting a confidence distribution rather than a posterior. The main practical contribution of this work is the development of a data-driven approach to drive ACDC in both Bayesian or frequentist contexts. The ACDC algorithm is data-driven by the selection of a data-dependent proposal function, the structure of which is quite general and adaptable to many settings. We explore two numerical examples that both verify the theoretical arguments in the development of ACDC and suggest instances in which ACDC outperform approximate Bayesian computing methods computationally.

</details>

<details>

<summary>2022-06-03 17:43:39 - Robust Multi-Objective Bayesian Optimization Under Input Noise</summary>

- *Samuel Daulton, Sait Cakmak, Maximilian Balandat, Michael A. Osborne, Enlu Zhou, Eytan Bakshy*

- `2202.07549v4` - [abs](http://arxiv.org/abs/2202.07549v4) - [pdf](http://arxiv.org/pdf/2202.07549v4)

> Bayesian optimization (BO) is a sample-efficient approach for tuning design parameters to optimize expensive-to-evaluate, black-box performance metrics. In many manufacturing processes, the design parameters are subject to random input noise, resulting in a product that is often less performant than expected. Although BO methods have been proposed for optimizing a single objective under input noise, no existing method addresses the practical scenario where there are multiple objectives that are sensitive to input perturbations. In this work, we propose the first multi-objective BO method that is robust to input noise. We formalize our goal as optimizing the multivariate value-at-risk (MVaR), a risk measure of the uncertain objectives. Since directly optimizing MVaR is computationally infeasible in many settings, we propose a scalable, theoretically-grounded approach for optimizing MVaR using random scalarizations. Empirically, we find that our approach significantly outperforms alternative methods and efficiently identifies optimal robust designs that will satisfy specifications across multiple metrics with high probability.

</details>

<details>

<summary>2022-06-03 18:48:25 - Bayesian and Frequentist Inference for Synthetic Controls</summary>

- *Ignacio Martinez, Jaume Vives-i-Bastida*

- `2206.01779v1` - [abs](http://arxiv.org/abs/2206.01779v1) - [pdf](http://arxiv.org/pdf/2206.01779v1)

> The synthetic control method has become a widely popular tool to estimate causal effects with observational data. Despite this, inference for synthetic control methods remains challenging. Often, inferential results rely on linear factor model data generating processes. In this paper, we characterize the conditions on the factor model primitives (the factor loadings) for which the statistical risk minimizers are synthetic controls (in the simplex). Then, we propose a Bayesian alternative to the synthetic control method that preserves the main features of the standard method and provides a new way of doing valid inference. We explore a Bernstein-von Mises style result to link our Bayesian inference to the frequentist inference. For linear factor model frameworks we show that a maximum likelihood estimator (MLE) of the synthetic control weights can consistently estimate the predictive function of the potential outcomes for the treated unit and that our Bayes estimator is asymptotically close to the MLE in the total variation sense. Through simulations, we show that there is convergence between the Bayes and frequentist approach even in sparse settings. Finally, we apply the method to re-visit the study of the economic costs of the German re-unification. The Bayesian synthetic control method is available in the bsynth R-package.

</details>

<details>

<summary>2022-06-03 23:32:32 - Accounting for shared covariates in semi-parametric Bayesian additive regression trees</summary>

- *Estevo B. Prado, Andrew C. Parnell, Keefe Murphy, Nathan McJames, Ann O'Shea, Rafael A. Moral*

- `2108.07636v6` - [abs](http://arxiv.org/abs/2108.07636v6) - [pdf](http://arxiv.org/pdf/2108.07636v6)

> We propose some extensions to semi-parametric models based on Bayesian additive regression trees (BART). In the semi-parametric BART paradigm, the response variable is approximated by a linear predictor and a BART model, where the linear component is responsible for estimating the main effects and BART accounts for non-specified interactions and non-linearities. Previous semi-parametric models based on BART have assumed that the set of covariates in the linear predictor and the BART model are mutually exclusive in an attempt to avoid poor coverage properties and reduce bias in the estimates of the parameters in the linear predictor. The main novelty in our approach lies in the way we change the tree-generation moves in BART to deal with this bias and resolve non-identifiability issues between the parametric and non-parametric components, even when they have covariates in common. This allows us to model complex interactions involving the covariates of primary interest, both among themselves and with those in the BART component. Our novel method is developed with a view to analysing data from an international education assessment, where certain predictors of students' achievements in mathematics are of particular interpretational interest. Through additional simulation studies and another application to a well-known benchmark dataset, we also show competitive performance when compared to regression models, alternative formulations of semi-parametric BART, and other tree-based methods. The implementation of the proposed method is available at \url{https://github.com/ebprado/CSP-BART}.

</details>

<details>

<summary>2022-06-04 20:32:39 - On Connecting Deep Trigonometric Networks with Deep Gaussian Processes: Covariance, Expressivity, and Neural Tangent Kernel</summary>

- *Chi-Ken Lu, Patrick Shafto*

- `2203.07411v2` - [abs](http://arxiv.org/abs/2203.07411v2) - [pdf](http://arxiv.org/pdf/2203.07411v2)

> Deep Gaussian Process (DGP) as a model prior in Bayesian learning intuitively exploits the expressive power in function composition. DGPs also offer diverse modeling capabilities, but inference becomes Achilles' heel as marginalization in latent function space is not tractable. With Bochner's theorem, DGP with squared exponential kernel can be viewed as a deep trigonometric network consisting of the random feature layers, sine and cosine activation units, and random weight layers. In the wide limit with a bottleneck, we show that the weight space view yield the same effective covariance functions which were obtained previously in function space. As such, DGPs can be translated into the deep trig networks, which is flexible and expressive as one can freely adopt different prior distributions over the parameters.Interestingly, the network representation enables the study of DGP's neural tangent kernel, which may reveal the mean of the intractable predictive distribution. Statistically, unlike the shallow networks, deep networks of finite width have covariance deviating from the limiting kernel, and the inner and outer widths may play different roles in learning.

</details>

<details>

<summary>2022-06-04 22:38:57 - Active Bayesian Causal Inference</summary>

- *Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf, Robert Peharz, Julius von Kgelgen*

- `2206.02063v1` - [abs](http://arxiv.org/abs/2206.02063v1) - [pdf](http://arxiv.org/pdf/2206.02063v1)

> Causal discovery and causal reasoning are classically treated as separate and consecutive tasks: one first infers the causal graph, and then uses it to estimate causal effects of interventions. However, such a two-stage approach is uneconomical, especially in terms of actively collected interventional data, since the causal query of interest may not require a fully-specified causal model. From a Bayesian perspective, it is also unnatural, since a causal query (e.g., the causal graph or some causal effect) can be viewed as a latent quantity subject to posterior inference -- other unobserved quantities that are not of direct interest (e.g., the full causal model) ought to be marginalized out in this process and contribute to our epistemic uncertainty. In this work, we propose Active Bayesian Causal Inference (ABCI), a fully-Bayesian active learning framework for integrated causal discovery and reasoning, which jointly infers a posterior over causal models and queries of interest. In our approach to ABCI, we focus on the class of causally-sufficient, nonlinear additive noise models, which we model using Gaussian processes. We sequentially design experiments that are maximally informative about our target causal query, collect the corresponding interventional data, and update our beliefs to choose the next experiment. Through simulations, we demonstrate that our approach is more data-efficient than several baselines that only focus on learning the full causal graph. This allows us to accurately learn downstream causal queries from fewer samples while providing well-calibrated uncertainty estimates for the quantities of interest.

</details>

<details>

<summary>2022-06-04 23:26:09 - A Further Look at the Bayes Blind Spot</summary>

- *Mark Shattuck, Carl Wagner*

- `2206.02068v1` - [abs](http://arxiv.org/abs/2206.02068v1) - [pdf](http://arxiv.org/pdf/2206.02068v1)

> Gyenis and Redei have demonstrated that any prior p on a finite algebra, however chosen, severely restricts the set of posteriors accessible from p by Jeffrey conditioning on a nontrivial partition. Their demonstration involves showing that the set of posteriors not accessible from p in this way (which they call the Bayes blind spot of p) is large with respect to three common measures of size, namely, having cardinality c, (normalized) Lebesgue measure 1, and Baire second category with respect to a natural topology. In the present paper, we establish analogous results for probability measures defined on any infinite sigma algebra of subsets of a denumerably infinite set. However, we have needed to employ distinctly different approaches to determine the cardinality, and especially, the topological and measure-theoretic sizes of the Bayes blind spot in the infinite case. Interestingly, all of the results that we establish for a single prior p continue to hold for the intersection of the Bayes blind spots of countably many priors. This leads us to conjecture that Bayesian learning itself might be just as culpable as the limitations imposed by priors in enabling the existence of large Bayes blind spots.

</details>

<details>

<summary>2022-06-05 03:48:42 - Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence Optimization</summary>

- *Hui Yuan, Chengzhuo Ni, Huazheng Wang, Xuezhou Zhang, Le Cong, Csaba Szepesvri, Mengdi Wang*

- `2206.02092v1` - [abs](http://arxiv.org/abs/2206.02092v1) - [pdf](http://arxiv.org/pdf/2206.02092v1)

> Directed Evolution (DE), a landmark wet-lab method originated in 1960s, enables discovery of novel protein designs via evolving a population of candidate sequences. Recent advances in biotechnology has made it possible to collect high-throughput data, allowing the use of machine learning to map out a protein's sequence-to-function relation. There is a growing interest in machine learning-assisted DE for accelerating protein optimization. Yet the theoretical understanding of DE, as well as the use of machine learning in DE, remains limited. In this paper, we connect DE with the bandit learning theory and make a first attempt to study regret minimization in DE. We propose a Thompson Sampling-guided Directed Evolution (TS-DE) framework for sequence optimization, where the sequence-to-function mapping is unknown and querying a single value is subject to costly and noisy measurements. TS-DE updates a posterior of the function based on collected measurements. It uses a posterior-sampled function estimate to guide the crossover recombination and mutation steps in DE. In the case of a linear model, we show that TS-DE enjoys a Bayesian regret of order $\tilde O(d^{2}\sqrt{MT})$, where $d$ is feature dimension, $M$ is population size and $T$ is number of rounds. This regret bound is nearly optimal, confirming that bandit learning can provably accelerate DE. It may have implications for more general sequence optimization and evolutionary algorithms.

</details>

<details>

<summary>2022-06-05 08:31:01 - Causal impact of severe events on electricity demand: The case of COVID-19 in Japan</summary>

- *Yasunobu Wakashiro*

- `2206.02122v1` - [abs](http://arxiv.org/abs/2206.02122v1) - [pdf](http://arxiv.org/pdf/2206.02122v1)

> As of May 2022, the coronavirus disease 2019 (COVID-19) still has a severe global impact on people's lives. Previous studies have reported that COVID-19 decreased the electricity demand in early 2020. However, our study found that the electricity demand increased in summer and winter even when the infection was widespread. The fact that the event has continued over two years suggests that it is essential to introduce the method which can estimate the impact of the event for long period considering seasonal fluctuations. We employed the Bayesian structural time-series model to estimate the causal impact of COVID-19 on electricity demand in Japan. The results indicate that behavioral restrictions due to COVID-19 decreased the daily electricity demand (-5.1% in weekdays, -6.1% in holidays) in April and May 2020 as indicated by previous studies. However, even in 2020, the results show that the demand increases in the hot summer and cold winter (the increasing rate is +14% in the period from 1st August to 15th September 2020, and +7.6% from 16th December 2020 to 15th January 2021). This study shows that the significant decrease in electricity demand for the business sector exceeded the increase in demand for the household sector in April and May 2020; however, the increase in demand for the households exceeded the decrease in demand for the business in hot summer and cold winter periods. Our result also implies that it is possible to run out of electricity when people's behavior changes even if they are less active.

</details>

<details>

<summary>2022-06-05 14:07:17 - Functional Ensemble Distillation</summary>

- *Coby Penso, Idan Achituve, Ethan Fetaya*

- `2206.02183v1` - [abs](http://arxiv.org/abs/2206.02183v1) - [pdf](http://arxiv.org/pdf/2206.02183v1)

> Bayesian models have many desirable properties, most notable is their ability to generalize from limited data and to properly estimate the uncertainty in their predictions. However, these benefits come at a steep computational cost as Bayesian inference, in most cases, is computationally intractable. One popular approach to alleviate this problem is using a Monte-Carlo estimation with an ensemble of models sampled from the posterior. However, this approach still comes at a significant computational cost, as one needs to store and run multiple models at test time. In this work, we investigate how to best distill an ensemble's predictions using an efficient model. First, we argue that current approaches that simply return distribution over predictions cannot compute important properties, such as the covariance between predictions, which can be valuable for further processing. Second, in many limited data settings, all ensemble members achieve nearly zero training loss, namely, they produce near-identical predictions on the training set which results in sub-optimal distilled models. To address both problems, we propose a novel and general distillation approach, named Functional Ensemble Distillation (FED), and we investigate how to best distill an ensemble in this setting. We find that learning the distilled model via a simple augmentation scheme in the form of mixup augmentation significantly boosts the performance. We evaluated our method on several tasks and showed that it achieves superior results in both accuracy and uncertainty estimation compared to current approaches.

</details>

<details>

<summary>2022-06-05 16:49:10 - Statistical Deep Learning for Spatial and Spatio-Temporal Data</summary>

- *Christopher K. Wikle, Andrew Zammit-Mangion*

- `2206.02218v1` - [abs](http://arxiv.org/abs/2206.02218v1) - [pdf](http://arxiv.org/pdf/2206.02218v1)

> Deep neural network models have become ubiquitous in recent years, and have been applied to nearly all areas of science, engineering, and industry. These models are particularly useful for data that have strong dependencies in space (e.g., images) and time (e.g., sequences). Indeed, deep models have also been extensively used by the statistical community to model spatial and spatio-temporal data through, for example, the use of multi-level Bayesian hierarchical models and deep Gaussian processes. In this review, we first present an overview of traditional statistical and machine learning perspectives for modeling spatial and spatio-temporal data, and then focus on a variety of hybrid models that have recently been developed for latent process, data, and parameter specifications. These hybrid models integrate statistical modeling ideas with deep neural network models in order to take advantage of the strengths of each modeling paradigm. We conclude by giving an overview of computational technologies that have proven useful for these hybrid models, and with a brief discussion on future research directions.

</details>

<details>

<summary>2022-06-05 20:58:42 - Information Threshold, Bayesian Inference and Decision-Making</summary>

- *Jacques Balayla*

- `2206.02266v1` - [abs](http://arxiv.org/abs/2206.02266v1) - [pdf](http://arxiv.org/pdf/2206.02266v1)

> We define the information threshold as the point of maximum curvature in the prior vs. posterior Bayesian curve, both of which are described as a function of the true positive and negative rates of the classification system in question. The nature of the threshold is such that for sufficiently adequate binary classification systems, retrieving excess information beyond the threshold does not significantly alter the reliability of our classification assessment. We hereby introduce the "marital status thought experiment" to illustrate this idea and report a previously undefined mathematical relationship between the Bayesian prior and posterior, which may have significant philosophical and epistemological implications in decision theory. Where the prior probability is a scalar between 0 and 1 given by $\phi$ and the posterior is a scalar between 0 and 1 given by $\rho$, then at the information threshold, $\phi_e$:   $\phi_e + \rho_e = 1$   Otherwise stated, given some degree of prior belief, we may assert its persuasiveness when sufficient quality evidence yields a posterior so that their combined sum equals 1. Retrieving further evidence beyond this point does not significantly improve the posterior probability, and may serve as a benchmark for confidence in decision-making.

</details>

<details>

<summary>2022-06-06 04:25:47 - Hybrid Models for Mixed Variables in Bayesian Optimization</summary>

- *Hengrui Luo, Younghyun Cho, James W. Demmel, Xiaoye S. Li, Yang Liu*

- `2206.01409v2` - [abs](http://arxiv.org/abs/2206.01409v2) - [pdf](http://arxiv.org/pdf/2206.01409v2)

> We systematically describe the problem of simultaneous surrogate modeling of mixed variables (i.e., continuous, integer and categorical variables) in the Bayesian optimization (BO) context. We provide a unified hybrid model using both Monte-Carlo tree search (MCTS) and Gaussian processes (GP) that encompasses and generalizes multiple state-of-the-art mixed BO surrogates. Based on the architecture, we propose applying a new dynamic model selection criterion among novel candidate families of covariance kernels, including non-stationary kernels and associated families. Different benchmark problems are studied and presented to support the superiority of our model, along with results highlighting the effectiveness of our method compared to most state-of-the-art mixed-variable methods in BO.

</details>

<details>

<summary>2022-06-06 08:48:58 - Continuous and Distribution-free Probabilistic Wind Power Forecasting: A Conditional Normalizing Flow Approach</summary>

- *Honglin Wen, Pierre Pinson, Jinghuan Ma, Jie Gu, Zhijian Jin*

- `2206.02433v1` - [abs](http://arxiv.org/abs/2206.02433v1) - [pdf](http://arxiv.org/pdf/2206.02433v1)

> We present a data-driven approach for probabilistic wind power forecasting based on conditional normalizing flow (CNF). In contrast with the existing, this approach is distribution-free (as for non-parametric and quantile-based approaches) and can directly yield continuous probability densities, hence avoiding quantile crossing. It relies on a base distribution and a set of bijective mappings. Both the shape parameters of the base distribution and the bijective mappings are approximated with neural networks. Spline-based conditional normalizing flow is considered owing to its non-affine characteristics. Over the training phase, the model sequentially maps input examples onto samples of base distribution, given the conditional contexts, where parameters are estimated through maximum likelihood. To issue probabilistic forecasts, one eventually maps samples of the base distribution into samples of a desired distribution. Case studies based on open datasets validate the effectiveness of the proposed model, and allows us to discuss its advantages and caveats with respect to the state of the art.

</details>

<details>

<summary>2022-06-06 08:56:56 - Information-theoretic Inducing Point Placement for High-throughput Bayesian Optimisation</summary>

- *Henry B. Moss, Sebastian W. Ober, Victor Picheny*

- `2206.02437v1` - [abs](http://arxiv.org/abs/2206.02437v1) - [pdf](http://arxiv.org/pdf/2206.02437v1)

> Sparse Gaussian Processes are a key component of high-throughput Bayesian optimisation (BO) loops -- an increasingly common setting where evaluation budgets are large and highly parallelised. By using representative subsets of the available data to build approximate posteriors, sparse models dramatically reduce the computational costs of surrogate modelling by relying on a small set of pseudo-observations, the so-called inducing points, in lieu of the full data set. However, current approaches to design inducing points are not appropriate within BO loops as they seek to reduce global uncertainty in the objective function. Thus, the high-fidelity modelling of promising and data-dense regions required for precise optimisation is sacrificed and computational resources are instead wasted on modelling areas of the space already known to be sub-optimal. Inspired by entropy-based BO methods, we propose a novel inducing point design that uses a principled information-theoretic criterion to select inducing points. By choosing inducing points to maximally reduce both global uncertainty and uncertainty in the maximum value of the objective function, we build surrogate models able to support high-precision high-throughput BO.

</details>

<details>

<summary>2022-06-06 09:25:54 - Component-wise iterative ensemble Kalman inversion for static Bayesian models with unknown measurement error covariance</summary>

- *Imke Botha, Matthew P. Adams, Dang Khuong Tran, Frederick R. Bennett, Christopher Drovandi*

- `2206.02451v1` - [abs](http://arxiv.org/abs/2206.02451v1) - [pdf](http://arxiv.org/pdf/2206.02451v1)

> The ensemble Kalman filter (EnKF) is a Monte Carlo approximation of the Kalman filter for high dimensional linear Gaussian state space models. EnKF methods have also been developed for parameter inference of static Bayesian models with a Gaussian likelihood, in a way that is analogous to likelihood tempering sequential Monte Carlo (SMC). These methods are commonly referred to as ensemble Kalman inversion (EKI). Unlike SMC, the inference from EKI is only asymptotically unbiased if the likelihood is linear Gaussian and the priors are Gaussian. However, EKI is significantly faster to run. Currently, a large limitation of EKI methods is that the covariance of the measurement error is assumed to be fully known. We develop a new method, which we call component-wise iterative ensemble Kalman inversion (CW-IEKI), that allows elements of the covariance matrix to be inferred alongside the model parameters at negligible extra cost. This novel method is compared to SMC on three different application examples: a model of nitrogen mineralisation in soil that is based on the Agricultural Production Systems Simulator (APSIM), a model predicting seagrass decline due to stress from water temperature and light, and a model predicting coral calcification rates. On all of these examples, we find that CW-IEKI has relatively similar predictive performance to SMC, albeit with greater uncertainty, and it has a significantly faster run time.

</details>

<details>

<summary>2022-06-06 12:06:13 - Sparse Bayesian Learning for Complex-Valued Rational Approximations</summary>

- *Felix Schneider, Iason Papaioannou, Gerhard Mller*

- `2206.02523v1` - [abs](http://arxiv.org/abs/2206.02523v1) - [pdf](http://arxiv.org/pdf/2206.02523v1)

> Surrogate models are used to alleviate the computational burden in engineering tasks, which require the repeated evaluation of computationally demanding models of physical systems, such as the efficient propagation of uncertainties. For models that show a strongly non-linear dependence on their input parameters, standard surrogate techniques, such as polynomial chaos expansion, are not sufficient to obtain an accurate representation of the original model response. Through applying a rational approximation instead, the approximation error can be efficiently reduced for models whose non-linearity is accurately described through a rational function. Specifically, our aim is to approximate complex-valued models. A common approach to obtain the coefficients in the surrogate is to minimize the sample-based error between model and surrogate in the least-square sense. In order to obtain an accurate representation of the original model and to avoid overfitting, the sample set has be two to three times the number of polynomial terms in the expansion. For models that require a high polynomial degree or are high-dimensional in terms of their input parameters, this number often exceeds the affordable computational cost. To overcome this issue, we apply a sparse Bayesian learning approach to the rational approximation. Through a specific prior distribution structure, sparsity is induced in the coefficients of the surrogate model. The denominator polynomial coefficients as well as the hyperparameters of the problem are determined through a type-II-maximum likelihood approach. We apply a quasi-Newton gradient-descent algorithm in order to find the optimal denominator coefficients and derive the required gradients through application of $\mathbb{CR}$-calculus.

</details>

<details>

<summary>2022-06-06 12:43:06 - $-$CoES</summary>

- *Aleksy Leeuwenkamp*

- `2206.02582v1` - [abs](http://arxiv.org/abs/2206.02582v1) - [pdf](http://arxiv.org/pdf/2206.02582v1)

> In this paper the $\Delta$-CoVaR method is extended in both the conditional and unconditional cases to be based on the Expected Shortfall (ES) using quantile regression with a more expansive distress definition. We find the resulting $\Delta$-CoES measure to be complementary to the $\Delta$-CoVaR and to be more effective than the $\Delta$-CoVaR in measuring short-term changes in systemic risk and in identifying heterogeneity in the systemic risk contributions of financial institutions and linkages between institutions due to its lower robustness. For regulators, risk managers and market participants these properties are interesting from an economic standpoint when they require the increased sensitivity and heterogeneity of the $\Delta$-CoES to set short-term capital requirements/risk limits, find problematic financial linkages, problematic financial institutions or have some kind of early warning system for the emergence of systemic risk. Lastly, the $\Delta$-CoES is straightforward to estimate and would fit within recent regulatory frameworks such as the FRTB. To show the statistical advantages and properties empirically, the $\Delta$-CoVaR and $\Delta$-CoES methods are used on a large sample (from 31-12-1970 to 31-12-2020 1564 firms) of daily equity data from US financial institutions both in a system and network fashion. On a sample of 9 US-based GSIBS we also show the properties and the utility of the $\Delta$-CoES when it comes to identifying problematic financial links.

</details>

<details>

<summary>2022-06-06 14:30:21 - Accelerating inference for stochastic kinetic models</summary>

- *Tom E. Lowe, Andrew Golightly, Chris Sherlock*

- `2206.02644v1` - [abs](http://arxiv.org/abs/2206.02644v1) - [pdf](http://arxiv.org/pdf/2206.02644v1)

> Stochastic kinetic models (SKMs) are increasingly used to account for the inherent stochasticity exhibited by interacting populations of species in areas such as epidemiology, population ecology and systems biology. Species numbers are modelled using a continuous-time stochastic process, and, depending on the application area of interest, this will typically take the form of a Markov jump process or an It\^o diffusion process. Widespread use of these models is typically precluded by their computational complexity. In particular, performing exact fully Bayesian inference in either modelling framework is challenging due to the intractability of the observed data likelihood, necessitating the use of computationally intensive techniques such as particle Markov chain Monte Carlo (particle MCMC). We propose to increase the computational and statistical efficiency of this approach by leveraging the tractability of an inexpensive surrogate derived directly from either the jump or diffusion process. The surrogate is used in three ways: in the design of a gradient-based parameter proposal, to construct an appropriate bridge construct and in the first stage of a delayed-acceptance step. We find that the resulting approach offers substantial gains in efficiency over a standard particle MCMC implementation.

</details>

<details>

<summary>2022-06-06 14:52:46 - Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees</summary>

- *Haotian Ju, Dongyue Li, Hongyang R. Zhang*

- `2206.02659v1` - [abs](http://arxiv.org/abs/2206.02659v1) - [pdf](http://arxiv.org/pdf/2206.02659v1)

> We consider transfer learning approaches that fine-tune a pretrained deep neural network on a target task. We investigate generalization properties of fine-tuning to understand the problem of overfitting, which often happens in practice. Previous works have shown that constraining the distance from the initialization of fine-tuning improves generalization. Using a PAC-Bayesian analysis, we observe that besides distance from initialization, Hessians affect generalization through the noise stability of deep neural networks against noise injections. Motivated by the observation, we develop Hessian distance-based generalization bounds for a wide range of fine-tuning methods. Next, we investigate the robustness of fine-tuning with noisy labels. We design an algorithm that incorporates consistent losses and distance-based regularization for fine-tuning. Additionally, we prove a generalization error bound of our algorithm under class conditional independent noise in the training dataset labels. We perform a detailed empirical study of our algorithm on various noisy environments and architectures. For example, on six image classification tasks whose training labels are generated with programmatic labeling, we show a 3.26% accuracy improvement over prior methods. Meanwhile, the Hessian distance measure of the fine-tuned network using our algorithm decreases by six times more than existing approaches.

</details>

<details>

<summary>2022-06-06 20:11:01 - Full-Information Estimation of Heterogeneous Agent Models Using Macro and Micro Data</summary>

- *Laura Liu, Mikkel Plagborg-Mller*

- `2101.04771v2` - [abs](http://arxiv.org/abs/2101.04771v2) - [pdf](http://arxiv.org/pdf/2101.04771v2)

> We develop a generally applicable full-information inference method for heterogeneous agent models, combining aggregate time series data and repeated cross sections of micro data. To handle unobserved aggregate state variables that affect cross-sectional distributions, we compute a numerically unbiased estimate of the model-implied likelihood function. Employing the likelihood estimate in a Markov Chain Monte Carlo algorithm, we obtain fully efficient and valid Bayesian inference. Evaluation of the micro part of the likelihood lends itself naturally to parallel computing. Numerical illustrations in models with heterogeneous households or firms demonstrate that the proposed full-information method substantially sharpens inference relative to using only macro data, and for some parameters micro data is essential for identification.

</details>

<details>

<summary>2022-06-07 03:23:37 - On Transportation of Mini-batches: A Hierarchical Approach</summary>

- *Khai Nguyen, Dang Nguyen, Quoc Nguyen, Tung Pham, Hung Bui, Dinh Phung, Trung Le, Nhat Ho*

- `2102.05912v5` - [abs](http://arxiv.org/abs/2102.05912v5) - [pdf](http://arxiv.org/pdf/2102.05912v5)

> Mini-batch optimal transport (m-OT) has been successfully used in practical applications that involve probability measures with a very high number of supports. The m-OT solves several smaller optimal transport problems and then returns the average of their costs and transportation plans. Despite its scalability advantage, the m-OT does not consider the relationship between mini-batches which leads to undesirable estimation. Moreover, the m-OT does not approximate a proper metric between probability measures since the identity property is not satisfied. To address these problems, we propose a novel mini-batch scheme for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds the optimal coupling between mini-batches and it can be seen as an approximation to a well-defined distance on the space of probability measures. Furthermore, we show that the m-OT is a limit of the entropic regularized version of the BoMb-OT when the regularized parameter goes to infinity. Finally, we carry out experiments on various applications including deep generative models, deep domain adaptation, approximate Bayesian computation, color transfer, and gradient flow to show that the BoMb-OT can be widely applied and performs well in various applications.

</details>

<details>

<summary>2022-06-07 06:26:46 - Relaxed Gaussian process interpolation: a goal-oriented approach to Bayesian optimization</summary>

- *Sbastien Petit, Julien Bect, Emmanuel Vazquez*

- `2206.03034v1` - [abs](http://arxiv.org/abs/2206.03034v1) - [pdf](http://arxiv.org/pdf/2206.03034v1)

> This work presents a new procedure for obtaining predictive distributions in the context of Gaussian process (GP) modeling, with a relaxation of the interpolation constraints outside some ranges of interest: the mean of the predictive distributions no longer necessarily interpolates the observed values when they are outside ranges of interest, but are simply constrained to remain outside. This method called relaxed Gaussian process (reGP) interpolation provides better predictive distributions in ranges of interest, especially in cases where a stationarity assumption for the GP model is not appropriate. It can be viewed as a goal-oriented method and becomes particularly interesting in Bayesian optimization, for example, for the minimization of an objective function, where good predictive distributions for low function values are important. When the expected improvement criterion and reGP are used for sequentially choosing evaluation points, the convergence of the resulting optimization algorithm is theoretically guaranteed (provided that the function to be optimized lies in the reproducing kernel Hilbert spaces attached to the known covariance of the underlying Gaussian process). Experiments indicate that using reGP instead of stationary GP models in Bayesian optimization is beneficial.

</details>

<details>

<summary>2022-06-07 07:45:57 - A Non-parametric Bayesian Model for Detecting Differential Item Functioning: An Application to Political Representation in the US</summary>

- *Yuki Shiraito, James Lo, Santiago Olivella*

- `2205.05934v2` - [abs](http://arxiv.org/abs/2205.05934v2) - [pdf](http://arxiv.org/pdf/2205.05934v2)

> A common approach when studying the quality of representation involves comparing the latent preferences of voters and legislators, commonly obtained by fitting an item-response theory (IRT) model to a common set of stimuli. Despite being exposed to the same stimuli, voters and legislators may not share a common understanding of how these stimuli map onto their latent preferences, leading to differential item-functioning (DIF) and incomparability of estimates. We explore the presence of DIF and incomparability of latent preferences obtained through IRT models by re-analyzing an influential survey data set, where survey respondents expressed their preferences on roll call votes that U.S. legislators had previously voted on. To do so, we propose defining a Dirichlet Process prior over item-response functions in standard IRT models. In contrast to typical multi-step approaches to detecting DIF, our strategy allows researchers to fit a single model, automatically identifying incomparable sub-groups with different mappings from latent traits onto observed responses. We find that although there is a group of voters whose estimated positions can be safely compared to those of legislators, a sizeable share of surveyed voters understand stimuli in fundamentally different ways. Ignoring these issues can lead to incorrect conclusions about the quality of representation.

</details>

<details>

<summary>2022-06-07 08:07:38 - Jackknife Partially Linear Model Averaging for the Conditional Quantile Prediction</summary>

- *Jing Lv*

- `2203.10248v2` - [abs](http://arxiv.org/abs/2203.10248v2) - [pdf](http://arxiv.org/pdf/2203.10248v2)

> Estimating the conditional quantile of the interested variable with respect to changes in the covariates is frequent in many economical applications as it can offer a comprehensive insight. In this paper, we propose a novel semiparametric model averaging to predict the conditional quantile even if all models under consideration are potentially misspecified. Specifically, we first build a series of non-nested partially linear sub-models, each with different nonlinear component. Then a leave-one-out cross-validation criterion is applied to choose the model weights. Under some regularity conditions, we have proved that the resulting model averaging estimator is asymptotically optimal in terms of minimizing the out-of-sample average quantile prediction error. Our modelling strategy not only effectively avoids the problem of specifying which a covariate should be nonlinear when one fits a partially linear model, but also results in a more accurate prediction than traditional model-based procedures because of the optimality of the selected weights by the cross-validation criterion. Simulation experiments and an illustrative application show that our proposed model averaging method is superior to other commonly used alternatives.

</details>

<details>

<summary>2022-06-07 09:56:37 - Ensemble Kalman Inversion for General Likelihoods</summary>

- *Samuel Duffield, Sumeetpal S. Singh*

- `2110.03034v3` - [abs](http://arxiv.org/abs/2110.03034v3) - [pdf](http://arxiv.org/pdf/2110.03034v3)

> In this letter we generalise Ensemble Kalman inversion techniques to general Bayesian models where previously they were restricted to additive Gaussian likelihoods - all in the difficult setting where the likelihood can be sampled from, but its density not necessarily evaluated.

</details>

<details>

<summary>2022-06-07 10:58:56 - Inferring Parsimonious Coupling Statistics in Nonlinear Dynamics with Variational Gaussian Processes</summary>

- *Ameer Ghouse, Gaetano Valenza*

- `2203.03868v5` - [abs](http://arxiv.org/abs/2203.03868v5) - [pdf](http://arxiv.org/pdf/2203.03868v5)

> Falsification is the basis for testing existing hypotheses, and a great danger is posed when results incorrectly reject our prior notions (false positives). Though nonparametric and nonlinear exploratory methods of uncovering coupling provide a flexible framework to study network configurations and discover causal graphs, multiple comparisons analyses make false positives more likely, exacerbating the need for their control. We aim to robustify the Gaussian Processes Convergent Cross-Mapping (GP-CCM) method through Variational Bayesian Gaussian Process modeling (VGP-CCM). We alleviate computational costs of integrating with conditional hyperparameter distributions through mean field approximations. This approximation model, in conjunction with permutation sampling of the null distribution, permits significance statistics that are more robust than permutation sampling with point hyperparameters. Simulated unidirectional Lorenz-Rossler systems as well as mechanistic models of neurovascular systems are used to evaluate the method. The results demonstrate that the proposed method yields improved specificity, showing promise to combat false positives

</details>

<details>

<summary>2022-06-07 12:20:22 - Shedding a PAC-Bayesian Light on Adaptive Sliced-Wasserstein Distances</summary>

- *Ruben Ohana, Kimia Nadjahi, Alain Rakotomamonjy, Liva Ralaivola*

- `2206.03230v1` - [abs](http://arxiv.org/abs/2206.03230v1) - [pdf](http://arxiv.org/pdf/2206.03230v1)

> The Sliced-Wasserstein distance (SW) is a computationally efficient and theoretically grounded alternative to the Wasserstein distance. Yet, the literature on its statistical properties with respect to the distribution of slices, beyond the uniform measure, is scarce. To bring new contributions to this line of research, we leverage the PAC-Bayesian theory and the central observation that SW actually hinges on a slice-distribution-dependent Gibbs risk, the kind of quantity PAC-Bayesian bounds have been designed to characterize. We provide four types of results: i) PAC-Bayesian generalization bounds that hold on what we refer as adaptive Sliced-Wasserstein distances, i.e. distances defined with respect to any distribution of slices, ii) a procedure to learn the distribution of slices that yields a maximally discriminative SW, by optimizing our PAC-Bayesian bounds, iii) an insight on how the performance of the so-called distributional Sliced-Wasserstein distance may be explained through our theory, and iv) empirical illustrations of our findings.

</details>

<details>

<summary>2022-06-07 13:04:26 - Certified Dimension Reduction for Bayesian Updating with the Cross-Entropy Method</summary>

- *Max Ehre, Rafael Flock, Martin Fueder, Iason Papaioannou, Daniel Straub*

- `2206.03249v1` - [abs](http://arxiv.org/abs/2206.03249v1) - [pdf](http://arxiv.org/pdf/2206.03249v1)

> In inverse problems, the parameters of a model are estimated based on observations of the model response. The Bayesian approach is powerful for solving such problems; one formulates a prior distribution for the parameter state that is updated with the observations to compute the posterior parameter distribution. Solving for the posterior distribution can be challenging when, e.g., prior and posterior significantly differ from one another and/or the parameter space is high-dimensional. We use a sequence of importance sampling measures that arise by tempering the likelihood to approach inverse problems exhibiting a significant distance between prior and posterior. Each importance sampling measure is identified by cross-entropy minimization as proposed in the context of Bayesian inverse problems in Engel et al. (2021). To efficiently address problems with high-dimensional parameter spaces we set up the minimization procedure in a low-dimensional subspace of the original parameter space. The principal idea is to analyse the spectrum of the second-moment matrix of the gradient of the log-likelihood function to identify a suitable subspace. Following Zahm et al. (2021), an upper bound on the Kullback-Leibler-divergence between full-dimensional and subspace posterior is provided, which can be utilized to determine the effective dimension of the inverse problem corresponding to a prescribed approximation error bound. We suggest heuristic criteria for optimally selecting the number of model and model gradient evaluations in each iteration of the importance sampling sequence. We investigate the performance of this approach using examples from engineering mechanics set in various parameter space dimensions.

</details>

<details>

<summary>2022-06-07 13:11:06 - Model selection for robust learning of mutational signatures using Negative Binomial non-negative matrix factorization</summary>

- *Marta Pelizzola, Ragnhild Laursen, Asger Hobolth*

- `2206.03257v1` - [abs](http://arxiv.org/abs/2206.03257v1) - [pdf](http://arxiv.org/pdf/2206.03257v1)

> The spectrum of mutations in a collection of cancer genomes can be described by a mixture of a few mutational signatures. The mutational signatures can be found using non-negative matrix factorization (NMF). To extract the mutational signatures we have to assume a distribution for the observed mutational counts and a number of mutational signatures. In most applications, the mutational counts are assumed to be Poisson distributed, but they are often overdispersed, and thus the Negative Binomial distribution is more appropriate. We demonstrate using a simulation study that Negative Binomial NMF requires fewer signatures than Poisson NMF to fit the data and we propose a Negative Binomial NMF with a patient specific overdispersion parameter to capture the variation across patients. We also introduce a robust model selection procedure inspired by cross-validation to determine the number of signatures. Furthermore we study the influence of the distributional assumption in relation to two classical model selection procedures: the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). In the presence of overdispersion we show that our model selection procedure is more robust at determining the correct number of signatures than state-of-the-art methods, which are overestimating the number of signatures. We apply our proposed analysis on a wide range of simulated data and on a data set from breast cancer patients. The code for our algorithms and analysis is available in the R package SigMoS and can be found at https://github.com/MartaPelizzola/SigMoS.

</details>

<details>

<summary>2022-06-07 17:31:21 - Robust Ranking of Happiness Outcomes: A Median Regression Perspective</summary>

- *Le-Yu Chen, Ekaterina Oparina, Nattavudh Powdthavee, Sorawoot Srisuma*

- `1902.07696v3` - [abs](http://arxiv.org/abs/1902.07696v3) - [pdf](http://arxiv.org/pdf/1902.07696v3)

> Ordered probit and logit models have been frequently used to estimate the mean ranking of happiness outcomes (and other ordinal data) across groups. However, it has been recently highlighted that such ranking may not be identified in most happiness applications. We suggest researchers focus on median comparison instead of the mean. This is because the median rank can be identified even if the mean rank is not. Furthermore, median ranks in probit and logit models can be readily estimated using standard statistical softwares. The median ranking, as well as ranking for other quantiles, can also be estimated semiparametrically and we provide a new constrained mixed integer optimization procedure for implementation. We apply it to estimate a happiness equation using General Social Survey data of the US.

</details>

<details>

<summary>2022-06-07 19:04:10 - Estimation of Optimal Dynamic Treatment Regimes using Gaussian Process Emulation</summary>

- *Daniel Rodriguez Duque, David A. Stephens, Erica E. M. Moodie*

- `2105.12259v2` - [abs](http://arxiv.org/abs/2105.12259v2) - [pdf](http://arxiv.org/pdf/2105.12259v2)

> In precision medicine, identifying optimal sequences of decision rules, termed dynamic treatment regimes (DTRs), is an important undertaking. One approach investigators may take to infer about optimal DTRs is via Bayesian dynamic Marginal Structural Models (MSMs). These models represent the expected outcome under adherence to a DTR for DTRs in a family indexed by a parameter $ \psi $; the function mapping regimes in the family to the expected outcome under adherence to a DTR is known as the value function. Models that allow for the straightforward identification of an optimal DTR may lead to biased estimates. If such a model is computationally tractable, common wisdom says that a grid-search for the optimal DTR may obviate this difficulty. In a Bayesian context, computational difficulties may be compounded if a posterior mean must be calculated at each grid point. We seek to alleviate these inferential challenges by implementing Gaussian Process ($ \mathcal{GP} $) optimization methods for estimators for the causal effect of adherence to a specified DTR. We examine how to identify optimal DTRs in settings where the value function is multi-modal, which are often not addressed in the DTR literature. We conclude that a $ \mathcal{GP} $ modeling approach that acknowledges noise in the estimated response surface leads to improved results. Additionally, we find that a grid-search may not always yield a robust solution and that it is often less efficient than a $ \mathcal{GP} $ approach. We illustrate the use of the proposed methods by analyzing a clinical dataset with the aim of quantifying the effect of different patterns of HIV therapy.

</details>

<details>

<summary>2022-06-07 22:52:59 - FedPop: A Bayesian Approach for Personalised Federated Learning</summary>

- *Nikita Kotelevskii, Maxime Vono, Eric Moulines, Alain Durmus*

- `2206.03611v1` - [abs](http://arxiv.org/abs/2206.03611v1) - [pdf](http://arxiv.org/pdf/2206.03611v1)

> Personalised federated learning (FL) aims at collaboratively learning a machine learning model taylored for each client. Albeit promising advances have been made in this direction, most of existing approaches works do not allow for uncertainty quantification which is crucial in many applications. In addition, personalisation in the cross-device setting still involves important issues, especially for new clients or those having small number of observations. This paper aims at filling these gaps. To this end, we propose a novel methodology coined FedPop by recasting personalised FL into the population modeling paradigm where clients' models involve fixed common population parameters and random effects, aiming at explaining data heterogeneity. To derive convergence guarantees for our scheme, we introduce a new class of federated stochastic optimisation algorithms which relies on Markov chain Monte Carlo methods. Compared to existing personalised FL methods, the proposed methodology has important benefits: it is robust to client drift, practical for inference on new clients, and above all, enables uncertainty quantification under mild computational and memory overheads. We provide non-asymptotic convergence guarantees for the proposed algorithms and illustrate their performances on various personalised federated learning tasks.

</details>

<details>

<summary>2022-06-08 00:06:48 - Bayesian additive regression trees for probabilistic programming</summary>

- *Miriana Quiroga, Pablo G Garay, Juan M. Alonso, Juan Martin Loyola, Osvaldo A Martin*

- `2206.03619v1` - [abs](http://arxiv.org/abs/2206.03619v1) - [pdf](http://arxiv.org/pdf/2206.03619v1)

> Bayesian additive regression trees (BART) is a non-parametric method to approximate functions. It is a black-box method based on the sum of many trees where priors are used to regularize inference, mainly by restricting trees' learning capacity so that no individual tree is able to explain the data, but rather the sum of trees. We discuss BART in the context of probabilistic programming languages (PPLs), specifically we introduce a BART implementation extending PyMC, a Python library for probabilistic programming. We present a few examples of models that can be built using this probabilistic programming-oriented version of BART, discuss recommendations for sample diagnostics and selection of model hyperparameters, and finally we close with limitations of the current approach and future extensions.

</details>

<details>

<summary>2022-06-08 03:06:02 - A Structural Dynamic Factor Model for Daily Global Stock Market Returns</summary>

- *Oliver B. Linton, Haihan Tang, Jianbin Wu*

- `2202.03638v3` - [abs](http://arxiv.org/abs/2202.03638v3) - [pdf](http://arxiv.org/pdf/2202.03638v3)

> Most stock markets are open for 6-8 hours per trading day. The Asian, European and American stock markets are separated in time by time-zone differences. We propose a statistical dynamic factor model for a large number of daily returns across multiple time zones. Our model has a common global factor as well as continental factors. Under a mild fixed-signs assumption, our model is identified and has a structural interpretation. We propose several estimators of the model: the maximum likelihood estimator-one day (MLE-one day), the quasi-maximum likelihood estimator (QMLE), an improved estimator from QMLE (QMLE-md), the QMLE-res (similar to MLE-one day), and a Bayesian estimator (Gibbs sampling). We establish consistency, the rates of convergence and the asymptotic distributions of the QMLE and the QMLE-md. We next provide a heuristic procedure for conducting inference for the MLE-one day and the QMLE-res. Monte Carlo simulations reveal that the MLE-one day, the QMLE-res and the QMLE-md work well. We then apply our model to two real data sets: (1) equity portfolio returns from Japan, Europe and the US; (2) MSCI equity indices of 41 developed and emerging markets. Some new insights about linkages among different markets are drawn.

</details>

<details>

<summary>2022-06-08 08:32:32 - Using Mixed-Effect Models to Learn Bayesian Networks from Related Data Sets</summary>

- *Marco Scutari, Christopher Marquis, Laura Azzimonti*

- `2206.03743v1` - [abs](http://arxiv.org/abs/2206.03743v1) - [pdf](http://arxiv.org/pdf/2206.03743v1)

> We commonly assume that data are a homogeneous set of observations when learning the structure of Bayesian networks. However, they often comprise different data sets that are related but not homogeneous because they have been collected in different ways or from different populations.   In our previous work (Azzimonti, Corani and Scutari, 2021), we proposed a closed-form Bayesian Hierarchical Dirichlet score for discrete data that pools information across related data sets to learn a single encompassing network structure, while taking into account the differences in their probabilistic structures. In this paper, we provide an analogous solution for learning a Bayesian network from continuous data using mixed-effects models to pool information across the related data sets. We study its structural, parametric, predictive and classification accuracy and we show that it outperforms both conditional Gaussian Bayesian networks (that do not perform any pooling) and classical Gaussian Bayesian networks (that disregard the heterogeneous nature of the data). The improvement is marked for low sample sizes and for unbalanced data sets.

</details>

<details>

<summary>2022-06-08 11:26:17 - Bayesian Predictive Decision Synthesis</summary>

- *Emily Tallman, Mike West*

- `2206.03815v1` - [abs](http://arxiv.org/abs/2206.03815v1) - [pdf](http://arxiv.org/pdf/2206.03815v1)

> Decision-guided perspectives on model uncertainty expand traditional statistical thinking about managing, comparing and combining inferences from sets of models. Bayesian predictive decision synthesis (BPDS) advances conceptual and theoretical foundations, and defines new methodology that explicitly integrates decision-analytic outcomes into the evaluation, comparison and potential combination of candidate models. BPDS extends recent theoretical and practical advances based on both Bayesian predictive synthesis and empirical goal-focused model uncertainty analysis. This is enabled by development of a novel subjective Bayesian perspective on model weighting in predictive decision settings. Illustrations come from applied contexts including optimal design for regression prediction and sequential time series forecasting for financial portfolio decisions.

</details>

<details>

<summary>2022-06-08 15:49:19 - Estimating the effects of a California gun control program with Multitask Gaussian Processes</summary>

- *Eli Ben-Michael, David Arbour, Avi Feller, Alex Franks, Steven Raphael*

- `2110.07006v2` - [abs](http://arxiv.org/abs/2110.07006v2) - [pdf](http://arxiv.org/pdf/2110.07006v2)

> Gun violence is a critical public safety concern in the United States. In 2006 California implemented a unique firearm monitoring program, the Armed and Prohibited Persons System (APPS), to address gun violence in the state. The APPS program first identifies those firearm owners who become prohibited from owning one due to federal or state law, then confiscates their firearms. Our goal is to assess the effect of APPS on California murder rates using annual, state-level crime data across the US for the years before and after the introduction of the program. To do so, we adapt a non-parametric Bayesian approach, multitask Gaussian Processes (MTGPs), to the panel data setting. MTGPs allow for flexible and parsimonious panel data models that nest many existing approaches and allow for direct control over both dependence across time and dependence across units, as well as natural uncertainty quantification. We extend this approach to incorporate non-Normal outcomes, auxiliary covariates, and multiple outcome series, which are all important in our application. We also show that this approach has attractive Frequentist properties, including a representation as a weighting estimator with separate weights over units and time periods. Applying this approach, we find that the increased monitoring and enforcement from the APPS program substantially decreased homicides in California. We also find that the effect on murder is driven entirely by declines in gun-related murder with no measurable effect on non-gun murder. Estimated cost per murder avoided are substantially lower than conventional estimates of the value of a statistical life, suggesting a very high benefit-cost ratio for this enforcement effort.

</details>

<details>

<summary>2022-06-08 16:13:04 - Neural Diffusion Processes</summary>

- *Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, Fergus Simpson*

- `2206.03992v1` - [abs](http://arxiv.org/abs/2206.03992v1) - [pdf](http://arxiv.org/pdf/2206.03992v1)

> Gaussian processes provide an elegant framework for specifying prior and posterior distributions over functions. They are, however, also computationally expensive, and limited by the expressivity of their covariance function. We propose Neural Diffusion Processes (NDPs), a novel approach based upon diffusion models, that learn to sample from distributions over functions. Using a novel attention block, we can incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs are able to capture functional distributions that are close to the true Bayesian posterior of a Gaussian process. This enables a variety of downstream tasks, including hyperparameter marginalisation and Bayesian optimisation.

</details>

<details>

<summary>2022-06-08 17:04:02 - Model Selection and Inference in Variational Longitudinal Distributed Lag Models</summary>

- *Mark J. Meyer, Selina Carter, Eileen McNeely, Elizabeth J. Malloy*

- `2206.04012v1` - [abs](http://arxiv.org/abs/2206.04012v1) - [pdf](http://arxiv.org/pdf/2206.04012v1)

> Flight-related health effects are a growing area of environmental health research with most work examining the concurrent impact of in-flight exposure on cardiac health. One understudied area is on the post-flight effects of in-flight exposures. Studies on the health effects of flight often collect a range of repeatedly sampled, time-varying exposure measurements both under crossover and longitudinal sampling designs. A natural choice to model the relationship of these lagged exposures on post-flight outcomes is the distributed lag model (DLM). However, longitudinal DLMs are a lightly studied class of models. In this article, we propose a class of models for analyzing longitudinal DLMs where the random effects can incorporate more general structures including random lags that arise from repeatedly sampling lagged exposures. We develop variational Bayesian algorithms to estimate model components under differing random effect structures, derive a variational AIC for model selection between these structures, and show how the converged variational estimates can fit into a framework for testing for the difference between two semiparametric curves. We then investigate the post-flight effects of in-flight, lagged exposures on heart health. We also perform simulation studies to evaluate the operating characteristics of our models.

</details>

<details>

<summary>2022-06-08 18:26:48 - Compositional Active Inference I: Bayesian Lenses. Statistical Games</summary>

- *Toby St. Clere Smithe*

- `2109.04461v2` - [abs](http://arxiv.org/abs/2109.04461v2) - [pdf](http://arxiv.org/pdf/2109.04461v2)

> We introduce the concepts of Bayesian lens, characterizing the bidirectional structure of exact Bayesian inference, and statistical game, formalizing the optimization objectives of approximate inference problems. We prove that Bayesian inversions compose according to the compositional lens pattern, and exemplify statistical games with a number of classic statistical concepts, from maximum likelihood estimation to generalized variational Bayesian methods. This paper is the first in a series laying the foundations for a compositional account of the theory of active inference, and we therefore pay particular attention to statistical games with a free-energy objective.

</details>

<details>

<summary>2022-06-08 19:21:30 - Bayesian multivariate logistic regression for superiority and inferiority decision-making under treatment heterogeneity</summary>

- *Xynthia Kavelaars, Joris Mulder, Maurits Kaptein*

- `2206.04133v1` - [abs](http://arxiv.org/abs/2206.04133v1) - [pdf](http://arxiv.org/pdf/2206.04133v1)

> The effects of a treatment may differ between patients with different characteristics. Addressing such treatment heterogeneity is crucial to identify which patients benefit from a treatment, but can be complex in the context of multiple correlated binary outcomes. The current paper presents a novel Bayesian method for estimation and inference for heterogeneous treatment effects in a multivariate binary setting. The framework is suitable for prediction of heterogeneous treatment effects and superiority/inferiority decision-making within subpopulations, while taking advantage of the size of the entire study sample.   We introduce a decision-making framework based on Bayesian multivariate logistic regression analysis with a P\'olya-Gamma expansion. The obtained regression coefficients are transformed into differences between success probabilities of treatments to allow for treatment comparison in terms of point estimation and superiority and/or inferiority decisions for different (sub)populations. Procedures for a priori sample size estimation under a non-informative prior distribution are included in the framework.   A numerical evaluation demonstrated that a) average and conditional treatment effect parameters could be estimated unbiasedly when the sample is large enough; b) decisions based on a priori sample size estimation resulted in anticipated error rates. Application to the International Stroke Trial dataset revealed a heterogeneous treatment effect: The model showed conditional treatment effects in opposite directions for patients with different levels of blood pressure, while the average treatment effect among the trial population was close to zero.

</details>

<details>

<summary>2022-06-09 02:26:22 - Learning Multitask Gaussian Bayesian Networks</summary>

- *Shuai Liu, Yixuan Qiu, Baojuan Li, Huaning Wang, Xiangyu Chang*

- `2205.05343v2` - [abs](http://arxiv.org/abs/2205.05343v2) - [pdf](http://arxiv.org/pdf/2205.05343v2)

> Major depressive disorder (MDD) requires study of brain functional connectivity alterations for patients, which can be uncovered by resting-state functional magnetic resonance imaging (rs-fMRI) data. We consider the problem of identifying alterations of brain functional connectivity for a single MDD patient. This is particularly difficult since the amount of data collected during an fMRI scan is too limited to provide sufficient information for individual analysis. Additionally, rs-fMRI data usually has the characteristics of incompleteness, sparsity, variability, high dimensionality and high noise. To address these problems, we proposed a multitask Gaussian Bayesian network (MTGBN) framework capable for identifying individual disease-induced alterations for MDD patients. We assume that such disease-induced alterations show some degrees of similarity with the tool to learn such network structures from observations to understanding of how system are structured jointly from related tasks. First, we treat each patient in a class of observation as a task and then learn the Gaussian Bayesian networks (GBNs) of this data class by learning from all tasks that share a default covariance matrix that encodes prior knowledge. This setting can help us to learn more information from limited data. Next, we derive a closed-form formula of the complete likelihood function and use the Monte-Carlo Expectation-Maximization(MCEM) algorithm to search for the approximately best Bayesian network structures efficiently. Finally, we assess the performance of our methods with simulated and real-world rs-fMRI data.

</details>

<details>

<summary>2022-06-09 06:26:54 - Data fission: splitting a single data point</summary>

- *James Leiner, Boyan Duan, Larry Wasserman, Aaditya Ramdas*

- `2112.11079v4` - [abs](http://arxiv.org/abs/2112.11079v4) - [pdf](http://arxiv.org/pdf/2112.11079v4)

> Suppose we observe a random vector $X$ from some distribution $P$ in a known family with unknown parameters. We ask the following question: when is it possible to split $X$ into two parts $f(X)$ and $g(X)$ such that neither part is sufficient to reconstruct $X$ by itself, but both together can recover $X$ fully, and the joint distribution of $(f(X),g(X))$ is tractable? As one example, if $X=(X_1,\dots,X_n)$ and $P$ is a product distribution, then for any $m<n$, we can split the sample to define $f(X)=(X_1,\dots,X_m)$ and $g(X)=(X_{m+1},\dots,X_n)$. Rasines and Young (2021) offers an alternative route of accomplishing this task through randomization of $X$ with additive Gaussian noise which enables post-selection inference in finite samples for Gaussian distributed data and asymptotically for non-Gaussian additive models. In this paper, we offer a more general methodology for achieving such a split in finite samples by borrowing ideas from Bayesian inference to yield a (frequentist) solution that can be viewed as a continuous analog of data splitting. We call our method data fission, as an alternative to data splitting, data carving and p-value masking. We exemplify the method on a few prototypical applications, such as post-selection inference for trend filtering and other regression problems.

</details>

<details>

<summary>2022-06-09 08:11:52 - Damage Identification in Fiber Metal Laminates using Bayesian Analysis with Model Order Reduction</summary>

- *Nanda Kishore Bellam Muralidhar, Carmen Grle, Natalie Rauter, Andrey Mikhaylenko, Rolf Lammering, Dirk A. Lorenz*

- `2206.04329v1` - [abs](http://arxiv.org/abs/2206.04329v1) - [pdf](http://arxiv.org/pdf/2206.04329v1)

> Fiber metal laminates (FML) are composite structures consisting of metals and fiber reinforced plastics (FRP) which have experienced an increasing interest as the choice of materials in aerospace and automobile industries. Due to a sophisticated built up of the material, not only the design and production of such structures is challenging but also its damage detection. This research work focuses on damage identification in FML with guided ultrasonic waves (GUW) through an inverse approach based on the Bayesian paradigm. As the Bayesian inference approach involves multiple queries of the underlying system, a parameterized reduced-order model (ROM) is used to closely approximate the solution with considerably less computational cost. The signals measured by the embedded sensors and the ROM forecasts are employed for the localization and characterization of damage in FML. In this paper, a Markov Chain Monte-Carlo (MCMC) based Metropolis-Hastings (MH) algorithm and an Ensemble Kalman filtering (EnKF) technique are deployed to identify the damage. Numerical tests illustrate the approaches and the results are compared in regard to accuracy and efficiency. It is found that both methods are successful in multivariate characterization of the damage with a high accuracy and were also able to quantify their associated uncertainties. The EnKF distinguishes itself with the MCMC-MH algorithm in the matter of computational efficiency. In this application of identifying the damage, the EnKF is approximately thrice faster than the MCMC-MH.

</details>

<details>

<summary>2022-06-09 09:46:28 - Normalized power priors always discount historical data</summary>

- *Samuel Pawel, Frederik Aust, Leonhard Held, Eric-Jan Wagenmakers*

- `2206.04379v1` - [abs](http://arxiv.org/abs/2206.04379v1) - [pdf](http://arxiv.org/pdf/2206.04379v1)

> Power priors are used for incorporating historical data in Bayesian analyses by taking the likelihood of the historical data raised to the power $\alpha$ as the prior distribution for the model parameters. The power parameter $\alpha$ is typically unknown and assigned a prior distribution, most commonly a beta distribution. Here, we give a novel theoretical result on the resulting marginal posterior distribution of $\alpha$ in case of the the normal and binomial model. Counterintuitively, when the current data perfectly mirror the historical data and the sample sizes from both data sets become arbitrarily large, the marginal posterior of $\alpha$ does not converge to a point mass at $\alpha = 1$ but approaches a distribution that hardly differs from the prior. The result implies that a complete pooling of historical and current data is impossible if a power prior with beta prior for $\alpha$ is used.

</details>

<details>

<summary>2022-06-09 14:25:39 - A Bayesian group sequential schema for ordinal endpoints</summary>

- *Chengxue Zhong, Haitao Pan, Hongyu Miao*

- `2108.06568v3` - [abs](http://arxiv.org/abs/2108.06568v3) - [pdf](http://arxiv.org/pdf/2108.06568v3)

> The ordinal endpoint is prevalent in clinical studies. For example, for the COVID-19, the most common endpoint used was 7-point ordinal scales. Another example is in phase II cancer studies, efficacy is often assessed as an ordinal variable based on a level of response of solid tumors with four categories: complete response, partial response, stable disease, and progression, though often a dichotomized approach is used in practices. However, there lack of designs for the ordinal endpoint despite Whitehead et al. (1993, 2017), Jaki et al. (2003) to list a few. In this paper, we propose a generic group sequential schema based on Bayesian methods for ordinal endpoints, including three methods, the proportional-odds-model (PO)-based, non-proportional-odds-model (NPO)-based, and PO/NPO switch-model-based designs, which makes our proposed methods generic to be able to deal with various scenarios. We conducted extensive simulations to demonstrate the desirable performances of the proposed method and an R package BayesOrdDesign has also been developed.

</details>

<details>

<summary>2022-06-09 17:36:17 - Regret Bounds for Information-Directed Reinforcement Learning</summary>

- *Botao Hao, Tor Lattimore*

- `2206.04640v1` - [abs](http://arxiv.org/abs/2206.04640v1) - [pdf](http://arxiv.org/pdf/2206.04640v1)

> Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning (RL). However, theoretical understanding of IDS for Markov Decision Processes (MDPs) is still limited. We develop novel information-theoretic tools to bound the information ratio and cumulative information gain about the learning target. Our theoretical results shed light on the importance of choosing the learning target such that the practitioners can balance the computation and regret bounds. As a consequence, we derive prior-free Bayesian regret bounds for vanilla-IDS which learns the whole environment under tabular finite-horizon MDPs. In addition, we propose a computationally-efficient regularized-IDS that maximizes an additive form rather than the ratio form and show that it enjoys the same regret bound as vanilla-IDS. With the aid of rate-distortion theory, we improve the regret bound by learning a surrogate, less informative environment. Furthermore, we extend our analysis to linear MDPs and prove similar regret bounds for Thompson sampling as a by-product.

</details>

<details>

<summary>2022-06-09 18:20:38 - Tackling covariate shift with node-based Bayesian neural networks</summary>

- *Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski*

- `2206.02435v2` - [abs](http://arxiv.org/abs/2206.02435v2) - [pdf](http://arxiv.org/pdf/2206.02435v2)

> Bayesian neural networks (BNNs) promise improved generalization under covariate shift by providing principled probabilistic representations of epistemic uncertainty. However, weight-based BNNs often struggle with high computational complexity of large-scale architectures and datasets. Node-based BNNs have recently been introduced as scalable alternatives, which induce epistemic uncertainty by multiplying each hidden node with latent random variables, while learning a point-estimate of the weights. In this paper, we interpret these latent noise variables as implicit representations of simple and domain-agnostic data perturbations during training, producing BNNs that perform well under covariate shift due to input corruptions. We observe that the diversity of the implicit corruptions depends on the entropy of the latent variables, and propose a straightforward approach to increase the entropy of these variables during training. We evaluate the method on out-of-distribution image classification benchmarks, and show improved uncertainty estimation of node-based BNNs under covariate shift due to input perturbations. As a side effect, the method also provides robustness against noisy training labels.

</details>

<details>

<summary>2022-06-09 19:07:04 - Valid and efficient imprecise-probabilistic inference across a spectrum of partial prior information</summary>

- *Ryan Martin*

- `2203.06703v3` - [abs](http://arxiv.org/abs/2203.06703v3) - [pdf](http://arxiv.org/pdf/2203.06703v3)

> Bayesian inference quantifies uncertainty directly and formally using classical probability theory, while frequentist inference does so indirectly and informally through the use of procedures with error rate control. Both have merits in the appropriate context, but the context isn't binary. If no prior information is available, then no prior distribution can be ruled out, so this context is best characterized as every prior. This implies there's an entire spectrum of contexts depending on what, if any, partial prior information is available, with "Bayesian" (one prior) and "frequentist" (every prior) on opposite extremes. Common examples between the two extremes include those high-dimensional problems where, e.g., sparsity assumptions are relevant but fall short of determining a complete prior distribution. This paper ties the two frameworks together by treating those cases where only partial prior information is available using the theory of imprecise probability. The end result is a unified framework of (imprecise-probabilistic) statistical inference with a new validity condition that implies both frequentist-style error rate control for derived procedures and Bayesian-style no-sure-loss properties, relative to the given partial prior information. This theory contains both the classical "Bayesian" and "frequentist" frameworks as special cases, since they're both valid in this new sense relative to their respective partial priors. Different constructions of these valid inferential models are considered, and compared based on their efficiency.

</details>

<details>

<summary>2022-06-09 19:14:52 - Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel Recombination</summary>

- *Masaki Adachi, Satoshi Hayakawa, Martin Jrgensen, Harald Oberhauser, Michael A. Osborne*

- `2206.04734v1` - [abs](http://arxiv.org/abs/2206.04734v1) - [pdf](http://arxiv.org/pdf/2206.04734v1)

> Calculation of Bayesian posteriors and model evidences typically requires numerical integration. Bayesian quadrature (BQ), a surrogate-model-based approach to numerical integration, is capable of superb sample efficiency, but its lack of parallelisation has hindered its practical applications. In this work, we propose a parallelised (batch) BQ method, employing techniques from kernel quadrature, that possesses a provably-exponential convergence rate. Additionally, just as with Nested Sampling, our method permits simultaneous inference of both posteriors and model evidence. Samples from our BQ surrogate model are re-selected to give a sparse set of samples, via a kernel recombination algorithm, requiring negligible additional time to increase the batch size. Empirically, we find that our approach significantly outperforms the sampling efficiency of both state-of-the-art BQ techniques and Nested Sampling in various real-world datasets, including lithium-ion battery analytics.

</details>

<details>

<summary>2022-06-09 19:28:56 - Direct and approximately valid probabilistic inference on a class of statistical functionals</summary>

- *Leonardo Cella, Ryan Martin*

- `2112.10232v2` - [abs](http://arxiv.org/abs/2112.10232v2) - [pdf](http://arxiv.org/pdf/2112.10232v2)

> Existing frameworks for probabilistic inference assume the quantity of interest is the parameter of a posited statistical model. In machine learning applications, however, often there is no statistical model/parameter; the quantity of interest is a statistical functional, a feature of the underlying distribution. Model-based methods can only handle such problems indirectly, via marginalization from a model parameter to the real quantity of interest. Here we develop a generalized inferential model (IM) framework for direct probabilistic uncertainty quantification on the quantity of interest. In particular, we construct a data-dependent, bootstrap-based possibility measure for uncertainty quantification and inference. We then prove that this new approach provides approximately valid inference in the sense that the plausibility values assigned to hypotheses about the unknowns are asymptotically well-calibrated in a frequentist sense. Among other things, this implies that confidence regions for the underlying functional derived from our proposed IM are approximately valid. The method is shown to perform well in key examples, including quantile regression, and in a personalized medicine application.

</details>

<details>

<summary>2022-06-09 21:19:07 - Joint Entropy Search For Maximally-Informed Bayesian Optimization</summary>

- *Carl Hvarfner, Frank Hutter, Luigi Nardi*

- `2206.04771v1` - [abs](http://arxiv.org/abs/2206.04771v1) - [pdf](http://arxiv.org/pdf/2206.04771v1)

> Information-theoretic Bayesian optimization techniques have become popular for optimizing expensive-to-evaluate black-box functions due to their non-myopic qualities. Entropy Search and Predictive Entropy Search both consider the entropy over the optimum in the input space, while the recent Max-value Entropy Search considers the entropy over the optimal value in the output space. We propose Joint Entropy Search (JES), a novel information-theoretic acquisition function that considers an entirely new quantity, namely the entropy over the joint optimal probability density over both input and output space. To incorporate this information, we consider the reduction in entropy from conditioning on fantasized optimal input/output pairs. The resulting approach primarily relies on standard GP machinery and removes complex approximations typically associated with information-theoretic methods. With minimal computational overhead, JES shows superior decision-making, and yields state-of-the-art performance for information-theoretic approaches across a wide suite of tasks. As a light-weight approach with superior results, JES provides a new go-to acquisition function for Bayesian optimization.

</details>

<details>

<summary>2022-06-10 06:57:53 - Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!</summary>

- *Luis Gruber, Gregor Kastner*

- `2206.04902v1` - [abs](http://arxiv.org/abs/2206.04902v1) - [pdf](http://arxiv.org/pdf/2206.04902v1)

> Vectorautogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinking priors, have shown to be successful in improving prediction performance. In the present paper we introduce the recently developed $R^2$-induced Dirichlet-decomposition prior to the VAR framework and compare it to refinements of well-known priors in the VAR literature. We demonstrate the virtues of the proposed prior in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing Illusion of Sparsity debate. We find that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames; dynamic model averaging, however, can combine the merits of both worlds. All priors are implemented using the reduced-form VAR and all models feature stochastic volatility in the variance-covariance matrix.

</details>

<details>

<summary>2022-06-10 08:56:27 - The perils of Kremlin's influence: evidence from Ukraine</summary>

- *Chiara Natalie Focacci, Mitja Kovac, Rok Spruk*

- `2206.04950v1` - [abs](http://arxiv.org/abs/2206.04950v1) - [pdf](http://arxiv.org/pdf/2206.04950v1)

> We examine the contribution of institutional integration to the institutional quality. To this end, we exploit the 2007 political crisis in Ukraine and examine the effects of staying out of the European Union for 28 Ukrainian provinces in the period 1996-2020. We construct novel subnational estimates of institutional quality for Ukraine and central and eastern European countries based on the latent residual component extraction of institutional quality from the existing governance indicators by making use of Bayesian posterior analysis under non-informative objective prior function. By comparing the residualized institutional quality trajectories of Ukrainian provinces with their central and eastern European peers that were admitted to the European Union in 2004 and after, we assess the institutional quality cost of being under Russian political influence and interference. Based on the large-scale synthetic control analysis, we find evidence of large-scale negative institutional quality effects of staying out of the European Union such as heightened political instability and rampant deterioration of the rule of law and control of corruption. Statistical significance of the estimated effects is evaluated across a comprehensive placebo simulation with more than 34 billion placebo averages for each institutional quality outcome.

</details>

<details>

<summary>2022-06-10 09:21:14 - $p$-Sparsified Sketches for Fast Multiple Output Kernel Methods</summary>

- *Tamim El Ahmad, Pierre Laforgue, Florence d'Alch-Buc*

- `2206.03827v2` - [abs](http://arxiv.org/abs/2206.03827v2) - [pdf](http://arxiv.org/pdf/2206.03827v2)

> Kernel methods are learning algorithms that enjoy solid theoretical foundations while suffering from important computational limitations. Sketching, that consists in looking for solutions among a subspace of reduced dimension, is a widely studied approach to alleviate this numerical burden. However, fast sketching strategies, such as non-adaptive subsampling, significantly degrade the guarantees of the algorithms, while theoretically-accurate sketches, such as the Gaussian one, turn out to remain relatively slow in practice. In this paper, we introduce the $p$-sparsified sketches, that combine the benefits from both approaches to achieve a good tradeoff between statistical accuracy and computational efficiency. To support our method, we derive excess risk bounds for both single and multiple output problems, with generic Lipschitz losses, providing new guarantees for a wide range of applications, from robust regression to multiple quantile regression. We also provide empirical evidences of the superiority of our sketches over recent SOTA approaches.

</details>

<details>

<summary>2022-06-10 11:08:31 - Posterior contraction for deep Gaussian process priors</summary>

- *Gianluca Finocchio, Johannes Schmidt-Hieber*

- `2105.07410v2` - [abs](http://arxiv.org/abs/2105.07410v2) - [pdf](http://arxiv.org/pdf/2105.07410v2)

> We study posterior contraction rates for a class of deep Gaussian process priors applied to the nonparametric regression problem under a general composition assumption on the regression function. It is shown that the contraction rates can achieve the minimax convergence rate (up to $\log n$ factors), while being adaptive to the underlying structure and smoothness of the target function. The proposed framework extends the Bayesian nonparametric theory for Gaussian process priors. We discuss the computational challenges of sampling from the posterior distribution.

</details>

<details>

<summary>2022-06-10 11:33:50 - Bayesian Inference of Stochastic Dynamical Networks</summary>

- *Yasen Wang, Junyang Jin, Jorge Goncalves*

- `2206.00858v2` - [abs](http://arxiv.org/abs/2206.00858v2) - [pdf](http://arxiv.org/pdf/2206.00858v2)

> Network inference has been extensively studied in several fields, such as systems biology and social sciences. Learning network topology and internal dynamics is essential to understand mechanisms of complex systems. In particular, sparse topologies and stable dynamics are fundamental features of many real-world continuous-time (CT) networks. Given that usually only a partial set of nodes are able to observe, in this paper, we consider linear CT systems to depict networks since they can model unmeasured nodes via transfer functions. Additionally, measurements tend to be noisy and with low and varying sampling frequencies. For this reason, we consider CT models since discrete-time approximations often require fine-grained measurements and uniform sampling steps. The developed method applies dynamical structure functions (DSFs) derived from linear stochastic differential equations (SDEs) to describe networks of measured nodes. A numerical sampling method, preconditioned Crank-Nicolson (pCN), is used to refine coarse-grained trajectories to improve inference accuracy. The convergence property of the developed method is robust to the dimension of data sources. Monte Carlo simulations indicate that the developed method outperforms state-of-the-art methods including group sparse Bayesian learning (GSBL), BINGO, kernel-based methods, dynGENIE3, GENIE3, and ARNI. The simulations include random and ring networks, and a synthetic biological network. These are challenging networks, suggesting that the developed method can be applied under a wide range of contexts, such as gene regulatory networks, social networks, and communication systems.

</details>

<details>

<summary>2022-06-10 12:12:41 - Scalable Deep Gaussian Markov Random Fields for General Graphs</summary>

- *Joel Oskarsson, Per Sidn, Fredrik Lindsten*

- `2206.05032v1` - [abs](http://arxiv.org/abs/2206.05032v1) - [pdf](http://arxiv.org/pdf/2206.05032v1)

> Machine learning methods on graphs have proven useful in many applications due to their ability to handle generally structured data. The framework of Gaussian Markov Random Fields (GMRFs) provides a principled way to define Gaussian models on graphs by utilizing their sparsity structure. We propose a flexible GMRF model for general graphs built on the multi-layer structure of Deep GMRFs, originally proposed for lattice graphs only. By designing a new type of layer we enable the model to scale to large graphs. The layer is constructed to allow for efficient training using variational inference and existing software frameworks for Graph Neural Networks. For a Gaussian likelihood, close to exact Bayesian inference is available for the latent field. This allows for making predictions with accompanying uncertainty estimates. The usefulness of the proposed model is verified by experiments on a number of synthetic and real world datasets, where it compares favorably to other both Bayesian and deep learning methods.

</details>

<details>

<summary>2022-06-10 13:55:19 - PAVI: Plate-Amortized Variational Inference</summary>

- *Louis Rouillard, Thomas Moreau, Demian Wassermann*

- `2206.05111v1` - [abs](http://arxiv.org/abs/2206.05111v1) - [pdf](http://arxiv.org/pdf/2206.05111v1)

> Given some observed data and a probabilistic generative model, Bayesian inference aims at obtaining the distribution of a model's latent parameters that could have yielded the data. This task is challenging for large population studies where thousands of measurements are performed over a cohort of hundreds of subjects, resulting in a massive latent parameter space. This large cardinality renders off-the-shelf Variational Inference (VI) computationally impractical. In this work, we design structured VI families that can efficiently tackle large population studies. To this end, our main idea is to share the parameterization and learning across the different i.i.d. variables in a generative model -symbolized by the model's plates. We name this concept plate amortization, and illustrate the powerful synergies it entitles, resulting in expressive, parsimoniously parameterized and orders of magnitude faster to train large scale hierarchical variational distributions. We illustrate the practical utility of PAVI through a challenging Neuroimaging example featuring a million latent parameters, demonstrating a significant step towards scalable and expressive Variational Inference.

</details>

<details>

<summary>2022-06-10 15:57:23 - Dynamic mean field programming</summary>

- *George Stamatescu*

- `2206.05200v1` - [abs](http://arxiv.org/abs/2206.05200v1) - [pdf](http://arxiv.org/pdf/2206.05200v1)

> A dynamic mean field theory is developed for model based Bayesian reinforcement learning in the large state space limit. In an analogy with the statistical physics of disordered systems, the transition probabilities are interpreted as couplings, and value functions as deterministic spins, and thus the sampled transition probabilities are considered to be quenched random variables. The results reveal that, under standard assumptions, the posterior over Q-values is asymptotically independent and Gaussian across state-action pairs, for infinite horizon problems. The finite horizon case exhibits the same behaviour for all state-actions pairs at each time but has an additional correlation across time, for each state-action pair. The results also hold for policy evaluation. The Gaussian statistics can be computed from a set of coupled mean field equations derived from the Bellman equation, which we call dynamic mean field programming (DMFP). For Q-value iteration, approximate equations are obtained by appealing to extreme value theory, and closed form expressions are found in the independent and identically distributed case. The Lyapunov stability of these closed form equations is studied.

</details>

<details>

<summary>2022-06-10 16:03:29 - Empirical Likelihood Based Bayesian Variable Selection</summary>

- *Yichen Cheng, Yichuan Zhao*

- `2206.05204v1` - [abs](http://arxiv.org/abs/2206.05204v1) - [pdf](http://arxiv.org/pdf/2206.05204v1)

> Empirical likelihood is a popular nonparametric statistical tool that does not require any distributional assumptions. In this paper, we explore the possibility of conducting variable selection via Bayesian empirical likelihood. We show theoretically that when the prior distribution satisfies certain mild conditions, the corresponding Bayesian empirical likelihood estimators are posteriorly consistent and variable selection consistent. As special cases, we show the prior of Bayesian empirical likelihood LASSO and SCAD satisfies such conditions and thus can identify the non-zero elements of the parameters with probability tending to 1. In addition, it is easy to verify that those conditions are met for other widely used priors such as ridge, elastic net and adaptive LASSO. Empirical likelihood depends on a parameter that needs to be obtained by numerically solving a non-linear equation. Thus, there exists no conjugate prior for the posterior distribution, which causes the slow convergence of the MCMC sampling algorithm in some cases. To solve this problem, we propose a novel approach, which uses an approximation distribution as the proposal. The computational results demonstrate quick convergence for the examples used in the paper. We use both simulation and real data analyses to illustrate the advantages of the proposed methods.

</details>

<details>

<summary>2022-06-10 16:17:48 - On the safe use of prior densities for Bayesian model selection</summary>

- *F. Llorente, L. Martino, E. Curbelo, J. Lopez-Santiago, D. Delgado*

- `2206.05210v1` - [abs](http://arxiv.org/abs/2206.05210v1) - [pdf](http://arxiv.org/pdf/2206.05210v1)

> The application of Bayesian inference for the purpose of model selection is very popular nowadays. In this framework, models are compared through their marginal likelihoods, or their quotients, called Bayes factors. However, marginal likelihoods depends on the prior choice. For model selection, even diffuse priors can be actually very informative, unlike for the parameter estimation problem. Furthermore, when the prior is improper, the marginal likelihood of the corresponding model is undetermined. In this work, we discuss the issue of prior sensitivity of the marginal likelihood and its role in model selection. We also comment on the use of uninformative priors, which are very common choices in practice. Several practical suggestions are discussed and many possible solutions, proposed in the literature, to design objective priors for model selection are described. Some of them also allow the use of improper priors. The connection between the marginal likelihood approach and the well-known information criteria is also presented. We describe the main issues and possible solutions by illustrative numerical examples, providing also some related code. One of them involving a real-world application on exoplanet detection.

</details>

<details>

<summary>2022-06-10 20:43:35 - Efficient Bayesian computation for low-photon imaging problems</summary>

- *Savvas Melidonis, Paul Dobson, Yoann Altmann, Marcelo Pereyra, Konstantinos C. Zygalakis*

- `2206.05350v1` - [abs](http://arxiv.org/abs/2206.05350v1) - [pdf](http://arxiv.org/pdf/2206.05350v1)

> This paper studies a new and highly efficient Markov chain Monte Carlo (MCMC) methodology to perform Bayesian inference in low-photon imaging problems, with particular attention to situations involving observation noise processes that deviate significantly from Gaussian noise, such as binomial, geometric and low-intensity Poisson noise. These problems are challenging for many reasons. From an inferential viewpoint, low-photon numbers lead to severe identifiability issues, poor stability and high uncertainty about the solution. Moreover, low-photon models often exhibit poor regularity properties that make efficient Bayesian computation difficult; e.g., hard non-negativity constraints, non-smooth priors, and log-likelihood terms with exploding gradients. More precisely, the lack of suitable regularity properties hinders the use of state-of-the-art Monte Carlo methods based on numerical approximations of the Langevin stochastic differential equation (SDE), as both the SDE and its numerical approximations behave poorly. We address this difficulty by proposing an MCMC methodology based on a reflected and regularised Langevin SDE, which is shown to be well-posed and exponentially ergodic under mild and easily verifiable conditions. This then allows us to derive four reflected proximal Langevin MCMC algorithms to perform Bayesian computation in low-photon imaging problems. The proposed approach is demonstrated with a range of experiments related to image deblurring, denoising, and inpainting under binomial, geometric and Poisson noise.

</details>

<details>

<summary>2022-06-10 22:38:19 - Bayesian Design with Sampling Windows for Complex Spatial Processes</summary>

- *Katie Buchhorn, Kerrie Mengersen, Edgar Santos-Fernandez, Erin E. Peterson, James M. McGree*

- `2206.05369v1` - [abs](http://arxiv.org/abs/2206.05369v1) - [pdf](http://arxiv.org/pdf/2206.05369v1)

> Optimal design facilitates intelligent data collection. In this paper, we introduce a fully Bayesian design approach for spatial processes with complex covariance structures, like those typically exhibited in natural ecosystems. Coordinate Exchange algorithms are commonly used to find optimal design points. However, collecting data at specific points is often infeasible in practice. Currently, there is no provision to allow for flexibility in the choice of design. We also propose an approach to find Bayesian sampling windows, rather than points, via Gaussian process emulation to identify regions of high design efficiency across a multi-dimensional space. These developments are motivated by two ecological case studies: monitoring water temperature in a river network system in the northwestern United States and monitoring submerged coral reefs off the north-west coast of Australia.

</details>

<details>

<summary>2022-06-11 07:12:04 - Enhancing Explainability of Hyperparameter Optimization via Bayesian Algorithm Execution</summary>

- *Julia Moosbauer, Giuseppe Casalicchio, Marius Lindauer, Bernd Bischl*

- `2206.05447v1` - [abs](http://arxiv.org/abs/2206.05447v1) - [pdf](http://arxiv.org/pdf/2206.05447v1)

> Despite all the benefits of automated hyperparameter optimization (HPO), most modern HPO algorithms are black-boxes themselves. This makes it difficult to understand the decision process which lead to the selected configuration, reduces trust in HPO, and thus hinders its broad adoption. Here, we study the combination of HPO with interpretable machine learning (IML) methods such as partial dependence plots. However, if such methods are naively applied to the experimental data of the HPO process in a post-hoc manner, the underlying sampling bias of the optimizer can distort interpretations. We propose a modified HPO method which efficiently balances the search for the global optimum w.r.t. predictive performance and the reliable estimation of IML explanations of an underlying black-box function by coupling Bayesian optimization and Bayesian Algorithm Execution. On benchmark cases of both synthetic objectives and HPO of a neural network, we demonstrate that our method returns more reliable explanations of the underlying black-box without a loss of optimization performance.

</details>

<details>

<summary>2022-06-11 17:06:52 - Bayesian Inverse Reinforcement Learning for Collective Animal Movement</summary>

- *Toryn L. J. Schafer, Christopher K. Wikle, Mevin B. Hooten*

- `2009.04003v3` - [abs](http://arxiv.org/abs/2009.04003v3) - [pdf](http://arxiv.org/pdf/2009.04003v3)

> Agent-based methods allow for defining simple rules that generate complex group behaviors. The governing rules of such models are typically set a priori and parameters are tuned from observed behavior trajectories. Instead of making simplifying assumptions across all anticipated scenarios, inverse reinforcement learning provides inference on the short-term (local) rules governing long term behavior policies by using properties of a Markov decision process. We use the computationally efficient linearly-solvable Markov decision process to learn the local rules governing collective movement for a simulation of the self propelled-particle (SPP) model and a data application for a captive guppy population. The estimation of the behavioral decision costs is done in a Bayesian framework with basis function smoothing. We recover the true costs in the SPP simulation and find the guppies value collective movement more than targeted movement toward shelter.

</details>

<details>

<summary>2022-06-12 02:47:29 - Density Regression and Uncertainty Quantification with Bayesian Deep Noise Neural Networks</summary>

- *Daiwei Zhang, Tianci Liu, Jian Kang*

- `2206.05643v1` - [abs](http://arxiv.org/abs/2206.05643v1) - [pdf](http://arxiv.org/pdf/2206.05643v1)

> Deep neural network (DNN) models have achieved state-of-the-art predictive accuracy in a wide range of supervised learning applications. However, accurately quantifying the uncertainty in DNN predictions remains a challenging task. For continuous outcome variables, an even more difficult problem is to estimate the predictive density function, which not only provides a natural quantification of the predictive uncertainty, but also fully captures the random variation in the outcome. In this work, we propose the Bayesian Deep Noise Neural Network (B-DeepNoise), which generalizes standard Bayesian DNNs by extending the random noise variable from the output layer to all hidden layers. The latent random noise equips B-DeepNoise with the flexibility to approximate highly complex predictive distributions and accurately quantify predictive uncertainty. For posterior computation, the unique structure of B-DeepNoise leads to a closed-form Gibbs sampling algorithm that iteratively simulates from the posterior full conditional distributions of the model parameters, circumventing computationally intensive Metropolis-Hastings methods. A theoretical analysis of B-DeepNoise establishes a recursive representation of the predictive distribution and decomposes the predictive variance with respect to the latent parameters. We evaluate B-DeepNoise against existing methods on benchmark regression datasets, demonstrating its superior performance in terms of prediction accuracy, uncertainty quantification accuracy, and uncertainty quantification efficiency. To illustrate our method's usefulness in scientific studies, we apply B-DeepNoise to predict general intelligence from neuroimaging features in the Adolescent Brain Cognitive Development (ABCD) project.

</details>

<details>

<summary>2022-06-12 04:20:11 - Variational Bayes Deep Operator Network: A data-driven Bayesian solver for parametric differential equations</summary>

- *Shailesh Garg, Souvik Chakraborty*

- `2206.05655v1` - [abs](http://arxiv.org/abs/2206.05655v1) - [pdf](http://arxiv.org/pdf/2206.05655v1)

> Neural network based data-driven operator learning schemes have shown tremendous potential in computational mechanics. DeepONet is one such neural network architecture which has gained widespread appreciation owing to its excellent prediction capabilities. Having said that, being set in a deterministic framework exposes DeepONet architecture to the risk of overfitting, poor generalization and in its unaltered form, it is incapable of quantifying the uncertainties associated with its predictions. We propose in this paper, a Variational Bayes DeepONet (VB-DeepONet) for operator learning, which can alleviate these limitations of DeepONet architecture to a great extent and give user additional information regarding the associated uncertainty at the prediction stage. The key idea behind neural networks set in Bayesian framework is that, the weights and bias of the neural network are treated as probability distributions instead of point estimates and, Bayesian inference is used to update their prior distribution. Now, to manage the computational cost associated with approximating the posterior distribution, the proposed VB-DeepONet uses \textit{variational inference}. Unlike Markov Chain Monte Carlo schemes, variational inference has the capacity to take into account high dimensional posterior distributions while keeping the associated computational cost low. Different examples covering mechanics problems like diffusion reaction, gravity pendulum, advection diffusion have been shown to illustrate the performance of the proposed VB-DeepONet and comparisons have also been drawn against DeepONet set in deterministic framework.

</details>

<details>

<summary>2022-06-12 08:28:32 - Solving the Poisson equation using coupled Markov chains</summary>

- *Randal Douc, Pierre E. Jacob, Anthony Lee, Dootika Vats*

- `2206.05691v1` - [abs](http://arxiv.org/abs/2206.05691v1) - [pdf](http://arxiv.org/pdf/2206.05691v1)

> This article draws connections between unbiased estimators constructed from coupled Markov chains that meet exactly after a random number of iterations, and solutions of the Poisson equation. We first show how such pairs of chains can be employed to obtain unbiased estimators of pointwise evaluations of solutions of the Poisson equation. We then propose new estimators of the asymptotic variance of Markov chain ergodic averages. We formally study the proposed estimators under realistic assumptions on the meeting times of the coupled chains and on the existence of moments of test functions under the target distribution. We illustrate their behaviour in toy examples and in a more challenging setting of high-dimensional Bayesian regression.

</details>

<details>

<summary>2022-06-12 12:47:09 - Analysis of Connection Times in Bipartite Network Data: Development of the Bayesian Latent Space Accumulator Model with Applications to Assessment Data</summary>

- *Jonghyun Yun, Hyunjoo Kim, Minjeong Jeon, Ick Hoon Jin*

- `2203.14306v2` - [abs](http://arxiv.org/abs/2203.14306v2) - [pdf](http://arxiv.org/pdf/2203.14306v2)

> Conventional social network analysis typically focuses on analyzing the structure of the connections between pairs of nodes in a sample dataset. However, the process and the consequences of how long it takes pairs of nodes to be connected, i.e., node connection times, on the network structure have been understudied in the literature. In this article, we propose a novel statistical approach, so-called the Bayesian latent space accumulator model, for modeling connection times and their influence on the structure of connections. We focus on a special type of bipartite network composed of respondents and test items, where connection outcomes are binary and mutually exclusive. To model connection times for each connection outcome, we leverage ideas from the competing risk modeling approach and embed latent spaces into the competing risk models to capture heterogeneous dependence structures of connection times across connection outcome types. The proposed approach is applied and illustrated with two real data examples.

</details>

<details>

<summary>2022-06-12 17:29:27 - Bivariate Inverse Topp-Leone Model to Counter Heterogeneous Data</summary>

- *Shikhar Tyagi*

- `2206.05798v1` - [abs](http://arxiv.org/abs/2206.05798v1) - [pdf](http://arxiv.org/pdf/2206.05798v1)

> In probability and statistics, reliable modeling of bivariate continuous characteristics remains a real insurmountable consideration. During analysis of bivariate data, we have to deal with heterogeneity that is present in data. Therefore, for dealing with such a scenario, we investigate a novel technique based on a Farlie-Gumbel-Morgenstern (FGM) copula and the inverse Topp-Leone (ITL) model in this study. The idea is to use the oscillating functionalities of the FGM copula and the flexibility of the ITL model to propose a serious bivariate solution for the modeling of bivariate lifetime phenomena to counter the heterogeneity present in data. Both theory and practice are developed. In particular, we determine the main functions related to the model, like the cumulative model function, probability density function, conditional density function, and various useful dependence measures for bivariate modeling. The model parameters are estimated using the maximum likelihood method and Bayesian framework of Markov Chain Monte Carlo (MCMC) methodology. Following that, model comparison methods are used to compare models. To explain the findings and show that better models are recommended, the famous Drought and Burr data sets are used.

</details>

<details>

<summary>2022-06-12 19:48:24 - Bayesian NVH metamodels to assess interior cabin noise using measurement databases</summary>

- *V. Prakash, O. Sauvage, J. Antoni, L. Gagliardini*

- `2207.02120v1` - [abs](http://arxiv.org/abs/2207.02120v1) - [pdf](http://arxiv.org/pdf/2207.02120v1)

> In recent years, a great emphasis has been put on engineering the acoustic signature of vehicles that represents the overall comfort level for passengers. Due to highly uncertain behavior of production cars, probabilistic metamodels or surrogates can be useful to estimate the NVH dispersion and assess different NVH risks. These metamodels follow physical behaviors and shall aid as a design space exploration tool during the early stage design process to support the NVH optimization. The measurement databases constitute different noise paths such as aerodynamic noise (wind-tunnel test), tire-pavement interaction noise (rolling noise), and noise due to electric motors (whining noise). This research work proposes a global NVH metamodeling technique for broadband noises such as aerodynamic and rolling noises exploiting the Bayesian framework that takes into account the prior (domain-expert) knowledge about complex physical mechanisms. Generalized additive models (GAMs) with polynomials and Gaussian basis functions are used to model the dependency of sound pressure level (SPL) on predictor variables. Moreover, parametric bootstrap algorithm based on data-generating mechanism using the point estimates is used to estimate the dispersion in unknown parameters. Probabilistic modelling is carried out using an open-source library PyMC3 that utilizes No-U-Turn sampler (NUTS) and the developed models are validated using Cross-Validation technique.

</details>

<details>

<summary>2022-06-13 02:49:27 - Posterior covariance information criterion for arbitrary loss functions</summary>

- *Yukito Iba, Keisuke Yano*

- `2206.05887v1` - [abs](http://arxiv.org/abs/2206.05887v1) - [pdf](http://arxiv.org/pdf/2206.05887v1)

> We propose a novel computationally low-cost method for estimating the predictive risks of Bayesian methods for arbitrary loss functions. The proposed method utilises posterior covariance and provides estimators of the Gibbs and the plugin generalization errors. We present theoretical guarantees of the proposed method, clarifying the connection between the widely applicable information criterion, the Bayesian sensitivity analysis, and the infinitesimal jackknife approximation of Bayesian leave-one-out cross validation. An application to differentially-private learning is also discussed.

</details>

<details>

<summary>2022-06-13 07:27:48 - A Bayesian Model to Estimate Abundance Based on Scarce Animal Vestige Data</summary>

- *Niamh Mimnagh, Iuri Ferreira, Luciano Verdade, Rafael de Andrade Moral*

- `2206.05944v1` - [abs](http://arxiv.org/abs/2206.05944v1) - [pdf](http://arxiv.org/pdf/2206.05944v1)

> We propose a modelling framework which allows for the estimation of abundances from trace counts. This indirect method of estimating abundance is attractive due to the relative affordability with which it may be carried out, and the reduction in possible risk posed to animals and humans when compared to direct methods for estimating animal abundance. We assess these methods by performing simulations which allow us to examine the accuracy of model estimates. The models are then fitted to several case studies to obtain abundance estimates for collared peccaries in Brazil, kit foxes in Arizona, red foxes in Italy and sika deer in Scotland. Simulation results reveal that these models produce accurate estimates of abundance at a range of sample sizes. In particular, this modelling framework produces accurate estimates when data is very scarce. The use of vestige counts in estimating abundance allows for the monitoring of species which may otherwise go undetected due to their reclusive nature. Additionally, the efficacy of these models when data is collected at very few transects will allow for the use of small-scale data collection programmes which may be carried out at reduced cost, when compared to larger-scale data collection.

</details>

<details>

<summary>2022-06-13 09:11:48 - Modeling the Machine Learning Multiverse</summary>

- *Samuel J. Bell, Onno P. Kampman, Jesse Dodge, Neil D. Lawrence*

- `2206.05985v1` - [abs](http://arxiv.org/abs/2206.05985v1) - [pdf](http://arxiv.org/pdf/2206.05985v1)

> Amid mounting concern about the reliability and credibility of machine learning research, we present a principled framework for making robust and generalizable claims: the Multiverse Analysis. Our framework builds upon the Multiverse Analysis (Steegen et al., 2016) introduced in response to psychology's own reproducibility crisis. To efficiently explore high-dimensional and often continuous ML search spaces, we model the multiverse with a Gaussian Process surrogate and apply Bayesian experimental design. Our framework is designed to facilitate drawing robust scientific conclusions about model performance, and thus our approach focuses on exploration rather than conventional optimization. In the first of two case studies, we investigate disputed claims about the relative merit of adaptive optimizers. Second, we synthesize conflicting research on the effect of learning rate on the large batch training generalization gap. For the machine learning community, the Multiverse Analysis is a simple and effective technique for identifying robust claims, for increasing transparency, and a step toward improved reproducibility.

</details>

<details>

<summary>2022-06-13 09:34:10 - The Bahadur representation of sample quantiles for associated sequences</summary>

- *Lahcen Douge*

- `2206.05995v1` - [abs](http://arxiv.org/abs/2206.05995v1) - [pdf](http://arxiv.org/pdf/2206.05995v1)

> In this paper, the Bahadur representation of sample quantiles based on associated sequences is established under polynomially decaying of covariances. The rate of approximation depends on the covariances decay degree and becomes close to the optimal rate obtained under independence when the covariances decrease fastly to 0.

</details>

<details>

<summary>2022-06-13 11:23:45 - Competing Persuaders in Zero-Sum Games</summary>

- *Dilip Ravindran, Zhihan Cui*

- `2008.08517v2` - [abs](http://arxiv.org/abs/2008.08517v2) - [pdf](http://arxiv.org/pdf/2008.08517v2)

> We study Bayesian Persuasion with multiple senders who have access to conditionally independent experiments (and possibly others). Senders have zero-sum preferences over information revealed. We characterize when any set of states can be pooled in equilibrium and when all equilibria are fully revealing. The state is fully revealed in every equilibrium if and only if sender utility functions are `globally nonlinear'. With two states, this is equivalent to some sender having nontrivial preferences. The upshot is that `most' zero-sum sender preferences result in full revelation. We explore what conditions are important for competition to result in such stark information revelation.

</details>

<details>

<summary>2022-06-13 12:45:21 - Deep Reinforcement Learning with Weighted Q-Learning</summary>

- *Andrea Cini, Carlo D'Eramo, Jan Peters, Cesare Alippi*

- `2003.09280v3` - [abs](http://arxiv.org/abs/2003.09280v3) - [pdf](http://arxiv.org/pdf/2003.09280v3)

> Reinforcement learning algorithms based on Q-learning are driving Deep Reinforcement Learning (DRL) research towards solving complex problems and achieving super-human performance on many of them. Nevertheless, Q-Learning is known to be positively biased since it learns by using the maximum over noisy estimates of expected values. Systematic overestimation of the action values coupled with the inherently high variance of DRL methods can lead to incrementally accumulate errors, causing learning algorithms to diverge. Ideally, we would like DRL agents to take into account their own uncertainty about the optimality of each action, and be able to exploit it to make more informed estimations of the expected return. In this regard, Weighted Q-Learning (WQL) effectively reduces bias and shows remarkable results in stochastic environments. WQL uses a weighted sum of the estimated action values, where the weights correspond to the probability of each action value being the maximum; however, the computation of these probabilities is only practical in the tabular setting. In this work, we provide methodological advances to benefit from the WQL properties in DRL, by using neural networks trained with Dropout as an effective approximation of deep Gaussian processes. In particular, we adopt the Concrete Dropout variant to obtain calibrated estimates of epistemic uncertainty in DRL. The estimator, then, is obtained by taking several stochastic forward passes through the action-value network and computing the weights in a Monte Carlo fashion. Such weights are Bayesian estimates of the probability of each action value corresponding to the maximum w.r.t. a posterior probability distribution estimated by Dropout. We show how our novel Deep Weighted Q-Learning algorithm reduces the bias w.r.t. relevant baselines and provides empirical evidence of its advantages on representative benchmarks.

</details>

<details>

<summary>2022-06-13 13:08:44 - The Sources of Statistical Bias Series: Simulated Demonstrations to Illustrate the Causes and Effects of Biases in Statistical Estimates</summary>

- *Ian A Silver*

- `2101.07097v12` - [abs](http://arxiv.org/abs/2101.07097v12) - [pdf](http://arxiv.org/pdf/2101.07097v12)

> When teaching and discussing statistical assumptions, our focus is oftentimes placed on how to test and address potential violations rather than the effects of violating assumptions on the estimates produced by our statistical models. The latter represents a potential avenue to help us better understand the impact of researcher degrees of freedom on the statistical estimates we produce. The Violating Assumptions Series is an endeavor I have undertaken to demonstrate the effects of violating assumptions on the estimates produced across various statistical models. The series will review assumptions associated with estimating causal associations, as well as more complicated statistical models including, but not limited to, multilevel models, path models, structural equation models, and Bayesian models. In addition to the primary goal, the series of posts is designed to illustrate how simulations can be used to develop a comprehensive understanding of applied statistics.

</details>

<details>

<summary>2022-06-13 13:53:42 - Interventions, Where and How? Experimental Design for Causal Models at Scale</summary>

- *Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schlkopf, Yarin Gal, Stefan Bauer*

- `2203.02016v2` - [abs](http://arxiv.org/abs/2203.02016v2) - [pdf](http://arxiv.org/pdf/2203.02016v2)

> Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-R\`enyi, Scale Free) for both linear and nonlinear SCMs as well as on the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.

</details>

<details>

<summary>2022-06-13 20:43:39 - Density Estimation with Autoregressive Bayesian Predictives</summary>

- *Sahra Ghalebikesabi, Chris Holmes, Edwin Fong, Brieuc Lehmann*

- `2206.06462v1` - [abs](http://arxiv.org/abs/2206.06462v1) - [pdf](http://arxiv.org/pdf/2206.06462v1)

> Bayesian methods are a popular choice for statistical inference in small-data regimes due to the regularization effect induced by the prior, which serves to counteract overfitting. In the context of density estimation, the standard Bayesian approach is to target the posterior predictive. In general, direct estimation of the posterior predictive is intractable and so methods typically resort to approximating the posterior distribution as an intermediate step. The recent development of recursive predictive copula updates, however, has made it possible to perform tractable predictive density estimation without the need for posterior approximation. Although these estimators are computationally appealing, they tend to struggle on non-smooth data distributions. This is largely due to the comparatively restrictive form of the likelihood models from which the proposed copula updates were derived. To address this shortcoming, we consider a Bayesian nonparametric model with an autoregressive likelihood decomposition and Gaussian process prior, which yields a data-dependent bandwidth parameter in the copula update. Further, we formulate a novel parameterization of the bandwidth using an autoregressive neural network that maps the data into a latent space, and is thus able to capture more complex dependencies in the data. Our extensions increase the modelling capacity of existing recursive Bayesian density estimators, achieving state-of-the-art results on tabular data sets.

</details>

<details>

<summary>2022-06-13 21:40:44 - On the Computational Complexity of Metropolis-Adjusted Langevin Algorithms for Bayesian Posterior Sampling</summary>

- *Rong Tang, Yun Yang*

- `2206.06491v1` - [abs](http://arxiv.org/abs/2206.06491v1) - [pdf](http://arxiv.org/pdf/2206.06491v1)

> In this paper, we study the computational complexity of sampling from a Bayesian posterior (or pseudo-posterior) using the Metropolis-adjusted Langevin algorithm (MALA). MALA first applies a discrete-time Langevin SDE to propose a new state, and then adjusts the proposed state using Metropolis-Hastings rejection. Most existing theoretical analysis of MALA relies on the smoothness and strongly log-concavity properties of the target distribution, which unfortunately is often unsatisfied in practical Bayesian problems. Our analysis relies on the statistical large sample theory, which restricts the deviation of the Bayesian posterior from being smooth and log-concave in a very specific manner. Specifically, we establish the optimal parameter dimension dependence of $d^{1/3}$ in the non-asymptotic mixing time upper bound for MALA after the burn-in period without assuming the smoothness and log-concavity of the target posterior density, where MALA is slightly modified by replacing the gradient with any subgradient if non-differentiable. In comparison, the well-known scaling limit for the classical Metropolis random walk (MRW) suggests a linear $d$ dimension dependence in its mixing time. Thus, our results formally verify the conventional wisdom that MALA, as a first-order method using gradient information, is more efficient than MRW as a zeroth-order method only using function value information in the context of Bayesian computation.

</details>

<details>

<summary>2022-06-14 01:36:19 - A Latent Mixture Model for Heterogeneous Causal Mechanisms in Mendelian Randomization</summary>

- *Daniel Iong, Qingyuan Zhao, Yang Chen*

- `2007.06476v3` - [abs](http://arxiv.org/abs/2007.06476v3) - [pdf](http://arxiv.org/pdf/2007.06476v3)

> Mendelian Randomization (MR) is a popular method in epidemiology and genetics that uses genetic variation as instrumental variables for causal inference. Existing MR methods usually assume most genetic variants are valid instrumental variables that identify a common causal effect. There is a general lack of awareness that this effect homogeneity assumption can be violated when there are multiple causal pathways involved, even if all the instrumental variables are valid. In this article, we introduce a latent mixture model MR-PATH that groups instruments that yield similar causal effect estimates together. We develop a Monte-Carlo EM algorithm to fit this mixture model, derive approximate confidence intervals for uncertainty quantification, and adopt a modified Bayesian Information Criterion (BIC) for model selection. We verify the efficacy of the Monte-Carlo EM algorithm, confidence intervals, and model selection criterion using numerical simulations. We identify potential mechanistic heterogeneity when applying our method to estimate the effect of high-density lipoprotein cholesterol on coronary heart disease and the effect of adiposity on type II diabetes.

</details>

<details>

<summary>2022-06-14 01:40:06 - Estimating a Causal Exposure Response Function with a Continuous Error-Prone Exposure: A Study of Fine Particulate Matter and All-Cause Mortality</summary>

- *Kevin P. Josey, Priyanka deSouza, Xiao Wu, Danielle Braun, Rachel Nethery*

- `2109.15264v2` - [abs](http://arxiv.org/abs/2109.15264v2) - [pdf](http://arxiv.org/pdf/2109.15264v2)

> Numerous studies have examined the associations between long-term exposure to fine particulate matter (PM2.5) and adverse health outcomes. Recently, many of these studies have begun to employ high-resolution predicted PM2.5 concentrations, which are subject to measurement error. Previous approaches for exposure measurement error correction have either been applied in non-causal settings or have only considered a categorical exposure. Moreover, most procedures have failed to account for uncertainty induced by error correction when fitting an exposure-response function (ERF). To remedy these deficiencies, we develop a multiple imputation framework that combines regression calibration and Bayesian techniques to estimate a causal ERF. We demonstrate how the output of the measurement error correction steps can be seamlessly integrated into a Bayesian additive regression trees (BART) estimator of the causal ERF. We also demonstrate how locally-weighted smoothing of the posterior samples from BART can be used to create a more accurate ERF estimate. Our proposed approach also properly propagates the exposure measurement error uncertainty to yield accurate standard error estimates. We assess the robustness of our proposed approach in an extensive simulation study. We then apply our methodology to estimate the effects of PM2.5 on all-cause mortality among Medicare enrollees in New England from 2000-2012.

</details>

<details>

<summary>2022-06-14 05:31:52 - Nonparametric inference on counterfactuals in first-price auctions</summary>

- *Pasha Andreyanov, Grigory Franguridi*

- `2106.13856v2` - [abs](http://arxiv.org/abs/2106.13856v2) - [pdf](http://arxiv.org/pdf/2106.13856v2)

> In a classical model of the first-price sealed-bid auction with independent private values, we develop nonparametric estimation and inference procedures for a class of policy-relevant metrics, such as total expected surplus and expected revenue under counterfactual reserve prices. Motivated by the linearity of these metrics in the quantile function of bidders' values, we propose a bid spacings-based estimator of the latter and derive its Bahadur-Kiefer expansion. This makes it possible to construct exact uniform confidence bands and assess the optimality of a given auction rule. Using the data on U.S. Forest Service timber auctions, we test whether setting zero reserve prices in these auctions was revenue maximizing.

</details>

<details>

<summary>2022-06-14 06:28:09 - Bayesian forecast combination using time-varying features</summary>

- *Li Li, Yanfei Kang, Feng Li*

- `2108.02082v3` - [abs](http://arxiv.org/abs/2108.02082v3) - [pdf](http://arxiv.org/pdf/2108.02082v3)

> In this work, we propose a novel framework for density forecast combination by constructing time-varying weights based on time series features, which is called Feature-based Bayesian Forecasting Model Averaging (FEBAMA). Our framework estimates weights in the forecast combination via Bayesian log predictive scores, in which the optimal forecasting combination is determined by time series features from historical information. In particular, we use an automatic Bayesian variable selection method to add weight to the importance of different features. To this end, our approach has better interpretability compared to other black-box forecasting combination schemes. We apply our framework to stock market data and M3 competition data. Based on our structure, a simple maximum-a-posteriori scheme outperforms benchmark methods, and Bayesian variable selection can further enhance the accuracy for both point and density forecasts.

</details>

<details>

<summary>2022-06-14 07:51:03 - 50 shades of Bayesian testing of hypotheses</summary>

- *Christian P Robert*

- `2206.06659v1` - [abs](http://arxiv.org/abs/2206.06659v1) - [pdf](http://arxiv.org/pdf/2206.06659v1)

> Hypothesis testing and model choice are quintessential questions for statistical inference and while the Bayesian paradigm seems ideally suited for answering these questions, it faces difficulties of its own ranging from prior modelling to calibration, to numerical implementation. This c

</details>

<details>

<summary>2022-06-14 08:51:20 - On the Role of Channel Capacity in Learning Gaussian Mixture Models</summary>

- *Elad Romanov, Tamir Bendory, Or Ordentlich*

- `2202.07707v2` - [abs](http://arxiv.org/abs/2202.07707v2) - [pdf](http://arxiv.org/pdf/2202.07707v2)

> This paper studies the sample complexity of learning the $k$ unknown centers of a balanced Gaussian mixture model (GMM) in $\mathbb{R}^d$ with spherical covariance matrix $\sigma^2\mathbf{I}$. In particular, we are interested in the following question: what is the maximal noise level $\sigma^2$, for which the sample complexity is essentially the same as when estimating the centers from labeled measurements? To that end, we restrict attention to a Bayesian formulation of the problem, where the centers are uniformly distributed on the sphere $\sqrt{d}\mathcal{S}^{d-1}$. Our main results characterize the exact noise threshold $\sigma^2$ below which the GMM learning problem, in the large system limit $d,k\to\infty$, is as easy as learning from labeled observations, and above which it is substantially harder. The threshold occurs at $\frac{\log k}{d} = \frac12\log\left( 1+\frac{1}{\sigma^2} \right)$, which is the capacity of the additive white Gaussian noise (AWGN) channel. Thinking of the set of $k$ centers as a code, this noise threshold can be interpreted as the largest noise level for which the error probability of the code over the AWGN channel is small. Previous works on the GMM learning problem have identified the minimum distance between the centers as a key parameter in determining the statistical difficulty of learning the corresponding GMM. While our results are only proved for GMMs whose centers are uniformly distributed over the sphere, they hint that perhaps it is the decoding error probability associated with the center constellation as a channel code that determines the statistical difficulty of learning the corresponding GMM, rather than just the minimum distance.

</details>

<details>

<summary>2022-06-14 10:04:41 - Deep Variational Implicit Processes</summary>

- *Luis A. Ortega, Simn Rodrguez Santana, Daniel Hernndez-Lobato*

- `2206.06720v1` - [abs](http://arxiv.org/abs/2206.06720v1) - [pdf](http://arxiv.org/pdf/2206.06720v1)

> Implicit processes (IPs) are a generalization of Gaussian processes (GPs). IPs may lack a closed-form expression but are easy to sample from. Examples include, among others, Bayesian neural networks or neural samplers. IPs can be used as priors over functions, resulting in flexible models with well-calibrated prediction uncertainty estimates. Methods based on IPs usually carry out function-space approximate inference, which overcomes some of the difficulties of parameter-space approximate inference. Nevertheless, the approximations employed often limit the expressiveness of the final model, resulting, \emph{e.g.}, in a Gaussian predictive distribution, which can be restrictive. We propose here a multi-layer generalization of IPs called the Deep Variational Implicit process (DVIP). This generalization is similar to that of deep GPs over GPs, but it is more flexible due to the use of IPs as the prior distribution over the latent functions. We describe a scalable variational inference algorithm for training DVIP and show that it outperforms previous IP-based methods and also deep GPs. We support these claims via extensive regression and classification experiments. We also evaluate DVIP on large datasets with up to several million data instances to illustrate its good scalability and performance.

</details>

<details>

<summary>2022-06-14 12:13:25 - A Surrogate-Assisted Uncertainty-Aware Bayesian Validation Framework and its Application to Coupling Free Flow and Porous-Medium Flow</summary>

- *Farid Mohammadi, Elissa Eggenweiler, Bernd Flemisch, Sergey Oladyshkin, Iryna Rybak, Martin Schneider, Kilian Weishaupt*

- `2106.13639v3` - [abs](http://arxiv.org/abs/2106.13639v3) - [pdf](http://arxiv.org/pdf/2106.13639v3)

> Existing model validation studies in geoscience often disregard or partly account for uncertainties in observations, model choices, and input parameters. In this work, we develop a statistical framework that incorporates a probabilistic modeling technique using a fully Bayesian approach to perform a quantitative uncertainty-aware validation. A Bayesian perspective on a validation task yields an optimal bias-variance trade-off against the reference data. It provides an integrative metric for model validation that incorporates parameter and conceptual uncertainty. Additionally, a surrogate modeling technique, namely Bayesian Sparse Polynomial Chaos Expansion, is employed to accelerate the computationally demanding Bayesian calibration and validation. We apply this validation framework to perform a comparative evaluation of models for coupling a free flow with a porous-medium flow. The correct choice of interface conditions and proper model parameters for such coupled flow systems is crucial for physically consistent modeling and accurate numerical simulations of applications. We develop a benchmark scenario that uses the Stokes equations to describe the free flow and considers different models for the porous-medium compartment and the coupling at the fluid--porous interface. These models include a porous-medium model using Darcy's law at the representative elementary volume scale with classical or generalized interface conditions and a pore-network model with its related coupling approach. We study the coupled flow problems' behaviors considering a benchmark case, where a pore-scale resolved model provides the reference solution. With the suggested framework, we perform sensitivity analysis, quantify the parametric uncertainties, demonstrate each model's predictive capabilities, and make a probabilistic model comparison.

</details>

<details>

<summary>2022-06-14 13:29:14 - Density Regression with Conditional Support Points</summary>

- *Yunlu Chen, Nan Zhang*

- `2206.06833v1` - [abs](http://arxiv.org/abs/2206.06833v1) - [pdf](http://arxiv.org/pdf/2206.06833v1)

> Density regression characterizes the conditional density of the response variable given the covariates, and provides much more information than the commonly used conditional mean or quantile regression. However, it is often computationally prohibitive in applications with massive data sets, especially when there are multiple covariates. In this paper, we develop a new data reduction approach for the density regression problem using conditional support points. After obtaining the representative data, we exploit the penalized likelihood method as the downstream estimation strategy. Based on the connections among the continuous ranked probability score, the energy distance, the $L_2$ discrepancy and the symmetrized Kullback-Leibler distance, we investigate the distributional convergence of the representative points and establish the rate of convergence of the density regression estimator. The usefulness of the methodology is illustrated by modeling the conditional distribution of power output given multivariate environmental factors using a large scale wind turbine data set. Supplementary materials for this article are available online.

</details>

<details>

<summary>2022-06-14 13:50:14 - On the Convergence of the Shapley Value in Parametric Bayesian Learning Games</summary>

- *Lucas Agussurja, Xinyi Xu, Bryan Kian Hsiang Low*

- `2205.07428v2` - [abs](http://arxiv.org/abs/2205.07428v2) - [pdf](http://arxiv.org/pdf/2205.07428v2)

> Measuring contributions is a classical problem in cooperative game theory where the Shapley value is the most well-known solution concept. In this paper, we establish the convergence property of the Shapley value in parametric Bayesian learning games where players perform a Bayesian inference using their combined data, and the posterior-prior KL divergence is used as the characteristic function. We show that for any two players, under some regularity conditions, their difference in Shapley value converges in probability to the difference in Shapley value of a limiting game whose characteristic function is proportional to the log-determinant of the joint Fisher information. As an application, we present an online collaborative learning framework that is asymptotically Shapley-fair. Our result enables this to be achieved without any costly computations of posterior-prior KL divergences. Only a consistent estimator of the Fisher information is needed. The effectiveness of our framework is demonstrated with experiments using real-world data.

</details>

<details>

<summary>2022-06-14 14:42:55 - Bayesian Fixed-domain Asymptotics for Covariance Parameters in a Gaussian Process Model</summary>

- *Cheng Li*

- `2010.02126v3` - [abs](http://arxiv.org/abs/2010.02126v3) - [pdf](http://arxiv.org/pdf/2010.02126v3)

> Gaussian process models typically contain finite dimensional parameters in the covariance function that need to be estimated from the data. We study the Bayesian fixed-domain asymptotics for the covariance parameters in a universal kriging model with an isotropic Matern covariance function, which has many applications in spatial statistics. We show that when the dimension of domain is less than or equal to three, the joint posterior distribution of the microergodic parameter and the range parameter can be factored independently into the product of their marginal posteriors under fixed-domain asymptotics. The posterior of the microergodic parameter is asymptotically close in total variation distance to a normal distribution with shrinking variance, while the posterior distribution of the range parameter does not converge to any point mass distribution in general. Our theory allows an unbounded prior support for the range parameter and flexible designs of sampling points. We further study the asymptotic efficiency and convergence rates in posterior prediction for the Bayesian kriging predictor with covariance parameters randomly drawn from their posterior distribution. In the special case of one-dimensional Ornstein-Uhlenbeck process, we derive explicitly the limiting posterior of the range parameter and the posterior convergence rate for asymptotic efficiency in posterior prediction. We verify these asymptotic results in numerical experiments.

</details>

<details>

<summary>2022-06-14 14:48:24 - A new algorithm for structural restrictions in Bayesian vector autoregressions</summary>

- *Dimitris Korobilis*

- `2206.06892v1` - [abs](http://arxiv.org/abs/2206.06892v1) - [pdf](http://arxiv.org/pdf/2206.06892v1)

> A comprehensive methodology for inference in vector autoregressions (VARs) using sign and other structural restrictions is developed. The reduced-form VAR disturbances are driven by a few common factors and structural identification restrictions can be incorporated in their loadings in the form of parametric restrictions. A Gibbs sampler is derived that allows for reduced-form parameters and structural restrictions to be sampled efficiently in one step. A key benefit of the proposed approach is that it allows for treating parameter estimation and structural inference as a joint problem. An additional benefit is that the methodology can scale to large VARs with multiple shocks, and it can be extended to accommodate non-linearities, asymmetries, and numerous other interesting empirical features. The excellent properties of the new algorithm for inference are explored using synthetic data experiments, and by revisiting the role of financial factors in economic fluctuations using identification based on sign restrictions.

</details>

<details>

<summary>2022-06-14 15:18:47 - Probabilistic forecasting of bus travel time with a Bayesian Gaussian mixture model</summary>

- *Xiaoxu Chen, Zhanhong Cheng, Jian Gang Jin, Martin Trepanier, Lijun Sun*

- `2206.06915v1` - [abs](http://arxiv.org/abs/2206.06915v1) - [pdf](http://arxiv.org/pdf/2206.06915v1)

> Accurate forecasting of bus travel time and its uncertainty is critical to service quality and operation of transit systems; for example, it can help passengers make better decisions on departure time, route choice, and even transport mode choice and also support transit operators to make informed decisions on tasks such as crew/vehicle scheduling and timetabling. However, most existing approaches in bus travel time forecasting are based on deterministic models that provide only point estimation. To this end, we develop in this paper a Bayesian probabilistic forecasting model for bus travel time. To characterize the strong dependencies/interactions between consecutive buses, we concatenate the link travel time vectors and the headway vector from a pair of two adjacent buses as a new augmented variable and model it with a constrained Multivariate Gaussian mixture distributions. This approach can naturally capture the interactions between adjacent buses (e.g., correlated speed and smooth variation of headway), handle missing values in data, and depict the multimodality in bus travel time distributions. Next, we assume different periods in a day share the same set of Gaussian components but different mixing coefficients to characterize the systematic temporal variations in bus operation. For model inference, we develop an efficient Markov chain Monte Carlo (MCMC) sampling algorithm to obtain the posterior distributions of model parameters and make probabilistic forecasting. We test the proposed model using the data from a twenty-link bus route in Guangzhou, China. Results show our approach significantly outperforms baseline models that overlook bus-to-bus interactions in terms of both predictive means and distributions. Besides forecasting, the parameters of the proposed model contain rich information for understanding/improving the bus service.

</details>

<details>

<summary>2022-06-14 16:46:13 - Highly Efficient Structural Learning of Sparse Staged Trees</summary>

- *Manuele Leonelli, Gherardo Varando*

- `2206.06970v1` - [abs](http://arxiv.org/abs/2206.06970v1) - [pdf](http://arxiv.org/pdf/2206.06970v1)

> Several structural learning algorithms for staged tree models, an asymmetric extension of Bayesian networks, have been defined. However, they do not scale efficiently as the number of variables considered increases. Here we introduce the first scalable structural learning algorithm for staged trees, which searches over a space of models where only a small number of dependencies can be imposed. A simulation study as well as a real-world application illustrate our routines and the practical use of such data-learned staged trees.

</details>

<details>

<summary>2022-06-14 17:07:19 - Concentration and robustness of discrepancy-based ABC via Rademacher complexity</summary>

- *Sirio Legramanti, Daniele Durante, Pierre Alquier*

- `2206.06991v1` - [abs](http://arxiv.org/abs/2206.06991v1) - [pdf](http://arxiv.org/pdf/2206.06991v1)

> Classical implementations of approximate Bayesian computation (ABC) employ summary statistics to measure the discrepancy among the observed data and the synthetic samples generated from each proposed value of the parameter of interest. However, finding effective summaries is challenging for most of the complex models for which ABC is required. This issue has motivated a growing literature on summary-free versions of ABC that leverage the discrepancy among the empirical distributions of the observed and synthetic data, rather than focusing on selected summaries. The effectiveness of these solutions has led to an increasing interest in the properties of the corresponding ABC posteriors, with a focus on concentration and robustness in asymptotic regimes. Although recent contributions have made key advancements, current theory mostly relies on existence arguments which are not immediate to verify and often yield bounds that are not readily interpretable, thus limiting the methodological implications of theoretical results. In this article we address such aspects by developing a novel unified and constructive framework, based on the concept of Rademacher complexity, to study concentration and robustness of ABC posteriors within the general class of integral probability semimetrics (IPS), that includes routinely-implemented discrepancies such as Wasserstein distance and MMD, and naturally extends classical summary-based ABC. For rejection ABC based on the IPS class, we prove that the theoretical properties of the ABC posterior in terms of concentration and robustness directly relate to the asymptotic behavior of the Rademacher complexity of the class of functions associated to each discrepancy. This result yields a novel understanding of the practical performance of ABC with specific discrepancies, as shown also in empirical studies, and allows to develop new theory guiding ABC calibration.

</details>

<details>

<summary>2022-06-14 23:05:56 - Unbiased Estimation using the Underdamped Langevin Dynamics</summary>

- *Hamza Ruzayqat, Neil K. Chada, Ajay Jasra*

- `2206.07202v1` - [abs](http://arxiv.org/abs/2206.07202v1) - [pdf](http://arxiv.org/pdf/2206.07202v1)

> In this work we consider the unbiased estimation of expectations w.r.t.~probability measures that have non-negative Lebesgue density, and which are known point-wise up-to a normalizing constant. We focus upon developing an unbiased method via the underdamped Langevin dynamics, which has proven to be popular of late due to applications in statistics and machine learning. Specifically in continuous-time, the dynamics can be constructed to admit the probability of interest as a stationary measure. We develop a novel scheme based upon doubly randomized estimation, which requires access only to time-discretized versions of the dynamics and are the ones that are used in practical algorithms. We prove, under standard assumptions, that our estimator is of finite variance and either has finite expected cost, or has finite cost with a high probability. To illustrate our theoretical findings we provide numerical experiments that verify our theory, which include challenging examples from Bayesian statistics and statistical physics.

</details>

<details>

<summary>2022-06-14 23:57:22 - Median of Means Principle for Bayesian Inference</summary>

- *Shunan Yao, Stanislav Minsker*

- `2203.06617v2` - [abs](http://arxiv.org/abs/2203.06617v2) - [pdf](http://arxiv.org/pdf/2203.06617v2)

> The topic of robustness is experiencing a resurgence of interest in the statistical and machine learning communities. In particular, robust algorithms making use of the so-called median of means estimator were shown to satisfy strong performance guarantees for many problems, including estimation of the mean, covariance structure as well as linear regression. In this work, we propose an extension of the median of means principle to the Bayesian framework, leading to the notion of the robust posterior distribution. In particular, we (a) quantify robustness of this posterior to outliers, (b) show that it satisfies a version of the Bernstein-von Mises theorem that connects Bayesian credible sets to the traditional confidence intervals, and (c) demonstrate that our approach performs well in applications.

</details>

<details>

<summary>2022-06-15 01:55:44 - Heterogeneous Distributed Lag Models to Estimate Personalized Effects of Maternal Exposures to Air Pollution</summary>

- *Daniel Mork, Marianthi-Anna Kioumourtzoglou, Marc Weisskopf, Brent A Coull, Ander Wilson*

- `2109.13763v2` - [abs](http://arxiv.org/abs/2109.13763v2) - [pdf](http://arxiv.org/pdf/2109.13763v2)

> Children's health studies support an association between maternal environmental exposures and children's birth outcomes. A common goal is to identify critical windows of susceptibility--periods during gestation with increased association between maternal exposures and a future outcome. The timing of the critical windows and magnitude of the associations are likely heterogeneous across different levels of individual, family, and neighborhood characteristics. Using an administrative Colorado birth cohort we estimate the individualized relationship between weekly exposures to fine particulate matter (PM2.5) during gestation and birth weight. To achieve this goal, we propose a statistical learning method combining distributed lag models and Bayesian additive regression trees to estimate critical windows at the individual level and identify characteristics that induce heterogeneity from a high-dimensional set of potential modifying factors. We find evidence of heterogeneity in the PM2.5-birth weight relationship, with some mother-child dyads showing a 3 times larger decrease in birth weight for an IQR increase in exposure (5.9 to 8.5 $\mu g/m^3$ PM2.5) compared to the population average. Specifically, we find increased susceptibility for non-Hispanic mothers who are either younger, have higher body mass index or lower educational attainment. Our case study is the first precision health study of critical windows.

</details>

<details>

<summary>2022-06-15 03:30:38 - CARD: Classification and Regression Diffusion Models</summary>

- *Xizewen Han, Huangjie Zheng, Mingyuan Zhou*

- `2206.07275v1` - [abs](http://arxiv.org/abs/2206.07275v1) - [pdf](http://arxiv.org/pdf/2206.07275v1)

> Learning the distribution of a continuous or categorical response variable $\boldsymbol y$ given its covariates $\boldsymbol x$ is a fundamental problem in statistics and machine learning. Deep neural network-based supervised learning algorithms have made great progress in predicting the mean of $\boldsymbol y$ given $\boldsymbol x$, but they are often criticized for their ability to accurately capture the uncertainty of their predictions. In this paper, we introduce classification and regression diffusion (CARD) models, which combine a denoising diffusion-based conditional generative model and a pre-trained conditional mean estimator, to accurately predict the distribution of $\boldsymbol y$ given $\boldsymbol x$. We demonstrate the outstanding ability of CARD in conditional distribution prediction with both toy examples and real-world datasets, the experimental results on which show that CARD in general outperforms state-of-the-art methods, including Bayesian neural network-based ones that are designed for uncertainty estimation, especially when the conditional distribution of $\boldsymbol y$ given $\boldsymbol x$ is multi-modal.

</details>

<details>

<summary>2022-06-15 09:29:41 - The Dual PC Algorithm for Structure Learning</summary>

- *Enrico Giudice, Jack Kuipers, Giusi Moffa*

- `2112.09036v3` - [abs](http://arxiv.org/abs/2112.09036v3) - [pdf](http://arxiv.org/pdf/2112.09036v3)

> Learning the graphical structure of Bayesian networks is key to describing data generating mechanisms in many complex applications but poses considerable computational challenges. Observational data can only identify the equivalence class of the directed acyclic graph underlying a Bayesian network model, and a variety of methods exist to tackle the problem. Under certain assumptions, the popular PC algorithm can consistently recover the correct equivalence class by reverse-engineering the conditional independence (CI) relationships holding in the variable distribution. Here, we propose the dual PC algorithm, a novel scheme to carry out the CI tests within the PC algorithm by leveraging the inverse relationship between covariance and precision matrices. By exploiting block matrix inversions we can simultaneously perform tests on partial correlations of complementary (or dual) conditioning sets. The multiple CI tests of the dual PC algorithm proceed by first considering marginal and full-order CI relationships and progressively moving to central-order ones. Simulation studies show that the dual PC algorithm outperforms the classic PC algorithm both in terms of run time and in recovering the underlying network structure, even in the presence of deviations from Gaussianity.

</details>

<details>

<summary>2022-06-15 10:23:19 - Multi-Objective Hyperparameter Optimization -- An Overview</summary>

- *Florian Karl, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, Janek Thomas, Jakob Richter, Michel Lang, Eduardo C. Garrido-Merchn, Juergen Branke, Bernd Bischl*

- `2206.07438v1` - [abs](http://arxiv.org/abs/2206.07438v1) - [pdf](http://arxiv.org/pdf/2206.07438v1)

> Hyperparameter optimization constitutes a large part of typical modern machine learning workflows. This arises from the fact that machine learning methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi- objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies, both from the domain of evolutionary algorithms and Bayesian optimization. We illustrate the utility of MOO in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability and robustness.

</details>

<details>

<summary>2022-06-15 14:20:14 - Bayesian Learning of Parameterised Quantum Circuits</summary>

- *Samuel Duffield, Marcello Benedetti, Matthias Rosenkranz*

- `2206.07559v1` - [abs](http://arxiv.org/abs/2206.07559v1) - [pdf](http://arxiv.org/pdf/2206.07559v1)

> Currently available quantum computers suffer from constraints including hardware noise and a limited number of qubits. As such, variational quantum algorithms that utilise a classical optimiser in order to train a parameterised quantum circuit have drawn significant attention for near-term practical applications of quantum technology. In this work, we take a probabilistic point of view and reformulate the classical optimisation as an approximation of a Bayesian posterior. The posterior is induced by combining the cost function to be minimised with a prior distribution over the parameters of the quantum circuit. We describe a dimension reduction strategy based on a maximum a posteriori point estimate with a Laplace prior. Experiments on the Quantinuum H1-2 computer show that the resulting circuits are faster to execute and less noisy than the circuits trained without the dimension reduction strategy. We subsequently describe a posterior sampling strategy based on stochastic gradient Langevin dynamics. Numerical simulations on three different problems show that the strategy is capable of generating samples from the full posterior and avoiding local optima.

</details>

<details>

<summary>2022-06-15 14:24:59 - Bayesian Federated Learning via Predictive Distribution Distillation</summary>

- *Shrey Bhatt, Aishwarya Gupta, Piyush Rai*

- `2206.07562v1` - [abs](http://arxiv.org/abs/2206.07562v1) - [pdf](http://arxiv.org/pdf/2206.07562v1)

> For most existing federated learning algorithms, each round consists of minimizing a loss function at each client to learn an optimal model at the client, followed by aggregating these client models at the server. Point estimation of the model parameters at the clients does not take into account the uncertainty in the models estimated at each client. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the client models for more accurate and robust predictions. Uncertainty also provides useful information for other important tasks, such as active learning and out-of-distribution (OOD) detection. We present a framework for Bayesian federated learning where each client infers the posterior predictive distribution using its training data and present various ways to aggregate these client-specific predictive distributions at the server. Since communicating and aggregating predictive distributions can be challenging and expensive, our approach is based on distilling each client's predictive distribution into a single deep neural network. This enables us to leverage advances in standard federated learning to Bayesian federated learning as well. Unlike some recent works that have tried to estimate model uncertainty of each client, our work also does not make any restrictive assumptions, such as the form of the client's posterior distribution. We evaluate our approach on classification in federated setting, as well as active learning and OOD detection in federated settings, on which our approach outperforms various existing federated learning baselines.

</details>

<details>

<summary>2022-06-15 14:41:43 - Calibrating Agent-based Models to Microdata with Graph Neural Networks</summary>

- *Joel Dyer, Patrick Cannon, J. Doyne Farmer, Sebastian M. Schmon*

- `2206.07570v1` - [abs](http://arxiv.org/abs/2206.07570v1) - [pdf](http://arxiv.org/pdf/2206.07570v1)

> Calibrating agent-based models (ABMs) to data is among the most fundamental requirements to ensure the model fulfils its desired purpose. In recent years, simulation-based inference methods have emerged as powerful tools for performing this task when the model likelihood function is intractable, as is often the case for ABMs. In some real-world use cases of ABMs, both the observed data and the ABM output consist of the agents' states and their interactions over time. In such cases, there is a tension between the desire to make full use of the rich information content of such granular data on the one hand, and the need to reduce the dimensionality of the data to prevent difficulties associated with high-dimensional learning tasks on the other. A possible resolution is to construct lower-dimensional time-series through the use of summary statistics describing the macrostate of the system at each time point. However, a poor choice of summary statistics can result in an unacceptable loss of information from the original dataset, dramatically reducing the quality of the resulting calibration. In this work, we instead propose to learn parameter posteriors associated with granular microdata directly using temporal graph neural networks. We will demonstrate that such an approach offers highly compelling inductive biases for Bayesian inference using the raw ABM microstates as output.

</details>

<details>

<summary>2022-06-15 14:59:42 - Non-Vacuous Generalisation Bounds for Shallow Neural Networks</summary>

- *Felix Biggs, Benjamin Guedj*

- `2202.01627v3` - [abs](http://arxiv.org/abs/2202.01627v3) - [pdf](http://arxiv.org/pdf/2202.01627v3)

> We focus on a specific class of shallow neural networks with a single hidden layer, namely those with $L_2$-normalised data and either a sigmoid-shaped Gaussian error function ("erf") activation or a Gaussian Error Linear Unit (GELU) activation. For these networks, we derive new generalisation bounds through the PAC-Bayesian theory; unlike most existing such bounds they apply to neural networks with deterministic rather than randomised parameters. Our bounds are empirically non-vacuous when the network is trained with vanilla stochastic gradient descent on MNIST and Fashion-MNIST.

</details>

<details>

<summary>2022-06-15 17:11:08 - Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling</summary>

- *Jiri Hron, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein*

- `2206.07673v1` - [abs](http://arxiv.org/abs/2206.07673v1) - [pdf](http://arxiv.org/pdf/2206.07673v1)

> We introduce repriorisation, a data-dependent reparameterisation which transforms a Bayesian neural network (BNN) posterior to a distribution whose KL divergence to the BNN prior vanishes as layer widths grow. The repriorisation map acts directly on parameters, and its analytic simplicity complements the known neural network Gaussian process (NNGP) behaviour of wide BNNs in function space. Exploiting the repriorisation, we develop a Markov chain Monte Carlo (MCMC) posterior sampling algorithm which mixes faster the wider the BNN. This contrasts with the typically poor performance of MCMC in high dimensions. We observe up to 50x higher effective sample size relative to no reparametrisation for both fully-connected and residual networks. Improvements are achieved at all widths, with the margin between reparametrised and standard BNNs growing with layer width.

</details>

<details>

<summary>2022-06-15 17:16:14 - A solution for the rare type match problem when using the DIP-STR marker system</summary>

- *Giulia Cereda, Richard D. Gill, Franco Taroni*

- `2206.07675v1` - [abs](http://arxiv.org/abs/2206.07675v1) - [pdf](http://arxiv.org/pdf/2206.07675v1)

> The rare type match problem is an evaluative challenging situation in which the analysis of a DNA profile reveals the presence of (at least) one allele which is not contained in the reference database. This situation is challenging because an estimate for the frequency of occurrence of the profile in a given population needs sophisticated evaluative procedures. The rare type match problem is very common when the DIP-STR marker system, which has proven itself very useful for dealing with unbalanced DNA mixtures, is used, essentially due to the limited size of the available database. The object-oriented Bayesian network proposed in Cereda, Biedermann, Hall and Taroni (2014) to assess the value of the evidence for general scenarios, was not designed to deal with this particular situation. In this paper, the model is extended and partially modified to be able to calculate the full Bayesian likelihood ratio in presence of any (observed and not yet observed) allele of a given profile. The method is based on the approach developed in Cereda (2017) for Y-STR data. Alternative solutions, such as the plug-in approximation and an empirical Bayesian methodology are also proposed and compared with the results obtained with the full Bayesian approach.

</details>

<details>

<summary>2022-06-15 18:53:23 - Bridging Bayesian, frequentist and fiducial (BFF) inferences using confidence distribution</summary>

- *Suzanne Thornton, Minge Xie*

- `2012.04464v3` - [abs](http://arxiv.org/abs/2012.04464v3) - [pdf](http://arxiv.org/pdf/2012.04464v3)

> Bayesian, frequentist and fiducial (BFF) inferences are much more congruous than they have been perceived historically in the scientific community (cf., Reid and Cox 2015; Kass 2011; Efron 1998). Most practitioners are probably more familiar with the two dominant statistical inferential paradigms, Bayesian inference and frequentist inference. The third, lesser known fiducial inference paradigm was pioneered by R.A. Fisher in an attempt to define an inversion procedure for inference as an alternative to Bayes' theorem. Although each paradigm has its own strengths and limitations subject to their different philosophical underpinnings, this article intends to bridge these different inferential methodologies through the lenses of confidence distribution theory and Monte-Carlo simulation procedures. This article attempts to understand how these three distinct paradigms, Bayesian, frequentist, and fiducial inference, can be unified and compared on a foundational level, thereby increasing the range of possible techniques available to both statistical theorists and practitioners across all fields.

</details>

<details>

<summary>2022-06-15 20:10:15 - Deep Bayesian inference for seismic imaging with tasks</summary>

- *Ali Siahkoohi, Gabrio Rizzuti, Felix J. Herrmann*

- `2110.04825v3` - [abs](http://arxiv.org/abs/2110.04825v3) - [pdf](http://arxiv.org/pdf/2110.04825v3)

> We propose to use techniques from Bayesian inference and deep neural networks to translate uncertainty in seismic imaging to uncertainty in tasks performed on the image, such as horizon tracking. Seismic imaging is an ill-posed inverse problem because of bandwidth and aperture limitations, which is hampered by the presence of noise and linearization errors. Many regularization methods, such as transform-domain sparsity promotion, have been designed to deal with the adverse effects of these errors, however, these methods run the risk of biasing the solution and do not provide information on uncertainty in the image space and how this uncertainty impacts certain tasks on the image. A systematic approach is proposed to translate uncertainty due to noise in the data to confidence intervals of automatically tracked horizons in the image. The uncertainty is characterized by a convolutional neural network (CNN) and to assess these uncertainties, samples are drawn from the posterior distribution of the CNN weights, used to parameterize the image. Compared to traditional priors, it is argued in the literature that these CNNs introduce a flexible inductive bias that is a surprisingly good fit for a diverse set of problems. The method of stochastic gradient Langevin dynamics is employed to sample from the posterior distribution. This method is designed to handle large scale Bayesian inference problems with computationally expensive forward operators as in seismic imaging. Aside from offering a robust alternative to maximum a posteriori estimate that is prone to overfitting, access to these samples allow us to translate uncertainty in the image, due to noise in the data, to uncertainty on the tracked horizons. For instance, it admits estimates for the pointwise standard deviation on the image and for confidence intervals on its automatically tracked horizons.

</details>

<details>

<summary>2022-06-15 21:03:12 - Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces</summary>

- *Samuel Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy*

- `2109.10964v4` - [abs](http://arxiv.org/abs/2109.10964v4) - [pdf](http://arxiv.org/pdf/2109.10964v4)

> Many real world scientific and industrial applications require optimizing multiple competing black-box objectives. When the objectives are expensive-to-evaluate, multi-objective Bayesian optimization (BO) is a popular approach because of its high sample efficiency. However, even with recent methodological advances, most existing multi-objective BO methods perform poorly on search spaces with more than a few dozen parameters and rely on global surrogate models that scale cubically with the number of observations. In this work we propose MORBO, a scalable method for multi-objective BO over high-dimensional search spaces. MORBO identifies diverse globally optimal solutions by performing BO in multiple local regions of the design space in parallel using a coordinated strategy. We show that MORBO significantly advances the state-of-the-art in sample efficiency for several high-dimensional synthetic problems and real world applications, including an optical display design problem and a vehicle design problem with 146 and 222 parameters, respectively. On these problems, where existing BO algorithms fail to scale and perform well, MORBO provides practitioners with order-of-magnitude improvements in sample efficiency over the current approach.

</details>

<details>

<summary>2022-06-15 22:06:45 - Deep Reference Priors: What is the best way to pretrain a model?</summary>

- *Yansong Gao, Rahul Ramesh, Pratik Chaudhari*

- `2202.00187v2` - [abs](http://arxiv.org/abs/2202.00187v2) - [pdf](http://arxiv.org/pdf/2202.00187v2)

> What is the best way to exploit extra data -- be it unlabeled data from the same task, or labeled data from a related task -- to learn a given task? This paper formalizes the question using the theory of reference priors. Reference priors are objective, uninformative Bayesian priors that maximize the mutual information between the task and the weights of the model. Such priors enable the task to maximally affect the Bayesian posterior, e.g., reference priors depend upon the number of samples available for learning the task and for very small sample sizes, the prior puts more probability mass on low-complexity models in the hypothesis space. This paper presents the first demonstration of reference priors for medium-scale deep networks and image-based data. We develop generalizations of reference priors and demonstrate applications to two problems. First, by using unlabeled data to compute the reference prior, we develop new Bayesian semi-supervised learning methods that remain effective even with very few samples per class. Second, by using labeled data from the source task to compute the reference prior, we develop a new pretraining method for transfer learning that allows data from the target task to maximally affect the Bayesian posterior. Empirical validation of these methods is conducted on image classification datasets. Code is available at https://github.com/grasp-lyrl/deep_reference_priors.

</details>

<details>

<summary>2022-06-16 03:30:35 - Does Bayesian Model Averaging improve polynomial extrapolations? Two toy problems as tests</summary>

- *M. A. Connell, I. Billig, D. R. Phillips*

- `2106.05906v2` - [abs](http://arxiv.org/abs/2106.05906v2) - [pdf](http://arxiv.org/pdf/2106.05906v2)

> We assess the accuracy of Bayesian polynomial extrapolations from small parameter values, x, to large values of x. We consider a set of polynomials of fixed order, intended as a proxy for a fixed-order effective field theory (EFT) description of data. We employ Bayesian Model Averaging (BMA) to combine results from different order polynomials (EFT orders). Our study considers two "toy problems" where the underlying function used to generate data sets is known. We use Bayesian parameter estimation to extract the polynomial coefficients that describe these data at low x. A "naturalness" prior is imposed on the coefficients, so that they are O(1). We Bayesian-Model-Average different polynomial degrees by weighting each according to its Bayesian evidence and compare the predictive performance of this Bayesian Model Average with that of the individual polynomials. The credibility intervals on the BMA forecast have the stated coverage properties more consistently than does the highest evidence polynomial, though BMA does not necessarily outperform every polynomial.

</details>

<details>

<summary>2022-06-16 08:52:44 - The convergent Indian buffet process</summary>

- *Ilsang Ohn*

- `2206.08002v1` - [abs](http://arxiv.org/abs/2206.08002v1) - [pdf](http://arxiv.org/pdf/2206.08002v1)

> We propose a new Bayesian nonparametric prior for latent feature models, which we call the convergent Indian buffet process (CIBP). We show that under the CIBP, the number of latent features is distributed as a Poisson distribution with the mean monotonically increasing but converging to a certain value as the number of objects goes to infinity. That is, the expected number of features is bounded above even when the number of objects goes to infinity, unlike the standard Indian buffet process under which the expected number of features increases with the number of objects. We provide two alternative representations of the CIBP based on a hierarchical distribution and a completely random measure, respectively, which are of independent interest. The proposed CIBP is assessed on a high-dimensional sparse factor model.

</details>

<details>

<summary>2022-06-16 10:40:58 - Inferring the spread of COVID-19: the role of time-varying reporting rate in epidemiological modelling</summary>

- *Adam Spannaus, Theodore Papamarkou, Samantha Erwin, J. Blair Christian*

- `2012.15306v4` - [abs](http://arxiv.org/abs/2012.15306v4) - [pdf](http://arxiv.org/pdf/2012.15306v4)

> The role of epidemiological models is crucial for informing public health officials during a public health emergency, such as the COVID-19 pandemic. However, traditional epidemiological models fail to capture the time-varying effects of mitigation strategies and do not account for under-reporting of active cases, thus introducing bias in the estimation of model parameters. To infer more accurate parameter estimates and to reduce the uncertainty of these estimates, we extend the SIR and SEIR epidemiological models with two time-varying parameters that capture the transmission rate and the rate at which active cases are reported to health officials. Using two real data sets of COVID-19 cases, we perform Bayesian inference via our SIR and SEIR models with time-varying transmission and reporting rates and via their standard counterparts with constant rates; our approach provides parameter estimates with more realistic interpretation, and one-week ahead predictions with reduced uncertainty. Furthermore, we find consistent under-reporting in the number of active cases in the data that we consider, suggesting that the initial phase of the pandemic was more widespread than previously reported.

</details>

<details>

<summary>2022-06-16 12:24:27 - Bayesian conjugacy in probit, tobit, multinomial probit and extensions: A review and new results</summary>

- *Niccolo' Anceschi, Augusto Fasano, Daniele Durante, Giacomo Zanella*

- `2206.08118v1` - [abs](http://arxiv.org/abs/2206.08118v1) - [pdf](http://arxiv.org/pdf/2206.08118v1)

> A broad class of models that routinely appear in several fields can be expressed as partially or fully discretized Gaussian linear regressions. Besides including basic Gaussian response settings, this class also encompasses probit, multinomial probit and tobit regression, among others, thereby yielding to one of the most widely-implemented families of models in applications. The relevance of such representations has stimulated decades of research in the Bayesian field, mostly motivated by the fact that, unlike for Gaussian linear regression, the posterior distribution induced by such models does not apparently belong to a known class, under the commonly-assumed Gaussian priors for the coefficients. This has motivated several solutions for posterior inference relying on sampling-based strategies or on deterministic approximations that, however, still experience computational and accuracy issues, especially in high dimensions. The scope of this article is to review, unify and extend recent advances in Bayesian inference and computation for this class of models. To address such a goal, we prove that the likelihoods induced by these formulations share a common analytical structure that implies conjugacy with a broad class of distributions, namely the unified skew-normals (SUN), that generalize Gaussians to skewed contexts. This result unifies and extends recent conjugacy properties for specific models within the class analyzed, and opens avenues for improved posterior inference, under a broader class of formulations and priors, via novel closed-form expressions, i.i.d. samplers from the exact SUN posteriors, and more accurate and scalable approximations from VB and EP. Such advantages are illustrated in simulations and are expected to facilitate the routine-use of these core Bayesian models, while providing a novel framework to study theoretical properties and develop future extensions.

</details>

<details>

<summary>2022-06-16 13:26:24 - Modeling rates of disease with missing categorical data</summary>

- *Rob Trangucci, Yang Chen, Jon Zelner*

- `2206.08161v1` - [abs](http://arxiv.org/abs/2206.08161v1) - [pdf](http://arxiv.org/pdf/2206.08161v1)

> Covariates like age, sex, and race/ethnicity provide invaluable insight to public health authorities trying to interpret surveillance data collected during a public health emergency such as the COVID-19 pandemic. However, the utility of such data is limited when many cases are missing key covariates. This problem is most concerning when this missingness likely depends on the values of missing covariates, i.e. they are not missing at random (NMAR). We propose a Bayesian parametric model that leverages joint information on spatial variation in the disease and covariate missingness processes and can accommodate both MAR and NMAR missingness. We show that the model is locally identifiable when the spatial distribution of the population covariates is known and observed cases can be associated with a spatial unit of observation. We also use a simulation study to investigate the model's finite-sample performance. We compare our model's performance on NMAR data against complete-case analysis and multiple imputation (MI), both of which are commonly used by public health researchers when confronted with missing categorical covariates. Finally, we model spatial variation in cumulative COVID-19 incidence in Wayne County, Michigan using data from the Michigan Department and Health and Human Services. The analysis suggests that population relative risk estimates by race during the early part of the COVID-19 pandemic in Michigan were understated for non-white residents compared to white residents when cases missing race were dropped or had these values imputed using MI.

</details>

<details>

<summary>2022-06-16 13:27:05 - Testing whether a Learning Procedure is Calibrated</summary>

- *Jon Cockayne, Matthew M. Graham, Chris J. Oates, T. J. Sullivan, Onur Teymur*

- `2012.12670v5` - [abs](http://arxiv.org/abs/2012.12670v5) - [pdf](http://arxiv.org/pdf/2012.12670v5)

> A learning procedure takes as input a dataset and performs inference for the parameters $\theta$ of a model that is assumed to have given rise to the dataset. Here we consider learning procedures whose output is a probability distribution, representing uncertainty about $\theta$ after seeing the dataset. Bayesian inference is a prime example of such a procedure, but one can also construct other learning procedures that return distributional output. This paper studies conditions for a learning procedure to be considered calibrated, in the sense that the true data-generating parameters are plausible as samples from its distributional output. A learning procedure whose inferences and predictions are systematically over- or under-confident will fail to be calibrated. On the other hand, a learning procedure that is calibrated need not be statistically efficient. A hypothesis-testing framework is developed in order to assess, using simulation, whether a learning procedure is calibrated. Several vignettes are presented to illustrate different aspects of the framework.

</details>

<details>

<summary>2022-06-16 13:44:11 - A Bernstein--von-Mises theorem for the Caldern problem with piecewise constant conductivities</summary>

- *Jan Bohr*

- `2206.08177v1` - [abs](http://arxiv.org/abs/2206.08177v1) - [pdf](http://arxiv.org/pdf/2206.08177v1)

> This note considers a finite dimensional statistical model for the Calder\'on problem with piecewise constant conductivities. In this setting it is shown that injectivity of the forward map and its linearisation suffice to prove the invertibility of the information operator, resulting in a Bernstein--von-Mises theorem and optimality guarantees for estimation by Bayesian posterior means.

</details>

<details>

<summary>2022-06-16 14:18:45 - Learning Physics between Digital Twins with Low-Fidelity Models and Physics-Informed Gaussian Processes</summary>

- *Michail Spitieris, Ingelin Steinsland*

- `2206.08201v1` - [abs](http://arxiv.org/abs/2206.08201v1) - [pdf](http://arxiv.org/pdf/2206.08201v1)

> A digital twin is a computer model that represents an individual, for example, a component, a patient or a process. In many situations, we want to gain knowledge about an individual from its data while incorporating imperfect physical knowledge and also learn from data from other individuals. In this paper, we introduce and demonstrate a fully Bayesian methodology for learning between digital twins in a setting where the physical parameters of each individual are of interest. For each individual, the methodology is based on Bayesian calibration with model discrepancy. Through the discrepancy, modelled as a Gaussian process, the imperfect low-fidelity physical model is accounted for. Using ideas from Bayesian hierarchical models, a joint probabilistic model of digital twins is constructed by connecting them through a new level in the hierarchy. For the physical parameters, the methodology can be seen as using a prior distribution in the individual model that is the posterior of the corresponding hyperparameter in the joint model. For learning the imperfect physics between individuals two approaches are introduced, one that assumes the same discrepancy for all individuals and one that can be seen as using a prior learned from all individuals for the parameters of the Gaussian processes representing the discrepancies. Based on recent advances related to physics-informed priors, Hamiltonian Monte Carlo methods and using these for inverse problems we set up an inference methodology that allows our approach to be computational feasible also for physical models based on partial differential equations and individual data that are not aligned. The methodology is demonstrated in two synthetic case studies, a toy example previously used in the literature extended to more individuals and an example based on a cardiovascular differential equation model relevant for the treatment of hypertension.

</details>

<details>

<summary>2022-06-16 15:44:54 - Contrasting random and learned features in deep Bayesian linear regression</summary>

- *Jacob A. Zavatone-Veth, William L. Tong, Cengiz Pehlevan*

- `2203.00573v2` - [abs](http://arxiv.org/abs/2203.00573v2) - [pdf](http://arxiv.org/pdf/2203.00573v2)

> Understanding how feature learning affects generalization is among the foremost goals of modern deep learning theory. Here, we study how the ability to learn representations affects the generalization performance of a simple class of models: deep Bayesian linear neural networks trained on unstructured Gaussian data. By comparing deep random feature models to deep networks in which all layers are trained, we provide a detailed characterization of the interplay between width, depth, data density, and prior mismatch. We show that both models display sample-wise double-descent behavior in the presence of label noise. Random feature models can also display model-wise double-descent if there are narrow bottleneck layers, while deep networks do not show these divergences. Random feature models can have particular widths that are optimal for generalization at a given data density, while making neural networks as wide or as narrow as possible is always optimal. Moreover, we show that the leading-order correction to the kernel-limit learning curve cannot distinguish between random feature models and deep networks in which all layers are trained. Taken together, our findings begin to elucidate how architectural details affect generalization performance in this simple class of deep regression models.

</details>

<details>

<summary>2022-06-16 16:33:20 - Sparse Quantile Regression</summary>

- *Le-Yu Chen, Sokbae Lee*

- `2006.11201v3` - [abs](http://arxiv.org/abs/2006.11201v3) - [pdf](http://arxiv.org/pdf/2006.11201v3)

> We consider both $\ell _{0}$-penalized and $\ell _{0}$-constrained quantile regression estimators. For the $\ell _{0}$-penalized estimator, we derive an exponential inequality on the tail probability of excess quantile prediction risk and apply it to obtain non-asymptotic upper bounds on the mean-square parameter and regression function estimation errors. We also derive analogous results for the $\ell _{0}$-constrained estimator. The resulting rates of convergence are nearly minimax-optimal and the same as those for $\ell _{1}$-penalized and non-convex penalized estimators. Further, we characterize expected Hamming loss for the $\ell _{0}$-penalized estimator. We implement the proposed procedure via mixed integer linear programming and also a more scalable first-order approximation algorithm. We illustrate the finite-sample performance of our approach in Monte Carlo experiments and its usefulness in a real data application concerning conformal prediction of infant birth weights (with $n\approx 10^{3}$ and up to $p>10^{3}$). In sum, our $\ell _{0}$-based method produces a much sparser estimator than the $\ell _{1}$-penalized and non-convex penalized approaches without compromising precision.

</details>

<details>

<summary>2022-06-16 17:04:53 - Weekly sequential Bayesian updating improves prediction of deaths at an early epidemic stage</summary>

- *Pedro Henrique da Costa Avelar, Natalia Del Coco, Luis C. Lamb, Sophia Tsoka, Jonathan Cardoso-Silva*

- `2104.01133v2` - [abs](http://arxiv.org/abs/2104.01133v2) - [pdf](http://arxiv.org/pdf/2104.01133v2)

> Background: Following the outbreak of the coronavirus epidemic in early 2020, municipalities, regional governments and policymakers worldwide had to plan their Non-Pharmaceutical Interventions (NPIs) amidst a scenario of great uncertainty. At this early stage of an epidemic, where no vaccine or medical treatment is in sight, algorithmic prediction can become a powerful tool to inform local policymaking. However, when we replicated one prominent epidemiological model to inform health authorities in a region in the south of Brazil, we found that this model relied too heavily on manually predetermined covariates and was too reactive to changes in data trends.   Methods: Our four proposed variations of the original method allow accessing data of daily reported infections and take into account the under-reporting of cases more explicitly. Two of the proposed versions also attempt to model the delay in test reporting. We simulated weekly forecasting of deaths from the period from 31/05/2020 until 31/01/2021. That workflow allowed us to run a lighter version of the model after the first calibration week. Google Mobility data, weekly updated, were used as covariates to the model at each simulated run.   Findings: The changes made the model significantly less reactive and more rapid in adapting to scenarios after a peak in deaths is observed. Assuming that reported cases were under-reported greatly benefited the model in its stability, and modelling retroactively-added data (due to the "hot" nature of the data used) had a negligible impact on performance.   Interpretation: Although not as reliable as death statistics, case statistics, when modelled in conjunction with an "overestimate" parameter, provide a good alternative for improving the forecasting of models, especially in long-range predictions and after the peak of an infection wave.

</details>

<details>

<summary>2022-06-16 17:59:48 - Scalable First-Order Bayesian Optimization via Structured Automatic Differentiation</summary>

- *Sebastian Ament, Carla Gomes*

- `2206.08366v1` - [abs](http://arxiv.org/abs/2206.08366v1) - [pdf](http://arxiv.org/pdf/2206.08366v1)

> Bayesian Optimization (BO) has shown great promise for the global optimization of functions that are expensive to evaluate, but despite many successes, standard approaches can struggle in high dimensions. To improve the performance of BO, prior work suggested incorporating gradient information into a Gaussian process surrogate of the objective, giving rise to kernel matrices of size $nd \times nd$ for $n$ observations in $d$ dimensions. Na\"ively multiplying with (resp. inverting) these matrices requires $\mathcal{O}(n^2d^2)$ (resp. $\mathcal{O}(n^3d^3$)) operations, which becomes infeasible for moderate dimensions and sample sizes. Here, we observe that a wide range of kernels gives rise to structured matrices, enabling an exact $\mathcal{O}(n^2d)$ matrix-vector multiply for gradient observations and $\mathcal{O}(n^2d^2)$ for Hessian observations. Beyond canonical kernel classes, we derive a programmatic approach to leveraging this type of structure for transformations and combinations of the discussed kernel classes, which constitutes a structure-aware automatic differentiation algorithm. Our methods apply to virtually all canonical kernels and automatically extend to complex kernels, like the neural network, radial basis function network, and spectral mixture kernels without any additional derivations, enabling flexible, problem-dependent modeling while scaling first-order BO to high $d$.

</details>

<details>

<summary>2022-06-16 19:36:17 - Generalised Bayesian Inference for Discrete Intractable Likelihood</summary>

- *Takuo Matsubara, Jeremias Knoblauch, Franois-Xavier Briol, Chris. J. Oates*

- `2206.08420v1` - [abs](http://arxiv.org/abs/2206.08420v1) - [pdf](http://arxiv.org/pdf/2206.08420v1)

> Discrete state spaces represent a major computational challenge to statistical inference, since the computation of normalisation constants requires summation over large or possibly infinite sets, which can be impractical. This paper addresses this computational challenge through the development of a novel generalised Bayesian inference procedure suitable for discrete intractable likelihood. Inspired by recent methodological advances for continuous data, the main idea is to update beliefs about model parameters using a discrete Fisher divergence, in lieu of the problematic intractable likelihood. The result is a generalised posterior that can be sampled using standard computational tools, such as Markov chain Monte Carlo, circumventing the intractable normalising constant. The statistical properties of the generalised posterior are analysed, with sufficient conditions for posterior consistency and asymptotic normality established. In addition, a novel and general approach to calibration of generalised posteriors is proposed. Applications are presented on lattice models for discrete spatial data and on multivariate models for count data, where in each case the methodology facilitates generalised Bayesian inference at low computational cost.

</details>

<details>

<summary>2022-06-16 20:02:02 - Applying Bayesian Hierarchical Probit Model to Interview Grade Evaluation</summary>

- *Yuki Ohnishi, Shinsuke Sugaya*

- `2003.11591v2` - [abs](http://arxiv.org/abs/2003.11591v2) - [pdf](http://arxiv.org/pdf/2003.11591v2)

> Job interviews are a fundamental activity for most corporations to acquire potential candidates, and for job seekers to get well-rewarded and fulfilling career opportunities. In many cases, interviews are conducted in multiple processes such as telephone interviews and several face-to-face interviews. At each stage, candidates are evaluated in various aspects. Among them, grade evaluation, such as a rating on a 1-4 scale, might be used as a reasonable method to evaluate candidates. However, because each evaluation is based on a subjective judgment of interviewers, the aggregated evaluations can be biased because the difference in toughness of interviewers is not examined. Additionally, it is noteworthy that the toughness of interviewers might vary depending on the interview round. As described herein, we propose an analytical framework of simultaneous estimation for both the true potential of candidates and toughness of interviewers' judgment considering job interview rounds, with algorithms to extract unseen knowledge of the true potential of candidates and toughness of interviewers as latent variables through analyzing grade data of job interviews. We apply a Bayesian Hierarchical Ordered Probit Model to the grade data from HRMOS, a cloud-based Applicant Tracking System (ATS) operated by BizReach, Inc., an IT start-up particularly addressing human-resource needs in Japan. Our model successfully quantifies the candidate potential and the interviewers' toughness. An interpretation and applications of the model are given along with a discussion of its place within hiring processes in real-world settings. The parameters are estimated by Markov Chain Monte Carlo (MCMC). A discussion of uncertainty, which is given by the posterior distribution of the parameters, is also provided along with the analysis.

</details>

<details>

<summary>2022-06-16 20:13:57 - Large Hybrid Time-Varying Parameter VARs</summary>

- *Joshua C. C. Chan*

- `2201.07303v2` - [abs](http://arxiv.org/abs/2201.07303v2) - [pdf](http://arxiv.org/pdf/2201.07303v2)

> Time-varying parameter VARs with stochastic volatility are routinely used for structural analysis and forecasting in settings involving a few endogenous variables. Applying these models to high-dimensional datasets has proved to be challenging due to intensive computations and over-parameterization concerns. We develop an efficient Bayesian sparsification method for a class of models we call hybrid TVP-VARs--VARs with time-varying parameters in some equations but constant coefficients in others. Specifically, for each equation, the new method automatically decides whether the VAR coefficients and contemporaneous relations among variables are constant or time-varying. Using US datasets of various dimensions, we find evidence that the parameters in some, but not all, equations are time varying. The large hybrid TVP-VAR also forecasts better than many standard benchmarks.

</details>

<details>

<summary>2022-06-16 20:42:27 - Fast and Accurate Variational Inference for Large Bayesian VARs with Stochastic Volatility</summary>

- *Joshua C. C. Chan, Xuewen Yu*

- `2206.08438v1` - [abs](http://arxiv.org/abs/2206.08438v1) - [pdf](http://arxiv.org/pdf/2206.08438v1)

> We propose a new variational approximation of the joint posterior distribution of the log-volatility in the context of large Bayesian VARs. In contrast to existing approaches that are based on local approximations, the new proposal provides a global approximation that takes into account the entire support of the joint distribution. In a Monte Carlo study we show that the new global approximation is over an order of magnitude more accurate than existing alternatives. We illustrate the proposed methodology with an application of a 96-variable VAR with stochastic volatility to measure global bank network connectedness.

</details>

<details>

<summary>2022-06-16 21:05:50 - Bayesian Spillover Graphs for Dynamic Networks</summary>

- *Grace Deng, David S. Matteson*

- `2203.01912v2` - [abs](http://arxiv.org/abs/2203.01912v2) - [pdf](http://arxiv.org/pdf/2203.01912v2)

> We present Bayesian Spillover Graphs (BSG), a novel method for learning temporal relationships, identifying critical nodes, and quantifying uncertainty for multi-horizon spillover effects in a dynamic system. BSG leverages both an interpretable framework via forecast error variance decompositions (FEVD) and comprehensive uncertainty quantification via Bayesian time series models to contextualize temporal relationships in terms of systemic risk and prediction variability. Forecast horizon hyperparameter $h$ allows for learning both short-term and equilibrium state network behaviors. Experiments for identifying source and sink nodes under various graph and error specifications show significant performance gains against state-of-the-art Bayesian Networks and deep-learning baselines. Applications to real-world systems also showcase BSG as an exploratory analysis tool for uncovering indirect spillovers and quantifying systemic risk.

</details>

<details>

<summary>2022-06-16 21:08:49 - Empirical Bayesian Approaches for Robust Constraint-based Causal Discovery under Insufficient Data</summary>

- *Zijun Cui, Naiyu Yin, Yuru Wang, Qiang Ji*

- `2206.08448v1` - [abs](http://arxiv.org/abs/2206.08448v1) - [pdf](http://arxiv.org/pdf/2206.08448v1)

> Causal discovery is to learn cause-effect relationships among variables given observational data and is important for many applications. Existing causal discovery methods assume data sufficiency, which may not be the case in many real world datasets. As a result, many existing causal discovery methods can fail under limited data. In this work, we propose Bayesian-augmented frequentist independence tests to improve the performance of constraint-based causal discovery methods under insufficient data: 1) We firstly introduce a Bayesian method to estimate mutual information (MI), based on which we propose a robust MI based independence test; 2) Secondly, we consider the Bayesian estimation of hypothesis likelihood and incorporate it into a well-defined statistical test, resulting in a robust statistical testing based independence test. We apply proposed independence tests to constraint-based causal discovery methods and evaluate the performance on benchmark datasets with insufficient samples. Experiments show significant performance improvement in terms of both accuracy and efficiency over SOTA methods.

</details>

<details>

<summary>2022-06-17 01:36:28 - Towards Data Assimilation in Level-Set Wildfire Models Using Bayesian Filtering</summary>

- *Joel Janek Dabrowski, Carolyn Huston, James Hilton, Stephane Mangeon, Petra Kuhnert*

- `2206.08501v1` - [abs](http://arxiv.org/abs/2206.08501v1) - [pdf](http://arxiv.org/pdf/2206.08501v1)

> The level-set method is a prominent approach to modelling the evolution of a fire over time based on a characterised rate of spread. It however does not provide a direct means for assimilating new data and quantifying uncertainty. Fire front predictions can be more accurate and agile if the models are able to assimilate data in real time. Furthermore, uncertainty estimation of the location and spread of the fire is critical for decision making. Using Bayesian filtering approaches, we extend the level-set method to allow for data assimilation and uncertainty quantification. We demonstrate these approaches on data from a controlled fire.

</details>

<details>

<summary>2022-06-17 03:57:27 - Capturing Actionable Dynamics with Structured Latent Ordinary Differential Equations</summary>

- *Paidamoyo Chapfuwa, Sherri Rose, Lawrence Carin, Edward Meeds, Ricardo Henao*

- `2202.12932v2` - [abs](http://arxiv.org/abs/2202.12932v2) - [pdf](http://arxiv.org/pdf/2202.12932v2)

> End-to-end learning of dynamical systems with black-box models, such as neural ordinary differential equations (ODEs), provides a flexible framework for learning dynamics from data without prescribing a mathematical model for the dynamics. Unfortunately, this flexibility comes at the cost of understanding the dynamical system, for which ODEs are used ubiquitously. Further, experimental data are collected under various conditions (inputs), such as treatments, or grouped in some way, such as part of sub-populations. Understanding the effects of these system inputs on system outputs is crucial to have any meaningful model of a dynamical system. To that end, we propose a structured latent ODE model that explicitly captures system input variations within its latent representation. Building on a static latent variable specification, our model learns (independent) stochastic factors of variation for each input to the system, thus separating the effects of the system inputs in the latent space. This approach provides actionable modeling through the controlled generation of time-series data for novel input combinations (or perturbations). Additionally, we propose a flexible approach for quantifying uncertainties, leveraging a quantile regression formulation. Results on challenging biological datasets show consistent improvements over competitive baselines in the controlled generation of observational data and inference of biologically meaningful system inputs.

</details>

<details>

<summary>2022-06-17 04:20:36 - Ensemble distributional forecasting for insurance loss reserving</summary>

- *Benjamin Avanzi, Yanfeng Li, Bernard Wong, Alan Xian*

- `2206.08541v1` - [abs](http://arxiv.org/abs/2206.08541v1) - [pdf](http://arxiv.org/pdf/2206.08541v1)

> Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data. This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments.   In this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple stochastic loss reserving models such that the strengths offered by different models can be utilised effectively. Criteria of choice consider the full distributional properties of the ensemble. A notable innovation of our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development, calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning.   Our ensemble reserving framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators).

</details>

<details>

<summary>2022-06-17 07:37:16 - Optimizing Sequential Experimental Design with Deep Reinforcement Learning</summary>

- *Tom Blau, Edwin V. Bonilla, Iadine Chades, Amir Dezfouli*

- `2202.00821v3` - [abs](http://arxiv.org/abs/2202.00821v3) - [pdf](http://arxiv.org/pdf/2202.00821v3)

> Bayesian approaches developed to solve the optimal design of sequential experiments are mathematically elegant but computationally challenging. Recently, techniques using amortization have been proposed to make these Bayesian approaches practical, by training a parameterized policy that proposes designs efficiently at deployment time. However, these methods may not sufficiently explore the design space, require access to a differentiable probabilistic model and can only optimize over continuous design spaces. Here, we address these limitations by showing that the problem of optimizing policies can be reduced to solving a Markov decision process (MDP). We solve the equivalent MDP with modern deep reinforcement learning techniques. Our experiments show that our approach is also computationally efficient at deployment time and exhibits state-of-the-art performance on both continuous and discrete design spaces, even when the probabilistic model is a black box.

</details>

<details>

<summary>2022-06-17 07:56:16 - Bayesian Model Averaging of Chain Event Graphs for Robust Explanatory Modelling</summary>

- *Peter Strong, Jim Q Smith*

- `2202.12352v2` - [abs](http://arxiv.org/abs/2202.12352v2) - [pdf](http://arxiv.org/pdf/2202.12352v2)

> Chain Event Graphs (CEGs) are a widely applicable class of probabilistic graphical model that can represent context-specific independence statements and asymmetric unfoldings of events in an easily interpretable way. Existing model selection literature on CEGs has largely focused on obtaining the maximum a posteriori (MAP) CEG. However, MAP selection is well-known to ignore model uncertainty. Here, we explore the use of Bayesian model averaging over this class. We demonstrate how this approach can quantify model uncertainty and leads to more robust inference by identifying shared features across multiple high-scoring models. Because the space of possible CEGs is huge, scoring models exhaustively for model averaging in all but small problems is prohibitive. However, we provide a simple modification of an existing model selection algorithm, that samples the model space, to illustrate the efficacy of Bayesian model averaging compared to more standard MAP modelling.

</details>

<details>

<summary>2022-06-17 08:27:41 - Optimal quasi-Bayesian reduced rank regression with incomplete response</summary>

- *The Tien Mai, Pierre Alquier*

- `2206.08619v1` - [abs](http://arxiv.org/abs/2206.08619v1) - [pdf](http://arxiv.org/pdf/2206.08619v1)

> The aim of reduced rank regression is to connect multiple response variables to multiple predictors. This model is very popular, especially in biostatistics where multiple measurements on individuals can be re-used to predict multiple outputs. Unfortunately, there are often missing data in such datasets, making it difficult to use standard estimation tools. In this paper, we study the problem of reduced rank regression where the response matrix is incomplete. We propose a quasi-Bayesian approach to this problem, in the sense that the likelihood is replaced by a quasi-likelihood. We provide a tight oracle inequality, proving that our method is adaptive to the rank of the coefficient matrix. We describe a Langevin Monte Carlo algorithm for the computation of the posterior mean. Numerical comparison on synthetic and real data show that our method are competitive to the state-of-the-art where the rank is chosen by cross validation, and sometimes lead to an improvement.

</details>

<details>

<summary>2022-06-17 09:50:46 - Meta-Learning Hypothesis Spaces for Sequential Decision-making</summary>

- *Parnian Kassraie, Jonas Rothfuss, Andreas Krause*

- `2202.00602v3` - [abs](http://arxiv.org/abs/2202.00602v3) - [pdf](http://arxiv.org/pdf/2202.00602v3)

> Obtaining reliable, adaptive confidence sets for prediction functions (hypotheses) is a central challenge in sequential decision-making tasks, such as bandits and model-based reinforcement learning. These confidence sets typically rely on prior assumptions on the hypothesis space, e.g., the known kernel of a Reproducing Kernel Hilbert Space (RKHS). Hand-designing such kernels is error prone, and misspecification may lead to poor or unsafe performance. In this work, we propose to meta-learn a kernel from offline data (Meta-KeL). For the case where the unknown kernel is a combination of known base kernels, we develop an estimator based on structured sparsity. Under mild conditions, we guarantee that our estimated RKHS yields valid confidence sets that, with increasing amounts of offline data, become as tight as those given the true unknown kernel. We demonstrate our approach on the kernelized bandit problem (a.k.a.~Bayesian optimization), where we establish regret bounds competitive with those given the true kernel. We also empirically evaluate the effectiveness of our approach on a Bayesian optimization task.

</details>

<details>

<summary>2022-06-17 11:45:27 - Bayesian Calibration of imperfect computer models using Physics-informed priors</summary>

- *Michail Spitieris, Ingelin Steinsland*

- `2201.06463v3` - [abs](http://arxiv.org/abs/2201.06463v3) - [pdf](http://arxiv.org/pdf/2201.06463v3)

> We introduce a computational efficient data-driven framework suitable for quantifying the uncertainty in physical parameters and model formulation of computer models, represented by differential equations. We construct physics-informed priors, which are multi-output GP priors that encode the model's structure in the covariance function. We extend this into a fully Bayesian framework that quantifies the uncertainty of physical parameters and model predictions. Since physical models often are imperfect descriptions of the real process, we allow the model to deviate from the observed data by considering a discrepancy function. To obtain the posterior distributions, we use Hamiltonian Monte Carlo sampling. We demonstrate our approach in a simulation study with hemodynamical models, which are time-dependent differential equations. Data are simulated from a more complex model than our modelling choice, and the aim is to learn physical parameters according to known mathematical connections. To demonstrate the flexibility of our approach, an example using the Heat equation, a space-time dependent differential equation where we consider a case of a biased data-acquisition process is also included. Finally, we fit the hemodynamic model using real data obtained in a medical trial.

</details>

<details>

<summary>2022-06-17 12:33:02 - Iterative importance sampling with Markov chain Monte Carlo sampling in robust Bayesian analysis</summary>

- *Ivette Raices Cruz, Johan Lindstrm, Matthias C. M. Troffaes, Ullrika Sahlin*

- `2206.08728v1` - [abs](http://arxiv.org/abs/2206.08728v1) - [pdf](http://arxiv.org/pdf/2206.08728v1)

> Bayesian inference under a set of priors, called robust Bayesian analysis, allows for estimation of parameters within a model and quantification of epistemic uncertainty in quantities of interest by bounded (or imprecise) probability. Iterative importance sampling can be used to estimate bounds on the quantity of interest by optimizing over the set of priors. A method for iterative importance sampling when the robust Bayesian inference rely on Markov chain Monte Carlo (MCMC) sampling is proposed. To accommodate the MCMC sampling in iterative importance sampling, a new expression for the effective sample size of the importance sampling is derived, which accounts for the correlation in the MCMC samples. To illustrate the proposed method for robust Bayesian analysis, iterative importance sampling with MCMC sampling is applied to estimate the lower bound of the overall effect in a previously published meta-analysis with a random effects model. The performance of the method compared to a grid search method and under different degrees of prior-data conflict is also explored.

</details>

<details>

<summary>2022-06-17 14:25:59 - Lagged couplings diagnose Markov chain Monte Carlo phylogenetic inference</summary>

- *Luke J. Kelly, Robin J. Ryder, Grgoire Clart*

- `2108.13328v2` - [abs](http://arxiv.org/abs/2108.13328v2) - [pdf](http://arxiv.org/pdf/2108.13328v2)

> Phylogenetic inference is an intractable statistical problem on a complex space. Markov chain Monte Carlo methods are the primary tool for Bayesian phylogenetic inference but it is challenging to construct efficient schemes to explore the associated posterior distribution or assess their performance. Existing approaches are unable to diagnose mixing or convergence of Markov schemes jointly across all components of a phylogenetic model. Lagged couplings of Markov chain Monte Carlo algorithms have recently been developed on simpler spaces to diagnose convergence and construct unbiased estimators. We describe a contractive coupling of Markov chains targeting a posterior distribution over a space of phylogenetic trees with branch lengths, scalar parameters and latent variables. We use these couplings to assess mixing and convergence of Markov chains jointly across all components of the phylogenetic model on trees with up to 200 leaves. Samples from our coupled chains may also be used to construct unbiased estimators.

</details>

<details>

<summary>2022-06-17 14:50:25 - Species Distribution Modeling with Expert Elicitation and Bayesian Calibration</summary>

- *Karel Kaurila, Sanna Kuningas, Antti Lappalainen, Jarno Vanhatalo*

- `2206.08817v1` - [abs](http://arxiv.org/abs/2206.08817v1) - [pdf](http://arxiv.org/pdf/2206.08817v1)

> Species distribution models (SDMs) are key tools in ecology, conservation and management of natural resources. They are commonly trained by scientific survey data but, since surveys are expensive, there is a need for complementary sources of information to train them. To this end, several authors have proposed to use expert elicitation since local citizen and substance area experts can hold valuable information on species distributions. Expert knowledge has been incorporated within SDMs, for example, through informative priors. However, existing approaches pose challenges related to assessment of the reliability of the experts. Since expert knowledge is inherently subjective and prone to biases, we should optimally calibrate experts' assessments and make inference on their reliability. Moreover, demonstrated examples of improved species distribution predictions using expert elicitation compared to using only survey data are few as well. In this work, we propose a novel approach to use expert knowledge on species distribution within SDMs and demonstrate that it leads to significantly better predictions. First, we propose expert elicitation process where experts summarize their belief on a species occurrence proability with maps. Second, we collect survey data to calibrate the expert assessments. Third, we propose a hierarchical Bayesian model that combines the two information sources and can be used to make predictions over the study area. We apply our methods to study the distribution of spring spawning pikeperch larvae in a coastal area of the Gulf of Finland. According to our results, the expert information significantly improves species distribution predictions compared to predictions conditioned on survey data only. However, experts' reliability also varies considerably, and even generally reliable experts had spatially structured biases in their assessments.

</details>

<details>

<summary>2022-06-17 16:54:56 - Scaling multi-species occupancy models to large citizen science datasets</summary>

- *Martin Ingram, Damjan Vukcevic, Nick Golding*

- `2206.08894v1` - [abs](http://arxiv.org/abs/2206.08894v1) - [pdf](http://arxiv.org/pdf/2206.08894v1)

> Citizen science datasets can be very large and promise to improve species distribution modelling, but detection is imperfect, risking bias when fitting models. In particular, observers may not detect species that are actually present. Occupancy models can estimate and correct for this observation process, and multi-species occupancy models exploit similarities in the observation process, which can improve estimates for rare species. However, the computational methods currently used to fit these models do not scale to large datasets. We develop approximate Bayesian inference methods and use graphics processing units (GPUs) to scale multi-species occupancy models to very large citizen science data. We fit multi-species occupancy models to one month of data from the eBird project consisting of 186,811 checklist records comprising 430 bird species. We evaluate the predictions on a spatially separated test set of 59,338 records, comparing two different inference methods -- Markov chain Monte Carlo (MCMC) and variational inference (VI) -- to occupancy models fitted to each species separately using maximum likelihood. We fitted models to the entire dataset using VI, and up to 32,000 records with MCMC. VI fitted to the entire dataset performed best, outperforming single-species models on both AUC (90.4% compared to 88.7%) and on log likelihood (-0.080 compared to -0.085). We also evaluate how well range maps predicted by the model agree with expert maps. We find that modelling the detection process greatly improves agreement and that the resulting maps agree as closely with expert maps as ones estimated using high quality survey data. Our results demonstrate that multi-species occupancy models are a compelling approach to model large citizen science datasets, and that, once the observation process is taken into account, they can model species distributions accurately.

</details>

<details>

<summary>2022-06-17 17:18:31 - Adapting the Linearised Laplace Model Evidence for Modern Deep Learning</summary>

- *Javier Antorn, David Janz, James Urquhart Allingham, Erik Daxberger, Riccardo Barbano, Eric Nalisnick, Jos Miguel Hernndez-Lobato*

- `2206.08900v1` - [abs](http://arxiv.org/abs/2206.08900v1) - [pdf](http://arxiv.org/pdf/2206.08900v1)

> The linearised Laplace method for estimating model uncertainty has received renewed attention in the Bayesian deep learning community. The method provides reliable error bars and admits a closed-form expression for the model evidence, allowing for scalable selection of model hyperparameters. In this work, we examine the assumptions behind this method, particularly in conjunction with model selection. We show that these interact poorly with some now-standard tools of deep learning--stochastic approximation methods and normalisation layers--and make recommendations for how to better adapt this classic method to the modern setting. We provide theoretical support for our recommendations and validate them empirically on MLPs, classic CNNs, residual networks with and without normalisation layers, generative autoencoders and transformers.

</details>

<details>

<summary>2022-06-17 21:29:55 - Bayesian Data Augmentation for Partially Observed Stochastic Compartmental Models</summary>

- *Shuying Wang, Stephen G. Walker*

- `2206.09018v1` - [abs](http://arxiv.org/abs/2206.09018v1) - [pdf](http://arxiv.org/pdf/2206.09018v1)

> Deterministic compartmental models are predominantly used in the modeling of infectious diseases, though stochastic models are considered more realistic, yet are complicated to estimate due to missing data. In this paper we present a novel algorithm for estimating the stochastic SIR/SEIR epidemic model within a Bayesian framework, which can be readily extended to more complex stochastic compartmental models. Specifically, based on the infinitesimal conditional independence properties of the model, we are able to find a proposal distribution for a Metropolis algorithm which is very close to the correct posterior distribution. As a consequence, rather than perform a Metropolis step updating one missing data point at a time, as in the current benchmark Markov chain Monte Carlo (MCMC) algorithm, we are able to extend our proposal to the entire set of missing observations. This improves the MCMC methods dramatically and makes the stochastic models now a viable modeling option. A number of real data illustrations and the necessary mathematical theory supporting our results are presented.

</details>

<details>

<summary>2022-06-18 01:35:19 - Why the Rich Get Richer? On the Balancedness of Random Partition Models</summary>

- *Changwoo J. Lee, Huiyan Sang*

- `2201.12697v2` - [abs](http://arxiv.org/abs/2201.12697v2) - [pdf](http://arxiv.org/pdf/2201.12697v2)

> Random partition models are widely used in Bayesian methods for various clustering tasks, such as mixture models, topic models, and community detection problems. While the number of clusters induced by random partition models has been studied extensively, another important model property regarding the balancedness of partition has been largely neglected. We formulate a framework to define and theoretically study the balancedness of exchangeable random partition models, by analyzing how a model assigns probabilities to partitions with different levels of balancedness. We demonstrate that the "rich-get-richer" characteristic of many existing popular random partition models is an inevitable consequence of two common assumptions: product-form exchangeability and projectivity. We propose a principled way to compare the balancedness of random partition models, which gives a better understanding of what model works better and what doesn't for different applications. We also introduce the "rich-get-poorer" random partition models and illustrate their application to entity resolution tasks.

</details>

<details>

<summary>2022-06-18 10:07:09 - Learning Multi-Task Gaussian Process Over Heterogeneous Input Domains</summary>

- *Haitao Liu, Kai Wu, Yew-Soon Ong, Chao Bian, Xiaomo Jiang, Xiaofang Wang*

- `2202.12636v3` - [abs](http://arxiv.org/abs/2202.12636v3) - [pdf](http://arxiv.org/pdf/2202.12636v3)

> Multi-task Gaussian process (MTGP) is a well-known non-parametric Bayesian model for learning correlated tasks effectively by transferring knowledge across tasks. But current MTGPs are usually limited to the multi-task scenario defined in the same input domain, leaving no space for tackling the heterogeneous case, i.e., the features of input domains vary over tasks. To this end, this paper presents a novel heterogeneous stochastic variational linear model of coregionalization (HSVLMC) model for simultaneously learning the tasks with varied input domains. Particularly, we develop the stochastic variational framework with Bayesian calibration that (i) takes into account the effect of dimensionality reduction raised by domain mappings in order to achieve effective input alignment; and (ii) employs a residual modeling strategy to leverage the inductive bias brought by prior domain mappings for better model inference. Finally, the superiority of the proposed model against existing LMC models has been extensively verified on diverse heterogeneous multi-task cases and a practical multi-fidelity steam turbine exhaust problem.

</details>

<details>

<summary>2022-06-18 10:56:02 - Bayesian Lesion Estimation with a Structured Spike-and-Slab Prior</summary>

- *Anna Menacher, Thomas E. Nichols, Chris Holmes, Habib Ganjgahi*

- `2206.09175v1` - [abs](http://arxiv.org/abs/2206.09175v1) - [pdf](http://arxiv.org/pdf/2206.09175v1)

> Neural demyelination and brain damage accumulated in white matter appear as hyperintense areas on MRI scans in the form of lesions. Modeling binary images at the population level, where each voxel represents the existence of a lesion, plays an important role in understanding aging and inflammatory diseases. We propose a scalable hierarchical Bayesian spatial model, called BLESS, capable of handling binary responses by placing continuous spike-and-slab mixture priors on spatially-varying parameters and enforcing spatial dependency on the parameter dictating the amount of sparsity within the probability of inclusion. The use of mean-field variational inference with dynamic posterior exploration, which is an annealing-like strategy that improves optimization, allows our method to scale to large sample sizes. Our method also accounts for underestimation of posterior variance due to variational inference by providing an approximate posterior sampling approach based on Bayesian bootstrap ideas and spike-and-slab priors with random shrinkage targets. Besides accurate uncertainty quantification, this approach is capable of producing novel cluster size based imaging statistics, such as credible intervals of cluster size, and measures of reliability of cluster occurrence. Lastly, we validate our results via simulation studies and an application to the UK Biobank, a large-scale lesion mapping study with a sample size of 40,000 subjects.

</details>

<details>

<summary>2022-06-18 12:30:06 - Efficient Aggregated Kernel Tests using Incomplete $U$-statistics</summary>

- *Antonin Schrab, Ilmun Kim, Benjamin Guedj, Arthur Gretton*

- `2206.09194v1` - [abs](http://arxiv.org/abs/2206.09194v1) - [pdf](http://arxiv.org/pdf/2206.09194v1)

> We propose a series of computationally efficient, nonparametric tests for the two-sample, independence and goodness-of-fit problems, using the Maximum Mean Discrepancy (MMD), Hilbert Schmidt Independence Criterion (HSIC), and Kernel Stein Discrepancy (KSD), respectively. Our test statistics are incomplete $U$-statistics, with a computational cost that interpolates between linear time in the number of samples, and quadratic time, as associated with classical $U$-statistic tests. The three proposed tests aggregate over several kernel bandwidths to detect departures from the null on various scales: we call the resulting tests MMDAggInc, HSICAggInc and KSDAggInc. For the test thresholds, we derive a quantile bound for wild bootstrapped incomplete $U$- statistics, which is of independent interest. We derive uniform separation rates for MMDAggInc and HSICAggInc, and quantify exactly the trade-off between computational efficiency and the attainable rates: this result is novel for tests based on incomplete $U$-statistics, to our knowledge. We further show that in the quadratic-time case, the wild bootstrap incurs no penalty to test power over more widespread permutation-based approaches, since both attain the same minimax optimal rates (which in turn match the rates that use oracle quantiles). We support our claims with numerical experiments on the trade-off between computational efficiency and test power. In the three testing frameworks, we observe that our proposed linear-time aggregated tests obtain higher power than current state-of-the-art linear-time kernel tests.

</details>

<details>

<summary>2022-06-18 16:13:58 - IID Sampling from Posterior Dirichlet Process Mixtures</summary>

- *Sourabh Bhattacharya*

- `2206.09233v1` - [abs](http://arxiv.org/abs/2206.09233v1) - [pdf](http://arxiv.org/pdf/2206.09233v1)

> The influence of Dirichlet process mixture is ubiquitous in the Bayesian nonparametrics literature. But sampling from its posterior distribution remains a challenge, despite the advent of various Markov chain Monte Carlo methods. The primary challenge is the infinite-dimensional setup, and even if the infinite-dimensional random measure is integrated out, high-dimensionality and discreteness still remain difficult issues to deal with.   In this article, exploiting the key ideas proposed in Bhattacharya (2021b), we propose a novel methodology for drawing iid realizations from posteriors of Dirichlet process mixtures. We focus in particular on the more general and flexible model of Bhattacharya (2008), so that the methods developed here are simply applicable to the traditional Dirichlet process mixture.   We illustrate our ideas on the well-known enzyme, acidity and the galaxy datasets, which are usually considered benchmark datasets for mixture applications. Generating 10, 000 iid realizations from the Dirichlet process mixture posterior of Bhattacharya (2008) given these datasets took 19 minutes, 8 minutes and 5 minutes, respectively, in our parallel implementation.

</details>

<details>

<summary>2022-06-18 20:00:07 - Efficient and Transferable Adversarial Examples from Bayesian Neural Networks</summary>

- *Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon, Koushik Sen*

- `2011.05074v4` - [abs](http://arxiv.org/abs/2011.05074v4) - [pdf](http://arxiv.org/pdf/2011.05074v4)

> An established way to improve the transferability of black-box evasion attacks is to craft the adversarial examples on an ensemble-based surrogate to increase diversity. We argue that transferability is fundamentally related to uncertainty. Based on a state-of-the-art Bayesian Deep Learning technique, we propose a new method to efficiently build a surrogate by sampling approximately from the posterior distribution of neural network weights, which represents the belief about the value of each parameter. Our extensive experiments on ImageNet, CIFAR-10 and MNIST show that our approach improves the success rates of four state-of-the-art attacks significantly (up to 83.2 percentage points), in both intra-architecture and inter-architecture transferability. On ImageNet, our approach can reach 94% of success rate while reducing training computations from 11.6 to 2.4 exaflops, compared to an ensemble of independently trained DNNs. Our vanilla surrogate achieves 87.5% of the time higher transferability than three test-time techniques designed for this purpose. Our work demonstrates that the way to train a surrogate has been overlooked, although it is an important element of transfer-based attacks. We are, therefore, the first to review the effectiveness of several training methods in increasing transferability. We provide new directions to better understand the transferability phenomenon and offer a simple but strong baseline for future work.

</details>

<details>

<summary>2022-06-18 22:10:25 - Approximate Bayesian Inference for the Interaction Types 1, 2, 3 and 4 with Application in Disease Mapping</summary>

- *Esmail Abdul Fattah, Haavard Rue*

- `2206.09287v1` - [abs](http://arxiv.org/abs/2206.09287v1) - [pdf](http://arxiv.org/pdf/2206.09287v1)

> We address in this paper a new approach for fitting spatiotemporal models with application in disease mapping using the interaction types 1,2,3, and 4. When we account for the spatiotemporal interactions in disease-mapping models, inference becomes more useful in revealing unknown patterns in the data. However, when the number of locations and/or the number of time points is large, the inference gets computationally challenging due to the high number of required constraints necessary for inference, and this holds for various inference architectures including Markov chain Monte Carlo (MCMC) and Integrated Nested Laplace Approximations (INLA). We re-formulate INLA approach based on dense matrices to fit the intrinsic spatiotemporal models with the four interaction types and account for the sum-to-zero constraints, and discuss how the new approach can be implemented in a high-performance computing framework. The computing time using the new approach does not depend on the number of constraints and can reach a 40-fold faster speed compared to INLA in realistic scenarios. This approach is verified by a simulation study and a real data application, and it is implemented in the R package INLAPLUS and the Python header function: inla1234().

</details>

<details>

<summary>2022-06-19 00:56:41 - Mathematical Theory of Bayesian Statistics for Unknown Information Source</summary>

- *Sumio Watanabe*

- `2206.05630v2` - [abs](http://arxiv.org/abs/2206.05630v2) - [pdf](http://arxiv.org/pdf/2206.05630v2)

> In statistical inference, uncertainty is unknown and all models are wrong. A person who makes a statistical model and a prior distribution is simultaneously aware that they are fictional and virtual candidates. In order to study such cases, several statistical measures have been constructed, such as cross validation, information criteria, and marginal likelihood, however, their mathematical properties have not yet been completely clarified when statistical models are under- and over- parametrized. In this paper, we introduce a place of mathematical theory of Bayesian statistics for unknown uncertainty, on which we show general properties of cross validation, information criteria, and marginal likelihood. The derived theory holds even if an unknown uncertainty is unrealizable by a statistical model or even if the posterior distribution cannot be approximated by any normal distribution, hence it gives a helpful standpoint for a person who cannot believe in any specific model and prior. The results are followings. (1) There exists a more precise statistical measure of the generalization loss than leave-one-out cross validation and information criterion based on the mathematical properties of them. (2) There exists a more efficient approximation method of the free energy, which is the minus log marginal likelihood, even if the posterior distribution cannot be approximated by any normal distribution. (3) And the prior distributions optimized by the cross validation and the widely applicable information criterion are asymptotically equivalent to each other, which are different from that by the marginal likelihood.

</details>

<details>

<summary>2022-06-19 03:19:59 - FRAPPE: $\underline{\text{F}}$ast $\underline{\text{Ra}}$nk $\underline{\text{App}}$roximation with $\underline{\text{E}}$xplainable Features for Tensors</summary>

- *William Shiao, Evangelos E. Papalexakis*

- `2206.09316v1` - [abs](http://arxiv.org/abs/2206.09316v1) - [pdf](http://arxiv.org/pdf/2206.09316v1)

> Tensor decompositions have proven to be effective in analyzing the structure of multidimensional data. However, most of these methods require a key parameter: the number of desired components. In the case of the CANDECOMP/PARAFAC decomposition (CPD), this value is known as the canonical rank and greatly affects the quality of the results. Existing methods use heuristics or Bayesian methods to estimate this value by repeatedly calculating the CPD, making them extremely computationally expensive. In this work, we propose FRAPPE and Self-FRAPPE: a cheaply supervised and a self-supervised method to estimate the canonical rank of a tensor without ever having to compute the CPD. We call FRAPPE cheaply supervised because it uses a fully synthetic training set without requiring real-world examples. We evaluate these methods on synthetic tensors, real tensors of known rank, and the weight tensor of a convolutional neural network. We show that FRAPPE and Self-FRAPPE offer large improvements in both effectiveness and speed, with a respective $15\%$ and $10\%$ improvement in MAPE and an $4000\times$ and $13\times$ improvement in evaluation speed over the best-performing baseline.

</details>

<details>

<summary>2022-06-19 06:46:39 - LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks</summary>

- *Aryaman Jeendgar, Aditya Pola, Soma S Dhavala, Snehanshu Saha*

- `2206.09333v1` - [abs](http://arxiv.org/abs/2206.09333v1) - [pdf](http://arxiv.org/pdf/2206.09333v1)

> High-throughput Genomics is ushering a new era in personalized health care, and targeted drug design and delivery. Mining these large datasets, and obtaining calibrated predictions is of immediate relevance and utility. In our work, we develop methods for Gene Expression Inference based on Deep neural networks. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of house keeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. However, check loss, used in quantile regression to drive the estimation process is not differentiable. We propose log-cosh as a smooth-alternative to the check loss. We apply our methods on GEO microarray dataset. We also extend the method to binary classification setting. Furthermore, we investigate other consequences of the smoothness of the loss in faster convergence.

</details>

<details>

<summary>2022-06-19 16:31:30 - Bayesian non-conjugate regression via variational belief updating</summary>

- *Cristian Castiglione, Mauro Bernardi*

- `2206.09444v1` - [abs](http://arxiv.org/abs/2206.09444v1) - [pdf](http://arxiv.org/pdf/2206.09444v1)

> We present an efficient semiparametric variational method to approximate the posterior distribution of Bayesian regression models combining subjective prior beliefs with an empirical risk function. Our results apply to all the mixed models predicting the data through a linear combination of the available covariates, including, as special cases, generalized linear mixed models, support vector machines, quantile and expectile regression. The iterative procedure designed for climbing the evidence lower bound only requires closed form updating formulas or the calculation of univariate numerical integrals, when no analytic solutions are available. Neither conjugacy nor elaborate data augmentation strategies are needed. As a generalization, we also extend our methodology in order to account for inducing sparsity and shrinkage priors, with particular attention to the generalizations of the Bayesian Lasso prior. The properties of the derived algorithm are then assessed through an extensive simulation study, in which we compare our proposal with Markov chain Monte Carlo, conjugate mean field variational Bayes and Laplace approximation in terms of posterior approximation accuracy and prediction error. A real data example is then presented through a probabilistic load forecasting application on the US power load consumption data.

</details>

<details>

<summary>2022-06-20 00:24:45 - Flexible and Hierarchical Prior for Bayesian Nonnegative Matrix Factorization</summary>

- *Jun Lu, Xuanyu Ye*

- `2205.11025v2` - [abs](http://arxiv.org/abs/2205.11025v2) - [pdf](http://arxiv.org/pdf/2205.11025v2)

> In this paper, we introduce a probabilistic model for learning nonnegative matrix factorization (NMF) that is commonly used for predicting missing values and finding hidden patterns in the data, in which the matrix factors are latent variables associated with each data dimension. The nonnegativity constraint for the latent factors is handled by choosing priors with support on the nonnegative subspace. Bayesian inference procedure based on Gibbs sampling is employed. We evaluate the model on several real-world datasets including MovieLens 100K and MovieLens 1M with different sizes and dimensions and show that the proposed Bayesian NMF GRRN model leads to better predictions and avoids overfitting compared to existing Bayesian NMF approaches.

</details>

<details>

<summary>2022-06-20 01:06:59 - Robust One Round Federated Learning with Predictive Space Bayesian Inference</summary>

- *Mohsin Hasan, Zehao Zhang, Kaiyang Guo, Mahdi Karami, Guojun Zhang, Xi Chen, Pascal Poupart*

- `2206.09526v1` - [abs](http://arxiv.org/abs/2206.09526v1) - [pdf](http://arxiv.org/pdf/2206.09526v1)

> Making predictions robust is an important challenge. A separate challenge in federated learning (FL) is to reduce the number of communication rounds, particularly since doing so reduces performance in heterogeneous data settings. To tackle both issues, we take a Bayesian perspective on the problem of learning a global model. We show how the global predictive posterior can be approximated using client predictive posteriors. This is unlike other works which aggregate the local model space posteriors into the global model space posterior, and are susceptible to high approximation errors due to the posterior's high dimensional multimodal nature. In contrast, our method performs the aggregation on the predictive posteriors, which are typically easier to approximate owing to the low-dimensionality of the output space. We present an algorithm based on this idea, which performs MCMC sampling at each client to obtain an estimate of the local posterior, and then aggregates these in one round to obtain a global ensemble model. Through empirical evaluation on several classification and regression tasks, we show that despite using one round of communication, the method is competitive with other FL techniques, and outperforms them on heterogeneous settings. The code is publicly available at https://github.com/hasanmohsin/FedPredSpace_1Round.

</details>

<details>

<summary>2022-06-20 05:33:12 - Combinatorial Bayesian Optimization with Random Mapping Functions to Convex Polytopes</summary>

- *Jungtaek Kim, Seungjin Choi, Minsu Cho*

- `2011.13094v2` - [abs](http://arxiv.org/abs/2011.13094v2) - [pdf](http://arxiv.org/pdf/2011.13094v2)

> Bayesian optimization is a popular method for solving the problem of global optimization of an expensive-to-evaluate black-box function. It relies on a probabilistic surrogate model of the objective function, upon which an acquisition function is built to determine where next to evaluate the objective function. In general, Bayesian optimization with Gaussian process regression operates on a continuous space. When input variables are categorical or discrete, an extra care is needed. A common approach is to use one-hot encoded or Boolean representation for categorical variables which might yield a combinatorial explosion problem. In this paper we present a method for Bayesian optimization in a combinatorial space, which can operate well in a large combinatorial space. The main idea is to use a random mapping which embeds the combinatorial space into a convex polytope in a continuous space, on which all essential process is performed to determine a solution to the black-box optimization in the combinatorial space. We describe our combinatorial Bayesian optimization algorithm and present its regret analysis. Numerical experiments demonstrate that our method shows satisfactory performance compared to existing methods.

</details>

<details>

<summary>2022-06-20 08:26:38 - Approximate Bayesian Computation with Domain Expert in the Loop</summary>

- *Ayush Bharti, Louis Filstroff, Samuel Kaski*

- `2201.12090v2` - [abs](http://arxiv.org/abs/2201.12090v2) - [pdf](http://arxiv.org/pdf/2201.12090v2)

> Approximate Bayesian computation (ABC) is a popular likelihood-free inference method for models with intractable likelihood functions. As ABC methods usually rely on comparing summary statistics of observed and simulated data, the choice of the statistics is crucial. This choice involves a trade-off between loss of information and dimensionality reduction, and is often determined based on domain knowledge. However, handcrafting and selecting suitable statistics is a laborious task involving multiple trial-and-error steps. In this work, we introduce an active learning method for ABC statistics selection which reduces the domain expert's work considerably. By involving the experts, we are able to handle misspecified models, unlike the existing dimension reduction methods. Moreover, empirical results show better posterior estimates than with existing methods, when the simulation budget is limited.

</details>

<details>

<summary>2022-06-20 12:29:07 - Intergenerational risk sharing in a Defined Contribution pension system: analysis with Bayesian optimization</summary>

- *An Chen, Motonobu Kanagawa, Fangyuan Zhang*

- `2106.13644v2` - [abs](http://arxiv.org/abs/2106.13644v2) - [pdf](http://arxiv.org/pdf/2106.13644v2)

> We study a fully funded, collective defined-contribution (DC) pension system with multiple overlapping generations. We investigate whether the welfare of participants can be improved by intergenerational risk sharing (IRS) implemented with a realistic investment strategy (e.g., no borrowing) and without an outside entity (e.g., share holders) that helps finance the pension fund. To implement IRS, the pension system uses an automatic adjustment rule for the indexation of individual accounts, which adapts to the notional funding ratio of the pension system. The pension system has two parameters that determine the investment strategy and the strength of the adjustment rule, which are optimized by expected utility maximization using Bayesian optimization. The volatility of the retirement benefits and that of the funding ratio are analyzed, and it is shown that the trade-off between them can be controlled by the optimal adjustment parameter to attain IRS. Compared with the optimal individual DC benchmark using the life-cycle strategy, the studied pension system with IRS is shown to improve the welfare of risk-averse participants, when the financial market is volatile.

</details>

<details>

<summary>2022-06-20 15:39:56 - Scalable Bayesian Network Structure Learning with Splines</summary>

- *Charupriya Sharma, Peter van Beek*

- `2110.14626v2` - [abs](http://arxiv.org/abs/2110.14626v2) - [pdf](http://arxiv.org/pdf/2110.14626v2)

> The graph structure of a Bayesian network (BN) can be learned from data using the well-known score-and-search approach. Previous work has shown that incorporating structured representations of the conditional probability distributions (CPDs) into the score-and-search approach can improve the accuracy of the learned graph. In this paper, we present a novel approach capable of learning the graph of a BN and simultaneously modelling linear and non-linear local probabilistic relationships between variables. We achieve this by a combination of feature selection to reduce the search space for local relationships and extending the score-and-search approach to incorporate modelling the CPDs over variables as Multivariate Adaptive Regression Splines (MARS). MARS are polynomial regression models represented as piecewise spline functions. We show on a set of discrete and continuous benchmark instances that our proposed approach can improve the accuracy of the learned graph while scaling to instances with a large number of variables.

</details>

<details>

<summary>2022-06-20 20:39:08 - Modelling Populations of Interaction Networks via Distance Metrics</summary>

- *George Bolt, Simn Lunagmez, Christopher Nemeth*

- `2206.09995v1` - [abs](http://arxiv.org/abs/2206.09995v1) - [pdf](http://arxiv.org/pdf/2206.09995v1)

> Network data arises through observation of relational information between a collection of entities. Recent work in the literature has independently considered when (i) one observes a sample of networks, connectome data in neuroscience being a ubiquitous example, and (ii) the units of observation within a network are edges or paths, such as emails between people or a series of page visits to a website by a user, often referred to as interaction network data. The intersection of these two cases, however, is yet to be considered. In this paper, we propose a new Bayesian modelling framework to analyse such data. Given a practitioner-specified distance metric between observations, we define families of models through location and scale parameters, akin to a Gaussian distribution, with subsequent inference of model parameters providing reasoned statistical summaries for this non-standard data structure. To facilitate inference, we propose specialised Markov chain Monte Carlo (MCMC) schemes capable of sampling from doubly-intractable posterior distributions over discrete and multi-dimensional parameter spaces. Through simulation studies we confirm the efficacy of our methodology and inference scheme, whilst its application we illustrate via an example analysis of a location-based social network (LSBN) data set.

</details>

<details>

<summary>2022-06-20 22:05:59 - Dynamic modeling of the Italians' attitude towards Covid-19</summary>

- *Emanuele Aliverti, Massimilano Russo*

- `2108.01194v2` - [abs](http://arxiv.org/abs/2108.01194v2) - [pdf](http://arxiv.org/pdf/2108.01194v2)

> We analyze repeated cross-sectional survey data collected by the Institute of Global Health Innovation, to characterize the perception and behavior of the Italian population during the Covid-19 pandemic, focusing on the period that spans from April to November 2020. To accomplish this goal, we propose a Bayesian dynamic latent-class regression model, that accounts for the effect of sampling bias including survey weights into the likelihood function. According to the proposed approach, attitudes towards Covid-19 are described via three ideal behaviors that are fixed over time, corresponding to different degrees of compliance with spread-preventive measures. The overall tendency toward a specific profile dynamically changes across survey waves via a latent Gaussian process regression, that adjusts for subject-specific covariates. We illustrate the dynamic evolution of Italians' behaviors during the pandemic, providing insights on how the proportion of ideal behaviors has varied during the phases of the lockdown, while measuring the effect of age, sex, region and employment of the respondents on the attitude toward Covid-19.

</details>

<details>

<summary>2022-06-21 02:37:40 - Quantum Speedup of Natural Gradient for Variational Bayes</summary>

- *Anna Lopatnikova, Minh-Ngoc Tran*

- `2106.05807v3` - [abs](http://arxiv.org/abs/2106.05807v3) - [pdf](http://arxiv.org/pdf/2106.05807v3)

> Variational Bayes (VB) is a critical method in machine learning and statistics, underpinning the recent success of Bayesian deep learning. The natural gradient is an essential component of efficient VB estimation, but it is prohibitively computationally expensive in high dimensions. We propose a computationally efficient regression-based method for natural gradient estimation, with convergence guarantees under standard assumptions. The method enables the use of quantum matrix inversion to further speed up VB. We demonstrate that the problem setup fulfills the conditions required for quantum matrix inversion to deliver computational efficiency. The method works with a broad range of statistical models and does not require special-purpose or simplified variational distributions.

</details>

<details>

<summary>2022-06-21 04:46:42 - A zero-inflated Bayesian nonparametric approach for identifying differentially abundant taxa in multigroup microbiome data with covariates</summary>

- *Archie Sachdeva, Somnath Datta, Subharup Guha*

- `2206.10108v1` - [abs](http://arxiv.org/abs/2206.10108v1) - [pdf](http://arxiv.org/pdf/2206.10108v1)

> Scientific studies conducted during the last two decades have established the central role of the microbiome in disease and health. Differential abundance analysis aims to identify microbial taxa associated with two or more sample groups defined by attributes such as disease subtype, geography, or environmental condition. The results, in turn, help clinical practitioners and researchers diagnose disease and develop new treatments more effectively. However, detecting differential abundance is uniquely challenging due to the high dimensionality, collinearity, sparsity, and compositionality of microbiome data. Further, there is a critical need for unified statistical approaches that can directly compare more than two groups and appropriately adjust for covariates. We develop a zero-inflated Bayesian nonparametric (ZIBNP) methodology that meets the multipronged challenges posed by microbiome data and identifies differentially abundant taxa in two or more groups, while also accounting for sample-specific covariates. The proposed hierarchical model flexibly adapts to unique data characteristics, casts the typically high proportion of zeros in a censoring framework, and mitigates high dimensionality and collinearity issues by utilizing the dimension reducing property of the semiparametric Chinese restaurant process. The approach relates the microbiome sampling depths to inferential precision and conforms with the compositional nature of microbiome data. In simulation studies and in the analyses of the CAnine Microbiome during Parasitism (CAMP) data on infected and uninfected dogs, and the Global Gut microbiome data on human subjects belonging to three geographical regions, we compare ZIBNP with established statistical methods for differential abundance analysis in the presence of covariates.

</details>

<details>

<summary>2022-06-21 10:36:57 - Efficient Bayesian Modeling of Binary and Categorical Data in R: The UPG Package</summary>

- *Gregor Zens, Sylvia Frhwirth-Schnatter, Helga Wagner*

- `2101.02506v2` - [abs](http://arxiv.org/abs/2101.02506v2) - [pdf](http://arxiv.org/pdf/2101.02506v2)

> In this vignette, we introduce the UPG package for efficient Bayesian inference in probit, logit, multinomial logit and binomial logit models. UPG offers a convenient estimation framework for balanced and imbalanced data settings where sampling efficiency is ensured through marginal data augmentation. UPG provides several methods for fast production of output tables and summary plots that are easily accessible to a broad range of users.

</details>

<details>

<summary>2022-06-21 14:36:44 - Crime in Philadelphia: Bayesian Clustering with Particle Optimization</summary>

- *Cecilia Balocchi, Sameer K. Deshpande, Edward I. George, Shane T. Jensen*

- `1912.00111v3` - [abs](http://arxiv.org/abs/1912.00111v3) - [pdf](http://arxiv.org/pdf/1912.00111v3)

> Accurate estimation of the change in crime over time is a critical first step towards better understanding of public safety in large urban environments. Bayesian hierarchical modeling is a natural way to study spatial variation in urban crime dynamics at the neighborhood level, since it facilitates principled ``sharing of information'' between spatially adjacent neighborhoods. Typically, however, cities contain many physical and social boundaries that may manifest as spatial discontinuities in crime patterns. In this situation, standard prior choices often yield overly-smooth parameter estimates, which can ultimately produce mis-calibrated forecasts. To prevent potential over-smoothing, we introduce a prior that partitions the set of neighborhoods into several clusters and encourages spatial smoothness within each cluster. In terms of model implementation, conventional stochastic search techniques are computationally prohibitive, as they must traverse a combinatorially vast space of partitions. We introduce an ensemble optimization procedure that simultaneously identifies several high probability partitions by solving one optimization problem using a new local search strategy. We then use the identified partitions to estimate crime trends in Philadelphia between 2006 and 2017. On simulated and real data, our proposed method demonstrates good estimation and partition selection performance.

</details>

<details>

<summary>2022-06-21 14:52:37 - Ridge Regularized Estimation of VAR Models for Inference</summary>

- *Giovanni Ballarin*

- `2105.00860v2` - [abs](http://arxiv.org/abs/2105.00860v2) - [pdf](http://arxiv.org/pdf/2105.00860v2)

> Ridge regression is a popular regularization method that has wide applicability, as many regression problems can be cast in this form. However, ridge is only seldom applied in the estimation of vector autoregressive models -- even though ridge naturally arises in Bayesian time series modeling. In this work I study ridge regression in the context of vector autoregressive process estimation and inference. The effects of shrinkage are analyzed and asymptotic theory is derived enabling inference. Frequentist and Bayesian ridge approaches are compared, and a hybrid VAR-LP estimator is proposed. Finally, the estimation of impulse response functions is evaluated with Monte Carlo simulations, and ridge regression is compared with a number of similar and competing methods.

</details>

<details>

<summary>2022-06-21 16:32:11 - Bayesian modeling and clustering for spatio-temporal areal data: an application to Italian unemployment</summary>

- *Alexander Mozdzen, Andrea Cremaschi, Annalisa Cadonna, Alessandra Guglielmi, Gregor Kastner*

- `2206.10509v1` - [abs](http://arxiv.org/abs/2206.10509v1) - [pdf](http://arxiv.org/pdf/2206.10509v1)

> Spatio-temporal areal data can be seen as a collection of time series which are spatially correlated according to a specific neighboring structure. Incorporating the temporal and spatial dimension into a statistical model poses challenges regarding the underlying theoretical framework as well as the implementation of efficient computational methods. We propose to include spatio-temporal random effects using a conditional autoregressive prior, where the temporal correlation is modeled through an autoregressive mean decomposition and the spatial correlation by the precision matrix inheriting the neighboring structure. Their joint distribution constitutes a Gaussian Markov Random Field, whose sparse precision matrix enables the usage of efficient sampling algorithms. We cluster the areal units using a nonparametric prior, thereby learning latent partitions of the areal units. The performance of the model is assessed via an application to study regional unemployment patterns in Italy. When compared to other spatial and spatio-temporal competitors, our model shows more precise estimates and the additional information obtained from the clustering allows for an extended economic interpretation of the unemployment rates of the Italian provinces.

</details>

<details>

<summary>2022-06-21 16:54:57 - Exact Bayesian inference for discretely observed Markov Jump Processes using finite rate matrices</summary>

- *Chris Sherlock, Andrew Golightly*

- `2001.02168v2` - [abs](http://arxiv.org/abs/2001.02168v2) - [pdf](http://arxiv.org/pdf/2001.02168v2)

> We present new methodologies for Bayesian inference on the rate parameters of a discretely observed continuous-time Markov jump processes with a countably infinite state space. The usual method of choice for inference, particle Markov chain Monte Carlo (particle MCMC), struggles when the observation noise is small. We consider the most challenging regime of exact observations and provide two new methodologies for inference in this case: the minimal extended state space algorithm (MESA) and the nearly minimal extended state space algorithm (nMESA). By extending the Markov chain Monte Carlo state space, both MESA and nMESA use the exponentiation of finite rate matrices to perform exact Bayesian inference on the Markov jump process even though its state space is countably infinite. Numerical experiments show improvements over particle MCMC of between a factor of three and several orders of magnitude.

</details>

<details>

<summary>2022-06-21 18:48:19 - Clustering microbiome data using mixtures of logistic normal multinomial models</summary>

- *Yuan Fang, Sanjeena Subedi*

- `2011.06682v2` - [abs](http://arxiv.org/abs/2011.06682v2) - [pdf](http://arxiv.org/pdf/2011.06682v2)

> Discrete data such as counts of microbiome taxa resulting from next-generation sequencing are routinely encountered in bioinformatics. Taxa count data in microbiome studies are typically high-dimensional, over-dispersed, and can only reveal relative abundance therefore being treated as compositional. Analyzing compositional data presents many challenges because they are restricted on a simplex. In a logistic normal multinomial model, the relative abundance is mapped from a simplex to a latent variable that exists on the real Euclidean space using the additive log-ratio transformation. While a logistic normal multinomial approach brings in flexibility for modeling the data, it comes with a heavy computational cost as the parameter estimation typically relies on Bayesian techniques. In this paper, we develop a novel mixture of logistic normal multinomial models for clustering microbiome data. Additionally, we utilize an efficient framework for parameter estimation using variational Gaussian approximations (VGA). Adopting a variational Gaussian approximation for the posterior of the latent variable reduces the computational overhead substantially. The proposed method is illustrated on simulated and real datasets.

</details>

<details>

<summary>2022-06-21 21:47:50 - Conformal Prediction Intervals for Markov Decision Process Trajectories</summary>

- *Thomas G. Dietterich, Jesse Hostetler*

- `2206.04860v2` - [abs](http://arxiv.org/abs/2206.04860v2) - [pdf](http://arxiv.org/pdf/2206.04860v2)

> Before delegating a task to an autonomous system, a human operator may want a guarantee about the behavior of the system. This paper extends previous work on conformal prediction for functional data and conformalized quantile regression to provide conformal prediction intervals over the future behavior of an autonomous system executing a fixed control policy on a Markov Decision Process (MDP). The prediction intervals are constructed by applying conformal corrections to prediction intervals computed by quantile regression. The resulting intervals guarantee that with probability $1-\delta$ the observed trajectory will lie inside the prediction interval, where the probability is computed with respect to the starting state distribution and the stochasticity of the MDP. The method is illustrated on MDPs for invasive species management and StarCraft2 battles.

</details>

<details>

<summary>2022-06-21 22:22:12 - Bayesian Tensor Factorized Mixed Effects Vector Autoregressive Processes for Inferring Granger Causality Patterns from High-Dimensional Neuroimage Data</summary>

- *Jingjing Fan, Kevin Sitek, Bharath Chandrasekaran, Abhra Sarkar*

- `2206.10757v1` - [abs](http://arxiv.org/abs/2206.10757v1) - [pdf](http://arxiv.org/pdf/2206.10757v1)

> Understanding the dynamics of functional brain connectivity patterns using noninvasive neuroimaging techniques is an important focus in human neuroscience. Vector autoregressive (VAR) processes and Granger causality analysis (GCA) have been extensively used to examine functional brain connectivity. While high-resolution neuroimage data are routinely collected now-a-days, the statistics literature on VAR models has remained heavily focused on small-to-moderate dimensional problems and single subject data. Motivated by these issues, we develop a novel Bayesian semiparametric VAR model that addresses the daunting dimensionality challenges by structuring the VAR coefficients matrices as a three-way tensor and then applying a tensor decomposition. A novel sparsity-inducing shrinkage prior allows data-adaptive dimension reduction, including automated lag selection. We also extend the approach to a novel mixed model for multi-subject neuroimaging data, capturing common brain connectivity patterns via shared fixed effects while also accommodating subject specific heterogeneity via random effects. Finally, GCA is performed via a posterior false discovery rate control procedure. We design a Markov chain Monte Carlo algorithm for posterior computation. We evaluate the methods' empirical performances through synthetic experiments. Applied to our motivating functional magnetic resonance imaging study, the proposed approach allows the directional connectivity of brain networks to be studied in fine detail, revealing meaningful but previously unsubstantiated cortical connectivity patterns.

</details>

<details>

<summary>2022-06-22 01:20:04 - Convergence Rates for Learning Linear Operators from Noisy Data</summary>

- *Maarten V. de Hoop, Nikola B. Kovachki, Nicholas H. Nelsen, Andrew M. Stuart*

- `2108.12515v2` - [abs](http://arxiv.org/abs/2108.12515v2) - [pdf](http://arxiv.org/pdf/2108.12515v2)

> This paper studies the learning of linear operators between infinite-dimensional Hilbert spaces. The training data comprises pairs of random input vectors in a Hilbert space and their noisy images under an unknown self-adjoint linear operator. Assuming that the operator is diagonalizable in a known basis, this work solves the equivalent inverse problem of estimating the operator's eigenvalues given the data. Adopting a Bayesian approach, the theoretical analysis establishes posterior contraction rates in the infinite data limit with Gaussian priors that are not directly linked to the forward map of the inverse problem. The main results also include learning-theoretic generalization error guarantees for a wide range of distribution shifts. These convergence rates quantify the effects of data smoothness and true eigenvalue decay or growth, for compact or unbounded operators, respectively, on sample complexity. Numerical evidence supports the theory in diagonal and non-diagonal settings.

</details>

<details>

<summary>2022-06-22 06:58:54 - From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses</summary>

- *Daniil Tiapkin, Denis Belomestny, Eric Moulines, Alexey Naumov, Sergey Samsonov, Yunhao Tang, Michal Valko, Pierre Menard*

- `2205.07704v2` - [abs](http://arxiv.org/abs/2205.07704v2) - [pdf](http://arxiv.org/pdf/2205.07704v2)

> We propose the Bayes-UCBVI algorithm for reinforcement learning in tabular, stage-dependent, episodic Markov decision process: a natural extension of the Bayes-UCB algorithm by Kaufmann et al. (2012) for multi-armed bandits. Our method uses the quantile of a Q-value function posterior as upper confidence bound on the optimal Q-value function. For Bayes-UCBVI, we prove a regret bound of order $\widetilde{O}(\sqrt{H^3SAT})$ where $H$ is the length of one episode, $S$ is the number of states, $A$ the number of actions, $T$ the number of episodes, that matches the lower-bound of $\Omega(\sqrt{H^3SAT})$ up to poly-$\log$ terms in $H,S,A,T$ for a large enough $T$. To the best of our knowledge, this is the first algorithm that obtains an optimal dependence on the horizon $H$ (and $S$) without the need for an involved Bernstein-like bonus or noise. Crucial to our analysis is a new fine-grained anti-concentration bound for a weighted Dirichlet sum that can be of independent interest. We then explain how Bayes-UCBVI can be easily extended beyond the tabular setting, exhibiting a strong link between our algorithm and Bayesian bootstrap (Rubin, 1981).

</details>

<details>

<summary>2022-06-22 11:13:18 - Diagnostic Tool for Out-of-Sample Model Evaluation</summary>

- *Ludvig Hult, Dave Zachariah, Petre Stoica*

- `2206.10982v1` - [abs](http://arxiv.org/abs/2206.10982v1) - [pdf](http://arxiv.org/pdf/2206.10982v1)

> Assessment of model fitness is an important step in many problems. Models are typically fitted to training data by minimizing a loss function, such as the squared-error or negative log-likelihood, and it is natural to desire low losses on future data. This letter considers the use of a test data set to characterize the out-of-sample losses of a model. We propose a simple model diagnostic tool that provides finite-sample guarantees under weak assumptions. The tool is computationally efficient and can be interpreted as an empirical quantile. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyper-parameter tuning.

</details>

<details>

<summary>2022-06-22 13:19:00 - Bayesian nonparametric scalar-on-image regression via Potts-Gibbs random partition models</summary>

- *Mica Teo Shu Xian, Sara Wade*

- `2206.11051v1` - [abs](http://arxiv.org/abs/2206.11051v1) - [pdf](http://arxiv.org/pdf/2206.11051v1)

> Scalar-on-image regression aims to investigate changes in a scalar response of interest based on high-dimensional imaging data. We propose a novel Bayesian nonparametric scalar-on-image regression model that utilises the spatial coordinates of the voxels to group voxels with similar effects on the response to have a common coefficient. We employ the Potts-Gibbs random partition model as the prior for the random partition in which the partition process is spatially dependent, thereby encouraging groups representing spatially contiguous regions. In addition, Bayesian shrinkage priors are utilised to identify the covariates and regions that are most relevant for the prediction. The proposed model is illustrated using the simulated data sets.

</details>

<details>

<summary>2022-06-22 13:22:42 - Discriminative Bayesian filtering lends momentum to the stochastic Newton method for minimizing log-convex functions</summary>

- *Michael C. Burkhart*

- `2104.12949v2` - [abs](http://arxiv.org/abs/2104.12949v2) - [pdf](http://arxiv.org/pdf/2104.12949v2)

> To minimize the average of a set of log-convex functions, the stochastic Newton method iteratively updates its estimate using subsampled versions of the full objective's gradient and Hessian. We contextualize this optimization problem as sequential Bayesian inference on a latent state-space model with a discriminatively-specified observation process. Applying Bayesian filtering then yields a novel optimization algorithm that considers the entire history of gradients and Hessians when forming an update. We establish matrix-based conditions under which the effect of older observations diminishes over time, in a manner analogous to Polyak's heavy ball momentum. We illustrate various aspects of our approach with an example and review other relevant innovations for the stochastic Newton method.

</details>

<details>

<summary>2022-06-22 15:30:21 - Cold Posteriors through PAC-Bayes</summary>

- *Konstantinos Pitas, Julyan Arbel*

- `2206.11173v1` - [abs](http://arxiv.org/abs/2206.11173v1) - [pdf](http://arxiv.org/pdf/2206.11173v1)

> We investigate the cold posterior effect through the lens of PAC-Bayes generalization bounds. We argue that in the non-asymptotic setting, when the number of training samples is (relatively) small, discussions of the cold posterior effect should take into account that approximate Bayesian inference does not readily provide guarantees of performance on out-of-sample data. Instead, out-of-sample error is better described through a generalization bound. In this context, we explore the connections between the ELBO objective from variational inference and the PAC-Bayes objectives. We note that, while the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally contain a temperature parameter $\lambda$ which is not restricted to be $\lambda=1$. For both regression and classification tasks, in the case of isotropic Laplace approximations to the posterior, we show how this PAC-Bayesian interpretation of the temperature parameter captures the cold posterior effect.

</details>

<details>

<summary>2022-06-22 17:07:58 - Graphical Evidence</summary>

- *Anindya Bhadra, Ksheera Sagar, Sayantan Banerjee, Jyotishka Datta*

- `2205.01016v2` - [abs](http://arxiv.org/abs/2205.01016v2) - [pdf](http://arxiv.org/pdf/2205.01016v2)

> Marginal likelihood, also known as model evidence, is a fundamental quantity in Bayesian statistics. It is used for model selection using Bayes factors or for empirical Bayes tuning of prior hyper-parameters. Yet, the calculation of evidence has remained a longstanding open problem in Gaussian graphical models. Currently, the only feasible solutions that exist are for special cases such as the Wishart or G-Wishart, in moderate dimensions. We present an application of Chib's technique that is applicable to a very broad class of priors under mild requirements. Specifically, the requirements are: (a) the priors on the diagonal terms on the precision matrix can be written as gamma or scale mixtures of gamma random variables and (b) those on the off-diagonal terms can be represented as normal or scale mixtures of normal. This includes structured priors such as the Wishart or G-Wishart, and more recently introduced element-wise priors, such as the Bayesian graphical lasso and the graphical horseshoe. Among these, the true marginal is known in an analytically closed form for Wishart, providing a useful validation of our approach. For the general setting of the other three, and several more priors satisfying conditions (a) and (b) above, the calculation of evidence has remained an open question that this article resolves under a unifying framework.

</details>

<details>

<summary>2022-06-22 17:35:30 - Coping with Information Loss and the Use of Auxiliary Sources of Data: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions</summary>

- *Silvia Calderazzo, Sergey Tarima, Carissa Reid, Nancy Flournoy, Tim Friede, Nancy Geller, James L Rosenberger, Nigel Stallard, Moreno Ursino, Marc Vandemeulebroecke, Kelly Van Lancker, Sarah Zohar*

- `2206.11238v1` - [abs](http://arxiv.org/abs/2206.11238v1) - [pdf](http://arxiv.org/pdf/2206.11238v1)

> Clinical trials disruption has always represented a non negligible part of the ending of interventional studies. While the SARS-CoV-2 (COVID-19) pandemic has led to an impressive and unprecedented initiation of clinical research, it has also led to considerable disruption of clinical trials in other disease areas, with around 80% of non-COVID-19 trials stopped or interrupted during the pandemic. In many cases the disrupted trials will not have the planned statistical power necessary to yield interpretable results. This paper describes methods to compensate for the information loss arising from trial disruptions by incorporating additional information available from auxiliary data sources. The methods described include the use of auxiliary data on baseline and early outcome data available from the trial itself and frequentist and Bayesian approaches for the incorporation of information from external data sources. The methods are illustrated by application to the analysis of artificial data based on the Primary care pediatrics Learning Activity Nutrition (PLAN) study, a clinical trial assessing a diet and exercise intervention for overweight children, that was affected by the COVID-19 pandemic. We show how all of the methods proposed lead to an increase in precision relative to use of complete case data only.

</details>

<details>

<summary>2022-06-22 17:40:38 - Persuasion with Ambiguous Receiver Preferences</summary>

- *Eitan Sapiro-Gheiler*

- `2109.11536v3` - [abs](http://arxiv.org/abs/2109.11536v3) - [pdf](http://arxiv.org/pdf/2109.11536v3)

> I describe a Bayesian persuasion problem where Receiver has a private type representing a cutoff for choosing Sender's preferred action, and Sender has maxmin preferences over all Receiver type distributions with known mean and bounds. This problem can be represented as a zero-sum game where Sender chooses a distribution of posterior mean beliefs that is a mean-preserving contraction of the prior over states, and an adversarial Nature chooses a Receiver type distribution with the known mean; the player with the higher realization from their chosen distribution wins. I formalize the connection between maxmin persuasion and similar games used to model political spending, all-pay auctions, and competitive persuasion. In both a standard binary-state setting and a new continuous-state setting, Sender optimally linearizes the prior distribution over states to create a distribution of posterior means that is uniform on a known interval with an atom at the lower bound of its support.

</details>

<details>

<summary>2022-06-22 17:54:57 - KSD Aggregated Goodness-of-fit Test</summary>

- *Antonin Schrab, Benjamin Guedj, Arthur Gretton*

- `2202.00824v3` - [abs](http://arxiv.org/abs/2202.00824v3) - [pdf](http://arxiv.org/pdf/2202.00824v3)

> We investigate properties of goodness-of-fit tests based on the Kernel Stein Discrepancy (KSD). We introduce a strategy to construct a test, called KSDAgg, which aggregates multiple tests with different kernels. KSDAgg avoids splitting the data to perform kernel selection (which leads to a loss in test power), and rather maximises the test power over a collection of kernels. We provide theoretical guarantees on the power of KSDAgg: we show it achieves the smallest uniform separation rate of the collection, up to a logarithmic term. KSDAgg can be computed exactly in practice as it relies either on a parametric bootstrap or on a wild bootstrap to estimate the quantiles and the level corrections. In particular, for the crucial choice of bandwidth of a fixed kernel, it avoids resorting to arbitrary heuristics (such as median or standard deviation) or to data splitting. We find on both synthetic and real-world data that KSDAgg outperforms other state-of-the-art adaptive KSD-based goodness-of-fit testing procedures.

</details>

<details>

<summary>2022-06-22 18:00:10 - Bayesian Nonparametrics for Offline Skill Discovery</summary>

- *Valentin Villecroze, Harry J. Braviner, Panteha Naderian, Chris J. Maddison, Gabriel Loaiza-Ganem*

- `2202.04675v3` - [abs](http://arxiv.org/abs/2202.04675v3) - [pdf](http://arxiv.org/pdf/2202.04675v3)

> Skills or low-level policies in reinforcement learning are temporally extended actions that can speed up learning and enable complex behaviours. Recent work in offline reinforcement learning and imitation learning has proposed several techniques for skill discovery from a set of expert trajectories. While these methods are promising, the number K of skills to discover is always a fixed hyperparameter, which requires either prior knowledge about the environment or an additional parameter search to tune it. We first propose a method for offline learning of options (a particular skill framework) exploiting advances in variational inference and continuous relaxations. We then highlight an unexplored connection between Bayesian nonparametrics and offline skill discovery, and show how to obtain a nonparametric version of our model. This version is tractable thanks to a carefully structured approximate posterior with a dynamically-changing number of options, removing the need to specify K. We also show how our nonparametric extension can be applied in other skill frameworks, and empirically demonstrate that our method can outperform state-of-the-art offline skill learning algorithms across a variety of environments. Our code is available at https://github.com/layer6ai-labs/BNPO .

</details>

<details>

<summary>2022-06-22 18:49:52 - A Bayesian Survival Model for Time-Varying Coefficients and Unobserved Heterogeneity</summary>

- *Peter Knaus, Daniel Winkler, Gerd Jomrich*

- `2206.11320v1` - [abs](http://arxiv.org/abs/2206.11320v1) - [pdf](http://arxiv.org/pdf/2206.11320v1)

> Dynamic survival models are a flexible tool for overcoming limitations of popular methods in the field of survival analysis. While this flexibility allows them to uncover more intricate relationships between covariates and the time-to-event, it also has them running the risk of overfitting. This paper proposes a solution to this issue based on state of the art global-local shrinkage priors and shows that they are able to effectively regularize the amount of time-variation observed in the parameters. Further, a novel approach to accounting for unobserved heterogeneity in the data through a dynamic factor model is introduced. An efficient MCMC sampler is developed and made available in an accompanying R package. Finally, the method is applied to a current data set of survival times of patients with adenocarcinoma of the gastroesophageal junction.

</details>

<details>

<summary>2022-06-22 19:38:52 - Bayesian model calibration for block copolymer self-assembly: Likelihood-free inference and expected information gain computation via measure transport</summary>

- *Ricardo Baptista, Lianghao Cao, Joshua Chen, Omar Ghattas, Fengyi Li, Youssef M. Marzouk, J. Tinsley Oden*

- `2206.11343v1` - [abs](http://arxiv.org/abs/2206.11343v1) - [pdf](http://arxiv.org/pdf/2206.11343v1)

> We consider the Bayesian calibration of models describing the phenomenon of block copolymer (BCP) self-assembly using image data produced by microscopy or X-ray scattering techniques. To account for the random long-range disorder in BCP equilibrium structures, we introduce auxiliary variables to represent this aleatory uncertainty. These variables, however, result in an integrated likelihood for high-dimensional image data that is generally intractable to evaluate. We tackle this challenging Bayesian inference problem using a likelihood-free approach based on measure transport together with the construction of summary statistics for the image data. We also show that expected information gains (EIGs) from the observed data about the model parameters can be computed with no significant additional cost. Lastly, we present a numerical case study based on the Ohta--Kawasaki model for diblock copolymer thin film self-assembly and top-down microscopy characterization. For calibration, we introduce several domain-specific energy- and Fourier-based summary statistics, and quantify their informativeness using EIG. We demonstrate the power of the proposed approach to study the effect of data corruptions and experimental designs on the calibration results.

</details>

<details>

<summary>2022-06-22 20:01:48 - Sequential Importance Sampling for Hybrid Model Bayesian Inference to Support Bioprocess Mechanism Learning and Robust Control</summary>

- *Wei Xie, Keqi Wang, Hua Zheng, Ben Feng*

- `2205.02410v3` - [abs](http://arxiv.org/abs/2205.02410v3) - [pdf](http://arxiv.org/pdf/2205.02410v3)

> Driven by the critical needs of biomanufacturing 4.0, we introduce a probabilistic knowledge graph hybrid model characterizing the risk- and science-based understanding of bioprocess mechanisms. It can faithfully capture the important properties, including nonlinear reactions, partially observed state, and nonstationary dynamics. Given very limited real process observations, we derive a posterior distribution quantifying model estimation uncertainty. To avoid the evaluation of intractable likelihoods, Approximate Bayesian Computation sampling with Sequential Monte Carlo (ABC-SMC) is utilized to approximate the posterior distribution. Under high stochastic and model uncertainties, it is computationally expensive to match output trajectories. Therefore, we create a linear Gaussian dynamic Bayesian network (LG-DBN) auxiliary likelihood-based ABC-SMC approach. Through matching the summary statistics driven through LG-DBN likelihood that can capture critical interactions and variations, the proposed algorithm can accelerate hybrid model inference, support process monitoring, and facilitate mechanism learning and robust control.

</details>

<details>

<summary>2022-06-22 21:05:45 - A joint latent class model of longitudinal and survival data with a time-varying membership probability</summary>

- *Ruoyu Miao, Christiana Charalambous*

- `2206.11384v1` - [abs](http://arxiv.org/abs/2206.11384v1) - [pdf](http://arxiv.org/pdf/2206.11384v1)

> Joint latent class modelling has been developed considerably in the past two decades. In some instances, the models are linked by the latent class k (i.e. the number of subgroups), in others they are joined by shared random effects or a heterogeneous random covariance matrix. We propose an extension to the joint latent class model (JLCM) in which probabilities of subjects being in latent class k can be set to vary with time. This can be a more flexible way to analyse the effect of treatments to patients. For example, a patient may be in period I at the first visit time and may move to period II at the second visit time, implying the treatment the patient had before might be noneffective at the following visit time. For a dataset with these particular features, the joint latent class model which allows jumps among different subgroups can potentially provide more information as well as more accurate estimation and prediction results compared to the basic JLCM. A Bayesian approach is used to do the estimation and a DIC criterion is used to decide the optimal number of classes. Simulation results indicate that the proposed model produces accurate results and the time-varying JLCM outperforms the basic JLCM. We also illustrate the performance of our proposed JLCM on the aids data (Goldman et al., 1996).

</details>

<details>

<summary>2022-06-22 22:34:36 - Automatic Zig-Zag sampling in practice</summary>

- *Alice Corbella, Simon E F Spencer, Gareth O Roberts*

- `2206.11410v1` - [abs](http://arxiv.org/abs/2206.11410v1) - [pdf](http://arxiv.org/pdf/2206.11410v1)

> Novel Monte Carlo methods to generate samples from a target distribution, such as a posterior from a Bayesian analysis, have rapidly expanded in the past decade. Algorithms based on Piecewise Deterministic Markov Processes (PDMPs), non-reversible continuous-time processes, are developing into their own research branch, thanks their important properties (e.g., correct invariant distribution, ergodicity, and super-efficiency). Nevertheless, practice has not caught up with the theory in this field, and the use of PDMPs to solve applied problems is not widespread. This might be due, firstly, to several implementational challenges that PDMP-based samplers present with and, secondly, to the lack of papers that showcase the methods and implementations in applied settings. Here, we address both these issues using one of the most promising PDMPs, the Zig-Zag sampler, as an archetypal example. After an explanation of the key elements of the Zig-Zag sampler, its implementation challenges are exposed and addressed. Specifically, the formulation of an algorithm that draws samples from a target distribution of interest is provided. Notably, the only requirement of the algorithm is a closed-form function to evaluate the target density of interest, and, unlike previous implementations, no further information on the target is needed. The performance of the algorithm is evaluated against another gradient-based sampler, and it is proven to be competitive, in simulation and real-data settings. Lastly, we demonstrate that the super-efficiency property, i.e. the ability to draw one independent sample at a lesser cost than evaluating the likelihood of all the data, can be obtained in practice.

</details>

<details>

<summary>2022-06-22 23:17:13 - Flexible Modeling of Multivariate Spatial Extremes</summary>

- *Yan Gong, Raphal Huser*

- `2206.11414v1` - [abs](http://arxiv.org/abs/2206.11414v1) - [pdf](http://arxiv.org/pdf/2206.11414v1)

> We develop a novel multi-factor copula model for multivariate spatial extremes, which is designed to capture the different combinations of marginal and cross-extremal dependence structures within and across different spatial random fields. Our proposed model, which can be seen as a multi-factor copula model, can capture all possible distinct combinations of extremal dependence structures within each individual spatial process while allowing flexible cross-process extremal dependence structures for both upper and lower tails. We show how to perform Bayesian inference for the proposed model using a Markov chain Monte Carlo algorithm based on carefully designed block proposals with an adaptive step size. In our real data application, we apply our model to study the upper and lower extremal dependence structures of the daily maximum air temperature (TMAX) and daily minimum air temperature (TMIN) from the state of Alabama in the southeastern United States. The fitted multivariate spatial model is found to provide a good fit in the lower and upper joint tails, both in terms of the spatial dependence structure within each individual process, as well as in terms of the cross-process dependence structure. Our results suggest that the TMAX and TMIN processes are quite strongly spatially dependent over the state of Alabama, and moderately cross-dependent. From a practical perspective, this implies that it may be worthwhile to model them jointly when interest lies in a computing spatial risk measures that involve both quantities.

</details>

<details>

<summary>2022-06-23 09:44:12 - A Bayesian Approach to Atmospheric Circulation Regime Assignment</summary>

- *Swinda K. J. Falkena, Jana de Wiljes, Antje Weisheimer, Theodore G. Shepherd*

- `2206.11576v1` - [abs](http://arxiv.org/abs/2206.11576v1) - [pdf](http://arxiv.org/pdf/2206.11576v1)

> The standard approach when studying atmospheric circulation regimes and their dynamics is to use a hard, categorical, regime assignment. That is, each atmospheric state is assigned to the regime it is closest to in distance. However, this may not always be the most appropriate approach as the regime assignment may be affected by small deviations in the distance to the regimes due to noise. To mitigate this we develop a probabilistic regime assignment using Bayes theorem. Bayes theorem tells us that the probability of a regime given the data can be determined by combining climatological likelihood with prior information. The regime probabilities at time t can be used to inform the prior probabilities at time t+1, which then is used to sequentially update the regime probabilities. We apply this approach to both reanalysis data and a seasonal hindcast ensemble incorporating knowledge of the transition probabilities between regimes. Furthermore, making use of the signal present within the ensemble to better inform the prior probabilities allows for identifying more pronounced interannual variability. The signal within the interannual variability of wintertime North Atlantic circulation regimes is assessed using both a categorical and regression approach, with the strongest signals found during very strong El Ni\~no years.

</details>

<details>

<summary>2022-06-23 10:08:45 - Dimension free non-asymptotic bounds on the accuracy of high dimensional Laplace approximation</summary>

- *Vladimir Spokoiny*

- `2204.11038v3` - [abs](http://arxiv.org/abs/2204.11038v3) - [pdf](http://arxiv.org/pdf/2204.11038v3)

> This note attempts to revisit the classical results on Laplace approximation in a modern non-asymptotic and dimension free form. Such an extension is motivated by applications to high dimensional statistical and optimization problems. The established results provide explicit non-asymptotic bounds on the quality of a Gaussian approximation of the posterior distribution in total variation distance in terms of the so called \emph{effective dimension} \( p_G \). This value is defined as interplay between information contained in the data and in the prior distribution. In the contrary to prominent Bernstein - von Mises results, the impact of the prior is not negligible and it allows to keep the effective dimension small or moderate even if the true parameter dimension is huge or infinite. We also address the issue of using a Gaussian approximation with inexact parameters with the focus on replacing the Maximum a Posteriori (MAP) value by the posterior mean and design the algorithm of Bayesian optimization based on Laplace iterations. The results are specified to the case of nonlinear inverse problem.

</details>

<details>

<summary>2022-06-23 12:10:38 - Robust Sequential Online Prediction with Dynamic Ensemble of Multiple Models: A Concise Introduction</summary>

- *Bin Liu*

- `2112.02374v3` - [abs](http://arxiv.org/abs/2112.02374v3) - [pdf](http://arxiv.org/pdf/2112.02374v3)

> In this paper, I give a concise introduction to a generic theoretical framework termed Bayesian Dynamic Ensemble of Multiple Models (BDEMM), which is used for robust sequential online prediction with time series data. This framework has three major features: (1) it employs a model pool, rather than a single model, to capture possible statistical regularities underlying the data; (2) the model pool consists of multiple weighted candidate models, wherein the model weights are adapted online to capture possible temporal evolutions of the data; (3) the adaptation for the model weights follows Bayesian formalism. These features together define BDEMM. To make this introduction comprehensive, I describe BDEMM from five perspectives, namely the basic theories, its different forms of algorithmic implementations, its applications, its connections to related research, open resources for algorithm implementations, followed by a discussion of practical issues for applying it and some open problems that are worth further research.

</details>

<details>

<summary>2022-06-23 16:44:22 - Scalable Multiple Network Inference with the Joint Graphical Horseshoe</summary>

- *Camilla Lingjrde, Benjamin P. Fairfax, Sylvia Richardson, Hlne Ruffieux*

- `2206.11820v1` - [abs](http://arxiv.org/abs/2206.11820v1) - [pdf](http://arxiv.org/pdf/2206.11820v1)

> Network models are useful tools for modelling complex associations. If a Gaussian graphical model is assumed, conditional independence is determined by the non-zero entries of the inverse covariance (precision) matrix of the data. The Bayesian graphical horseshoe estimator provides a robust and flexible framework for precision matrix inference, as it introduces local, edge-specific parameters which prevent over-shrinkage of non-zero off-diagonal elements. However, for many applications such as statistical omics, the current implementation based on Gibbs sampling becomes computationally inefficient or even unfeasible in high dimensions. Moreover, the graphical horseshoe has only been formulated for a single network, whereas interest has grown in the network analysis of multiple data sets that might share common structures. We propose (i) a scalable expectation conditional maximisation (ECM) algorithm for obtaining the posterior mode of the precision matrix in the graphical horseshoe, and (ii) a novel joint graphical horseshoe estimator, which borrows information across multiple related networks to improve estimation. We show, on both simulated and real omics data, that our single-network ECM approach is more scalable than the existing graphical horseshoe Gibbs implementation, while achieving the same level of accuracy. We also show that our joint-network proposal successfully leverages shared edge-specific information between networks while still retaining differences, outperforming state-of-the-art methods at any level of network similarity.

</details>

<details>

<summary>2022-06-23 17:41:36 - The Effective Sample Size in Bayesian Information Criterion for Level-Specific Fixed and Random Effects Selection in a Two-Level Nested Model</summary>

- *Sun-Joo Cho, Hao Wu, Matthew Naveiras*

- `2206.11880v1` - [abs](http://arxiv.org/abs/2206.11880v1) - [pdf](http://arxiv.org/pdf/2206.11880v1)

> Popular statistical software provides Bayesian information criterion (BIC) for multilevel models or linear mixed models. However, it has been observed that the combination of statistical literature and software documentation has led to discrepancies in the formulas of the BIC and uncertainties of the proper use of the BIC in selecting a multilevel model with respect to level-specific fixed and random effects. These discrepancies and uncertainties result from different specifications of sample size in the BIC's penalty term for multilevel models. In this study, we derive the BIC's penalty term for level-specific fixed and random effect selection in a two-level nested design. In this new version of BIC, called BIC_E, this penalty term is decomposed into two parts if the random effect variance-covariance matrix has full rank: (a) a term with the log of average sample size per cluster whose multiplier involves the overlapping number of dimensions between the column spaces of the random and fixed effect design matrices and (b) the total number of parameters times the log of the total number of clusters. Furthermore, we study the behavior of BIC_E in the presence of redundant random effects. The use of BIC_E is illustrated with a textbook example data set and a numerical demonstration shows that the derived formulae adheres to empirical values.

</details>

<details>

<summary>2022-06-23 19:35:58 - Deep Stable neural networks: large-width asymptotics and convergence rates</summary>

- *Stefano Favaro, Sandra Fortini, Stefano Peluchetti*

- `2108.02316v2` - [abs](http://arxiv.org/abs/2108.02316v2) - [pdf](http://arxiv.org/pdf/2108.02316v2)

> In modern deep learning, there is a recent and growing literature on the interplay between large-width asymptotic properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed weights, and Gaussian stochastic processes (SPs). Such an interplay has proved to be critical in Bayesian inference under Gaussian SP priors, kernel regression for infinitely wide deep NNs trained via gradient descent, and information propagation within infinitely wide NNs. Motivated by empirical analyses that show the potential of replacing Gaussian distributions with Stable distributions for the NN's weights, in this paper we present a rigorous analysis of the large-width asymptotic behaviour of (fully connected) feed-forward deep Stable NNs, i.e. deep NNs with Stable-distributed weights. We show that as the width goes to infinity jointly over the NN's layers, i.e. the ``joint growth" setting, a rescaled deep Stable NN converges weakly to a Stable SP whose distribution is characterized recursively through the NN's layers. Because of the non-triangular structure of the NN, this is a non-standard asymptotic problem, to which we propose an inductive approach of independent interest. Then, we establish sup-norm convergence rates of the rescaled deep Stable NN to the Stable SP, under the ``joint growth" and a ``sequential growth" of the width over the NN's layers. Such a result provides the difference between the ``joint growth" and the ``sequential growth" settings, showing that the former leads to a slower rate than the latter, depending on the depth of the layer and the number of inputs of the NN. Our work extends some recent results on infinitely wide limits for deep Gaussian NNs to the more general deep Stable NNs, providing the first result on convergence rates in the ``joint growth" setting.

</details>

<details>

<summary>2022-06-23 20:11:02 - Accelerated Information Gradient flow</summary>

- *Yifei Wang, Wuchen Li*

- `1909.02102v3` - [abs](http://arxiv.org/abs/1909.02102v3) - [pdf](http://arxiv.org/pdf/1909.02102v3)

> We present a framework for Nesterov's accelerated gradient flows in probability space to design efficient mean-field Markov chain Monte Carlo (MCMC) algorithms for Bayesian inverse problems. Here four examples of information metrics are considered, including Fisher-Rao metric, Wasserstein-2 metric, Kalman-Wasserstein metric and Stein metric. For both Fisher-Rao and Wasserstein-2 metrics, we prove convergence properties of accelerated gradient flows. In implementations, we propose a sampling-efficient discrete-time algorithm for Wasserstein-2, Kalman-Wasserstein and Stein accelerated gradient flows with a restart technique. We also formulate a kernel bandwidth selection method, which learns the gradient of logarithm of density from Brownian-motion samples. Numerical experiments, including Bayesian logistic regression and Bayesian neural network, show the strength of the proposed methods compared with state-of-the-art algorithms.

</details>

<details>

<summary>2022-06-24 05:34:13 - Functional Partial Membership Models</summary>

- *Nicholas Marco, Damala entrk, Shafali Jeste, Charlotte DiStefano, Abigail Dickinson, Donatello Telesca*

- `2206.12084v1` - [abs](http://arxiv.org/abs/2206.12084v1) - [pdf](http://arxiv.org/pdf/2206.12084v1)

> Partial membership models, or mixed membership models, are a flexible unsupervised learning method that allows each observation to belong to multiple clusters. In this paper, we propose a Bayesian partial membership model for functional data. By using the multivariate Karhunen-Lo\`eve theorem, we are able to derive a scalable representation of Gaussian processes that maintains data-driven learning of the covariance structure. Within this framework, we establish conditional posterior consistency given a known feature allocation matrix. Compared to previous work on partial membership models, our proposal allows for increased modeling flexibility, with the benefit of a directly interpretable mean and covariance structure. Our work is motivated by studies in functional brain imaging through electroencephalography (EEG) of children with autism spectrum disorder (ASD). In this context, our work formalizes the clinical notion of "spectrum" in terms of feature membership probabilities.

</details>

<details>

<summary>2022-06-24 05:44:09 - Variational inference for cutting feedback in misspecified models</summary>

- *Xuejun Yu, David J. Nott, Michael Stanley Smith*

- `2108.11066v2` - [abs](http://arxiv.org/abs/2108.11066v2) - [pdf](http://arxiv.org/pdf/2108.11066v2)

> Bayesian analyses combine information represented by different terms in a joint Bayesian model. When one or more of the terms is misspecified, it can be helpful to restrict the use of information from suspect model components to modify posterior inference. This is called "cutting feedback", and both the specification and computation of the posterior for such "cut models" is challenging. In this paper, we define cut posterior distributions as solutions to constrained optimization problems, and propose optimization-based variational methods for their computation. These methods are faster than existing Markov chain Monte Carlo (MCMC) approaches for computing cut posterior distributions by an order of magnitude. It is also shown that variational methods allow for the evaluation of computationally intensive conflict checks that can be used to decide whether or not feedback should be cut. Our methods are illustrated in a number of simulated and real examples, including an application where recent methodological advances that combine variational inference and MCMC within the variational optimization are used.

</details>

<details>

<summary>2022-06-24 13:30:48 - Bayesian Circular Lattice Filters for Computationally Efficient Estimation of Multivariate Time-Varying Autoregressive Models</summary>

- *Yuelei Sui, Scott H. Holan, Wen-Hsi Yang*

- `2206.12280v1` - [abs](http://arxiv.org/abs/2206.12280v1) - [pdf](http://arxiv.org/pdf/2206.12280v1)

> Nonstationary time series data exist in various scientific disciplines, including environmental science, biology, signal processing, econometrics, among others. Many Bayesian models have been developed to handle nonstationary time series. The time-varying vector autoregressive (TV-VAR) model is a well-established model for multivariate nonstationary time series. Nevertheless, in most cases, the large number of parameters presented by the model results in a high computational burden, ultimately limiting its usage. This paper proposes a computationally efficient multivariate Bayesian Circular Lattice Filter to extend the usage of the TV-VAR model to a broader class of high-dimensional problems. Our fully Bayesian framework allows both the autoregressive (AR) coefficients and innovation covariance to vary over time. Our estimation method is based on the Bayesian lattice filter (BLF), which is extremely computationally efficient and stable in univariate cases. To illustrate the effectiveness of our approach, we conduct a comprehensive comparison with other competing methods through simulation studies and find that, in most cases, our approach performs superior in terms of average squared error between the estimated and true time-varying spectral density. Finally, we demonstrate our methodology through applications to quarterly Gross Domestic Product (GDP) data and Northern California wind data.

</details>

<details>

<summary>2022-06-24 14:16:10 - MCMC for GLMMs</summary>

- *Vivekananda Roy*

- `2204.01866v2` - [abs](http://arxiv.org/abs/2204.01866v2) - [pdf](http://arxiv.org/pdf/2204.01866v2)

> Generalized linear mixed models (GLMMs) are often used for analyzing correlated non-Gaussian data. The likelihood function in a GLMM is available only as a high dimensional integral, and thus closed-form inference and prediction are not possible for GLMMs. Since the likelihood is not available in a closed-form, the associated posterior densities in Bayesian GLMMs are also intractable. Generally, Markov chain Monte Carlo (MCMC) algorithms are used for conditional simulation in GLMMs and exploring these posterior densities. In this article, we present different state of the art MCMC algorithms for fitting GLMMs. These MCMC algorithms include efficient data augmentation strategies, as well as diffusions based and Hamiltonian dynamics based methods. The Langevin and Hamiltonian Monte Carlo methods presented here are applicable to any GLMMs, and are illustrated using three most popular GLMMs, namely, the logistic and probit GLMMs for binomial data and the Poisson-log GLMM for count data. We also present efficient data augmentation algorithms for probit and logistic GLMMs. Some of these algorithms are compared using a numerical example.

</details>

<details>

<summary>2022-06-24 15:08:47 - Unified field theoretical approach to deep and recurrent neuronal networks</summary>

- *Kai Segadlo, Bastian Epping, Alexander van Meegen, David Dahmen, Michael Krmer, Moritz Helias*

- `2112.05589v3` - [abs](http://arxiv.org/abs/2112.05589v3) - [pdf](http://arxiv.org/pdf/2112.05589v3)

> Understanding capabilities and limitations of different network architectures is of fundamental importance to machine learning. Bayesian inference on Gaussian processes has proven to be a viable approach for studying recurrent and deep networks in the limit of infinite layer width, $n\to\infty$. Here we present a unified and systematic derivation of the mean-field theory for both architectures that starts from first principles by employing established methods from statistical physics of disordered systems. The theory elucidates that while the mean-field equations are different with regard to their temporal structure, they yet yield identical Gaussian kernels when readouts are taken at a single time point or layer, respectively. Bayesian inference applied to classification then predicts identical performance and capabilities for the two architectures. Numerically, we find that convergence towards the mean-field theory is typically slower for recurrent networks than for deep networks and the convergence speed depends non-trivially on the parameters of the weight prior as well as the depth or number of time steps, respectively. Our method exposes that Gaussian processes are but the lowest order of a systematic expansion in $1/n$ and we compute next-to-leading-order corrections which turn out to be architecture-specific. The formalism thus paves the way to investigate the fundamental differences between recurrent and deep architectures at finite widths $n$.

</details>

<details>

<summary>2022-06-24 16:26:08 - Predicting Value at Risk for Cryptocurrencies With Generalized Random Forests</summary>

- *Konstantin Grgen, Jonas Meirer, Melanie Schienle*

- `2203.08224v2` - [abs](http://arxiv.org/abs/2203.08224v2) - [pdf](http://arxiv.org/pdf/2203.08224v2)

> We study the prediction of Value at Risk (VaR) for cryptocurrencies. In contrast to classic assets, returns of cryptocurrencies are often highly volatile and characterized by large fluctuations around single events. Analyzing a comprehensive set of 105 major cryptocurrencies, we show that Generalized Random Forests (GRF) (Athey et al., 2019) adapted to quantile prediction have superior performance over other established methods such as quantile regression, GARCH-type and CAViaR models. This advantage is especially pronounced in unstable times and for classes of highly-volatile cryptocurrencies. Furthermore, we identify important predictors during such times and show their influence on forecasting over time. Moreover, a comprehensive simulation study also indicates that the GRF methodology is at least on par with existing methods in VaR predictions for standard types of financial returns and clearly superior in the cryptocurrency setup.

</details>

<details>

<summary>2022-06-24 16:49:43 - Beyond Gaussian processes: Flexible Bayesian modeling and inference for geostatistical processes</summary>

- *F. B. Gonalves, M. O. Prates, G. A. S. Aguilar*

- `2203.06437v3` - [abs](http://arxiv.org/abs/2203.06437v3) - [pdf](http://arxiv.org/pdf/2203.06437v3)

> This paper proposes a novel family of geostatistical models to account for features that cannot be properly accommodated by traditional Gaussian processes. The family is specified hierarchically and combines the infinite dimensional dynamics of Gaussian processes to that of any multivariate continuous distribution. This combination is stochastically defined through a latent Poisson process and the new family is called the Poisson-Gaussian Mixture Process - POGAMP. Whilst the attempt of defining a geostatistical process by assigning some arbitrary continuous distributions to be the finite-dimension distributions usually leads to non-valid processes, the finite-dimensional distributions of the POGAMP can be arbitrarily close to any continuous distribution and still define a valid process. Formal results to establish the existence and some important properties of the POGAMP, such as absolute continuity with respect to a Gaussian process measure, are provided. Also, a MCMC algorithm is carefully devised to perform Bayesian inference when the POGAMP is discretely observed in some space domain. Simulations are performed to empirically investigate the modelling properties of the POGAMP and the efficiency of the MCMC algorithm.

</details>

<details>

<summary>2022-06-24 17:24:12 - The Bayes Estimator of a Conditional Density: Consistency</summary>

- *Agustin G. Nogales*

- `2206.12379v1` - [abs](http://arxiv.org/abs/2206.12379v1) - [pdf](http://arxiv.org/pdf/2206.12379v1)

> In a Bayesian framework we prove that the optimal estimator of a conditional density is consistent.

</details>

<details>

<summary>2022-06-24 19:52:29 - Sparse precision matrix estimation in phenotypic trait evolution models</summary>

- *Felipe G. Pinheiro, Taiane S. Prass, Marc A. Suchard, Gabriela B. Cybis*

- `2206.12483v1` - [abs](http://arxiv.org/abs/2206.12483v1) - [pdf](http://arxiv.org/pdf/2206.12483v1)

> Phylogenetic trait evolution models allow for the estimation of evolutionary correlations between a set of traits observed in a sample of related organisms. By directly modeling the evolution of the traits on a phylogenetic tree in a Bayesian framework, the model's structure allows us to control for shared evolutionary history. In these models, relevant correlations are assessed through a post-process procedure based on the high posterior density interval of marginal correlations. However, the selected correlations alone may not provide all available information regarding trait relationships. Their association structure, in contrast, is likely to express some sparsity pattern and provide straightforward information about direct associations between traits. In order to employ a model-based method to identify this association structure we explore the use of Gaussian graphical models (GGM) for covariance selection. We model the precision matrix with a G-Wishart conjugate prior, which results in sparse precision estimates. We evaluate our approach through Monte Carlo simulations and applications that examine the association structure and evolutionary correlations of phenotypic traits in Darwin's finches and genomic and phenotypic traits in prokaryotes. Our approach provides accurate graph estimates and lower errors for the precision and correlation parameter estimates, especially for conditionally independent traits, which are the target for sparsity in GGMs.

</details>

<details>

<summary>2022-06-24 20:49:48 - Local Projections vs. VARs: Lessons From Thousands of DGPs</summary>

- *Dake Li, Mikkel Plagborg-Mller, Christian K. Wolf*

- `2104.00655v2` - [abs](http://arxiv.org/abs/2104.00655v2) - [pdf](http://arxiv.org/pdf/2104.00655v2)

> We conduct a simulation study of Local Projection (LP) and Vector Autoregression (VAR) estimators of structural impulse responses across thousands of data generating processes, designed to mimic the properties of the universe of U.S. macroeconomic data. Our analysis considers various identification schemes and several variants of LP and VAR estimators. A clear bias-variance trade-off emerges: LP estimators have lower bias than VAR estimators but substantially higher variance at intermediate and long horizons. Consequently, unless researchers are overwhelmingly concerned with bias, shrinkage via Bayesian VARs or penalized LPs is attractive.

</details>

<details>

<summary>2022-06-25 12:47:52 - Optimal Experimental Design for Inverse Problems in the Presence of Observation Correlations</summary>

- *Ahmed Attia, Emil Constantinescu*

- `2007.14476v3` - [abs](http://arxiv.org/abs/2007.14476v3) - [pdf](http://arxiv.org/pdf/2007.14476v3)

> Optimal experimental design (OED) is the general formalism of sensor placement and decisions about the data collection strategy for engineered or natural experiments. This approach is prevalent in many critical fields such as battery design, numerical weather prediction, geosciences, and environmental and urban studies. State-of-the-art computational methods for experimental design, however, do not accommodate correlation structure in observational errors produced by many expensive-to-operate devices such as X-ray machines or radar and satellite retrievals. Discarding evident data correlations leads to biased results, poor data collection decisions, and waste of valuable resources. We present a general formulation of the OED formalism for model-constrained large-scale Bayesian linear inverse problems, where measurement errors are generally correlated. The proposed approach utilizes the Hadamard product of matrices to formulate the weighted likelihood and is valid for both finite- and infinite- dimensional Bayesian inverse problems. We also discuss widely used approaches for relaxation of the binary OED problem, in light of the proposed pointwise weighting approach, and present a clear interpretation of the relaxed design and its effect on the observational error covariance. Extensive numerical experiments are carried out for empirical verification of the proposed approach by using an advection-diffusion model, where the objective is to optimally place a small set of sensors, under a limited budget, to predict the concentration of a contaminant in a bounded domain.

</details>

<details>

<summary>2022-06-25 14:28:05 - Scalable Spike-and-Slab</summary>

- *Niloy Biswas, Lester Mackey, Xiao-Li Meng*

- `2204.01668v2` - [abs](http://arxiv.org/abs/2204.01668v2) - [pdf](http://arxiv.org/pdf/2204.01668v2)

> Spike-and-slab priors are commonly used for Bayesian variable selection, due to their interpretability and favorable statistical properties. However, existing samplers for spike-and-slab posteriors incur prohibitive computational costs when the number of variables is large. In this article, we propose Scalable Spike-and-Slab ($S^3$), a scalable Gibbs sampling implementation for high-dimensional Bayesian regression with the continuous spike-and-slab prior of George and McCulloch (1993). For a dataset with $n$ observations and $p$ covariates, $S^3$ has order $\max\{ n^2 p_t, np \}$ computational cost at iteration $t$ where $p_t$ never exceeds the number of covariates switching spike-and-slab states between iterations $t$ and $t-1$ of the Markov chain. This improves upon the order $n^2 p$ per-iteration cost of state-of-the-art implementations as, typically, $p_t$ is substantially smaller than $p$. We apply $S^3$ on synthetic and real-world datasets, demonstrating orders of magnitude speed-ups over existing exact samplers and significant gains in inferential quality over approximate samplers with comparable cost.

</details>

<details>

<summary>2022-06-26 03:02:38 - Scalable and optimal Bayesian inference for sparse covariance matrices via screened beta-mixture prior</summary>

- *Kyoungjae Lee, Seongil Jo, Jaeyong Lee*

- `2206.12773v1` - [abs](http://arxiv.org/abs/2206.12773v1) - [pdf](http://arxiv.org/pdf/2206.12773v1)

> In this paper, we propose a scalable Bayesian method for sparse covariance matrix estimation by incorporating a continuous shrinkage prior with a screening procedure. In the first step of the procedure, the off-diagonal elements with small correlations are screened based on their sample correlations. In the second step, the posterior of the covariance with the screened elements fixed at $0$ is computed with the beta-mixture prior. The screened elements of the covariance significantly increase the efficiency of the posterior computation. The simulation studies and real data applications show that the proposed method can be used for the high-dimensional problem with the `large p, small n'. In some examples in this paper, the proposed method can be computed in a reasonable amount of time, while no other existing Bayesian methods work for the same problems. The proposed method has also sound theoretical properties. The screening procedure has the sure screening property and the selection consistency, and the posterior has the optimal minimax or nearly minimax convergence rate under the Frobeninus norm.

</details>

<details>

<summary>2022-06-26 14:47:25 - A Model for Censored Reliability Data with Two Dependent Failure Modes and Prediction of Future Failures</summary>

- *Aakash Agrawal, Debanjan Mitra, Ayon Ganguly*

- `2206.12892v1` - [abs](http://arxiv.org/abs/2206.12892v1) - [pdf](http://arxiv.org/pdf/2206.12892v1)

> Quite often, we observe reliability data with two failure modes that may influence each other, resulting in a setting of dependent failure modes. Here, we discuss modelling of censored reliability data with two dependent failure modes by using a bivariate Weibull model with distinct shape parameters which we construct as an extension of the well-known Marshall-Olkin bivariate exponential model in reliability. Likelihood inference for modelling censored reliability data with two dependent failure modes by using the proposed bivariate Weibull distribution with distinct shape parameters is discussed. Bayesian analysis for this issue is also discussed. Through a Monte Carlo simulation study, the proposed methods of inference are observed to provide satisfactory results. A problem of practical interest for reliability engineers is to predict field failures of units at a future time. Frequentist and Bayesian methods for prediction of future failures are developed in this setting of censored reliability data with two dependent failure modes. An illustrative example based on a real data on device failure with two failure modes is presented. The model and methodology presented in this article provide a complete and comprehensive treatment of modelling censored reliability data with two dependent failure modes, and address some practical prediction issues.

</details>

<details>

<summary>2022-06-26 19:38:53 - High Dimensional Bayesian Optimization with Kernel Principal Component Analysis</summary>

- *Kirill Antonov, Elena Raponi, Hao Wang, Carola Doerr*

- `2204.13753v2` - [abs](http://arxiv.org/abs/2204.13753v2) - [pdf](http://arxiv.org/pdf/2204.13753v2)

> Bayesian Optimization (BO) is a surrogate-based global optimization strategy that relies on a Gaussian Process regression (GPR) model to approximate the objective function and an acquisition function to suggest candidate points. It is well-known that BO does not scale well for high-dimensional problems because the GPR model requires substantially more data points to achieve sufficient accuracy and acquisition optimization becomes computationally expensive in high dimensions. Several recent works aim at addressing these issues, e.g., methods that implement online variable selection or conduct the search on a lower-dimensional sub-manifold of the original search space. Advancing our previous work of PCA-BO that learns a linear sub-manifold, this paper proposes a novel kernel PCA-assisted BO (KPCA-BO) algorithm, which embeds a non-linear sub-manifold in the search space and performs BO on this sub-manifold. Intuitively, constructing the GPR model on a lower-dimensional sub-manifold helps improve the modeling accuracy without requiring much more data from the objective function. Also, our approach defines the acquisition function on the lower-dimensional sub-manifold, making the acquisition optimization more manageable.   We compare the performance of KPCA-BO to a vanilla BO and to PCA-BO on the multi-modal problems of the COCO/BBOB benchmark suite. Empirical results show that KPCA-BO outperforms BO in terms of convergence speed on most test problems, and this benefit becomes more significant when the dimensionality increases. For the 60D functions, KPCA-BO achieves better results than PCA-BO for many test cases. Compared to the vanilla BO, it efficiently reduces the CPU time required to train the GPR model and to optimize the acquisition function compared to the vanilla BO.

</details>

<details>

<summary>2022-06-27 00:06:05 - Enriched standard conjugate priors and the right invariant prior for Wishart distributions</summary>

- *Hidemasa Oda, Fumiyasu Komaki*

- `2101.04919v4` - [abs](http://arxiv.org/abs/2101.04919v4) - [pdf](http://arxiv.org/pdf/2101.04919v4)

> The prediction of the variance-covariance matrix of the multivariate normal distribution is important in the multivariate analysis. We investigated Bayesian predictive distributions for Wishart distributions under the Kullback-Leibler divergence. The conditional reducibility of the family of Wishart distributions enables us to decompose the risk of a Bayesian predictive distribution. We considered a recently introduced class of prior distributions, which is called the family of enriched standard conjugate prior distributions, and compared the Bayesian predictive distributions based on these prior distributions. Furthermore, we studied the performance of the Bayesian predictive distribution based on the reference prior distribution in the family and showed that there exists a prior distribution in the family that dominates the reference prior distribution. Our study provides new insight into the multivariate analysis when there exists an ordered inferential importance for the independent variables.

</details>

<details>

<summary>2022-06-27 03:55:27 - A General Recipe for Likelihood-free Bayesian Optimization</summary>

- *Jiaming Song, Lantao Yu, Willie Neiswanger, Stefano Ermon*

- `2206.13035v1` - [abs](http://arxiv.org/abs/2206.13035v1) - [pdf](http://arxiv.org/pdf/2206.13035v1)

> The acquisition function, a critical component in Bayesian optimization (BO), can often be written as the expectation of a utility function under a surrogate model. However, to ensure that acquisition functions are tractable to optimize, restrictions must be placed on the surrogate model and utility function. To extend BO to a broader class of models and utilities, we propose likelihood-free BO (LFBO), an approach based on likelihood-free inference. LFBO directly models the acquisition function without having to separately perform inference with a probabilistic surrogate model. We show that computing the acquisition function in LFBO can be reduced to optimizing a weighted classification problem, where the weights correspond to the utility being chosen. By choosing the utility function for expected improvement (EI), LFBO outperforms various state-of-the-art black-box optimization methods on several real-world optimization problems. LFBO can also effectively leverage composite structures of the objective function, which further improves its regret by several orders of magnitude.

</details>

<details>

<summary>2022-06-27 10:34:45 - A Robust Efficient Dynamic Mechanism</summary>

- *Endre Cska*

- `2110.15219v2` - [abs](http://arxiv.org/abs/2110.15219v2) - [pdf](http://arxiv.org/pdf/2110.15219v2)

> Athey and Segal introduced an efficient budget-balanced mechanism for a dynamic stochastic model with quasilinear payoffs and private values, using the solution concept of perfect Bayesian equilibrium. We show that this implementation is not robust in multiple senses, especially for at least 3 agents. For example, we will show a generic setup where all efficient strategy profiles can be eliminated by iterative elimination of weakly dominated strategies. Furthermore, this model used strong assumptions about the information of the agents, and the mechanism was not robust to the relaxation of these assumptions. In this paper, we will show a different mechanism that implements efficiency under weaker assumptions and uses the stronger solution concept of ``efficient Nash equilibrium with guaranteed expected payoffs''.

</details>

<details>

<summary>2022-06-27 14:49:48 - Distributional Gaussian Processes Layers for Out-of-Distribution Detection</summary>

- *Sebastian G. Popescu, David J. Sharp, James H. Cole, Konstantinos Kamnitsas, Ben Glocker*

- `2206.13346v1` - [abs](http://arxiv.org/abs/2206.13346v1) - [pdf](http://arxiv.org/pdf/2206.13346v1)

> Machine learning models deployed on medical imaging tasks must be equipped with out-of-distribution detection capabilities in order to avoid erroneous predictions. It is unsure whether out-of-distribution detection models reliant on deep neural networks are suitable for detecting domain shifts in medical imaging. Gaussian Processes can reliably separate in-distribution data points from out-of-distribution data points via their mathematical construction. Hence, we propose a parameter efficient Bayesian layer for hierarchical convolutional Gaussian Processes that incorporates Gaussian Processes operating in Wasserstein-2 space to reliably propagate uncertainty. This directly replaces convolving Gaussian Processes with a distance-preserving affine operator on distributions. Our experiments on brain tissue-segmentation show that the resulting architecture approaches the performance of well-established deterministic segmentation algorithms (U-Net), which has not been achieved with previous hierarchical Gaussian Processes. Moreover, by applying the same segmentation model to out-of-distribution data (i.e., images with pathology such as brain tumors), we show that our uncertainty estimates result in out-of-distribution detection that outperforms the capabilities of previous Bayesian networks and reconstruction-based approaches that learn normative distributions. To facilitate future work our code is publicly available.

</details>

<details>

<summary>2022-06-27 15:12:26 - Approximation of bayesian Hawkes process models with Inlabru</summary>

- *Francesco Serafini, Finn Lindgren, Mark Naylor*

- `2206.13360v1` - [abs](http://arxiv.org/abs/2206.13360v1) - [pdf](http://arxiv.org/pdf/2206.13360v1)

> Hawkes process are very popular mathematical tools for modelling phenomena exhibiting a self-exciting behaviour. Typical examples are earthquakes occurrence, wild-fires, crime violence, trade exchange, and social network activity. The widespread use of Hawkes process in different fields calls for fast, reproducible, reliable, easy-to-code techniques to implement such models. We offer a technique to perform approximate Bayesian inference of Hawkes process parameters based on the use of the R-package Inlabru. Inlabru, in turn, relies on the INLA methodology to approximate the posterior of the parameters. The approximation is based on a decomposition of the Hakwes process likelihood in three parts, which are linearly approximated separately. The linear approximation is performed with respect to the mode of the posterior parameters distribution, which is determined with an iterative gradient-based method. The approximation of the posterior parameters is therefore deterministic, ensuring full reproducibility of the results. The proposed technique only required the user to provide the functions to calculate the different parts of the decomposed likelihood, while the optimization is performed through the R-package Inlabru. The limitations of this approach include the functional form of the different likelihood parts, which needs to be as linear as possible with respect to the parameters of the model. Moreover, care should be taken of the numerical stability of the provided functions.

</details>

<details>

<summary>2022-06-27 16:27:20 - Exact Convergence Analysis for Metropolis-Hastings Independence Samplers in Wasserstein Distances</summary>

- *Austin Brown, Galin L. Jones*

- `2111.10406v2` - [abs](http://arxiv.org/abs/2111.10406v2) - [pdf](http://arxiv.org/pdf/2111.10406v2)

> Under mild assumptions, we show the sharp convergence rate in total variation is also sharp in weaker Wasserstein distances for the Metropolis-Hastings independence sampler. We derive exact convergence expressions for general Wasserstein distances when initialization is at a specific point. Using optimization, we construct a novel centered independent proposal to develop exact convergence rates in Bayesian quantile regression and many generalized linear model settings. We show the exact convergence rate can be upper bounded in Bayesian binary response regression (e.g. logistic and probit) when the sample size and dimension grow together.

</details>

<details>

<summary>2022-06-27 17:45:41 - Bayesian Quickest Detection of Propagating Spatial Events</summary>

- *Topi Halme, Eyal Nitzan, Visa Koivunen*

- `2104.04335v3` - [abs](http://arxiv.org/abs/2104.04335v3) - [pdf](http://arxiv.org/pdf/2104.04335v3)

> Rapid detection of spatial events that propagate across a sensor network is of wide interest in many modern applications. In particular, in communications, radar, IoT, environmental monitoring, and biosurveillance, we may observe propagating fields or particles. In this paper, we propose Bayesian sequential single and multiple change-point detection procedures for the rapid detection of such phenomena. Using a dynamic programming framework we derive the structure of the optimal single-event quickest detection procedure, which minimizes the average detection delay (ADD) subject to a false alarm probability upper bound. The multi-sensor system configuration is arbitrary and sensors may be mobile. In the rare event regime, the optimal procedure converges to a more practical threshold test on the posterior probability of the change point. A convenient recursive computation of this posterior probability is derived by using the propagation characteristics of the spatial event. The ADD of the posterior probability threshold test is analyzed in the asymptotic regime, and specific analysis is conducted in the setting of detecting random Gaussian signals affected by path loss. Then, we show how the proposed procedure is easy to extend for detecting multiple propagating spatial events in parallel in a multiple hypothesis testing setting. A method that provides strict false discovery rate (FDR) control is proposed. In the simulation section, it is demonstrated that exploiting the spatial properties of the event decreases the ADD compared to procedures that do not utilize this information, even under model mismatch.

</details>

<details>

<summary>2022-06-27 20:06:01 - Ensemble-Based Experimental Design for Targeting Data Acquisition to Inform Climate Models</summary>

- *Oliver R. A. Dunbar, Michael F. Howland, Tapio Schneider, Andrew M. Stuart*

- `2201.06998v2` - [abs](http://arxiv.org/abs/2201.06998v2) - [pdf](http://arxiv.org/pdf/2201.06998v2)

> Data required to calibrate uncertain GCM parameterizations are often only available in limited regions or time periods, for example, observational data from field campaigns, or data generated in local high-resolution simulations. This raises the question of where and when to acquire additional data to be maximally informative about parameterizations in a GCM. Here we construct a new ensemble-based parallel algorithm to automatically target data acquisition to regions and times that maximize the uncertainty reduction, or information gain, about GCM parameters. The algorithm uses a Bayesian framework that exploits a quantified distribution of GCM parameters as a measure of uncertainty. This distribution is informed by time-averaged climate statistics restricted to local regions and times. The algorithm is embedded in the recently developed calibrate-emulate-sample (CES) framework, which performs efficient model calibration and uncertainty quantification with only $\mathcal{O}(10^2)$ model evaluations, compared with $\mathcal{O}(10^5)$ evaluations typically needed for traditional approaches to Bayesian calibration. We demonstrate the algorithm with an idealized GCM, with which we generate surrogates of local data. In this perfect-model setting, we calibrate parameters and quantify uncertainties in a quasi-equilibrium convection scheme in the GCM. We consider targeted data that are (i) localized in space for statistically stationary simulations, and (ii) localized in space and time for seasonally varying simulations. In these proof-of-concept applications, the calculated information gain reflects the reduction in parametric uncertainty obtained from Bayesian inference when harnessing a targeted sample of data. The largest information gain typically, but not always, results from regions near the intertropical convergence zone (ITCZ).

</details>

<details>

<summary>2022-06-27 21:47:15 - The role of unobservable characteristics in friendship network formation</summary>

- *Pablo Braas-Garza, Lorenzo Ductor, Jaromr Kovrk*

- `2206.13641v1` - [abs](http://arxiv.org/abs/2206.13641v1) - [pdf](http://arxiv.org/pdf/2206.13641v1)

> Inbreeding homophily is a prevalent feature of human social networks with important individual and group-level social, economic, and health consequences. The literature has proposed an overwhelming number of dimensions along which human relationships might sort, without proposing a unified empirically-grounded framework for their categorization. We exploit rich data on a sample of University freshmen with very similar characteristic - age, race and education- and contrast the relative importance of observable vs. unobservables characteristics in their friendship formation. We employ Bayesian Model Averaging, a methodology explicitly designed to target model uncertainty and to assess the robustness of each candidate attribute while predicting friendships. We show that, while observable features such as assignment of students to sections, gender, and smoking are robust key determinants of whether two individuals befriend each other, unobservable attributes, such as personality, cognitive abilities, economic preferences, or socio-economic aspects, are largely sensible to the model specification, and are not important predictors of friendships.

</details>

<details>

<summary>2022-06-27 22:56:17 - Data Assimilation in Operator Algebras</summary>

- *David Freeman, Dimitrios Giannakis, Brian Mintz, Abbas Ourmazd, Joanna Slawinska*

- `2206.13659v1` - [abs](http://arxiv.org/abs/2206.13659v1) - [pdf](http://arxiv.org/pdf/2206.13659v1)

> We develop an algebraic framework for sequential data assimilation of partially observed dynamical systems. In this framework, Bayesian data assimilation is embedded in a non-abelian operator algebra, which provides a representation of observables by multiplication operators and probability densities by density operators (quantum states). In the algebraic approach, the forecast step of data assimilation is represented by a quantum operation induced by the Koopman operator of the dynamical system. Moreover, the analysis step is described by a quantum effect, which generalizes the Bayesian observational update rule. Projecting this formulation to finite-dimensional matrix algebras leads to new computational data assimilation schemes that are (i) automatically positivity-preserving; and (ii) amenable to consistent data-driven approximation using kernel methods for machine learning. Moreover, these methods are natural candidates for implementation on quantum computers. Applications to data assimilation of the Lorenz 96 multiscale system and the El Nino Southern Oscillation in a climate model show promising results in terms of forecast skill and uncertainty quantification.

</details>

<details>

<summary>2022-06-28 01:49:28 - Bayesian decision theory for tree-based adaptive screening tests with an application to youth delinquency</summary>

- *Chelsea Krantsevich, P. Richard Hahn, Yi Zheng, Charles Katz*

- `2106.10364v3` - [abs](http://arxiv.org/abs/2106.10364v3) - [pdf](http://arxiv.org/pdf/2106.10364v3)

> Crime prevention strategies based on early intervention depend on accurate risk assessment instruments for identifying high risk youth. It is important in this context that the instruments be convenient to administer, which means, in particular, that they should also be reasonably brief; adaptive screening tests are useful for this purpose. Adaptive tests constructed using classification and regression trees are becoming a popular alternative to traditional Item Response Theory (IRT) approaches for adaptive testing. However, tree-based adaptive tests lack a principled criterion for terminating the test. This paper develops a Bayesian decision theory framework for measuring the trade-off between brevity and accuracy, when considering tree-based adaptive screening tests of different lengths. We also present a novel method for designing tree-based adaptive tests, motivated by this framework. The framework and associated adaptive test method are demonstrated through an application to youth delinquency risk assessment in Honduras; it is shown that an adaptive test requiring a subject to answer fewer than 10 questions can identify high risk youth nearly as accurately as an unabridged survey containing 173 items.

</details>

<details>

<summary>2022-06-28 11:21:41 - The split Gibbs sampler revisited: improvements to its algorithmic structure and augmented target distribution</summary>

- *Marcelo Pereyra, Luis A. Vargas-Mieles, Konstantinos C. Zygalakis*

- `2206.13894v1` - [abs](http://arxiv.org/abs/2206.13894v1) - [pdf](http://arxiv.org/pdf/2206.13894v1)

> This paper proposes a new accelerated proximal Markov chain Monte Carlo (MCMC) methodology to perform Bayesian computation efficiently in imaging inverse problems. The proposed methodology is derived from the Langevin diffusion process and stems from tightly integrating two state-of-the-art proximal Langevin MCMC samplers, SK-ROCK and split Gibbs sampling (SGS), which employ distinctively different strategies to improve convergence speed. More precisely, we show how to integrate, at the level of the Langevin diffusion process, the proximal SK-ROCK sampler which is based on a stochastic Runge-Kutta-Chebyshev approximation of the diffusion, with the model augmentation and relaxation strategy that SGS exploits to speed up Bayesian computation at the expense of asymptotic bias. This leads to a new and faster proximal SK-ROCK sampler that combines the accelerated quality of the original SK-ROCK sampler with the computational benefits of augmentation and relaxation. Moreover, rather than viewing the augmented and relaxed model as an approximation of the target model, positioning relaxation in a bias-variance trade-off, we propose to regard the augmented and relaxed model as a generalisation of the target model. This then allows us to carefully calibrate the amount of relaxation in order to simultaneously improve the accuracy of the model (as measured by the model evidence) and the sampler's convergence speed. To achieve this, we derive an empirical Bayesian method to automatically estimate the optimal amount of relaxation by maximum marginal likelihood estimation. The proposed methodology is demonstrated with a range of numerical experiments related to image deblurring and inpainting, as well as with comparisons with alternative approaches from the state of the art.

</details>

<details>

<summary>2022-06-28 12:02:05 - Statistical Depth based Normalization and Outlier Detection of Gene Expression Data</summary>

- *Alicia Nieto-Reyes, Javier Cabrera*

- `2206.13928v1` - [abs](http://arxiv.org/abs/2206.13928v1) - [pdf](http://arxiv.org/pdf/2206.13928v1)

> Normalization and outlier detection belong to the preprocessing of gene expression data. We propose a natural normalization procedure based on statistical data depth which normalizes to the distribution of gene expressions of the most representative gene expression of the group. This differ from the standard method of quantile normalization, based on the coordinate-wise median array that lacks of the well-known properties of the one-dimensional median. The statistical data depth maintains those good properties. Gene expression data are known for containing outliers. Although detecting outlier genes in a given gene expression dataset has been broadly studied, these methodologies do not apply for detecting outlier samples, given the difficulties posed by the high dimensionality but low sample size structure of the data. The standard procedures used for detecting outlier samples are visual and based on dimension reduction techniques; instances are multidimensional scaling and spectral map plots. For detecting outlier genes in a given gene expression dataset, we propose an analytical procedure and based on the Tukey's concept of outlier and the notion of statistical depth, as previous methodologies lead to unassertive and wrongful outliers. We reveal the outliers of four datasets; as a necessary step for further research.

</details>

<details>

<summary>2022-06-28 12:29:13 - Dynamic Memory for Interpretable Sequential Optimisation</summary>

- *Srivas Chennu, Andrew Maher, Jamie Martin, Subash Prabanantham*

- `2206.13960v1` - [abs](http://arxiv.org/abs/2206.13960v1) - [pdf](http://arxiv.org/pdf/2206.13960v1)

> Real-world applications of reinforcement learning for recommendation and experimentation faces a practical challenge: the relative reward of different bandit arms can evolve over the lifetime of the learning agent. To deal with these non-stationary cases, the agent must forget some historical knowledge, as it may no longer be relevant to minimise regret. We present a solution to handling non-stationarity that is suitable for deployment at scale, to provide business operators with automated adaptive optimisation. Our solution aims to provide interpretable learning that can be trusted by humans, whilst responding to non-stationarity to minimise regret. To this end, we develop an adaptive Bayesian learning agent that employs a novel form of dynamic memory. It enables interpretability through statistical hypothesis testing, by targeting a set point of statistical power when comparing rewards and adjusting its memory dynamically to achieve this power. By design, the agent is agnostic to different kinds of non-stationarity. Using numerical simulations, we compare its performance against an existing proposal and show that, under multiple non-stationary scenarios, our agent correctly adapts to real changes in the true rewards. In all bandit solutions, there is an explicit trade-off between learning and achieving maximal performance. Our solution sits on a different point on this trade-off when compared to another similarly robust approach: we prioritise interpretability, which relies on more learning, at the cost of some regret. We describe the architecture of a large-scale deployment of automatic optimisation-as-a-service where our agent achieves interpretability whilst adapting to changing circumstances.

</details>

<details>

<summary>2022-06-28 14:44:09 - A Bayesian hierarchical model for improving exercise rehabilitation in mechanically ventilated ICU patients</summary>

- *Luke Hardcastle, Samuel Livingstone, Claire Black, Federico Ricciardi, Gianluca Baio*

- `2206.14047v1` - [abs](http://arxiv.org/abs/2206.14047v1) - [pdf](http://arxiv.org/pdf/2206.14047v1)

> Patients who are mechanically ventilated in the intensive care unit (ICU) participate in exercise as a component of their rehabilitation to ameliorate the long-term impact of critical illness on their physical function. The effective implementation of these programmes is hindered, however, by the lack of a scientific method for quantifying an individual patient's exercise intensity level in real time, which results in a broad one-size-fits-all approach to rehabilitation and sub-optimal patient outcomes. In this work we have developed a Bayesian hierarchical model with temporally correlated latent Gaussian processes to predict $\dot VO_2$, a physiological measure of exercise intensity, using readily available physiological data. Inference was performed using Integrated Nested Laplace Approximation. For practical use by clinicians $\dot VO_2$ was classified into exercise intensity categories. Internal validation using leave-one-patient-out cross-validation was conducted based on these classifications, and the role of probabilistic statements describing the classification uncertainty was investigated.

</details>

<details>

<summary>2022-06-28 17:08:16 - ABC for model selection and parameter estimation of drill-string bit-rock interaction models and stochastic stability</summary>

- *Daniel A Castello, Thiago G Ritto*

- `2206.14609v1` - [abs](http://arxiv.org/abs/2206.14609v1) - [pdf](http://arxiv.org/pdf/2206.14609v1)

> The bit-rock interaction considerably affects the dynamics of a drill string. One critical condition is the stick-slip oscillations, where torsional vibrations are high; the bit angular speed varies from zero to about two times (or more) the top drive nominal angular speed. In addition, uncertainties should be taken into account when calibrating (identifying) the bit-rock interaction parameters. This paper proposes a procedure to estimate the parameters of four bit-rock interaction models, one of which is new, and at the same time select the most suitable model, given the available field data. The approximate Bayesian computation (ABC) is used for this purpose. An approximate posterior probability density function is obtained for the parameters of each model, which allows uncertainty to be analyzed. Furthermore, the impact of the uncertainties of the selected models on the torsional stability map (varying the nominal top drive angular speed and the weight on bit) of the system is evaluated.

</details>

<details>

<summary>2022-06-28 17:08:53 - Rethinking Optimization with Differentiable Simulation from a Global Perspective</summary>

- *Rika Antonova, Jingyun Yang, Krishna Murthy Jatavallabhula, Jeannette Bohg*

- `2207.00167v1` - [abs](http://arxiv.org/abs/2207.00167v1) - [pdf](http://arxiv.org/pdf/2207.00167v1)

> Differentiable simulation is a promising toolkit for fast gradient-based policy optimization and system identification. However, existing approaches to differentiable simulation have largely tackled scenarios where obtaining smooth gradients has been relatively easy, such as systems with mostly smooth dynamics. In this work, we study the challenges that differentiable simulation presents when it is not feasible to expect that a single descent reaches a global optimum, which is often a problem in contact-rich scenarios. We analyze the optimization landscapes of diverse scenarios that contain both rigid bodies and deformable objects. In dynamic environments with highly deformable objects and fluids, differentiable simulators produce rugged landscapes with nonetheless useful gradients in some parts of the space. We propose a method that combines Bayesian optimization with semi-local 'leaps' to obtain a global search method that can use gradients effectively, while also maintaining robust performance in regions with noisy gradients. We show that our approach outperforms several gradient-based and gradient-free baselines on an extensive set of experiments in simulation, and also validate the method using experiments with a real robot and deformables. Videos and supplementary materials are available at https://tinyurl.com/globdiff

</details>

<details>

<summary>2022-06-28 18:08:32 - Bayesian Structure Learning with Generative Flow Networks</summary>

- *Tristan Deleu, Antnio Gis, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer, Yoshua Bengio*

- `2202.13903v2` - [abs](http://arxiv.org/abs/2202.13903v2) - [pdf](http://arxiv.org/pdf/2202.13903v2)

> In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.

</details>

<details>

<summary>2022-06-28 20:09:35 - Dynamic Co-Quantile Regression</summary>

- *Timo Dimitriadis, Yannick Hoga*

- `2206.14275v1` - [abs](http://arxiv.org/abs/2206.14275v1) - [pdf](http://arxiv.org/pdf/2206.14275v1)

> The popular systemic risk measure CoVaR (conditional Value-at-Risk) is widely used in economics and finance. Formally, it is defined as an (extreme) quantile of one variable (e.g., losses in the financial system) conditional on some other variable (e.g., losses in a bank's shares) being in distress and, hence, measures the spillover of risks. In this article, we propose a dynamic "Co-Quantile Regression", which jointly models VaR and CoVaR semiparametrically. We propose a two-step M-estimator drawing on recently proposed bivariate scoring functions for the pair (VaR, CoVaR). Among others, this allows for the estimation of joint dynamic forecasting models for (VaR, CoVaR). We prove the asymptotic normality of the proposed estimator and simulations illustrate its good finite-sample properties. We apply our co-quantile regression to correct the statistical inference in the existing literature on CoVaR, and to generate CoVaR forecasts for real financial data, which are shown to be superior to existing methods.

</details>

<details>

<summary>2022-06-28 22:06:09 - Model Assessment for a Generalised Bayesian Structural Equation Model</summary>

- *Konstantinos Vamvourellis, Konstantinos Kalogeropoulos, Irini Moustaki*

- `2104.01603v3` - [abs](http://arxiv.org/abs/2104.01603v3) - [pdf](http://arxiv.org/pdf/2104.01603v3)

> The paper proposes a novel model assessment paradigm aiming to address shortcoming of posterior predictive $p-$values, which provide the default metric of fit for Bayesian structural equation modelling (BSEM). The model framework of the paper focuses on the approximate zero approach, according to which parameters that would before set to zero (e.g. factor loadings) are now formulated to be approximate zero via informative priors (Muthen and Asparouhov, 2012). The introduced model assessment procedure monitors the out-of-sample predictive performance of the fitted model, and together with a list of guidelines we provide, one can investigate whether the hypothesised model is supported by the data. We incorporate scoring rules and cross-validation to supplement existing model assessment metrics for Bayesian SEM. The proposed tools can be applied to models for both categorical and continuous data. The modelling of categorical and non-normally distributed continuous data is facilitated with the introduction of an item-individual random effect that can also be used for outlier detection. We study the performance of the proposed methodology via simulations. The factor model for continuous and binary data is fitted to data on the `Big-5' personality scale and the Fagerstrom test for nicotine dependence respectively.

</details>

<details>

<summary>2022-06-28 22:07:57 - Bayesian Multi-task Variable Selection with an Application to Differential DAG Analysis</summary>

- *Guanxun Li, Quan Zhou*

- `2206.14303v1` - [abs](http://arxiv.org/abs/2206.14303v1) - [pdf](http://arxiv.org/pdf/2206.14303v1)

> In this paper, we study the Bayesian multi-task variable selection problem, where the goal is to select activated variables for multiple related data sets simultaneously. Our proposed method generalizes the spike-and-slab prior to multiple data sets, and we prove its posterior consistency in high-dimensional regimes. To calculate the posterior distribution, we propose a novel variational Bayes algorithm based on the recently developed "sum of single effects" model of Wang et al. (2020). Finally, motivated by differential gene network analysis in biology, we extend our method to joint learning of multiple directed acyclic graphical models. Both simulation studies and real gene expression data analysis are conducted to show the effectiveness of the proposed method.

</details>

<details>

<summary>2022-06-29 11:41:32 - Adjoint-aided inference of Gaussian process driven differential equations</summary>

- *Paterne Gahungu, Christopher W Lanyon, Mauricio A Alvarez, Engineer Bainomugisha, Michael Smith, Richard D. Wilkinson*

- `2202.04589v2` - [abs](http://arxiv.org/abs/2202.04589v2) - [pdf](http://arxiv.org/pdf/2202.04589v2)

> Linear systems occur throughout engineering and the sciences, most notably as differential equations. In many cases the forcing function for the system is unknown, and interest lies in using noisy observations of the system to infer the forcing, as well as other unknown parameters. In differential equations, the forcing function is an unknown function of the independent variables (typically time and space), and can be modelled as a Gaussian process (GP). In this paper we show how the adjoint of a linear system can be used to efficiently infer forcing functions modelled as GPs, using a truncated basis expansion of the GP kernel. We show how exact conjugate Bayesian inference for the truncated GP can be achieved, in many cases with substantially lower computation than would be required using MCMC methods. We demonstrate the approach on systems of both ordinary and partial differential equations, and show that the basis expansion approach approximates well the true forcing with a modest number of basis vectors. Finally, we show how to infer point estimates for the non-linear model parameters, such as the kernel length-scales, using Bayesian optimisation.

</details>

<details>

<summary>2022-06-29 13:06:42 - Auditing Ranked Voting Elections with Dirichlet-Tree Models: First Steps</summary>

- *Floyd Everest, Michelle Blom, Philip B. Stark, Peter J. Stuckey, Vanessa Teague, Damjan Vukcevic*

- `2206.14605v1` - [abs](http://arxiv.org/abs/2206.14605v1) - [pdf](http://arxiv.org/pdf/2206.14605v1)

> Ranked voting systems, such as instant-runoff voting (IRV) and single transferable vote (STV), are used in many places around the world. They are more complex than plurality and scoring rules, presenting a challenge for auditing their outcomes: there is no known risk-limiting audit (RLA) method for STV other than a full hand count.   We present a new approach to auditing ranked systems that uses a statistical model, a Dirichlet-tree, that can cope with high-dimensional parameters in a computationally efficient manner. We demonstrate this approach with a ballot-polling Bayesian audit for IRV elections. Although the technique is not known to be risk-limiting, we suggest some strategies that might allow it to be calibrated to limit risk.

</details>

<details>

<summary>2022-06-29 17:14:09 - On quantiles, continuity and robustness</summary>

- *Riccardo Passeggeri, Nancy Reid*

- `2206.06998v2` - [abs](http://arxiv.org/abs/2206.06998v2) - [pdf](http://arxiv.org/pdf/2206.06998v2)

> We consider the geometric quantile and various definitions of the component-wise quantile in infinite dimensions and show their existence, uniqueness and continuity. Building on these results, we introduce and study the properties of the quantile-of-estimators (QoE) estimator, a robustification of a large class of estimators. For example, given an estimator that is asymptotically normal, the QoE estimator is asymptotically normal even in the presence of contaminated data.

</details>

<details>

<summary>2022-06-30 07:24:08 - Prediction of Dilatory Behavior in eLearning: A Comparison of Multiple Machine Learning Models</summary>

- *Christof Imhof, Ioan-Sorin Comsa, Martin Hlosta, Behnam Parsaeifard, Ivan Moser, Per Bergamin*

- `2206.15079v1` - [abs](http://arxiv.org/abs/2206.15079v1) - [pdf](http://arxiv.org/pdf/2206.15079v1)

> Procrastination, the irrational delay of tasks, is a common occurrence in online learning. Potential negative consequences include higher risk of drop-outs, increased stress, and reduced mood. Due to the rise of learning management systems and learning analytics, indicators of such behavior can be detected, enabling predictions of future procrastination and other dilatory behavior. However, research focusing on such predictions is scarce. Moreover, studies involving different types of predictors and comparisons between the predictive performance of various methods are virtually non-existent. In this study, we aim to fill these research gaps by analyzing the performance of multiple machine learning algorithms when predicting the delayed or timely submission of online assignments in a higher education setting with two categories of predictors: subjective, questionnaire-based variables and objective, log-data based indicators extracted from a learning management system. The results show that models with objective predictors consistently outperform models with subjective predictors, and a combination of both variable types perform slightly better. For each of these three options, a different approach prevailed (Gradient Boosting Machines for the subjective, Bayesian multilevel models for the objective, and Random Forest for the combined predictors). We conclude that careful attention should be paid to the selection of predictors and algorithms before implementing such models in learning management systems.

</details>

<details>

<summary>2022-06-30 11:52:57 - Uncovering Patterns for Adverse Pregnancy Outcomes with Spatial Analysis: Evidence from Philadelphia</summary>

- *Cecilia Balocchi, Ray Bai, Jessica Liu, Silvia P. Caneln, Edward I. George, Yong Chen, Mary R. Boland*

- `2105.04981v2` - [abs](http://arxiv.org/abs/2105.04981v2) - [pdf](http://arxiv.org/pdf/2105.04981v2)

> In this case study, we analyze the risk of stillbirth and preterm birth in Philadelphia from 2010 to 2017. We exploit a rich electronic health records dataset (45,919 deliveries at hospitals within Penn Medicine), and augmented with neighborhood data from 363 census tracts of Philadelphia. We conduct a two-stage statistical analysis. In the first stage, we introduce a Bayesian spatial logistic regression model to study the patient-specific risk of stillbirth and preterm birth. Our model accounts for heterogeneity and spatial autocorrelation between neighborhoods. We find both patient-level characteristics (e.g. self-identified racial/ethnic group) and neighborhood-level characteristics (e.g. violent crime) are highly associated with patient-specific risk of both outcomes. In the second stage, we aggregate the estimates from our spatial model to quantify neighborhood risks of stillbirth and preterm birth. We find that neighborhoods in West Philadelphia and North Philadelphia are at highest risk of these outcomes. Specifically, neighborhoods with higher rates of women living in poverty or on public assistance have higher risk, while neighborhoods with higher rates of women who are college-educated or in the labor force have lower risk. Our Bayesian approach provides meaningful uncertainty measures for these neighborhood risk probabilities and would be useful for public health interventions.

</details>

<details>

<summary>2022-06-30 14:50:55 - On Bayesian Dirichlet Scores for Staged Trees and Chain Event Graphs</summary>

- *Conor Hughes, Peter Strong, Aditi Shenvi*

- `2206.15322v1` - [abs](http://arxiv.org/abs/2206.15322v1) - [pdf](http://arxiv.org/pdf/2206.15322v1)

> Chain event graphs (CEGs) are a recent family of probabilistic graphical models that have emerged as a suitable alternative to Bayesian networks (BNs) for asymmetric processes. These asymmetries include context-specific independencies and structural asymmetries (i.e. structural zeros and structural missing values). Model selection in CEGs is done through an intermediate model called staged trees, and similar to BNs, this can be done through a score-based approach. Moreover, a CEG is uniquely defined by its staged tree. In BNs, the Bayesian Dirichlet equivalent uniform (BDeu) score - obtained through a specific hyperparameter setting in the Bayesian Dirichlet score function - is popular for score-based model selection for its desirable theoretical properties such as ease of hyperparameter setting, preservation of effective sample size, and score equivalence. It has been shown that, under standard assumptions, the BD score function can analogously be defined for staged trees and thereby, for CEGs. However, unlike in BNs, there has been little research into the effects of hyperparameter setting in the BD score function on both these models. In this paper, we derive a BDeu score for staged trees and CEGs. Further, we explore the relationship between the BD sparse (BDs) score, proposed for BNs that contain unobserved configurations of its variables within a dataset, and the BDeu for staged trees and CEGs. Through this relationship, we demonstrate the favourable properties of CEGs in modelling processes with sparsity or asymmetry.

</details>

<details>

<summary>2022-06-30 16:34:42 - A Latent Restoring Force Approach to Nonlinear System Identification</summary>

- *Timothy J. Rogers, Tobias Friis*

- `2109.10681v2` - [abs](http://arxiv.org/abs/2109.10681v2) - [pdf](http://arxiv.org/pdf/2109.10681v2)

> Identification of nonlinear dynamic systems remains a significant challenge across engineering. This work suggests an approach based on Bayesian filtering to extract and identify the contribution of an unknown nonlinear term in the system which can be seen as an alternative viewpoint on restoring force surface type approaches. To achieve this identification, the contribution which is the nonlinear restoring force is modelled, initially, as a Gaussian process in time. That Gaussian process is converted into a state-space model and combined with the linear dynamic component of the system. Then, by inference of the filtering and smoothing distributions, the internal states of the system and the nonlinear restoring force can be extracted. In possession of these states a nonlinear model can be constructed. The approach is demonstrated to be effective in both a simulated case study and on an experimental benchmark dataset.

</details>

<details>

<summary>2022-06-30 17:53:59 - Bayesian Causal Inference: A Critical Review</summary>

- *Fan Li, Peng Ding, Fabrizia Mealli*

- `2206.15460v1` - [abs](http://arxiv.org/abs/2206.15460v1) - [pdf](http://arxiv.org/pdf/2206.15460v1)

> This paper provides a critical review of the Bayesian perspective of causal inference based on the potential outcomes framework. We review the causal estimands, identification assumptions, and general structure of Bayesian inference of causal effects. We highlight issues that are unique to Bayesian causal inference, including the role of the propensity score, definition of identifiability, and choice of priors in both low and high dimensional regimes. We point out the central role of covariate overlap and more generally the design stage in Bayesian causal inference. We extend the discussion to two complex assignment mechanisms: instrumental variable and time-varying treatments. Throughout, we illustrate the key concepts via examples.

</details>

<details>

<summary>2022-06-30 20:24:20 - Robust subgroup discovery</summary>

- *Hugo Manuel Proena, Peter Grnwald, Thomas Bck, Matthijs van Leeuwen*

- `2103.13686v4` - [abs](http://arxiv.org/abs/2103.13686v4) - [pdf](http://arxiv.org/pdf/2103.13686v4)

> We introduce the problem of robust subgroup discovery, i.e., finding a set of interpretable descriptions of subsets that 1) stand out with respect to one or more target attributes, 2) are statistically robust, and 3) non-redundant. Many attempts have been made to mine either locally robust subgroups or to tackle the pattern explosion, but we are the first to address both challenges at the same time from a global modelling perspective. First, we formulate the broad model class of subgroup lists, i.e., ordered sets of subgroups, for univariate and multivariate targets that can consist of nominal or numeric variables, including traditional top-1 subgroup discovery in its definition. This novel model class allows us to formalise the problem of optimal robust subgroup discovery using the Minimum Description Length (MDL) principle, where we resort to optimal Normalised Maximum Likelihood and Bayesian encodings for nominal and numeric targets, respectively. Second, finding optimal subgroup lists is NP-hard. Therefore, we propose SSD++, a greedy heuristic that finds good subgroup lists and guarantees that the most significant subgroup found according to the MDL criterion is added in each iteration. In fact, the greedy gain is shown to be equivalent to a Bayesian one-sample proportion, multinomial, or t-test between the subgroup and dataset marginal target distributions plus a multiple hypothesis testing penalty. Furthermore, we empirically show on 54 datasets that SSD++ outperforms previous subgroup discovery methods in terms of quality, generalisation on unseen data, and subgroup list size.

</details>

<details>

<summary>2022-06-30 20:50:39 - A Bayesian 'sandwich' for variance estimation and hypothesis testing</summary>

- *Kendrick Qijun Li, Kenneth Martin Rice*

- `2207.00100v1` - [abs](http://arxiv.org/abs/2207.00100v1) - [pdf](http://arxiv.org/pdf/2207.00100v1)

> Many frequentist methods have large-sample Bayesian analogs, but widely-used "sandwich" or "robust" covariance estimates are an exception. We propose such an analog, as the Bayes rule under a form of balanced loss function, that combines elements of standard parametric inference with fidelity of the data to the model. Our development is general, for essentially any regression setting with independent outcomes. Besides being the large-sample equivalent of its frequentist counterpart, we show by simulation that the Bayesian robust standard error can faithfully quantify the variability of parameter estimates even under model misspecification -- thus retaining the major attraction of the original frequentist version. We demonstrate some advantages of our Bayesian analog's standard error estimate when studying the association between age and systolic blood pressure in NHANES.

</details>

<details>

<summary>2022-06-30 23:41:47 - Optimizing Training Trajectories in Variational Autoencoders via Latent Bayesian Optimization Approach</summary>

- *Arpan Biswas, Rama Vasudevan, Maxim Ziatdinov, Sergei V. Kalinin*

- `2207.00128v1` - [abs](http://arxiv.org/abs/2207.00128v1) - [pdf](http://arxiv.org/pdf/2207.00128v1)

> Unsupervised and semi-supervised ML methods such as variational autoencoders (VAE) have become widely adopted across multiple areas of physics, chemistry, and materials sciences due to their capability in disentangling representations and ability to find latent manifolds for classification and regression of complex experimental data. Like other ML problems, VAEs require hyperparameter tuning, e.g., balancing the Kullback Leibler (KL) and reconstruction terms. However, the training process and resulting manifold topology and connectivity depend not only on hyperparameters, but also their evolution during training. Because of the inefficiency of exhaustive search in a high-dimensional hyperparameter space for the expensive to train models, here we explored a latent Bayesian optimization (zBO) approach for the hyperparameter trajectory optimization for the unsupervised and semi-supervised ML and demonstrate for joint-VAE with rotational invariances. We demonstrate an application of this method for finding joint discrete and continuous rotationally invariant representations for MNIST and experimental data of a plasmonic nanoparticles material system. The performance of the proposed approach has been discussed extensively, where it allows for any high dimensional hyperparameter tuning or trajectory optimization of other ML models.

</details>


## 2022-07

<details>

<summary>2022-07-01 03:02:30 - Guided sequential ABC schemes for intractable Bayesian models</summary>

- *Umberto Picchini, Massimiliano Tamborrino*

- `2206.12235v2` - [abs](http://arxiv.org/abs/2206.12235v2) - [pdf](http://arxiv.org/pdf/2206.12235v2)

> Sequential algorithms such as sequential importance sampling (SIS) and sequential Monte Carlo (SMC) have proven fundamental in Bayesian inference for models not admitting a readily available likelihood function. For approximate Bayesian computation (ABC), sequential Monte Carlo ABC is the state-of-art sampler. However, since the ABC paradigm is intrinsically wasteful, sequential ABC schemes can benefit from well-targeted proposal samplers that efficiently avoid improbable parameter regions. We contribute to the ABC modeller's toolbox with novel proposal samplers that are conditional to summary statistics of the data. In a sense, the proposed parameters are "guided" to rapidly reach regions of the posterior surface that are compatible with the observed data. This speeds up the convergence of these sequential samplers, thus reducing the computational effort, while preserving the accuracy in the inference. We provide a variety of guided samplers for both SIS-ABC and SMC-ABC easing inference for challenging case-studies, including hierarchical models with high-dimensional summary statistics (21 parameters to infer using 180 summaries) and a simulation study of cell movements (using more than 400 summaries).

</details>

<details>

<summary>2022-07-01 03:55:26 - Adaptation of the Tuning Parameter in General Bayesian Inference with Robust Divergence</summary>

- *Shouto Yonekura, Shonosuke Sugasawa*

- `2106.06902v4` - [abs](http://arxiv.org/abs/2106.06902v4) - [pdf](http://arxiv.org/pdf/2106.06902v4)

> We introduce a methodology for robust Bayesian estimation with robust divergence (e.g., density power divergence or {\gamma}-divergence), indexed by a single tuning parameter. It is well known that the posterior density induced by robust divergence gives highly robust estimators against outliers if the tuning parameter is appropriately and carefully chosen. In a Bayesian framework, one way to find the optimal tuning parameter would be using evidence (marginal likelihood). However, we numerically illustrate that evidence induced by the density power divergence does not work to select the optimal tuning parameter since robust divergence is not regarded as a statistical model. To overcome the problems, we treat the exponential of robust divergence as an unnormalized statistical model, and we estimate the tuning parameter via minimizing the Hyvarinen score. We also provide adaptive computational methods based on sequential Monte Carlo (SMC) samplers, which enables us to obtain the optimal tuning parameter and samples from posterior distributions simultaneously. The empirical performance of the proposed method through simulations and an application to real data are also provided.

</details>

<details>

<summary>2022-07-01 13:45:33 - Flexible Bayesian Product Mixture Models for Vector Autoregressions</summary>

- *Suprateek Kundu, Joshua Lukemire*

- `2111.08743v2` - [abs](http://arxiv.org/abs/2111.08743v2) - [pdf](http://arxiv.org/pdf/2111.08743v2)

> Bayesian non-parametric methods based on Dirichlet process mixtures have seen tremendous success in various domains and are appealing in being able to borrow information by clustering samples that share identical parameters. However, such methods can face hurdles in heterogeneous settings where objects are expected to cluster only along a subset of axes or where clusters of samples share only a subset of identical parameters. We overcome such limitations by developing a novel class of product of Dirichlet process location-scale mixtures that enable independent clustering at multiple scales, which result in varying levels of information sharing across samples. First, we develop the approach for independent multivariate data. Subsequently we generalize it to multivariate time-series data under the framework of multi-subject Vector Autoregressive (VAR) models that is our primary focus, which go beyond parametric single-subject VAR models. We establish posterior consistency and develop efficient posterior computation for implementation. Extensive numerical studies involving VAR models show distinct advantages over competing methods, in terms of estimation, clustering, and feature selection accuracy. Our resting state fMRI analysis from the Human Connectome Project reveals biologically interpretable connectivity differences between distinct intelligence groups, while another air pollution application illustrates the superior forecasting accuracy compared to alternate methods.

</details>

<details>

<summary>2022-07-01 15:54:23 - Semi-nonparametric Estimation of Operational Risk Capital with Extreme Loss Events</summary>

- *Heng Z. Chen, Stephen R. Cosslett*

- `2111.11459v2` - [abs](http://arxiv.org/abs/2111.11459v2) - [pdf](http://arxiv.org/pdf/2111.11459v2)

> Bank operational risk capital modeling using the Basel II advanced measurement approach (AMA) often lead to a counter-intuitive capital estimate of value at risk at 99.9% due to extreme loss events. To address this issue, a flexible semi-nonparametric (SNP) model is introduced using the change of variables technique to enrich the family of distributions to handle extreme loss events. The SNP models are proved to have the same maximum domain of attraction (MDA) as the parametric kernels, and it follows that the SNP models are consistent with the extreme value theory peaks over threshold method but with different shape and scale parameters from the kernels. By using the simulation dataset generated from a mixture of distributions with both light and heavy tails, the SNP models in the Frechet and Gumbel MDAs are shown to fit the tail dataset satisfactorily through increasing the number of model parameters. The SNP model quantile estimates at 99.9 percent are not overly sensitive towards the body-tail threshold change, which is in sharp contrast to the parametric models. When applied to a bank operational risk dataset with three Basel event types, the SNP model provides a significant improvement in the goodness of fit to the two event types with heavy tails, yielding an intuitive capital estimate that is in the same magnitude as the event type total loss. Since the third event type does not have a heavy tail, the parametric model yields an intuitive capital estimate, and the SNP model cannot provide additional improvement. This research suggests that the SNP model may enable banks to continue with the AMA or its partial use to obtain an intuitive operational risk capital estimate when the simple non-model based Basic Indicator Approach or Standardized Approach are not suitable per Basel Committee Banking Supervision OPE10 (2019).

</details>

<details>

<summary>2022-07-01 18:00:05 - Function-space Inference with Sparse Implicit Processes</summary>

- *Simn Rodrguez Santana, Bryan Zaldivar, Daniel Hernndez-Lobato*

- `2110.07618v2` - [abs](http://arxiv.org/abs/2110.07618v2) - [pdf](http://arxiv.org/pdf/2110.07618v2)

> Implicit Processes (IPs) represent a flexible framework that can be used to describe a wide variety of models, from Bayesian neural networks, neural samplers and data generators to many others. IPs also allow for approximate inference in function-space. This change of formulation solves intrinsic degenerate problems of parameter-space approximate inference concerning the high number of parameters and their strong dependencies in large models. For this, previous works in the literature have attempted to employ IPs both to set up the prior and to approximate the resulting posterior. However, this has proven to be a challenging task. Existing methods that can tune the prior IP result in a Gaussian predictive distribution, which fails to capture important data patterns. By contrast, methods producing flexible predictive distributions by using another IP to approximate the posterior process cannot tune the prior IP to the observed data. We propose here the first method that can accomplish both goals. For this, we rely on an inducing-point representation of the prior IP, as often done in the context of sparse Gaussian processes. The result is a scalable method for approximate inference with IPs that can tune the prior IP parameters to the data, and that provides accurate non-Gaussian predictive distributions.

</details>

<details>

<summary>2022-07-01 18:41:28 - Tractable Uncertainty for Structure Learning</summary>

- *Benjie Wang, Matthew Wicker, Marta Kwiatkowska*

- `2204.14170v2` - [abs](http://arxiv.org/abs/2204.14170v2) - [pdf](http://arxiv.org/pdf/2204.14170v2)

> Bayesian structure learning allows one to capture uncertainty over the causal directed acyclic graph (DAG) responsible for generating given data. In this work, we present Tractable Uncertainty for STructure learning (TRUST), a framework for approximate posterior inference that relies on probabilistic circuits as the representation of our posterior belief. In contrast to sample-based posterior approximations, our representation can capture a much richer space of DAGs, while also being able to tractably reason about the uncertainty through a range of useful inference queries. We empirically show how probabilistic circuits can be used as an augmented representation for structure learning methods, leading to improvement in both the quality of inferred structures and posterior uncertainty. Experimental results on conditional query answering further demonstrate the practical utility of the representational capacity of TRUST.

</details>

<details>

<summary>2022-07-01 23:22:40 - Engagement Maximization</summary>

- *Benjamin Hbert, Weijie Zhong*

- `2207.00685v1` - [abs](http://arxiv.org/abs/2207.00685v1) - [pdf](http://arxiv.org/pdf/2207.00685v1)

> We consider the problem of a rational, Bayesian agent receiving signals over time for the purpose of taking an action. The agent chooses when to stop and take an action based on her current beliefs, and prefers (all else equal) to act sooner rather than later. However, the signals received by the agent are determined by a principal, whose objective is to maximize engagement (the total attention paid by the agent to the signals). We show that engagement maximization by the principal minimizes the agent's welfare; the agent does no better than if she gathered no information at all. Relative to a benchmark in which the agent chooses which signals to acquire, engagement maximization leads to excessive information acquisition and to more extreme beliefs. We show that an optimal strategy involves "suspensive signals" that lead the agent's belief to change while keeping it "less certain than the prior" and "decisive signals" that lead the agent's belief to jump to the stopping region.

</details>

<details>

<summary>2022-07-01 23:36:47 - Rapidly Mixing Multiple-try Metropolis Algorithms for Model Selection Problems</summary>

- *Hyunwoong Chang, Changwoo J. Lee, Zhao Tang Luo, Huiyan Sang, Quan Zhou*

- `2207.00689v1` - [abs](http://arxiv.org/abs/2207.00689v1) - [pdf](http://arxiv.org/pdf/2207.00689v1)

> The multiple-try Metropolis (MTM) algorithm is an extension of the Metropolis-Hastings (MH) algorithm by selecting the proposed state among multiple trials according to some weight function. Although MTM has gained great popularity owing to its faster empirical convergence and mixing than the standard MH algorithm, its theoretical mixing property is rarely studied in the literature due to its complex proposal scheme. We prove that MTM can achieve a mixing time bound smaller than that of MH by a factor of the number of trials under a general setting applicable to high-dimensional model selection problems. Our theoretical results motivate a new class of weight functions and guide the choice of the number of trials, which leads to improved performance than standard MTM algorithms. We support our theoretical results by extensive simulation studies with several Bayesian model selection problems: variable selection, stochastic block models, and spatial clustering models.

</details>

<details>

<summary>2022-07-02 01:56:57 - Double soft-thresholded model for multi-group scalar on vector-valued image regression</summary>

- *Arkaprava Roy, Zhou Lan*

- `2206.09819v2` - [abs](http://arxiv.org/abs/2206.09819v2) - [pdf](http://arxiv.org/pdf/2206.09819v2)

> In this paper, we develop a novel spatial variable selection method for scalar on vector-valued image regression in a multi-group setting. Here, 'vector-valued image' refers to the imaging datasets that contain vector-valued information at each pixel/voxel location, such as in RGB color images, multimodal medical images, DTI imaging, etc. The focus of this work is to identify the spatial locations in the image having an important effect on the scalar outcome measure. Specifically, the overall effect of each voxel is of interest. We thus develop a novel shrinkage prior by soft-thresholding the \ell_2 norm of a latent multivariate Gaussian process. It will allow us to estimate sparse and piecewise-smooth spatially varying vector-valued regression coefficient functions. For posterior inference, an efficient MCMC algorithm is developed. We establish the posterior contraction rate for parameter estimation and consistency for variable selection of the proposed Bayesian model, assuming that the true regression coefficients are Holder smooth. Finally, we demonstrate the advantages of the proposed method in simulation studies and further illustrate in an ADNI dataset for modeling MMSE scores based on DTI-based vector-valued imaging markers.

</details>

<details>

<summary>2022-07-02 09:24:14 - Modeling Multivariate Positive-Valued Time Series Using R-INLA</summary>

- *Chiranjit Dutta, Nalini Ravishanker, Sumanta Basu*

- `2206.05374v2` - [abs](http://arxiv.org/abs/2206.05374v2) - [pdf](http://arxiv.org/pdf/2206.05374v2)

> In this paper we describe fast Bayesian statistical analysis of vector positive-valued time series, with application to interesting financial data streams. We discuss a flexible level correlated model (LCM) framework for building hierarchical models for vector positive-valued time series. The LCM allows us to combine marginal gamma distributions for the positive-valued component responses, while accounting for association among the components at a latent level. We use integrated nested Laplace approximation (INLA) for fast approximate Bayesian modeling via the R-INLA package, building custom functions to handle this setup. We use the proposed method to model interdependencies between realized volatility measures from several stock indexes.

</details>

<details>

<summary>2022-07-02 16:59:37 - Tree ensemble kernels for Bayesian optimization with known constraints over mixed-feature spaces</summary>

- *Alexander Thebelt, Calvin Tsay, Robert M. Lee, Nathan Sudermann-Merx, David Walz, Behrang Shafei, Ruth Misener*

- `2207.00879v1` - [abs](http://arxiv.org/abs/2207.00879v1) - [pdf](http://arxiv.org/pdf/2207.00879v1)

> Tree ensembles can be well-suited for black-box optimization tasks such as algorithm tuning and neural architecture search, as they achieve good predictive performance with little to no manual tuning, naturally handle discrete feature spaces, and are relatively insensitive to outliers in the training data. Two well-known challenges in using tree ensembles for black-box optimization are (i) effectively quantifying model uncertainty for exploration and (ii) optimizing over the piece-wise constant acquisition function. To address both points simultaneously, we propose using the kernel interpretation of tree ensembles as a Gaussian Process prior to obtain model variance estimates, and we develop a compatible optimization formulation for the acquisition function. The latter further allows us to seamlessly integrate known constraints to improve sampling efficiency by considering domain-knowledge in engineering settings and modeling search space symmetries, e.g., hierarchical relationships in neural architecture search. Our framework performs as well as state-of-the-art methods for unconstrained black-box optimization over continuous/discrete features and outperforms competing methods for problems combining mixed-variable feature spaces and known input constraints.

</details>

<details>

<summary>2022-07-03 06:06:12 - Low probability states, data statistics, and entropy estimation</summary>

- *Damin G. Hernndez, Ahmed Roman, Ilya Nemenman*

- `2207.00962v1` - [abs](http://arxiv.org/abs/2207.00962v1) - [pdf](http://arxiv.org/pdf/2207.00962v1)

> A fundamental problem in analysis of complex systems is getting a reliable estimate of entropy of their probability distributions over the state space. This is difficult because unsampled states can contribute substantially to the entropy, while they do not contribute to the Maximum Likelihood estimator of entropy, which replaces probabilities by the observed frequencies. Bayesian estimators overcome this obstacle by introducing a model of the low-probability tail of the probability distribution. Which statistical features of the observed data determine the model of the tail, and hence the output of such estimators, remains unclear. Here we show that well-known entropy estimators for probability distributions on discrete state spaces model the structure of the low probability tail based largely on few statistics of the data: the sample size, the Maximum Likelihood estimate, the number of coincidences among the samples, the dispersion of the coincidences. We derive approximate analytical entropy estimators for undersampled distributions based on these statistics, and we use the results to propose an intuitive understanding of how the Bayesian entropy estimators work.

</details>

<details>

<summary>2022-07-03 18:39:03 - Mathematical Foundations of Graph-Based Bayesian Semi-Supervised Learning</summary>

- *Nicolas Garca Trillos, Daniel Sanz-Alonso, Ruiyi Yang*

- `2207.01093v1` - [abs](http://arxiv.org/abs/2207.01093v1) - [pdf](http://arxiv.org/pdf/2207.01093v1)

> In recent decades, science and engineering have been revolutionized by a momentous growth in the amount of available data. However, despite the unprecedented ease with which data are now collected and stored, labeling data by supplementing each feature with an informative tag remains to be challenging. Illustrative tasks where the labeling process requires expert knowledge or is tedious and time-consuming include labeling X-rays with a diagnosis, protein sequences with a protein type, texts by their topic, tweets by their sentiment, or videos by their genre. In these and numerous other examples, only a few features may be manually labeled due to cost and time constraints. How can we best propagate label information from a small number of expensive labeled features to a vast number of unlabeled ones? This is the question addressed by semi-supervised learning (SSL).   This article overviews recent foundational developments on graph-based Bayesian SSL, a probabilistic framework for label propagation using similarities between features. SSL is an active research area and a thorough review of the extant literature is beyond the scope of this article. Our focus will be on topics drawn from our own research that illustrate the wide range of mathematical tools and ideas that underlie the rigorous study of the statistical accuracy and computational efficiency of graph-based Bayesian SSL.

</details>

<details>

<summary>2022-07-04 00:44:30 - Modeling Randomly Walking Volatility with Chained Gamma Distributions</summary>

- *Di Zhang, Qiang Niu, Youzhou Zhou*

- `2207.01151v1` - [abs](http://arxiv.org/abs/2207.01151v1) - [pdf](http://arxiv.org/pdf/2207.01151v1)

> Volatility clustering is a common phenomenon in financial time series. Typically, linear models are used to describe the temporal autocorrelation of the (logarithmic) variance of returns. Considering the difficulty in estimation of this model, we construct a Dynamic Bayesian Network, which utilizes the conjugate prior relation of normal-gamma and gamma-gamma, so that at each node, its posterior form locally remains unchanged. This makes it possible to quickly find approximate solutions using variational methods. Furthermore, we ensure that the volatility expressed by the model is an independent incremental process after inserting dummy gamma nodes between adjacent time steps. We have found that, this model has two advantages: 1) It can be proved that it can express heavier tails than Gaussians, i.e., have positive excess kurtosis, compared to popular linear models. 2) If the variational inference(VI) is used for state estimation, it runs much faster than Monte Carlo(MC) methods, since the calculation of the posterior uses only basic arithmetic operations. And, its convergence process is deterministic.   We tested the model, named Gam-Chain, using recent Crypto, Nasdaq, and Forex records of varying resolutions. The results show that: 1) In the same case of using MC, this model can achieve comparable state estimation results with the regular lognormal chain. 2) In the case of only using VI, this model can obtain accuracy that are slightly worse than MC, but still acceptable in practice; 3) Only using VI, the running time of Gam-Chain, under the most conservative settings, can be reduced to below 20% of that based on the lognormal chain via MC.

</details>

<details>

<summary>2022-07-04 07:06:45 - Look beyond labels: Incorporating functional summary information in Bayesian neural networks</summary>

- *Vishnu Raj, Tianyu Cui, Markus Heinonen, Pekka Marttinen*

- `2207.01234v1` - [abs](http://arxiv.org/abs/2207.01234v1) - [pdf](http://arxiv.org/pdf/2207.01234v1)

> Bayesian deep learning offers a principled approach to train neural networks that accounts for both aleatoric and epistemic uncertainty. In variational inference, priors are often specified over the weight parameters, but they do not capture the true prior knowledge in large and complex neural network architectures. We present a simple approach to incorporate summary information about the predicted probability (such as sigmoid or softmax score) outputs in Bayesian neural networks (BNNs). The available summary information is incorporated as augmented data and modeled with a Dirichlet process, and we derive the corresponding \emph{Summary Evidence Lower BOund}. We show how the method can inform the model about task difficulty or class imbalance. Extensive empirical experiments show that, with negligible computational overhead, the proposed method yields a BNN with a better calibration of uncertainty.

</details>

<details>

<summary>2022-07-04 08:27:29 - Inverse Problems and Data Assimilation with Connections to Machine Learning</summary>

- *Daniel Sanz-Alonso, Andrew M. Stuart, Armeen Taeb*

- `1810.06191v3` - [abs](http://arxiv.org/abs/1810.06191v3) - [pdf](http://arxiv.org/pdf/1810.06191v3)

> These notes are designed with the aim of providing a clear and concise introduction to the subjects of Inverse Problems and Data Assimilation, and their inter-relations, together with citations to some relevant literature in this area. The first half of the notes is dedicated to studying the Bayesian framework for inverse problems. Techniques such as importance sampling and Markov Chain Monte Carlo (MCMC) methods are introduced; these methods have the desirable property that in the limit of an infinite number of samples they reproduce the full posterior distribution. Since it is often computationally intensive to implement these methods, especially in high dimensional problems, approximate techniques such as approximating the posterior by a Dirac or a Gaussian distribution are discussed. The second half of the notes cover data assimilation. This refers to a particular class of inverse problems in which the unknown parameter is the initial condition of a dynamical system, and in the stochastic dynamics case the subsequent states of the system, and the data comprises partial and noisy observations of that (possibly stochastic) dynamical system. We will also demonstrate that methods developed in data assimilation may be employed to study generic inverse problems, by introducing an artificial time to generate a sequence of probability measures interpolating from the prior to the posterior. The third and final part of the notes describes various topics which blend the theory of inverse problems, data assimilation, and machine learning. Whilst ideas from machine learning appear in the first two parts of the notes, the final part overviews the main ways in which machine learning is impacting on, and has the potential to impact on, both the subjects of inverse problems and data assimilation.

</details>

<details>

<summary>2022-07-04 08:59:41 - Discrepancy-based Inference for Intractable Generative Models using Quasi-Monte Carlo</summary>

- *Ziang Niu, Johanna Meier, Franois-Xavier Briol*

- `2106.11561v3` - [abs](http://arxiv.org/abs/2106.11561v3) - [pdf](http://arxiv.org/pdf/2106.11561v3)

> Intractable generative models are models for which the likelihood is unavailable but sampling is possible. Most approaches to parameter inference in this setting require the computation of some discrepancy between the data and the generative model. This is for example the case for minimum distance estimation and approximate Bayesian computation. These approaches require sampling a high number of realisations from the model for different parameter values, which can be a significant challenge when simulating is an expensive operation. In this paper, we propose to enhance this approach by enforcing "sample diversity" in simulations of our models. This will be implemented through the use of quasi-Monte Carlo (QMC) point sets. Our key results are sample complexity bounds which demonstrate that, under smoothness conditions on the generator, QMC can significantly reduce the number of samples required to obtain a given level of accuracy when using three of the most common discrepancies: the maximum mean discrepancy, the Wasserstein distance, and the Sinkhorn divergence. This is complemented by a simulation study which highlights that an improved accuracy is sometimes also possible in some settings which are not covered by the theory.

</details>

<details>

<summary>2022-07-04 11:11:01 - Bayesian posterior repartitioning for nested sampling</summary>

- *Xi Chen, Farhan Feroz, Michael Hobson*

- `1908.04655v3` - [abs](http://arxiv.org/abs/1908.04655v3) - [pdf](http://arxiv.org/pdf/1908.04655v3)

> Priors in Bayesian analyses often encode informative domain knowledge that can be useful in making the inference process more efficient. Occasionally, however, priors may be unrepresentative of the parameter values for a given dataset, which can result in inefficient parameter space exploration, or even incorrect inferences, particularly for nested sampling (NS) algorithms. Simply broadening the prior in such cases may be inappropriate or impossible in some applications. Hence our previous solution to this problem, known as posterior repartitioning (PR), redefines the prior and likelihood while keeping their product fixed, so that the posterior inferences and evidence estimates remain unchanged, but the efficiency of the NS process is significantly increased. In its most practical form, PR raises the prior to some power beta, which is introduced as an auxiliary variable that must be determined on a case-by-case basis, usually by lowering beta from unity according to some pre-defined `annealing schedule' until the resulting inferences converge to a consistent solution. Here we present a very simple yet powerful alternative Bayesian approach, in which beta is instead treated as a hyperparameter that is inferred from the data alongside the original parameters of the problem, and then marginalised over to obtain the final inference. We show through numerical examples that this Bayesian PR (BPR) method provides a very robust, self-adapting and computationally efficient `hands-off' solution to the problem of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the original PR method, we show that even for representative priors BPR has a negligible computational overhead relative to standard nesting sampling, which suggests that it should be used as the default in all NS analyses.

</details>

<details>

<summary>2022-07-04 15:41:02 - Variational Neural Networks</summary>

- *Illia Oleksiienko, Dat Thanh Tran, Alexandros Iosifidis*

- `2207.01524v1` - [abs](http://arxiv.org/abs/2207.01524v1) - [pdf](http://arxiv.org/pdf/2207.01524v1)

> Bayesian Neural Networks (BNNs) provide a tool to estimate the uncertainty of a neural network by considering a distribution over weights and sampling different models for each input. In this paper, we propose a method for uncertainty estimation in neural networks called Variational Neural Network that, instead of considering a distribution over weights, generates parameters for the output distribution of a layer by transforming its inputs with learnable sub-layers. In uncertainty quality estimation experiments, we show that VNNs achieve better uncertainty quality than Monte Carlo Dropout or Bayes By Backpropagation methods.

</details>

<details>

<summary>2022-07-05 02:16:06 - Efficient data augmentation techniques for some classes of state space models</summary>

- *Linda S. L. Tan*

- `1712.08887v3` - [abs](http://arxiv.org/abs/1712.08887v3) - [pdf](http://arxiv.org/pdf/1712.08887v3)

> Data augmentation improves the convergence of iterative algorithms, such as the EM algorithm and Gibbs sampler by introducing carefully designed latent variables. In this article, we first propose a data augmentation scheme for the first-order autoregression plus noise model, where optimal values of working parameters introduced for recentering and rescaling of the latent states, can be derived analytically by minimizing the fraction of missing information in the EM algorithm. The proposed data augmentation scheme is then utilized to design efficient Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of some non-Gaussian and nonlinear state space models, via a mixture of normals approximation coupled with a block-specific reparametrization strategy. Applications on simulated and benchmark real datasets indicate that the proposed MCMC sampler can yield improvements in simulation efficiency compared with centering, noncentering and even the ancillarity-sufficiency interweaving strategy.

</details>

<details>

<summary>2022-07-05 03:25:37 - Approximate Gibbs sampler for Bayesian Huberized lasso</summary>

- *Jun Kawakami, Shintaro Hashimoto*

- `2204.00237v2` - [abs](http://arxiv.org/abs/2204.00237v2) - [pdf](http://arxiv.org/pdf/2204.00237v2)

> The Bayesian lasso is well-known as a Bayesian alternative for Lasso. Although the advantage of the Bayesian lasso is capable of full probabilistic uncertain quantification for parameters, the corresponding posterior distribution can be sensitive to outliers. To overcome such problem, robust Bayesian regression models have been proposed in recent years. In this paper, we consider the robust and efficient estimation for the Bayesian Huberized lasso regression in fully Bayesian perspective. A new posterior computation algorithm for the Bayesian Huberized lasso regression is proposed. The proposed approximate Gibbs sampler is based on the approximation of full conditional distribution and it is possible to estimate a tuning parameter for robustness of the pseudo-Huber loss function. Some theoretical properties of the posterior distribution are also derived. We illustrate performance of the proposed method through simulation studies and real data examples.

</details>

<details>

<summary>2022-07-05 05:12:05 - Stochastic Variational Methods in Generalized Hidden Semi-Markov Models to Characterize Functionality in Random Heteropolymers</summary>

- *Yun Zhou, Boying Gong, Tao Jiang, Ting Xu, Haiyan Huang*

- `2207.01813v1` - [abs](http://arxiv.org/abs/2207.01813v1) - [pdf](http://arxiv.org/pdf/2207.01813v1)

> Recent years have seen substantial advances in the development of biofunctional materials using synthetic polymers. The growing problem of elusive sequence-functionality relations for most biomaterials has driven researchers to seek more effective tools and analysis methods. In this study, statistical models are used to study sequence features of the recently reported random heteropolymers (RHP), which transport protons across lipid bilayers selectively and rapidly like natural proton channels. We utilized the probabilistic graphical model framework and developed a generalized hidden semi-Markov model (GHSMM-RHP) to extract the function-determining sequence features, including the transmembrane segments within a chain and the sequence heterogeneity among different chains. We developed stochastic variational methods for efficient inference on parameter estimation and predictions, and empirically studied their computational performance from a comparative perspective on Bayesian (i.e., stochastic variational Bayes) versus frequentist (i.e., stochastic variational expectation-maximization) frameworks that have been studied separately before. The real data results agree well with the laboratory experiments, and suggest GHSMM-RHP's potential in predicting protein-like behavior at the polymer-chain level.

</details>

<details>

<summary>2022-07-05 06:16:28 - Input-State-Parameter-Noise Identification and Virtual Sensing in Dynamical Systems: A Bayesian Expectation-Maximization (BEM) Perspective</summary>

- *Daniz Teymouri, Omid Sedehi, Lambros S. Katafygiotis, Costas Papadimitriou*

- `2207.01828v1` - [abs](http://arxiv.org/abs/2207.01828v1) - [pdf](http://arxiv.org/pdf/2207.01828v1)

> Structural identification and damage detection can be generalized as the simultaneous estimation of input forces, physical parameters, and dynamical states. Although Kalman-type filters are efficient tools to address this problem, the calibration of noise covariance matrices is cumbersome. For instance, calibration of input noise covariance matrix in augmented or dual Kalman filters is a critical task since a slight variation in its value can adversely affect estimations. The present study develops a Bayesian Expectation-Maximization (BEM) methodology for the uncertainty quantification and propagation in coupled input-state-parameter-noise identification problems. It also proposes the incorporation of input dummy observations for stabilizing low-frequency components of the latent states and mitigating potential drifts. In this respect, the covariance matrix of the dummy observations is also calibrated based on the measured data. Additionally, an explicit formulation is provided to study the theoretical observability of the Bayesian estimators, which helps characterize the minimum sensor requirements. Ultimately, the BEM is tested and verified through numerical and experimental examples, wherein sensor configurations, multiple input forces, and abrupt stiffness changes are investigated. It is confirmed that the BEM provides accurate estimations of states, input, and parameters while characterizing the degree of belief in these estimations based on the posterior uncertainties driven by applying a Bayesian perspective.

</details>

<details>

<summary>2022-07-05 07:17:43 - Meta-Learning a Real-Time Tabular AutoML Method For Small Data</summary>

- *Noah Hollmann, Samuel Mller, Katharina Eggensperger, Frank Hutter*

- `2207.01848v1` - [abs](http://arxiv.org/abs/2207.01848v1) - [pdf](http://arxiv.org/pdf/2207.01848v1)

> We present TabPFN, an AutoML method that is competitive with the state of the art on small tabular datasets while being over 1,000$\times$ faster. Our method is very simple: it is fully entailed in the weights of a single neural network, and a single forward pass directly yields predictions for a new dataset. Our AutoML method is meta-learned using the Transformer-based Prior-Data Fitted Network (PFN) architecture and approximates Bayesian inference with a prior that is based on assumptions of simplicity and causal structures. The prior contains a large space of structural causal models and Bayesian neural networks with a bias for small architectures and thus low complexity. Furthermore, we extend the PFN approach to differentiably calibrate the prior's hyperparameters on real data. By doing so, we separate our abstract prior assumptions from their heuristic calibration on real data. Afterwards, the calibrated hyperparameters are fixed and TabPFN can be applied to any new tabular dataset at the push of a button. Finally, on 30 datasets from the OpenML-CC18 suite we show that our method outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with predictions produced in less than a second. We provide all our code and our final trained TabPFN in the supplementary materials.

</details>

<details>

<summary>2022-07-05 09:47:33 - $$VAE: a stochastic process prior for Bayesian deep learning with MCMC</summary>

- *Swapnil Mishra, Seth Flaxman, Tresnia Berah, Harrison Zhu, Mikko Pakkanen, Samir Bhatt*

- `2002.06873v5` - [abs](http://arxiv.org/abs/2002.06873v5) - [pdf](http://arxiv.org/pdf/2002.06873v5)

> Stochastic processes provide a mathematically elegant way model complex data. In theory, they provide flexible priors over function classes that can encode a wide range of interesting assumptions. In practice, however, efficient inference by optimisation or marginalisation is difficult, a problem further exacerbated with big data and high dimensional input spaces. We propose a novel variational autoencoder (VAE) called the prior encoding variational autoencoder ($\pi$VAE). The $\pi$VAE is finitely exchangeable and Kolmogorov consistent, and thus is a continuous stochastic process. We use $\pi$VAE to learn low dimensional embeddings of function classes. We show that our framework can accurately learn expressive function classes such as Gaussian processes, but also properties of functions to enable statistical inference (such as the integral of a log Gaussian process). For popular tasks, such as spatial interpolation, $\pi$VAE achieves state-of-the-art performance both in terms of accuracy and computational efficiency. Perhaps most usefully, we demonstrate that the low dimensional independently distributed latent space representation learnt provides an elegant and scalable means of performing Bayesian inference for stochastic processes within probabilistic programming languages such as Stan.

</details>

<details>

<summary>2022-07-05 09:49:45 - Honest Confidence Bands for Isotonic Quantile Curves</summary>

- *Lutz Duembgen, Lukas Luethi*

- `2206.13069v2` - [abs](http://arxiv.org/abs/2206.13069v2) - [pdf](http://arxiv.org/pdf/2206.13069v2)

> We provide confidence bands for isotonic quantile curves in nonparametric univariate regression with guaranteed given coverage probability. The method is an adaptation of the confidence bands of Duembgen and Johns (2004) for isotonic median curves.

</details>

<details>

<summary>2022-07-05 10:06:30 - A Probabilistic State Space Model for Joint Inference from Differential Equations and Data</summary>

- *Jonathan Schmidt, Nicholas Krmer, Philipp Hennig*

- `2103.10153v3` - [abs](http://arxiv.org/abs/2103.10153v3) - [pdf](http://arxiv.org/pdf/2103.10153v3)

> Mechanistic models with differential equations are a key component of scientific applications of machine learning. Inference in such models is usually computationally demanding, because it involves repeatedly solving the differential equation. The main problem here is that the numerical solver is hard to combine with standard inference techniques. Recent work in probabilistic numerics has developed a new class of solvers for ordinary differential equations (ODEs) that phrase the solution process directly in terms of Bayesian filtering. We here show that this allows such methods to be combined very directly, with conceptual and numerical ease, with latent force models in the ODE itself. It then becomes possible to perform approximate Bayesian inference on the latent force as well as the ODE solution in a single, linear complexity pass of an extended Kalman filter / smoother - that is, at the cost of computing a single ODE solution. We demonstrate the expressiveness and performance of the algorithm by training, among others, a non-parametric SIRD model on data from the COVID-19 outbreak.

</details>

<details>

<summary>2022-07-05 10:26:57 - Variational Bayes for high-dimensional proportional hazards models with applications within gene expression</summary>

- *Michael Komodromos, Eric Aboagye, Marina Evangelou, Sarah Filippi, Kolyan Ray*

- `2112.10270v2` - [abs](http://arxiv.org/abs/2112.10270v2) - [pdf](http://arxiv.org/pdf/2112.10270v2)

> Few Bayesian methods for analyzing high-dimensional sparse survival data provide scalable variable selection, effect estimation and uncertainty quantification. Such methods often either sacrifice uncertainty quantification by computing maximum a posteriori estimates, or quantify the uncertainty at high (unscalable) computational expense. We bridge this gap and develop an interpretable and scalable Bayesian proportional hazards model for prediction and variable selection, referred to as SVB. Our method, based on a mean-field variational approximation, overcomes the high computational cost of MCMC whilst retaining useful features, providing a posterior distribution for the parameters and offering a natural mechanism for variable selection via posterior inclusion probabilities. The performance of our proposed method is assessed via extensive simulations and compared against other state-of-the-art Bayesian variable selection methods, demonstrating comparable or better performance. Finally, we demonstrate how the proposed method can be used for variable selection on two transcriptomic datasets with censored survival outcomes, and how the uncertainty quantification offered by our method can be used to provide an interpretable assessment of patient risk.

</details>

<details>

<summary>2022-07-05 11:30:34 - Parallel and distributed Bayesian modelling for analysing high-dimensional spatio-temporal count data</summary>

- *E. Orozco-Acosta, A. Adin, M. D. Ugarte*

- `2201.08323v2` - [abs](http://arxiv.org/abs/2201.08323v2) - [pdf](http://arxiv.org/pdf/2201.08323v2)

> This paper proposes a general procedure to analyse high-dimensional spatio-temporal count data, with special emphasis on relative risks estimation in cancer epidemiology. More precisely, we present a pragmatic and simple idea that permits to fit hierarchical spatio-temporal models when the number of small areas is very large. Model fitting is carried out using integrated nested Laplace approximations over a partition of the spatial domain. We also use parallel and distributed strategies to speed up computations in a setting where Bayesian model fitting is generally prohibitively time-consuming and even unfeasible. The whole procedure is evaluated in a simulation study with a twofold objective: to estimate risks accurately and to detect extreme risk areas while avoiding false positives/negatives. We show that our method outperforms classical global models. A real data analysis comparing the global models and the new procedure is also presented.

</details>

<details>

<summary>2022-07-05 11:39:45 - Variational Inference of Dynamic Factor Models with Arbitrary Missing Data</summary>

- *Erik Spnberg*

- `2207.01976v1` - [abs](http://arxiv.org/abs/2207.01976v1) - [pdf](http://arxiv.org/pdf/2207.01976v1)

> Dynamic factor models are typically estimated by point-estimation methods, disregarding parameter uncertainty. We propose a new method accounting for parameter uncertainty by means of posterior approximation, using variational inference. Our approach allows for any arbitrary pattern of missing data, including different sample sizes and mixed frequencies. It also yields a straight-forward estimation algorithm absent of time-consuming simulation techniques. In empirical examples using both small and large models, we compare our method to full Bayesian estimation from MCMC-simulations. Generally, the approximation captures factor features and parameters well, with vast computational gains. The resulting predictive distributions are approximated to a very high precision, almost indistinguishable from MCMC both in and out of sample, in a tiny fraction of computational time.

</details>

<details>

<summary>2022-07-05 14:20:15 - Assessing inter-rater reliability with heterogeneous variance components models: Flexible approach accounting for contextual variables</summary>

- *Patrcia Martinkov, Frantiek Barto, Marek Brabec*

- `2207.02071v1` - [abs](http://arxiv.org/abs/2207.02071v1) - [pdf](http://arxiv.org/pdf/2207.02071v1)

> Inter-rater reliability (IRR), which is a prerequisite of high-quality ratings and assessments, may be affected by contextual variables such as the rater's or ratee's gender, major, or experience. Identification of such heterogeneity sources in IRR is important for implementation of policies with the potential to decrease measurement error and to increase IRR by focusing on the most relevant subgroups. In this study, we propose a flexible approach for assessing IRR in cases of heterogeneity due to covariates by directly modeling differences in variance components. We use Bayes factors to select the best performing model, and we suggest using Bayesian model-averaging as an alternative approach for obtaining IRR and variance component estimates, allowing us to account for model uncertainty. We use inclusion Bayes factors considering the whole model space to provide evidence for or against differences in variance components due to covariates. The proposed method is compared with other Bayesian and frequentist approaches in a simulation study, and we demonstrate its superiority in some situations. Finally, we provide real data examples from grant proposal peer-review, demonstrating the usefulness of this method and its flexibility in the generalization of more complex designs.

</details>

<details>

<summary>2022-07-05 16:10:04 - Bayesian model selection for multilevel models using marginal likelihoods</summary>

- *Tom Edinburgh, Ari Ercole, Stephen J. Eglen*

- `2207.02144v1` - [abs](http://arxiv.org/abs/2207.02144v1) - [pdf](http://arxiv.org/pdf/2207.02144v1)

> Multilevel linear models allow flexible statistical modelling of complex data with different levels of stratification. Identifying the most appropriate model from the large set of possible candidates is a challenging problem. In the Bayesian setting, the standard approach is a comparison of models using the model evidence or the Bayes factor. However, in all but the simplest of cases, direct computation of these quantities is impossible. Markov Chain Monte Carlo approaches are widely used, such as sequential Monte Carlo, but it is not always clear how well such techniques perform in practice. We present an improved method for estimation of the log model evidence, by an intermediate analytic computation of a marginal likelihood, integrated over non-variance parameters. This reduces the dimensionality of the Monte Carlo sampling algorithm, which in turn yields more consistent estimates. We illustrate this method on a popular multilevel dataset containing levels of radon in homes in the US state of Minnesota.

</details>

<details>

<summary>2022-07-05 17:58:33 - Offline RL Policies Should be Trained to be Adaptive</summary>

- *Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, Sergey Levine*

- `2207.02200v1` - [abs](http://arxiv.org/abs/2207.02200v1) - [pdf](http://arxiv.org/pdf/2207.02200v1)

> Offline RL algorithms must account for the fact that the dataset they are provided may leave many facets of the environment unknown. The most common way to approach this challenge is to employ pessimistic or conservative methods, which avoid behaviors that are too dissimilar from those in the training dataset. However, relying exclusively on conservatism has drawbacks: performance is sensitive to the exact degree of conservatism, and conservative objectives can recover highly suboptimal policies. In this work, we propose that offline RL methods should instead be adaptive in the presence of uncertainty. We show that acting optimally in offline RL in a Bayesian sense involves solving an implicit POMDP. As a result, optimal policies for offline RL must be adaptive, depending not just on the current state but rather all the transitions seen so far during evaluation.We present a model-free algorithm for approximating this optimal adaptive policy, and demonstrate the efficacy of learning such adaptive policies in offline RL benchmarks.

</details>

<details>

<summary>2022-07-05 22:38:45 - Forecasting with a Panel Tobit Model</summary>

- *Laura Liu, Hyungsik Roger Moon, Frank Schorfheide*

- `2110.14117v2` - [abs](http://arxiv.org/abs/2110.14117v2) - [pdf](http://arxiv.org/pdf/2110.14117v2)

> We use a dynamic panel Tobit model with heteroskedasticity to generate forecasts for a large cross-section of short time series of censored observations. Our fully Bayesian approach allows us to flexibly estimate the cross-sectional distribution of heterogeneous coefficients and then implicitly use this distribution as prior to construct Bayes forecasts for the individual time series. In addition to density forecasts, we construct set forecasts that explicitly target the average coverage probability for the cross-section. We present a novel application in which we forecast bank-level loan charge-off rates for small banks.

</details>

<details>

<summary>2022-07-05 22:47:09 - The R Package BHAM: Fast and Scalable Bayesian Hierarchical Additive Model for High-dimensional Data</summary>

- *Boyi Guo, Nengjun Yi*

- `2207.02348v1` - [abs](http://arxiv.org/abs/2207.02348v1) - [pdf](http://arxiv.org/pdf/2207.02348v1)

> BHAM is a freely avaible R pakcage that implments Bayesian hierarchical additive models for high-dimensional clinical and genomic data. The package includes functions that generalized additive model, and Cox additive model with the spike-and-slab LASSO prior. These functions implement scalable and stable algorithms to estimate parameters. BHAM also provides utility functions to construct additive models in high dimensional settings, select optimal models, summarize bi-level variable selection results, and visualize nonlinear effects. The package can facilitate flexible modeling of large-scale molecular data, i.e. detecting susceptible variables and infering disease diagnostic and prognostic. In this article, we describe the models, algorithms and related features implemented in BHAM. The package is freely available via the public GitHub repository https://github.com/boyiguo1/BHAM.

</details>

<details>

<summary>2022-07-06 01:13:30 - Bayesian Trend Filtering via Proximal Markov Chain Monte Carlo</summary>

- *Qiang Heng, Hua Zhou, Eric C. Chi*

- `2201.00092v2` - [abs](http://arxiv.org/abs/2201.00092v2) - [pdf](http://arxiv.org/pdf/2201.00092v2)

> Proximal Markov Chain Monte Carlo is a novel construct that lies at the intersection of Bayesian computation and convex optimization, which helped popularize the use of nondifferentiable priors in Bayesian statistics. Existing formulations of proximal MCMC, however, require hyperparameters and regularization parameters to be prespecified. In this work, we extend the paradigm of proximal MCMC through introducing a novel new class of nondifferentiable priors called epigraph priors. As a proof of concept, we place trend filtering, which was originally a nonparametric regression problem, in a parametric setting to provide a posterior median fit along with credible intervals as measures of uncertainty. The key idea is to replace the nonsmooth term in the posterior density with its Moreau-Yosida envelope, which enables the application of the gradient-based MCMC sampler Hamiltonian Monte Carlo. The proposed method identifies the appropriate amount of smoothing in a data-driven way, thereby automating regularization parameter selection. Compared with conventional proximal MCMC methods, our method is mostly tuning free, achieving simultaneous calibration of the mean, scale and regularization parameters in a fully Bayesian framework.

</details>

<details>

<summary>2022-07-06 02:54:37 - Statistical inference of random graphs with a surrogate likelihood function</summary>

- *Dingbo Wu, Fangzheng Xie*

- `2207.01702v2` - [abs](http://arxiv.org/abs/2207.01702v2) - [pdf](http://arxiv.org/pdf/2207.01702v2)

> Spectral estimators have been broadly applied to statistical network analysis but they do not incorporate the likelihood information of the network sampling model. This paper proposes a novel surrogate likelihood function for statistical inference of a class of popular network models referred to as random dot product graphs. In contrast to the structurally complicated exact likelihood function, the surrogate likelihood function has a separable structure and is log-concave yet approximates the exact likelihood function well. From the frequentist perspective, we study the maximum surrogate likelihood estimator and establish the accompanying theory. We show its existence, uniqueness, large sample properties, and that it improves upon the baseline spectral estimator with a smaller sum of squared errors. A computationally convenient stochastic gradient descent algorithm is designed for finding the maximum surrogate likelihood estimator in practice. From the Bayesian perspective, we establish the Bernstein--von Mises theorem of the posterior distribution with the surrogate likelihood function and show that the resulting credible sets have the correct frequentist coverage. The empirical performance of the proposed surrogate-likelihood-based methods is validated through the analyses of simulation examples and a real-world Wikipedia graph dataset. An R package implementing the proposed computation algorithms is publicly available at https://fangzheng-xie.github.io./publication/ .

</details>

<details>

<summary>2022-07-06 12:41:37 - Epistemic Neural Networks</summary>

- *Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, Benjamin Van Roy*

- `2107.08924v5` - [abs](http://arxiv.org/abs/2107.08924v5) - [pdf](http://arxiv.org/pdf/2107.08924v5)

> Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. Conventional neural networks lack this capability and, since most research has focused on marginal predictions, this shortcoming has been largely overlooked. We introduce the epistemic neural network (ENN) as an interface for models that represent uncertainty as required to generate useful joint predictions. While prior approaches to uncertainty modeling such as Bayesian neural networks can be expressed as ENNs, this new interface facilitates comparison of joint predictions and the design of novel architectures and algorithms. In particular, we introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. We demonstrate this efficacy across synthetic data, ImageNet, and some reinforcement learning tasks. As part of this effort we open-source experiment code.

</details>

<details>

<summary>2022-07-06 14:59:21 - Scalable Bayesian transport maps for high-dimensional non-Gaussian spatial fields</summary>

- *Matthias Katzfuss, Florian Schfer*

- `2108.04211v4` - [abs](http://arxiv.org/abs/2108.04211v4) - [pdf](http://arxiv.org/pdf/2108.04211v4)

> A multivariate distribution can be described by a triangular transport map from the target distribution to a simple reference distribution. We propose Bayesian nonparametric inference on the transport map by modeling its components using Gaussian processes. This enables regularization and uncertainty quantification of the map estimation, while still resulting in a closed-form and invertible posterior map. We then focus on inferring the distribution of a nonstationary spatial field from a small number of replicates. We develop specific transport-map priors that are highly flexible and are motivated by the behavior of a large class of stochastic processes. Our approach is scalable to high-dimensional distributions due to data-dependent sparsity and parallel computations. We also discuss extensions, including Dirichlet process mixtures for flexible marginals. We present numerical results to demonstrate the accuracy, scalability, and usefulness of our methods, including statistical emulation of non-Gaussian climate-model output.

</details>

<details>

<summary>2022-07-06 16:54:36 - Improved conformalized quantile regression</summary>

- *Martim Sousa, Ana Maria Tom, Jos Moreira*

- `2207.02808v1` - [abs](http://arxiv.org/abs/2207.02808v1) - [pdf](http://arxiv.org/pdf/2207.02808v1)

> Conformalized quantile regression is a procedure that inherits the advantages of conformal prediction and quantile regression. That is, we use quantile regression to estimate the true conditional quantile and then apply a conformal step on a calibration set to ensure marginal coverage. In this way, we get adaptive prediction intervals that account for heteroscedasticity. However, the aforementioned conformal step lacks adaptiveness as described in (Romano et al., 2019). To overcome this limitation, instead of applying a single conformal step after estimating conditional quantiles with quantile regression, we propose to cluster the explanatory variables weighted by their permutation importance with an optimized k-means and apply k conformal steps. To show that this improved version outperforms the classic version of conformalized quantile regression and is more adaptive to heteroscedasticity, we extensively compare the prediction intervals of both in open datasets.

</details>

<details>

<summary>2022-07-06 19:21:19 - The Posterior Predictive Null</summary>

- *Gemma E. Moran, John P. Cunningham, David M. Blei*

- `2112.03333v2` - [abs](http://arxiv.org/abs/2112.03333v2) - [pdf](http://arxiv.org/pdf/2112.03333v2)

> Bayesian model criticism is an important part of the practice of Bayesian statistics. Traditionally, model criticism methods have been based on the predictive check, an adaptation of goodness-of-fit testing to Bayesian modeling and an effective method to understand how well a model captures the distribution of the data. In modern practice, however, researchers iteratively build and develop many models, exploring a space of models to help solve the problem at hand. While classical predictive checks can help assess each one, they cannot help the researcher understand how the models relate to each other. This paper introduces the posterior predictive null check (PPN), a method for Bayesian model criticism that helps characterize the relationships between models. The idea behind the PPN is to check whether data from one model's predictive distribution can pass a predictive check designed for another model. This form of criticism complements the classical predictive check by providing a comparative tool. A collection of PPNs, which we call a PPN study, can help us understand which models are equivalent and which models provide different perspectives on the data. With mixture models, we demonstrate how a PPN study, along with traditional predictive checks, can help select the number of components by the principle of parsimony. With probabilistic factor models, we demonstrate how a PPN study can help understand relationships between different classes of models, such as linear models and models based on neural networks. Finally, we analyze data from the literature on predictive checks to show how a PPN study can improve the practice of Bayesian model criticism. Code to replicate the results in this paper is available at \url{https://github.com/gemoran/ppn-code}.

</details>

<details>

<summary>2022-07-07 01:21:06 - Posterior consistency and characteristic functional of normalized random measures with independent increments</summary>

- *Junxi Zhang, Yaozhong Hu*

- `2207.03032v1` - [abs](http://arxiv.org/abs/2207.03032v1) - [pdf](http://arxiv.org/pdf/2207.03032v1)

> In this paper, we obtain explicit form of the posterior moments and characteristic functional for normalized random measures with independent increments (NRMIs) in terms of their associated Levy intensities, which is a class of Bayesian nonparametric priors that have been studied widely in the literature. These results are applied to solve the posterior consistency problem, the results of which are illustrated with examples.

</details>

<details>

<summary>2022-07-07 03:56:59 - Pre-trained Gaussian processes for Bayesian optimization</summary>

- *Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, Zoubin Ghahramani*

- `2109.08215v4` - [abs](http://arxiv.org/abs/2109.08215v4) - [pdf](http://arxiv.org/pdf/2109.08215v4)

> Bayesian optimization (BO) has become a popular strategy for global optimization of many expensive real-world functions. Contrary to a common belief that BO is suited to optimizing black-box functions, it actually requires domain knowledge on characteristics of those functions to deploy BO successfully. Such domain knowledge often manifests in Gaussian process priors that specify initial beliefs on functions. However, even with expert knowledge, it is not an easy task to select a prior. This is especially true for hyperparameter tuning problems on complex machine learning models, where landscapes of tuning objectives are often difficult to comprehend. We seek an alternative practice for setting these functional priors. In particular, we consider the scenario where we have data from similar functions that allow us to pre-train a tighter distribution a priori. Theoretically, we show a bounded regret of BO with pre-trained priors. To verify our approach in realistic model training setups, we collected a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art models on popular image and text datasets, as well as a protein sequence dataset. Our results show that on average, our method is able to locate good hyperparameters at least 3 times more efficiently than the best competing methods.

</details>

<details>

<summary>2022-07-07 04:05:12 - Sequential estimation of quantiles with applications to A/B-testing and best-arm identification</summary>

- *Steven R. Howard, Aaditya Ramdas*

- `1906.09712v5` - [abs](http://arxiv.org/abs/1906.09712v5) - [pdf](http://arxiv.org/pdf/1906.09712v5)

> We propose confidence sequences -- sequences of confidence intervals which are valid uniformly over time -- for quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We give methods both for tracking a fixed quantile and for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible $\sqrt{t^{-1} \log\log t}$ rate, along with a non-asymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the same rate. The latter strengthens Smirnov's empirical process law of the iterated logarithm and extends the Dvoretzky-Kiefer-Wolfowitz inequality to hold uniformly over time. We give a new algorithm and sample complexity bound for selecting an arm with an approximately best quantile in a multi-armed bandit framework. In simulations, our method requires fewer samples than existing methods by a factor of five to fifty.

</details>

<details>

<summary>2022-07-07 04:15:26 - Playing Divide-and-Choose Given Uncertain Preferences</summary>

- *Jamie Tucker-Foltz, Richard Zeckhauser*

- `2207.03076v1` - [abs](http://arxiv.org/abs/2207.03076v1) - [pdf](http://arxiv.org/pdf/2207.03076v1)

> We study the classic divide-and-choose method for equitably allocating divisible goods between two players who are rational, self-interested Bayesian agents. The players have additive private values for the goods. The prior distributions on those values are independent and common knowledge.   We characterize the structure of optimal divisions in the divide-and-choose game and show how to efficiently compute equilibria. We identify several striking differences between optimal strategies in the cases of known versus unknown preferences. Most notably, the divider has a compelling "diversification" incentive in creating the chooser's two options. This incentive, hereto unnoticed, leads to multiple goods being divided at equilibrium, quite contrary to the divider's optimal strategy when preferences are known.   In many contexts, such as buy-and-sell provisions between partners, or in judging fairness, it is important to assess the relative expected utilities of the divider and chooser. Those utilities, we show, depend on the players' uncertainties about each other's values, the number of goods being divided, and whether the divider can offer multiple alternative divisions. We prove that, when values are independently and identically distributed across players and goods, the chooser is strictly better off for a small number of goods, while the divider is strictly better off for a large number of goods.

</details>

<details>

<summary>2022-07-07 04:42:54 - Pre-training helps Bayesian optimization too</summary>

- *Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, Zoubin Ghahramani*

- `2207.03084v1` - [abs](http://arxiv.org/abs/2207.03084v1) - [pdf](http://arxiv.org/pdf/2207.03084v1)

> Bayesian optimization (BO) has become a popular strategy for global optimization of many expensive real-world functions. Contrary to a common belief that BO is suited to optimizing black-box functions, it actually requires domain knowledge on characteristics of those functions to deploy BO successfully. Such domain knowledge often manifests in Gaussian process priors that specify initial beliefs on functions. However, even with expert knowledge, it is not an easy task to select a prior. This is especially true for hyperparameter tuning problems on complex machine learning models, where landscapes of tuning objectives are often difficult to comprehend. We seek an alternative practice for setting these functional priors. In particular, we consider the scenario where we have data from similar functions that allow us to pre-train a tighter distribution a priori. To verify our approach in realistic model training setups, we collected a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art models on popular image and text datasets, as well as a protein sequence dataset. Our results show that on average, our method is able to locate good hyperparameters at least 3 times more efficiently than the best competing methods.

</details>

<details>

<summary>2022-07-07 05:20:18 - A Pair of Novel Priors for Improving and Extending the Conditional MLE</summary>

- *T. Yanagimoto, Y. Miyata*

- `2207.03092v1` - [abs](http://arxiv.org/abs/2207.03092v1) - [pdf](http://arxiv.org/pdf/2207.03092v1)

> A Bayesian estimator aiming at improving the conditional MLE is proposed by introducing a pair of priors. After explaining the conditional MLE by the posterior mode under a prior, we define a promising estimator by the posterior mean under a corresponding prior. The prior is equivalent to the reference prior in familiar models. Advantages of the present approach include two different optimality properties of the induced estimator, the ease of various extensions and the possible treatments for a finite sample size. The existing approaches are discussed and critiqued.

</details>

<details>

<summary>2022-07-07 09:23:03 - Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions</summary>

- *Patrick Has, Frdric Crou, Mathias Rousset*

- `2207.03182v1` - [abs](http://arxiv.org/abs/2207.03182v1) - [pdf](http://arxiv.org/pdf/2207.03182v1)

> Atmospheric motion vectors (AMVs) extracted from satellite imagery are the only wind observations with good global coverage. They are important features for feeding numerical weather prediction (NWP) models. Several Bayesian models have been proposed to estimate AMVs. Although critical for correct assimilation into NWP models, very few methods provide a thorough characterization of the estimation errors. The difficulty of estimating errors stems from the specificity of the posterior distribution, which is both very high dimensional, and highly ill-conditioned due to a singular likelihood, which becomes critical in particular in the case of missing data (unobserved pixels). This work studies the evaluation of the expected error of AMVs using gradient-based Markov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose a tempering strategy, which amounts to sampling a local approximation of the joint posterior distribution of AMVs and image variables in the neighborhood of a point estimate. In addition, we provide efficient preconditioning with the covariance related to the prior family itself (fractional Brownian motion), with possibly different hyper-parameters. From a theoretical point of view, we show that under regularity assumptions, the family of tempered posterior distributions converges in distribution as temperature decreases to an {optimal} Gaussian approximation at a point estimate given by the Maximum A Posteriori (MAP) log-density. From an empirical perspective, we evaluate the proposed approach based on some quantitative Bayesian evaluation criteria. Our numerical simulations performed on synthetic and real meteorological data reveal a significant gain in terms of accuracy of the AMV point estimates and of their associated expected error estimates, but also a substantial acceleration in the convergence speed of the MCMC algorithms.

</details>

<details>

<summary>2022-07-07 10:05:19 - Comparing Confidence Intervals for a Binomial Proportion with the Interval Score</summary>

- *Lisa J. Hofer, Leonhard Held*

- `2207.03199v1` - [abs](http://arxiv.org/abs/2207.03199v1) - [pdf](http://arxiv.org/pdf/2207.03199v1)

> There are over 55 different ways to construct a confidence respectively credible interval (CI) for the binomial proportion. Methods to compare them are necessary to decide which should be used in practice. The interval score has been suggested to compare prediction intervals. This score is a proper scoring rule that combines the coverage as a measure of calibration and the width as a measure of sharpness. We evaluate eleven CIs for the binomial proportion based on the expected interval score and propose a summary measure which can take into account different weighting of the underlying true proportion. Under uniform weighting, the expected interval score recommends the Wilson CI or Bayesian credible intervals with a uniform prior. If extremely low or high proportions receive more weight, the score recommends Bayesian credible intervals based on Jeffreys' prior. While more work is needed to theoretically justify the use of the interval score for the comparison of CIs, our results suggest that it constitutes a useful method to combine coverage and width in one measure. This novel approach could also be used in other applications.

</details>

<details>

<summary>2022-07-07 11:24:50 - Challenges and Pitfalls of Bayesian Unlearning</summary>

- *Ambrish Rawat, James Requeima, Wessel Bruinsma, Richard Turner*

- `2207.03227v1` - [abs](http://arxiv.org/abs/2207.03227v1) - [pdf](http://arxiv.org/pdf/2207.03227v1)

> Machine unlearning refers to the task of removing a subset of training data, thereby removing its contributions to a trained model. Approximate unlearning are one class of methods for this task which avoid the need to retrain the model from scratch on the retained data. Bayes' rule can be used to cast approximate unlearning as an inference problem where the objective is to obtain the updated posterior by dividing out the likelihood of deleted data. However this has its own set of challenges as one often doesn't have access to the exact posterior of the model parameters. In this work we examine the use of the Laplace approximation and Variational Inference to obtain the updated posterior. With a neural network trained for a regression task as the guiding example, we draw insights on the applicability of Bayesian unlearning in practical scenarios.

</details>

<details>

<summary>2022-07-07 11:50:10 - A Bayesian Survival Tree Partition Model Using Latent Gaussian Processes</summary>

- *Richard D. Payne, Nilabja Guha, Bani K. Mallick*

- `2207.03242v1` - [abs](http://arxiv.org/abs/2207.03242v1) - [pdf](http://arxiv.org/pdf/2207.03242v1)

> Survival models are used to analyze time-to-event data in a variety of disciplines. Proportional hazard models provide interpretable parameter estimates, but proportional hazards assumptions are not always appropriate. Non-parametric models are more flexible but often lack a clear inferential framework. We propose a Bayesian tree partition model which is both flexible and inferential. Inference is obtained through the posterior tree structure and flexibility is preserved by modeling the the hazard function in each partition using a latent exponentiated Gaussian process. An efficient reversible jump Markov chain Monte Carlo algorithm is accomplished by marginalizing the parameters in each partition element via a Laplace approximation. Consistency properties for the estimator are established. The method can be used to help determine subgroups as well as prognostic and/or predictive biomarkers in time-to-event data. The method is applied to a liver survival dataset and is compared with some existing methods on simulated data.

</details>

<details>

<summary>2022-07-07 12:07:19 - A Metropolized adaptive subspace algorithm for high-dimensional Bayesian variable selection</summary>

- *Christian Staerk, Maria Kateri, Ioannis Ntzoufras*

- `2105.01039v2` - [abs](http://arxiv.org/abs/2105.01039v2) - [pdf](http://arxiv.org/pdf/2105.01039v2)

> A simple and efficient adaptive Markov Chain Monte Carlo (MCMC) method, called the Metropolized Adaptive Subspace (MAdaSub) algorithm, is proposed for sampling from high-dimensional posterior model distributions in Bayesian variable selection. The MAdaSub algorithm is based on an independent Metropolis-Hastings sampler, where the individual proposal probabilities of the explanatory variables are updated after each iteration using a form of Bayesian adaptive learning, in a way that they finally converge to the respective covariates' posterior inclusion probabilities. We prove the ergodicity of the algorithm and present a parallel version of MAdaSub with an adaptation scheme for the proposal probabilities based on the combination of information from multiple chains. The effectiveness of the algorithm is demonstrated via various simulated and real data examples, including a high-dimensional problem with more than 20,000 covariates.

</details>

<details>

<summary>2022-07-07 14:46:09 - bqror: An R package for Bayesian Quantile Regression in Ordinal Models</summary>

- *Prajual Maheshwari, Mohammad Arshad Rahman*

- `2109.13606v2` - [abs](http://arxiv.org/abs/2109.13606v2) - [pdf](http://arxiv.org/pdf/2109.13606v2)

> This article describes an R package bqror that estimates Bayesian quantile regression for ordinal models introduced in Rahman (2016). The paper classifies ordinal models into two types and offers computationally efficient, yet simple, Markov chain Monte Carlo (MCMC) algorithms for estimating ordinal quantile regression. The generic ordinal model with 3 or more outcomes (labeled ORI model) is estimated by a combination of Gibbs sampling and Metropolis-Hastings algorithm. Whereas an ordinal model with exactly 3 outcomes (labeled ORII model) is estimated using Gibbs sampling only. In line with the Bayesian literature, we suggest using marginal likelihood for comparing alternative quantile regression models and explain how to compute the same. The models and their estimation procedures are illustrated via multiple simulation studies and implemented in two applications. The article also describes several other functions contained within the bqror package, which are necessary for estimation, inference, and assessing model fit.

</details>

