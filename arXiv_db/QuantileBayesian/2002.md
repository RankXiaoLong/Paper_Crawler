# 2002

## TOC

- [2002-06](#2002-06)
- [2002-12](#2002-12)

## 2002-06

<details>

<summary>2002-06-19 11:42:02 - Fast algorithms of Bayesian Segmentation of Images</summary>

- *B. A. Zalesky*

- `0206184v2` - [abs](http://arxiv.org/abs/0206184v2) - [pdf](http://arxiv.org/pdf/math/0206184v2)

> The network flow optimization approach is offered for Bayesian segmentation of gray-scale and color images. It is supposed image pixels are characterized by a feature function taking finite number of arbitrary rational values (it can be either intensity values or other characteristics of images). The clusters of homogeneous pixels are described by labels with values in another set of rational numbers. They are assumed to be dependent and distributed according to either the exponential or the Gaussian Gibbs law. Instead traditionally used local neighborhoods of nearest pixels the completely connected graph of dependence of all pixels is employed for the Gibbs prior distributions.   The methods developed reduce the problem of segmentation to the problem of determination of the minimum network cuts.

</details>


## 2002-12

<details>

<summary>2002-12-01 00:00:00 - Pattern theory: the mathematics of perception</summary>

- *David Mumford*

- `0212400v1` - [abs](http://arxiv.org/abs/0212400v1) - [pdf](http://arxiv.org/pdf/math/0212400v1)

> Is there a mathematical theory underlying intelligence? Control theory addresses the output side, motor control, but the work of the last 30 years has made clear that perception is a matter of Bayesian statistical inference, based on stochastic models of the signals delivered by our senses and the structures in the world producing them. We will start by sketching the simplest such model, the hidden Markov model for speech, and then go on illustrate the complications, mathematical issues and challenges that this has led to.

</details>

<details>

<summary>2002-12-02 15:22:25 - Why Maximum Entropy? A Non-axiomatic Approach</summary>

- *M. Grendar, Jr., M. Grendar*

- `0212005v1` - [abs](http://arxiv.org/abs/0212005v1) - [pdf](http://arxiv.org/pdf/math-ph/0212005v1)

> Ill-posed inverse problems of the form y = X p where y is J-dimensional vector of a data, p is m-dimensional probability vector which cannot be measured directly and matrix X of observable variables is a known J,m matrix, J < m, are frequently solved by Shannon's entropy maximization (MaxEnt). Several axiomatizations were proposed to justify the MaxEnt method (also) in this context. The main aim of the presented work is two-fold: 1) to view the concept of complementarity of MaxEnt and Maximum Likelihood (ML) tasks from a geometric perspective, and consequently 2) to provide an intuitive and non-axiomatic answer to the 'Why MaxEnt?' question.

</details>

